{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"canVODpy","text":"","path":["canVODpy"],"tags":[]},{"location":"#canvodpy","level":1,"title":"canVODpy","text":"<p>An Open Python Ecosystem for GNSS-Transmissometry Canopy VOD Retrievals</p> <p>canVODpy aims to be the central community-driven software suite for deriving and analyzing canopy Vegetation Optical Depth (VOD) from GNSS signal-to-noise ratio observations.</p> <p> </p> <p>Get started View on GitHub</p>","path":["canVODpy"],"tags":[]},{"location":"#processing-pipeline","level":2,"title":"Processing Pipeline","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"Data Acquisition\"]\n        RINEX[\"RINEX v3.04\\nObservation Files\\n(SNR, Pseudorange, Phase)\"]\n        SP3[\"SP3 Precise\\nEphemerides\\n(satellite orbits)\"]\n        CLK[\"CLK Precise\\nClock Corrections\"]\n    end\n\n    subgraph DOWNLOAD[\"Auxiliary Data Retrieval\"]\n        FTP[\"FTP Download\\n(ESA primary,\\nNASA CDDIS fallback)\"]\n        CACHE[\"Local File Cache\\n(SP3/CLK per DOY)\"]\n    end\n\n    subgraph PREPROCESS[\"Auxiliary Preprocessing\"]\n        PARSE_SP3[\"Parse SP3\\n(ECEF satellite positions\\n+ velocities)\"]\n        PARSE_CLK[\"Parse CLK\\n(satellite clock offsets)\"]\n        HERMITE[\"Hermite Spline\\nInterpolation\\n(cubic, velocity-aware)\"]\n        LINEAR[\"Piecewise Linear\\nInterpolation\\n(clock corrections)\"]\n        MERGE_AUX[\"Merge Interpolated\\nAuxiliary Data\\n(Zarr cache)\"]\n    end\n\n    subgraph PARALLEL[\"Parallel RINEX Processing (ProcessPoolExecutor)\"]\n        READ[\"Read RINEX\\n(per hourly file)\"]\n        SID[\"Signal ID Mapping\\n(sv|band|code)\"]\n        SLICE[\"Slice Auxiliary to\\nObservation Epochs\\n(nearest-neighbour)\"]\n        SCS[\"Spherical Coordinate\\nTransformation\\n(ECEF to r, theta, phi)\"]\n    end\n\n    subgraph STORAGE[\"Versioned Storage\"]\n        ICECHUNK[\"Icechunk Repository\\n(append per epoch,\\ncommit per file)\"]\n        META[\"Metadata Tracking\\n(RINEX hash, snapshot ID,\\nepoch range)\"]\n    end\n\n    subgraph GRIDDING[\"Hemispheric Grid Assignment\"]\n        GRID[\"Grid Construction\\n(equal-area, HEALPix,\\ngeodesic, ...)\"]\n        KDTREE[\"KDTree Cell Assignment\\n(O(n log m) spatial query)\"]\n    end\n\n    subgraph VOD[\"VOD Retrieval\"]\n        PAIR[\"Canopy-Reference\\nDataset Pairing\"]\n        TAU[\"Zeroth-Order\\nTau-Omega Inversion\"]\n        FORMULA[\"DELTA_SNR = SNR_canopy - SNR_ref\\ntransmissivity = 10^(DELTA_SNR/10)\\nVOD = -ln(transmissivity) * cos(theta)\"]\n    end\n\n    subgraph OUTPUT[\"Output\"]\n        VOD_DS[\"VOD Dataset\\n(per sid, epoch, cell)\"]\n        VIZ[\"Hemispheric\\nVisualization\\n(polar projection)\"]\n    end\n\n    SP3 --&gt; FTP\n    CLK --&gt; FTP\n    FTP --&gt; CACHE\n    CACHE --&gt; PARSE_SP3\n    CACHE --&gt; PARSE_CLK\n    PARSE_SP3 --&gt; HERMITE\n    PARSE_CLK --&gt; LINEAR\n    HERMITE --&gt; MERGE_AUX\n    LINEAR --&gt; MERGE_AUX\n\n    RINEX --&gt; READ\n    READ --&gt; SID\n    MERGE_AUX --&gt; SLICE\n    SID --&gt; SLICE\n    SLICE --&gt; SCS\n    SCS --&gt; ICECHUNK\n    ICECHUNK --&gt; META\n\n    ICECHUNK --&gt; GRID\n    GRID --&gt; KDTREE\n    KDTREE --&gt; PAIR\n    PAIR --&gt; TAU\n    TAU --&gt; FORMULA\n    FORMULA --&gt; VOD_DS\n    VOD_DS --&gt; VIZ</code></pre>","path":["canVODpy"],"tags":[]},{"location":"#packages","level":2,"title":"Packages","text":"Package Description canvod-readers RINEX v3.04 observation file parsing with validation canvod-auxiliary SP3 ephemeris and CLK clock correction processing canvod-grids Hemispheric grid implementations (HEALPix, equal-area) canvod-vod VOD estimation using the tau-omega model canvod-store Versioned storage via Icechunk canvod-viz 2D and 3D hemispheric visualization canvod-utils Configuration management and CLI canvodpy Umbrella package providing unified access","path":["canVODpy"],"tags":[]},{"location":"#quick-start","level":2,"title":"Quick Start","text":"<pre><code>pip install canvodpy\n</code></pre>","path":["canVODpy"],"tags":[]},{"location":"#level-1-convenience-two-lines","level":3,"title":"Level 1 — Convenience (two lines)","text":"<pre><code>from canvodpy import process_date, calculate_vod\n\ndata = process_date(\"Rosalia\", \"2025001\")\nvod  = calculate_vod(\"Rosalia\", \"canopy_01\", \"reference_01\", \"2025001\")\n</code></pre>","path":["canVODpy"],"tags":[]},{"location":"#level-2-fluent-workflow-deferred-execution","level":3,"title":"Level 2 — Fluent workflow (deferred execution)","text":"<pre><code>import canvodpy\n\nresult = (canvodpy.workflow(\"Rosalia\")\n    .read(\"2025001\")\n    .preprocess()\n    .grid(\"equal_area\", angular_resolution=5.0)\n    .vod(\"canopy_01\", \"reference_01\")\n    .result())\n</code></pre>","path":["canVODpy"],"tags":[]},{"location":"#level-3-vodworkflow-eager-with-logging","level":3,"title":"Level 3 — VODWorkflow (eager, with logging)","text":"<pre><code>from canvodpy import VODWorkflow\n\nwf  = VODWorkflow(site=\"Rosalia\", grid=\"equal_area\")\nvod = wf.calculate_vod(\"canopy_01\", \"reference_01\", \"2025001\")\n</code></pre>","path":["canVODpy"],"tags":[]},{"location":"#level-4-functional-stateless-airflow-ready","level":3,"title":"Level 4 — Functional (stateless, Airflow-ready)","text":"<pre><code>from canvodpy import read_rinex, create_grid, assign_grid_cells\n\nds   = read_rinex(path, reader=\"rinex3\")\ngrid = create_grid(grid_type=\"equal_area\", angular_resolution=5.0)\nds   = assign_grid_cells(ds, grid)\n</code></pre>","path":["canVODpy"],"tags":[]},{"location":"#technology","level":2,"title":"Technology","text":"Python 3.13+ uv + uv_build xarray + NumPy Icechunk / Zarr ruff + ty pytest Zensical just","path":["canVODpy"],"tags":[]},{"location":"#publications","level":2,"title":"Publications","text":"<p>Bader, N. F. (2026). canVODpy: An Open Python Ecosystem for GNSS-Transmissometry Canopy VOD Retrievals (v0.1.0-beta.2). Zenodo. https://doi.org/10.5281/zenodo.18636775</p>","path":["canVODpy"],"tags":[]},{"location":"#affiliation","level":2,"title":"Affiliation","text":"<p>Climate and Environmental Remote Sensing Research Unit (CLIMERS), Department of Geodesy and Geoinformation, TU Wien (Vienna University of Technology).</p> <p>https://www.tuwien.at/en/mg/geo/climers</p>","path":["canVODpy"],"tags":[]},{"location":"CONTRIBUTING/","level":1,"title":"Contributing","text":"<p>Contributions are welcome. This guide covers the development setup and contribution workflow.</p>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#required-tools","level":2,"title":"Required Tools","text":"<p>Two external tools must be installed separately (not managed by <code>uv sync</code>):</p>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#uv-python-package-manager","level":3,"title":"uv (Python Package Manager)","text":"<pre><code># macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or via package manager\nbrew install uv\n</code></pre> <p>uv documentation</p>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#just-command-runner","level":3,"title":"just (Command Runner)","text":"<pre><code># macOS/Linux\ncurl --proto '=https' --tlsv1.2 -sSf https://just.systems/install.sh | bash\n\n# Or via package manager\nbrew install just\n</code></pre> <p>just documentation</p>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#types-of-contributions","level":2,"title":"Types of Contributions","text":"","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#report-bugs","level":3,"title":"Report Bugs","text":"<p>Report bugs at https://github.com/nfb2021/canvodpy/issues. Include operating system, local setup details, and steps to reproduce.</p>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#fix-bugs","level":3,"title":"Fix Bugs","text":"<p>Issues tagged \"bug\" and \"help wanted\" are open for contributions.</p>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#implement-features","level":3,"title":"Implement Features","text":"<p>Issues tagged \"enhancement\" and \"help wanted\" are open for contributions.</p>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#write-documentation","level":3,"title":"Write Documentation","text":"<p>Improvements to documentation, docstrings, or external articles are appreciated.</p>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#submit-feedback","level":3,"title":"Submit Feedback","text":"<p>File feature proposals at https://github.com/nfb2021/canvodpy/issues. Keep the scope narrow and explain the intended behavior.</p>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#development-workflow","level":2,"title":"Development Workflow","text":"<ol> <li> <p>Install required tools (uv and just).</p> </li> <li> <p>Fork and clone the repository:    <pre><code>git clone git@github.com:your_name_here/canvodpy.git\ncd canvodpy\n</code></pre></p> </li> <li> <p>Verify tools and install dependencies:    <pre><code>just check-dev-tools\nuv sync\njust hooks\n</code></pre></p> </li> <li> <p>Create a feature branch:    <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre></p> </li> <li> <p>Make changes and verify:    <pre><code>just test\njust check\n</code></pre></p> </li> <li> <p>Commit using conventional commits:    <pre><code>git commit -m \"feat(readers): add support for RINEX 4.0 format\"\n</code></pre></p> </li> <li> <p>Push and create a pull request:    <pre><code>git push origin name-of-your-bugfix-or-feature\n</code></pre></p> </li> </ol>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#commit-message-format","level":3,"title":"Commit Message Format","text":"<pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n</code></pre> <p>Types: <code>feat</code>, <code>fix</code>, <code>docs</code>, <code>refactor</code>, <code>test</code>, <code>chore</code>, <code>perf</code>, <code>ci</code></p> <p>Scopes: <code>readers</code>, <code>aux</code>, <code>grids</code>, <code>vod</code>, <code>store</code>, <code>viz</code>, <code>utils</code>, <code>docs</code>, <code>ci</code>, <code>deps</code></p> <p>Examples: <pre><code>git commit -m \"feat(vod): add tau-omega calculator\"\ngit commit -m \"fix(readers): handle empty RINEX files\"\ngit commit -m \"docs: update installation instructions\"\ngit commit -m \"feat(viz)!: redesign 3D plotting API\"  # Breaking change\n</code></pre></p> <p>See Conventional Commits for the full specification.</p>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#common-commands","level":2,"title":"Common Commands","text":"<pre><code>just --list                    # Show all commands\njust test                      # Run all tests\njust test-coverage             # With coverage report\njust test-package canvod-grids # Specific package\njust check                     # Lint + format + type-check\njust docs                      # Preview documentation\n</code></pre>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#pull-request-guidelines","level":2,"title":"Pull Request Guidelines","text":"<ol> <li>Include tests for new functionality.</li> <li>Update documentation if adding features.</li> <li>Ensure compatibility with Python 3.13+.</li> </ol>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#workspace-development","level":2,"title":"Workspace Development","text":"<p>This project uses a monorepo structure with multiple packages:</p> <ul> <li>Work on individual packages in <code>packages/</code> or <code>canvodpy/</code></li> <li>Run package-specific commands: <code>just check-package canvod-readers</code></li> <li>Run workspace-wide commands: <code>just check</code>, <code>just test</code></li> <li>All packages share a single lockfile and virtual environment</li> </ul>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#code-quality","level":2,"title":"Code Quality","text":"<ul> <li>ruff for linting and formatting</li> <li>ty for type checking</li> <li>pytest for testing with coverage</li> </ul> <p>Run <code>just check</code> before committing.</p>","path":["Development","Contributing"],"tags":[]},{"location":"CONTRIBUTING/#deploying","level":2,"title":"Deploying","text":"<p>For maintainers:</p> <pre><code>just bump minor\ngit push\ngit push --tags\n</code></pre> <p>GitHub Actions publishes to PyPI when a new tag is pushed.</p>","path":["Development","Contributing"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/","level":1,"title":"Naming Refactor Plan","text":"<p>Audit date: 2026-02-07 Branch: to be created from <code>main</code> after current work is merged.</p>","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#critical-public-api-renames-require-deprecation-aliases","level":2,"title":"Critical: Public API renames (require deprecation aliases)","text":"Current name Package Problem Suggested name <code>MyIcechunkStore</code> canvod-store <code>My</code> prefix (tutorial artifact) <code>IcechunkStore</code> <code>add_cell_ids_to_ds_fast</code> canvod-grids <code>_fast</code> with no slow counterpart <code>assign_cell_ids</code> <code>add_cell_ids_to_vod_fast</code> canvod-grids <code>_fast</code> suffix <code>assign_cell_ids_to_vod</code> <code>prep_aux_ds</code> canvod-auxiliary triple abbreviation <code>prepare_auxiliary</code> <code>preprocess_aux_for_interpolation</code> canvod-auxiliary overly long (borderline) <code>preprocess_auxiliary</code> <p>Each public rename needs a backward-compatible alias in <code>__init__.py</code> with a deprecation warning for at least one minor release.</p>","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#high-internal-but-widely-referenced","level":2,"title":"High: Internal but widely referenced","text":"","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#speedalgorithm-qualifiers","level":3,"title":"Speed/algorithm qualifiers","text":"Current name File Problem Suggested name <code>astropy_hampel_vectorized_fast</code> <code>canvod-grids/grids/analysis/sigma_clip_filter.py:212</code> library name + <code>_fast</code> <code>hampel_filter_vectorized</code> <code>astropy_hampel_ultra_fast</code> <code>canvod-grids/grids/analysis/sigma_clip_filter.py:399</code> library + superlative speed <code>sigma_clip_filter</code> <code>_compute_spherical_coords_fast</code> <code>canvodpy/orchestrator/processor.py:246</code> no slow counterpart <code>_compute_spherical_coords</code> <code>_append_to_icechunk_slow</code> <code>canvodpy/orchestrator/processor.py:914</code> speed qualifier <code>_append_to_icechunk_sequential</code> <code>_append_to_icechunk_coord_distrbtd</code> <code>canvodpy/orchestrator/processor.py:2625</code> misspelling <code>_append_to_icechunk_distributed</code> <code>_append_to_icechunk_native_context_manager</code> <code>canvodpy/orchestrator/processor.py:2502</code> 6 words <code>_append_to_icechunk_transactional</code>","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#versionmagic-number-suffixes","level":3,"title":"Version/magic-number suffixes","text":"Current name File Problem Suggested name <code>parsed_rinex_data_gen_v2</code> <code>canvod-store/store/reader.py:415</code> version suffix merge into <code>parsed_rinex_data_gen</code> <code>parsed_rinex_data_gen_2_receivers</code> <code>canvodpy/orchestrator/processor.py:1899</code> magic number in name <code>parsed_rinex_data_gen_multi</code> or consolidate","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#overly-long-names","level":3,"title":"Overly long names","text":"Current name File Problem Suggested name <code>create_processed_data_fast_hampel_complete</code> <code>canvod-grids/grids/workflows/adapted_workflow.py:233</code> 6 words <code>run_hampel_pipeline</code> <code>create_processed_data_hampel_parallel_complete</code> <code>canvod-grids/grids/workflows/adapted_workflow.py:290</code> 6 words <code>run_hampel_pipeline_parallel</code> <code>_create_processed_data_fast_hampel</code> <code>canvod-grids/grids/workflows/adapted_workflow.py:572</code> 5 words + <code>_fast</code> <code>_run_hampel_filter</code> <code>generate_filename_based_on_type</code> <code>canvod-auxiliary/auxiliary/core/base.py:216</code>, <code>ephemeris/reader.py:81</code>, <code>clock/reader.py:80</code> wordy (3 locations) <code>build_filename</code> <code>normalize_datetime_for_comparison</code> <code>canvod-grids/grids/workflows/adapted_workflow.py:55</code> wordy <code>normalize_datetime</code> <code>check_temporal_coverage_compatibility</code> <code>canvod-grids/grids/workflows/adapted_workflow.py:142</code> borderline <code>check_temporal_coverage</code> <code>safe_temporal_aggregate_to_branch</code> <code>canvod-store/store/store.py:2645</code> <code>safe_</code> prefix is vague <code>aggregate_to_branch</code> <code>create_rinex_netcdf_with_signal_id</code> <code>canvod-readers/readers/rinex/v3_04.py:1369</code> leaks output format <code>to_signal_id_dataset</code>","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#cryptic-abbreviations","level":3,"title":"Cryptic abbreviations","text":"Current name File Problem Suggested name <code>epochrecordinfo_dt_to_numpy_dt</code> <code>canvod-readers/readers/rinex/v3_04.py:1154</code> no word separators, double <code>dt</code> <code>epoch_record_to_datetime64</code> <code>get_channel_used_by_SV</code> <code>canvod-readers/readers/gnss_specs/constellations.py:811</code> PEP 8 violation (noqa suppressed) <code>get_channel_for_sv</code> <code>freqs_G1_G2_lut</code> <code>canvod-readers/readers/gnss_specs/constellations.py:865</code> dense abbreviation + PEP 8 (noqa) <code>build_glonass_frequency_lut</code>","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#implementation-detail-leaks","level":3,"title":"Implementation-detail leaks","text":"Current name File Problem Suggested name <code>_convert_to_polars_freq</code> <code>canvod-grids/grids/aggregation.py:487</code> leaks \"polars\" <code>_normalize_freq_string</code> <code>_preprocess_aux_data_with_hermite</code> <code>canvodpy/orchestrator/processor.py:551</code> leaks algorithm name <code>_preprocess_auxiliary_data</code> <code>hemigrid_polars_storage_methods.py</code> <code>canvod-store/store/grid_adapters/</code> leaks \"polars\" in module name deprecate module, move logic","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#medium-structural-renames","level":2,"title":"Medium: Structural renames","text":"","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#directory-rename","level":3,"title":"Directory rename","text":"Current Problem Suggested <code>canvod-grids/grids/grids_impl/</code> <code>_impl</code> suffix <code>builders/</code> <p>This requires updating all imports from <code>canvod.grids.grids_impl</code> to <code>canvod.grids.builders</code>. Leave a re-export shim in <code>grids_impl/__init__.py</code> for one release.</p>","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#file-renames","level":3,"title":"File renames","text":"Current Problem Suggested <code>constants_CLEANED.py</code> process artifact in name merge into <code>constants.py</code> or rename to <code>constants_v2.py</code> then consolidate <code>timing_diagnostics_new_api.py</code> <code>_new_api</code> temporal suffix <code>timing_diagnostics.py</code>","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#class-rename","level":3,"title":"Class rename","text":"Current File Problem Suggested <code>MyIcechunkStore</code> <code>canvod-store/store/store.py:42</code> <code>My</code> prefix <code>IcechunkStore</code> <p>This is the highest-impact rename. Used across notebooks, tests, and downstream code. Provide <code>MyIcechunkStore = IcechunkStore</code> alias with deprecation warning.</p>","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#function-that-should-be-documentation","level":3,"title":"Function that should be documentation","text":"Current File Problem <code>adapt_existing_rnxv3obs_class</code> <code>canvod-readers/readers/rinex/v3_04.py:1822</code> Returns a string of integration instructions; not code <p>Remove and move content to a doc page or docstring.</p>","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#low-naming-conventions-to-establish","level":2,"title":"Low: Naming conventions to establish","text":"","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#verb-consistency-for-data-retrieval","level":3,"title":"Verb consistency for data retrieval","text":"<p>Establish project-wide convention: - <code>get_*</code> = return metadata, config, lightweight (no I/O) - <code>read_*</code> = read raw data from file/store (I/O bound) - <code>load_*</code> = read + parse/construct (higher-level, may combine multiple reads)</p> <p>Current violations are too numerous to list individually. Apply convention during refactor.</p>","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#builder-pattern-compute-methods","level":3,"title":"Builder-pattern <code>compute()</code> methods","text":"<p><code>WeightCalculator.compute()</code> and <code>SpatialMaskBuilder.compute()</code> are acceptable given the builder pattern (<code>builder.add_*().compute()</code>). No action needed.</p>","path":["Naming Refactor Plan"],"tags":[]},{"location":"NAMING_REFACTOR_PLAN/#execution-order","level":2,"title":"Execution order","text":"<ol> <li><code>MyIcechunkStore</code> -&gt; <code>IcechunkStore</code> (highest visibility)</li> <li><code>add_cell_ids_to_ds_fast</code> / <code>add_cell_ids_to_vod_fast</code> (public API)</li> <li><code>prep_aux_ds</code> (public API)</li> <li><code>grids_impl/</code> -&gt; <code>builders/</code> (structural)</li> <li>Internal speed qualifiers (<code>_fast</code>, <code>_slow</code>, <code>_ultra_fast</code>)</li> <li>Overly long names</li> <li>Abbreviations and PEP 8 violations</li> <li>Verb consistency pass</li> </ol>","path":["Naming Refactor Plan"],"tags":[]},{"location":"RELEASING/","level":1,"title":"Release Process","text":"<p>Last Updated: 2026-02-04 Audience: Maintainers</p>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#prerequisites","level":2,"title":"Prerequisites","text":"<ul> <li>All CI checks passing</li> <li>Main branch is stable</li> <li>Push access to the repository</li> <li>All changes committed and pushed</li> </ul>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#release-types","level":2,"title":"Release Types","text":"Type Version Bump Example Description Patch 0.1.X 0.1.0 -&gt; 0.1.1 Bug fixes only, backward compatible Minor 0.X.0 0.1.0 -&gt; 0.2.0 New features, backward compatible Major X.0.0 0.9.0 -&gt; 1.0.0 Breaking changes, requires migration guide","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#step-by-step-process","level":2,"title":"Step-by-Step Process","text":"","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#1-prepare","level":3,"title":"1. Prepare","text":"<pre><code>git checkout main\ngit pull origin main\njust test\njust check\n</code></pre>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#2-create-release","level":3,"title":"2. Create Release","text":"<pre><code>just release 0.2.0\n</code></pre> <p>This command: 1. Runs all tests 2. Generates CHANGELOG.md from commits 3. Bumps version in all packages 4. Creates git tag <code>vX.Y.Z</code> 5. Commits changes</p>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#3-review","level":3,"title":"3. Review","text":"<pre><code>git log --oneline -5\ngit tag | tail -1\n</code></pre>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#4-push","level":3,"title":"4. Push","text":"<pre><code>git push origin main\ngit push origin --tags\n</code></pre>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#5-publish-github-release","level":3,"title":"5. Publish GitHub Release","text":"<p>The <code>.github/workflows/release.yml</code> workflow detects the new tag and creates a draft release. Review and publish at https://github.com/nfb2021/canvodpy/releases.</p>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#6-pypi-publishing-future","level":3,"title":"6. PyPI Publishing (Future)","text":"<p>Once configured, publishing the GitHub release triggers PyPI upload.</p>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#troubleshooting","level":2,"title":"Troubleshooting","text":"<p>Tests fail during release: Fix failing tests before retrying.</p> <p>Version bump fails: Use explicit <code>X.Y.Z</code> format without <code>v</code> prefix.</p> <p>Tag already exists: Delete with <code>git tag -d v0.2.0</code> and recreate.</p> <p>Workflow did not trigger: Verify tag matches pattern <code>v*.*.*</code> at https://github.com/nfb2021/canvodpy/actions.</p>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#manual-release-fallback","level":2,"title":"Manual Release (Fallback)","text":"<pre><code>just changelog v0.2.0\n</code></pre> <p>Then create a release manually at https://github.com/nfb2021/canvodpy/releases/new.</p>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#post-release","level":2,"title":"Post-Release","text":"<ul> <li>Monitor for issues related to the new release</li> <li>Create Zenodo snapshot for DOI (optional)</li> <li>Update citation information</li> </ul>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"RELEASING/#see-also","level":2,"title":"See Also","text":"<ul> <li>VERSIONING.md</li> <li>CONTRIBUTING.md</li> </ul>","path":["Release & Publishing","Release Process"],"tags":[]},{"location":"VERSIONING/","level":1,"title":"Versioning Strategy","text":"<p>Last Updated: 2026-02-04 Status: Active</p>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#overview","level":2,"title":"Overview","text":"<p>canvodpy uses unified semantic versioning across all packages in the monorepo. All packages share the same version number and are released together.</p>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#rationale","level":2,"title":"Rationale","text":"","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#fair-principles-and-scientific-reproducibility","level":3,"title":"FAIR Principles and Scientific Reproducibility","text":"<p>Unified versioning supports the FAIR principles for scientific software: - A single version number enables unambiguous citation in scientific papers - Environment recreation requires only one version identifier - Compatible package combinations are guaranteed by the unified release</p>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#user-experience","level":3,"title":"User Experience","text":"<p>A single version number simplifies usage: - <code>pip install canvodpy==0.2.0</code> installs a known-compatible set - Individual packages share the same version: <code>pip install canvod-readers==0.2.0</code></p>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#monorepo-with-sollbruchstellen","level":3,"title":"Monorepo with Sollbruchstellen","text":"<p>The monorepo provides modularity during development while maintaining coherence at release time: - Development: Independent package development and testing - Release: Coordinated releases with unified version - Installation: Individual packages installable if needed</p>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#semantic-versioning","level":2,"title":"Semantic Versioning","text":"<p>Following Semantic Versioning 2.0.0:</p> <pre><code>MAJOR.MINOR.PATCH\n</code></pre> Increment Meaning Example MAJOR Breaking API changes 0.9.0 -&gt; 1.0.0 MINOR New features (backward compatible) 0.1.0 -&gt; 0.2.0 PATCH Bug fixes (backward compatible) 0.1.0 -&gt; 0.1.1 <p>Pre-release versions: <code>0.2.0-alpha.1</code>, <code>0.2.0-beta.1</code>, <code>0.2.0-rc.1</code></p>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#version-management","level":2,"title":"Version Management","text":"","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#creating-a-release","level":3,"title":"Creating a Release","text":"<pre><code>just release 0.2.0    # Minor release\njust release 0.1.1    # Patch release\njust release 1.0.0    # Major release\n</code></pre> <p>This runs tests, generates a changelog, bumps the version in all packages, and creates a git tag.</p>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#manual-version-bump","level":3,"title":"Manual Version Bump","text":"<pre><code>just bump minor       # 0.1.0 -&gt; 0.2.0\njust bump patch       # 0.1.0 -&gt; 0.1.1\njust bump major       # 0.1.0 -&gt; 1.0.0\n</code></pre>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#version-files","level":2,"title":"Version Files","text":"<p>All package versions are synchronized via commitizen:</p> <pre><code>[tool.commitizen]\nversion = \"0.1.0\"\nversion_files = [\n    \"canvodpy/pyproject.toml:version\",\n    \"packages/canvod-readers/pyproject.toml:version\",\n    \"packages/canvod-auxiliary/pyproject.toml:version\",\n    \"packages/canvod-grids/pyproject.toml:version\",\n    \"packages/canvod-vod/pyproject.toml:version\",\n    \"packages/canvod-store/pyproject.toml:version\",\n    \"packages/canvod-viz/pyproject.toml:version\",\n    \"packages/canvod-utils/pyproject.toml:version\",\n]\n</code></pre>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#git-tags","level":2,"title":"Git Tags","text":"<p>Tags follow the format <code>vMAJOR.MINOR.PATCH</code> (e.g., <code>v0.1.0</code>, <code>v1.0.0</code>, <code>v0.2.0-beta.1</code>).</p>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#deprecation-policy","level":2,"title":"Deprecation Policy","text":"<ol> <li>Version N: Feature deprecated with warning</li> <li>Version N+1: Louder warning, migration guide in docs</li> <li>Version N+2: Feature removed, MAJOR version bump</li> </ol>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#citation","level":2,"title":"Citation","text":"","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#recommended-format","level":3,"title":"Recommended Format","text":"<pre><code>@software{canvodpy2026,\n  author = {Bader, Nicolas and Contributors},\n  title = {canvodpy: GNSS Vegetation Optical Depth Analysis},\n  version = {0.2.0},\n  year = {2026},\n  url = {https://github.com/nfb2021/canvodpy},\n  doi = {10.5281/zenodo.XXXXXXX}\n}\n</code></pre>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#in-text","level":3,"title":"In-text","text":"<p>\"Analysis was performed using canvodpy v0.2.0 (Bader et al., 2026).\"</p>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"VERSIONING/#see-also","level":2,"title":"See Also","text":"<ul> <li>Semantic Versioning</li> <li>Conventional Commits</li> <li>RELEASING.md</li> </ul>","path":["Release & Publishing","Versioning Strategy"],"tags":[]},{"location":"architecture/","level":1,"title":"Monorepo Structure","text":"","path":["Architecture","Monorepo Structure"],"tags":[]},{"location":"architecture/#overview","level":2,"title":"Overview","text":"<p>canVODpy is organized as a monorepo containing eight Python packages for GNSS vegetation optical depth analysis. All packages reside in a single repository while maintaining technical independence: each can be developed, tested, and published separately.</p>","path":["Architecture","Monorepo Structure"],"tags":[]},{"location":"architecture/#package-organization","level":2,"title":"Package Organization","text":"<pre><code>canVODpy Monorepo\n  canvod-readers    RINEX v3.04 observation file readers\n  canvod-auxiliary   SP3 ephemeris and CLK clock correction processing\n  canvod-grids      Hemispheric grid implementations\n  canvod-vod        VOD estimation algorithms\n  canvod-store      Icechunk/Zarr storage backends\n  canvod-viz        Visualization utilities\n  canvod-utils      Configuration and CLI tools\n  canvodpy          Umbrella package (re-exports all subpackages)\n</code></pre> <pre><code>graph LR\n    subgraph FOUNDATION[\"Foundation Layer\"]\n        UTILS[\"canvod-utils\\n\\nConfiguration (Pydantic)\\nDate utilities (YYYYDOY)\\nShared tooling\"]\n    end\n\n    subgraph DATAIO[\"Data I/O Layer\"]\n        READERS[\"canvod-readers\\n\\nRINEX v3.04 parser\\nSignal ID mapping\\nData directory matching\"]\n        AUX[\"canvod-auxiliary\\n\\nSP3/CLK retrieval\\nHermite interpolation\\nFTP download management\"]\n    end\n\n    subgraph STORE_LAYER[\"Persistence Layer\"]\n        STORE[\"canvod-store\\n\\nIcechunk versioned storage\\nSite/receiver management\\nMetadata tracking\"]\n    end\n\n    subgraph COMPUTE[\"Computation Layer\"]\n        VOD[\"canvod-vod\\n\\nVOD calculator (ABC)\\nTau-Omega inversion\\nExtensible algorithms\"]\n        GRIDS[\"canvod-grids\\n\\nHemispheric grids (7 types)\\nKDTree cell assignment\\nGrid I/O operations\"]\n    end\n\n    subgraph PRESENT[\"Presentation Layer\"]\n        VIZ[\"canvod-viz\\n\\n2D polar projections\\n3D interactive surfaces\\nTime-series plots\"]\n    end\n\n    subgraph ORCHESTRATION[\"Orchestration Layer\"]\n        CANVODPY[\"canvodpy\\n\\nPipeline orchestrator\\nFactory system\\nPublic API (3 levels)\"]\n    end\n\n    READERS -.-&gt; UTILS\n    AUX -.-&gt; READERS\n    AUX -.-&gt; UTILS\n    STORE -.-&gt; AUX\n    STORE -.-&gt; READERS\n    STORE -.-&gt; UTILS\n    GRIDS -.-&gt; UTILS\n    VIZ -.-&gt; GRIDS\n\n    CANVODPY ==&gt; READERS\n    CANVODPY ==&gt; AUX\n    CANVODPY ==&gt; STORE\n    CANVODPY ==&gt; VOD\n    CANVODPY ==&gt; GRIDS\n    CANVODPY ==&gt; VIZ</code></pre>","path":["Architecture","Monorepo Structure"],"tags":[]},{"location":"architecture/#key-design-decisions","level":2,"title":"Key Design Decisions","text":"","path":["Architecture","Monorepo Structure"],"tags":[]},{"location":"architecture/#namespace-packages","level":3,"title":"Namespace Packages","text":"<p>All packages share the <code>canvod.*</code> namespace, providing a unified import API:</p> <pre><code>from canvod.readers import Rnxv3Obs\nfrom canvod.grids import EqualAreaBuilder\nfrom canvod.vod import VODCalculator\n</code></pre> <p>Each import originates from a different installable package, but the shared namespace presents a coherent interface. See Namespace Packages for implementation details.</p>","path":["Architecture","Monorepo Structure"],"tags":[]},{"location":"architecture/#workspace-architecture","level":3,"title":"Workspace Architecture","text":"<p>All packages share a single virtual environment and lockfile:</p> <ul> <li>One <code>uv sync</code> installs all packages in editable mode</li> <li>Dependencies are resolved together, preventing version conflicts</li> <li>Each package maintains its own <code>pyproject.toml</code> for independent publishing</li> </ul>","path":["Architecture","Monorepo Structure"],"tags":[]},{"location":"architecture/#package-independence","level":3,"title":"Package Independence","text":"<p>Each package can be installed independently:</p> <pre><code>pip install canvod-readers          # Just the readers\npip install canvod-grids canvod-vod # Grids and VOD only\npip install canvodpy                # Everything\n</code></pre>","path":["Architecture","Monorepo Structure"],"tags":[]},{"location":"architecture/#directory-structure","level":2,"title":"Directory Structure","text":"<pre><code>canvodpy/                           # Repository root\n  packages/                         # Independent packages\n    canvod-readers/\n      src/\n        canvod/                     # Namespace (no __init__.py)\n          readers/                  # Package code\n            __init__.py\n      tests/\n      pyproject.toml\n      README.md\n    canvod-auxiliary/                # Same structure\n      ...\n  canvodpy/                         # Umbrella package\n    src/\n      canvodpy/\n        __init__.py                 # Re-exports all subpackages\n  docs/                             # Centralized documentation\n  pyproject.toml                    # Workspace configuration\n  uv.lock                          # Shared lockfile\n  Justfile                          # Task runner commands\n</code></pre>","path":["Architecture","Monorepo Structure"],"tags":[]},{"location":"architecture/#dependency-flow","level":2,"title":"Dependency Flow","text":"<pre><code>canvod-readers    (no inter-package dependencies)\ncanvod-grids      (no inter-package dependencies)\ncanvod-vod        (no inter-package dependencies)\ncanvod-utils      (no inter-package dependencies)\ncanvod-auxiliary   depends on canvod-readers\ncanvod-store      depends on canvod-grids\ncanvod-viz        depends on canvod-grids\ncanvodpy          depends on all packages\n</code></pre> <p>The dependency graph is intentionally flat: four foundation packages have zero inter-package dependencies, and three consumer packages each depend on exactly one foundation package. Maximum dependency depth is 1.</p>","path":["Architecture","Monorepo Structure"],"tags":[]},{"location":"architecture/#complete-processing-flow","level":2,"title":"Complete Processing Flow","text":"<p>The following diagram shows the full logical flow of canVODpy — from YAML configuration through data discovery, auxiliary data retrieval, parallel RINEX processing, versioned storage, hemispheric grid assignment, VOD retrieval, and output.</p> <pre><code>flowchart TD\n    %% ── Configuration ──\n    subgraph CFG[\"Configuration\"]\n        YAML[\"YAML Config Files\\n(processing, sites, sids)\"]\n        PYDANTIC[\"Pydantic Validation\\n+ Deep Merge with Defaults\"]\n        CONFIG[\"CanvodConfig\"]\n    end\n\n    %% ── Site Initialization ──\n    subgraph INIT[\"Site Initialization\"]\n        SITE[\"Site(name)\\nLoad receiver configs,\\ninitialize stores\"]\n        RINEX_STORE[\"RINEX Icechunk Store\\n(versioned observations)\"]\n        VOD_STORE[\"VOD Icechunk Store\\n(versioned retrievals)\"]\n    end\n\n    %% ── Data Discovery ──\n    subgraph DISCOVERY[\"Data Discovery (per DOY)\"]\n        MATCHER[\"PairDataDirMatcher\\n(ThreadPoolExecutor)\"]\n        EXPAND[\"SCS Expansion\\nReference receivers expanded\\nper canopy position\"]\n        SCHEDULE[\"Processing Schedule\\n{date: {group: (dir, type, pos_dir)}}\"]\n    end\n\n    %% ── Auxiliary Data (once per DOY) ──\n    subgraph AUX[\"Auxiliary Data Pipeline (sequential, once per DOY)\"]\n        FTP[\"FTP Download\\nESA primary / NASA fallback\"]\n        SP3_PARSE[\"Parse SP3 Ephemerides\\n(ECEF positions + velocities)\"]\n        CLK_PARSE[\"Parse CLK Corrections\\n(satellite clock offsets)\"]\n        EPOCH_GRID[\"Generate 24h Epoch Grid\\n(n = 86400 / sampling_interval)\"]\n        HERMITE[\"Hermite Spline Interpolation\\n(ephemerides, cubic,\\nvelocity-aware)\"]\n        LINEAR[\"Piecewise Linear Interpolation\\n(clock corrections,\\nwindow=9, jump_thr=1e-6)\"]\n        AUX_ZARR[\"Merged Auxiliary Zarr\\n(cached per DOY)\"]\n    end\n\n    %% ── Parallel RINEX Processing ──\n    subgraph PARALLEL[\"Parallel RINEX Processing (ProcessPoolExecutor, n workers)\"]\n        subgraph WORKER[\"Per Hourly File (independent process)\"]\n            READ[\"1. Read RINEX v3.04\\n(Rnxv3Obs.to_ds)\"]\n            FILTER[\"2. Filter Signals\\n(keep_vars, keep_sids)\"]\n            SLICE_AUX[\"3. Slice Auxiliary\\n(nearest-epoch match)\"]\n            COMMON_SID[\"4. Intersect SIDs\\n(RINEX ∩ auxiliary)\"]\n            SPHERICAL[\"5. Spherical Coordinates\\n(ECEF to r, theta, phi\\nrelative to receiver position)\"]\n        end\n    end\n\n    %% ── Storage ──\n    subgraph WRITE[\"Icechunk Storage (sequential)\"]\n        HASH_CHECK[\"Check RINEX File Hash\\n(skip if exists)\"]\n        NORMALIZE[\"Normalize Encodings\\n(dtype compatibility)\"]\n        APPEND[\"Append to Group\\n(epoch dimension)\"]\n        COMMIT[\"Atomic Commit\\n(snapshot ID)\"]\n        META[\"Update Metadata\\n(hash, epochs, snapshot,\\ntimestamp)\"]\n    end\n\n    %% ── Grid Assignment ──\n    subgraph GRID[\"Hemispheric Grid Assignment\"]\n        BUILD_GRID[\"Construct Grid\\n(equal-area, HEALPix,\\ngeodesic, fibonacci, ...)\"]\n        KDTREE[\"Build KDTree\\n(cell centres in\\nCartesian coordinates)\"]\n        ASSIGN[\"Spatial Query\\n(O(n log m) per observation)\"]\n    end\n\n    %% ── VOD Retrieval ──\n    subgraph VOD[\"VOD Retrieval\"]\n        LOAD_C[\"Load Canopy Dataset\\n(from RINEX store)\"]\n        LOAD_R[\"Load Reference Dataset\\n(from RINEX store,\\nSCS-matched group)\"]\n        DELTA[\"Compute Delta-SNR\\n(SNR_canopy - SNR_ref)\"]\n        TRANSMISSIVITY[\"Convert to Transmissivity\\n(10^(Delta-SNR/10))\"]\n        TAU[\"Tau-Omega Inversion\\nVOD = -ln(T) * cos(theta)\"]\n        VOD_DS[\"VOD Dataset\\n(per sid, epoch, cell)\"]\n    end\n\n    %% ── Output ──\n    subgraph OUTPUT[\"Output\"]\n        VOD_WRITE[\"Write to VOD Store\\n(versioned, per analysis pair)\"]\n        VIZ_2D[\"2D Hemispheric Plot\\n(polar projection)\"]\n        VIZ_3D[\"3D Interactive Surface\\n(Plotly)\"]\n        EXPORT[\"Data Export\\n(NetCDF, CSV, Zarr)\"]\n    end\n\n    %% ── Connections ──\n\n    %% Config flow\n    YAML --&gt; PYDANTIC --&gt; CONFIG\n    CONFIG --&gt; SITE\n    SITE --&gt; RINEX_STORE\n    SITE --&gt; VOD_STORE\n\n    %% Discovery\n    CONFIG --&gt; MATCHER\n    MATCHER --&gt; EXPAND\n    EXPAND --&gt; SCHEDULE\n\n    %% Auxiliary pipeline\n    SCHEDULE --&gt; FTP\n    FTP --&gt; SP3_PARSE\n    FTP --&gt; CLK_PARSE\n    SP3_PARSE --&gt; HERMITE\n    CLK_PARSE --&gt; LINEAR\n    SCHEDULE --&gt; EPOCH_GRID\n    EPOCH_GRID --&gt; HERMITE\n    EPOCH_GRID --&gt; LINEAR\n    HERMITE --&gt; AUX_ZARR\n    LINEAR --&gt; AUX_ZARR\n\n    %% Parallel processing\n    SCHEDULE --&gt; READ\n    READ --&gt; FILTER\n    FILTER --&gt; SLICE_AUX\n    AUX_ZARR --&gt; SLICE_AUX\n    SLICE_AUX --&gt; COMMON_SID\n    COMMON_SID --&gt; SPHERICAL\n\n    %% Storage\n    SPHERICAL --&gt; HASH_CHECK\n    HASH_CHECK --&gt; NORMALIZE\n    NORMALIZE --&gt; APPEND\n    APPEND --&gt; COMMIT\n    COMMIT --&gt; META\n    META --&gt; RINEX_STORE\n\n    %% Grid\n    RINEX_STORE --&gt; BUILD_GRID\n    BUILD_GRID --&gt; KDTREE\n    KDTREE --&gt; ASSIGN\n\n    %% VOD\n    ASSIGN --&gt; LOAD_C\n    ASSIGN --&gt; LOAD_R\n    LOAD_C --&gt; DELTA\n    LOAD_R --&gt; DELTA\n    DELTA --&gt; TRANSMISSIVITY\n    TRANSMISSIVITY --&gt; TAU\n    TAU --&gt; VOD_DS\n\n    %% Output\n    VOD_DS --&gt; VOD_WRITE\n    VOD_WRITE --&gt; VOD_STORE\n    VOD_DS --&gt; VIZ_2D\n    VOD_DS --&gt; VIZ_3D\n    VOD_DS --&gt; EXPORT</code></pre>","path":["Architecture","Monorepo Structure"],"tags":[]},{"location":"architecture/#trade-offs","level":2,"title":"Trade-offs","text":"<p>Advantages: - Clear separation of concerns between packages - Users install only the components they need - Independent testing and development per package - Smaller dependency trees for individual packages</p> <p>Costs: - Additional configuration files per package - Developers must understand the namespace package mechanism - Coordinated releases required for version consistency</p>","path":["Architecture","Monorepo Structure"],"tags":[]},{"location":"build-system/","level":1,"title":"Build System","text":"","path":["Development","Build System"],"tags":[]},{"location":"build-system/#distribution-formats","level":2,"title":"Distribution Formats","text":"<p>Python packages are distributed in two formats:</p> <p>Source distribution (sdist): <code>canvod_readers-0.1.0.tar.gz</code> -- contains source code and metadata. Requires a build step during installation.</p> <p>Wheel (built distribution): <code>canvod_readers-0.1.0-py3-none-any.whl</code> -- pre-built, ready to install by copying to site-packages. Preferred format for installation.</p> <p>Wheel filename components: <pre><code>canvod_readers-0.1.0-py3-none-any.whl\n  package name - version - python - ABI - platform\n</code></pre></p>","path":["Development","Build System"],"tags":[]},{"location":"build-system/#build-backend-configuration","level":2,"title":"Build Backend Configuration","text":"<pre><code># packages/canvod-readers/pyproject.toml\n[build-system]\nrequires = [\"uv_build&gt;=0.9.17,&lt;0.10.0\"]\nbuild-backend = \"uv_build\"\n\n[tool.uv.build-backend]\nmodule-name = \"canvod.readers\"       # Dot creates namespace package\n</code></pre> <p>The dotted <code>module-name</code> instructs uv_build to create a namespace package structure: <code>canvod/</code> contains no <code>__init__.py</code>, while <code>canvod/readers/</code> is the actual module.</p>","path":["Development","Build System"],"tags":[]},{"location":"build-system/#building","level":2,"title":"Building","text":"<pre><code>cd packages/canvod-readers\nuv build\n</code></pre> <p>This produces: - <code>dist/canvod_readers-0.1.0.tar.gz</code> - <code>dist/canvod_readers-0.1.0-py3-none-any.whl</code></p>","path":["Development","Build System"],"tags":[]},{"location":"build-system/#wheel-contents","level":3,"title":"Wheel Contents","text":"<pre><code>canvod/                           # Namespace directory (no __init__.py)\n  readers/                        # Module directory\n    __init__.py\n    base.py\n    rinex/\n      ...\ncanvod_readers-0.1.0.dist-info/   # Metadata\n  METADATA\n  WHEEL\n  RECORD\n</code></pre>","path":["Development","Build System"],"tags":[]},{"location":"build-system/#building-all-packages","level":3,"title":"Building All Packages","text":"<pre><code>just dist                         # Build all packages\n</code></pre>","path":["Development","Build System"],"tags":[]},{"location":"build-system/#package-metadata","level":2,"title":"Package Metadata","text":"<pre><code>[project]\nname = \"canvod-readers\"\nversion = \"0.1.0\"\ndescription = \"GNSS data format readers for canVODpy\"\nreadme = \"README.md\"\nlicense = {text = \"Apache-2.0\"}\nauthors = [{name = \"Nicolas Bader\", email = \"nicolas.bader@geo.tuwien.ac.at\"}]\nkeywords = [\"gnss\", \"rinex\", \"geodesy\"]\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: Apache Software License\",\n    \"Programming Language :: Python :: 3.13\",\n    \"Topic :: Scientific/Engineering :: GIS\",\n]\nrequires-python = \"&gt;=3.13\"\ndependencies = [\"numpy&gt;=1.24\", \"pandas&gt;=2.0\"]\n\n[project.urls]\nHomepage = \"https://github.com/nfb2021/canvodpy\"\nRepository = \"https://github.com/nfb2021/canvodpy\"\nIssues = \"https://github.com/nfb2021/canvodpy/issues\"\n</code></pre>","path":["Development","Build System"],"tags":[]},{"location":"build-system/#publishing-to-pypi","level":2,"title":"Publishing to PyPI","text":"","path":["Development","Build System"],"tags":[]},{"location":"build-system/#test-on-testpypi","level":3,"title":"Test on TestPyPI","text":"<pre><code>cd packages/canvod-readers\nuv build\nuv publish --repository testpypi\npip install --index-url https://test.pypi.org/simple/ canvod-readers\n</code></pre>","path":["Development","Build System"],"tags":[]},{"location":"build-system/#publish-to-production","level":3,"title":"Publish to Production","text":"<pre><code>uv build\nuv publish\n</code></pre>","path":["Development","Build System"],"tags":[]},{"location":"build-system/#version-management","level":2,"title":"Version Management","text":"<p>All packages follow Semantic Versioning with unified version numbers.</p> <pre><code>just bump patch    # 0.1.0 -&gt; 0.1.1\njust bump minor    # 0.1.0 -&gt; 0.2.0\njust bump major    # 0.1.0 -&gt; 1.0.0\n</code></pre> <p>Version bumps update <code>pyproject.toml</code> in all packages, create a git commit, and tag the release.</p>","path":["Development","Build System"],"tags":[]},{"location":"build-system/#including-extra-files","level":2,"title":"Including Extra Files","text":"<pre><code>[tool.uv.build-backend]\nmodule-name = \"canvod.readers\"\ninclude = [\"src/canvod/readers/data/*.dat\", \"LICENSE\", \"README.md\"]\nexclude = [\"src/canvod/readers/tests/\", \"*.pyc\"]\n</code></pre>","path":["Development","Build System"],"tags":[]},{"location":"build-system/#console-scripts","level":2,"title":"Console Scripts","text":"<pre><code>[project.scripts]\ncanvod-read = \"canvod.readers.cli:main\"\n</code></pre>","path":["Development","Build System"],"tags":[]},{"location":"build-system/#troubleshooting","level":2,"title":"Troubleshooting","text":"<p>ModuleNotFoundError: Verify <code>module-name</code> in <code>pyproject.toml</code> uses dotted notation.</p> <p>Wrong package structure: Ensure <code>module-name = \"canvod.readers\"</code> (with dot), not <code>\"canvod_readers\"</code>.</p> <p>Build failures: Run <code>uv build --verbose</code> for detailed diagnostics.</p>","path":["Development","Build System"],"tags":[]},{"location":"dependencies/","level":1,"title":"Package Dependencies","text":"<p>Inter-package dependency relationships and independence metrics for the canVODpy monorepo.</p>","path":["Architecture","Package Dependencies"],"tags":[]},{"location":"dependencies/#dependency-graph","level":2,"title":"Dependency Graph","text":"<pre><code>graph TD\n    canvod_readers[\"canvod-readers&lt;br/&gt;GNSS data format readers\"]\n    canvod_aux[\"canvod-auxiliary&lt;br/&gt;Auxiliary data augmentation\"]\n    canvod_vod[\"canvod-vod&lt;br/&gt;VOD calculation\"]\n    canvod_viz[\"canvod-viz&lt;br/&gt;Visualization utilities\"]\n    canvod_utils[\"canvod-utils&lt;br/&gt;Configuration &amp; utilities\"]\n    canvod_grids[\"canvod-grids&lt;br/&gt;Grid operations\"]\n    canvod_store[\"canvod-store&lt;br/&gt;Storage management\"]\n\n    canvod_aux --&gt; canvod_readers\n    canvod_viz --&gt; canvod_grids\n    canvod_store --&gt; canvod_grids</code></pre> <p>Legend: - Green (Stable): No dependencies, used by other packages - Pink (Leaf): Has dependencies, not depended upon by others</p>","path":["Architecture","Package Dependencies"],"tags":[]},{"location":"dependencies/#independence-metrics","level":2,"title":"Independence Metrics","text":"Package Dependencies Dependents Instability Independence canvod-readers 0 1 0.00 1.00 canvod-grids 0 2 0.00 1.00 canvod-vod 0 0 0.00 1.00 canvod-utils 0 0 0.00 1.00 canvod-auxiliary 1 0 1.00 0.83 canvod-viz 1 0 1.00 0.83 canvod-store 1 0 1.00 0.83","path":["Architecture","Package Dependencies"],"tags":[]},{"location":"dependencies/#metric-definitions","level":3,"title":"Metric Definitions","text":"<ul> <li>Efferent coupling (Ce): Number of packages this package depends on. Lower values indicate greater independence.</li> <li>Afferent coupling (Ca): Number of packages that depend on this package. Higher values indicate greater reusability.</li> <li>Instability (I): <code>Ce / (Ce + Ca)</code>. 0.0 = maximally stable (foundation), 1.0 = maximally unstable (leaf).</li> <li>Independence: <code>1 - (Ce / total_packages)</code>. 1.0 = no inter-package dependencies.</li> </ul>","path":["Architecture","Package Dependencies"],"tags":[]},{"location":"dependencies/#architecture-summary","level":2,"title":"Architecture Summary","text":"<ul> <li>No circular dependencies</li> <li>4 packages with zero inter-package dependencies (57%)</li> <li>3 total internal dependency edges</li> <li>Maximum dependency depth: 1</li> </ul>","path":["Architecture","Package Dependencies"],"tags":[]},{"location":"dependencies/#dependency-layers","level":3,"title":"Dependency Layers","text":"<pre><code>Layer 0 (Foundation, 0 dependencies):\n  canvod-readers, canvod-grids, canvod-vod, canvod-utils\n\nLayer 1 (Consumers, 1 dependency each):\n  canvod-auxiliary (depends on canvod-readers)\n  canvod-viz (depends on canvod-grids)\n  canvod-store (depends on canvod-grids)\n</code></pre> <p>The two-layer structure simplifies testing (test Layer 0 first, then Layer 1) and ensures that changes to foundation packages do not cascade between siblings.</p>","path":["Architecture","Package Dependencies"],"tags":[]},{"location":"dependencies/#extractability","level":2,"title":"Extractability","text":"<p>All packages can be extracted to independent repositories with zero or minimal changes:</p> <pre><code># Foundation packages: extract directly\npackages/canvod-readers/  -&gt; independent repo\npackages/canvod-grids/    -&gt; independent repo\npackages/canvod-vod/      -&gt; independent repo\npackages/canvod-utils/    -&gt; independent repo\n\n# Consumer packages: extract with one PyPI dependency\npackages/canvod-auxiliary/ -&gt; independent repo (+ canvod-readers)\npackages/canvod-viz/      -&gt; independent repo (+ canvod-grids)\npackages/canvod-store/    -&gt; independent repo (+ canvod-grids)\n</code></pre>","path":["Architecture","Package Dependencies"],"tags":[]},{"location":"dependencies/#regeneration","level":2,"title":"Regeneration","text":"<pre><code>just deps-report    # Full metrics report\njust deps-graph     # Mermaid diagram\n</code></pre>","path":["Architecture","Package Dependencies"],"tags":[]},{"location":"namespace-packages/","level":1,"title":"Namespace Packages","text":"","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#overview","level":2,"title":"Overview","text":"<p>canVODpy uses Python 3.3+ implicit namespace packages to allow seven independent packages to share the <code>canvod.*</code> import prefix. This enables a unified API while keeping packages independently installable.</p> <pre><code>from canvod.readers import Rnxv3Obs        # from canvod-readers package\nfrom canvod.auxiliary import Sp3File        # from canvod-auxiliary package\nfrom canvod.grids import EqualAreaBuilder   # from canvod-grids package\n</code></pre>","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#mechanism","level":2,"title":"Mechanism","text":"","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#directory-structure","level":3,"title":"Directory Structure","text":"<p>A namespace package is created by omitting <code>__init__.py</code> from the shared parent directory:</p> <pre><code>canvod-readers/\n  src/\n    canvod/              # Namespace directory -- NO __init__.py\n      readers/           # Package directory\n        __init__.py      # Regular package\n</code></pre> <p>When Python encounters a directory without <code>__init__.py</code>, it treats it as a namespace package. Multiple installed packages can each contribute a subdirectory under the same namespace without conflict.</p>","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#comparison-with-regular-packages","level":3,"title":"Comparison with Regular Packages","text":"<p>Regular package (only one package can claim the name): <pre><code>src/\n  canvod/\n    __init__.py        # Makes this a regular package -- blocks other contributors\n    readers/\n      __init__.py\n</code></pre></p> <p>Namespace package (multiple packages share the name): <pre><code>src/\n  canvod/              # No __init__.py -- namespace is open for extension\n    readers/\n      __init__.py\n</code></pre></p>","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#build-configuration","level":2,"title":"Build Configuration","text":"<p>The <code>uv_build</code> backend is configured with a dotted <code>module-name</code> to produce correct namespace structure:</p> <pre><code># packages/canvod-readers/pyproject.toml\n[build-system]\nrequires = [\"uv_build&gt;=0.9.17,&lt;0.10.0\"]\nbuild-backend = \"uv_build\"\n\n[tool.uv.build-backend]\nmodule-name = \"canvod.readers\"  # Dot indicates namespace package\n</code></pre> <p>The dot in <code>\"canvod.readers\"</code> instructs uv_build to: - Treat <code>canvod</code> as a namespace (do not include <code>__init__.py</code> for it) - Package only the <code>readers</code> subdirectory</p>","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#wheel-contents","level":2,"title":"Wheel Contents","text":"<p>A built wheel contains the namespace structure without a top-level <code>__init__.py</code>:</p> <pre><code>canvod_readers-0.1.0-py3-none-any.whl\n  canvod/\n    readers/\n      __init__.py\n      base.py\n      rinex/\n        ...\n  canvod_readers-0.1.0.dist-info/\n    METADATA\n    WHEEL\n    RECORD\n</code></pre>","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#combined-installation","level":2,"title":"Combined Installation","text":"<p>When multiple packages are installed, Python merges them under one namespace:</p> <pre><code>site-packages/\n  canvod/\n    readers/      # From canvod-readers\n    auxiliary/    # From canvod-auxiliary\n    grids/        # From canvod-grids\n    vod/          # From canvod-vod\n    store/        # From canvod-store\n    viz/          # From canvod-viz\n</code></pre>","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#import-resolution","level":2,"title":"Import Resolution","text":"<p>When Python processes <code>from canvod.readers import Rnxv3Obs</code>:</p> <ol> <li>It finds <code>canvod</code> as a namespace (no <code>__init__.py</code>)</li> <li>It locates <code>readers</code> within <code>canvod</code> (has <code>__init__.py</code> -- regular package)</li> <li>It imports <code>Rnxv3Obs</code> from <code>canvod/readers/__init__.py</code></li> </ol>","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#verification","level":2,"title":"Verification","text":"<pre><code>import canvod\nprint(canvod.__file__)   # AttributeError -- namespace packages have no __file__\n\nfrom canvod import readers\nprint(readers.__file__)  # Prints the file path -- regular package\n</code></pre>","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#common-pitfalls","level":2,"title":"Common Pitfalls","text":"","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#adding-__init__py-to-the-namespace-directory","level":3,"title":"Adding <code>__init__.py</code> to the Namespace Directory","text":"<p>Creating <code>src/canvod/__init__.py</code> converts the namespace into a regular package, preventing other packages from contributing to it.</p>","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#incorrect-module-name-configuration","level":3,"title":"Incorrect module-name Configuration","text":"<pre><code># Incorrect -- creates regular package, not namespace\nmodule-name = \"canvod_readers\"\n\n# Correct -- creates namespace package\nmodule-name = \"canvod.readers\"\n</code></pre>","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"namespace-packages/#precedents","level":2,"title":"Precedents","text":"<p>Namespace packages are used by several major Python projects:</p> <ul> <li>Azure SDK: <code>azure.storage</code>, <code>azure.compute</code>, <code>azure.ai</code></li> <li>Google Cloud: <code>google.cloud.storage</code>, <code>google.cloud.compute</code></li> <li>Zope: <code>zope.interface</code>, <code>zope.component</code></li> <li>Sphinx extensions: <code>sphinxcontrib.*</code></li> </ul>","path":["Architecture","Namespace Packages"],"tags":[]},{"location":"tooling/","level":1,"title":"Development Tooling","text":"","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#overview","level":2,"title":"Overview","text":"<p>canVODpy uses a modern Python toolchain, primarily from the Astral ecosystem. This page documents each tool and its role.</p>","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#core-tools","level":2,"title":"Core Tools","text":"","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#uv-package-manager","level":3,"title":"uv -- Package Manager","text":"<p>Documentation</p> <p>uv manages Python versions, virtual environments, dependency resolution, package installation, and builds. It replaces pip, venv, pip-tools, and twine.</p> <pre><code>uv sync                    # Install dependencies\nuv add numpy               # Add a dependency\nuv run pytest              # Run command in the environment\nuv build                   # Build the package\n</code></pre> <p>Configuration is specified in <code>pyproject.toml</code>:</p> <pre><code>[project]\ndependencies = [\"numpy&gt;=1.24\", \"pandas&gt;=2.0\"]\n\n[dependency-groups]\ndev = [\"pytest&gt;=8.0\", \"ruff&gt;=0.14\"]\n</code></pre>","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#uv_build-build-backend","level":3,"title":"uv_build -- Build Backend","text":"<p>Documentation</p> <p>uv_build creates wheel and source distributions from Python packages. It provides native support for namespace packages via the dotted <code>module-name</code> configuration.</p> <pre><code>[build-system]\nrequires = [\"uv_build&gt;=0.9.17,&lt;0.10.0\"]\nbuild-backend = \"uv_build\"\n\n[tool.uv.build-backend]\nmodule-name = \"canvod.readers\"  # Dot creates namespace package\n</code></pre> <p>Note: <code>canvod-utils</code> uses <code>hatchling</code> as its build backend instead.</p>","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#ruff-linter-and-formatter","level":3,"title":"ruff -- Linter and Formatter","text":"<p>Documentation</p> <p>ruff provides linting and formatting in a single tool, replacing flake8, pylint, black, and isort. It implements 700+ rules from multiple linting tools.</p> <pre><code>ruff check .          # Lint\nruff check . --fix    # Lint with auto-fix\nruff format .         # Format\n</code></pre> <p>Configuration:</p> <pre><code>[tool.ruff]\nline-length = 88\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"W\", \"I\", \"N\", \"UP\", \"B\", \"SIM\", \"C4\", \"RUF\", \"PIE\", \"PT\"]\n</code></pre>","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#ty-type-checker","level":3,"title":"ty -- Type Checker","text":"<p>ty checks Python type annotations, replacing mypy. It is currently in early development (alpha).</p> <pre><code>ty check .\n</code></pre>","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#supporting-tools","level":2,"title":"Supporting Tools","text":"","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#just-task-runner","level":3,"title":"just -- Task Runner","text":"<p>Documentation</p> <p>just provides a command runner with simpler syntax than Make. It is used for all common development tasks and in CI/CD.</p> <pre><code>just test             # Run tests\njust check            # Lint + format + type-check\njust docs             # Build and serve documentation\njust --list           # Show all available commands\n</code></pre>","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#pytest-testing-framework","level":3,"title":"pytest -- Testing Framework","text":"<p>Documentation</p> <p>pytest runs the test suite and generates coverage reports. Tests reside in each package's <code>tests/</code> directory.</p> <pre><code>uv run pytest                    # All tests\nuv run pytest --cov=canvod       # With coverage\n</code></pre>","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#pre-commit-git-hooks","level":3,"title":"pre-commit -- Git Hooks","text":"<p>Documentation</p> <p>pre-commit runs ruff and other checks automatically before each commit.</p> <pre><code>just hooks            # Install hooks\n</code></pre> <p>Configuration in <code>.pre-commit-config.yaml</code>:</p> <pre><code>repos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    hooks:\n      - id: ruff-check\n        args: [--fix]\n      - id: ruff-format\n</code></pre>","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#myst-documentation","level":3,"title":"MyST -- Documentation","text":"<p>Documentation</p> <p>MyST is a Markdown-based documentation system supporting Jupyter notebooks, Mermaid diagrams, and cross-references.</p> <pre><code>uv run myst           # Preview docs locally\n</code></pre>","path":["Development","Development Tooling"],"tags":[]},{"location":"tooling/#tool-comparison","level":2,"title":"Tool Comparison","text":"Task Traditional canVODpy Package management pip uv Environments venv uv (built-in) Linting flake8 + pylint ruff Formatting black + isort ruff Type checking mypy ty Building setuptools uv_build Task runner make just Documentation Sphinx MyST","path":["Development","Development Tooling"],"tags":[]},{"location":"api/canvod-auxiliary/","level":1,"title":"canvod.auxiliary API Reference","text":"<p>SP3 ephemeris and CLK clock correction processing, interpolation, coordinate transformations, and the GNSS product registry.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#package","level":2,"title":"Package","text":"<p>canvod-aux: Auxiliary data augmentation for GNSS VOD analysis</p> <p>Handles downloading, parsing, and interpolating SP3 ephemerides and clock corrections for GNSS satellite data processing.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File","level":2,"title":"<code>Sp3File</code>","text":"<p>               Bases: <code>AuxFile</code></p> <p>Handler for SP3 orbit files with multi-product support.</p> <p>Now supports all IGS analysis centers via product registry: COD, GFZ, ESA, JPL, IGS, WHU, GRG, SHA</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic dataclass with <code>arbitrary_types_allowed=True</code>.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File--attributes","level":4,"title":"Attributes","text":"<p>date : str     String in YYYYDOY format. agency : str     Agency code (e.g., \"COD\", \"GFZ\", \"ESA\"). product_type : str     Product type (\"final\", \"rapid\"). ftp_server : str     Base URL for downloads. local_dir : Path     Local storage directory. add_velocities : bool | None, default True     Whether to compute velocities. dimensionless : bool | None, default True     Whether to strip units (store magnitudes only). product_spec : ProductSpec | None, optional     Product specification resolved from the registry.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass Sp3File(AuxFile):\n    \"\"\"Handler for SP3 orbit files with multi-product support.\n\n    Now supports all IGS analysis centers via product registry:\n    COD, GFZ, ESA, JPL, IGS, WHU, GRG, SHA\n\n    Notes\n    -----\n    This is a Pydantic dataclass with `arbitrary_types_allowed=True`.\n\n    Attributes\n    ----------\n    date : str\n        String in YYYYDOY format.\n    agency : str\n        Agency code (e.g., \"COD\", \"GFZ\", \"ESA\").\n    product_type : str\n        Product type (\"final\", \"rapid\").\n    ftp_server : str\n        Base URL for downloads.\n    local_dir : Path\n        Local storage directory.\n    add_velocities : bool | None, default True\n        Whether to compute velocities.\n    dimensionless : bool | None, default True\n        Whether to strip units (store magnitudes only).\n    product_spec : ProductSpec | None, optional\n        Product specification resolved from the registry.\n    \"\"\"\n\n    date: str\n    agency: str\n    product_type: str\n    ftp_server: str\n    local_dir: Path\n    add_velocities: bool | None = True\n    dimensionless: bool | None = True\n    product_spec: ProductSpec | None = None\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize with product validation.\"\"\"\n        self.file_type = [\"orbit\"]\n        self.local_dir = Path(self.local_dir)\n        self.local_dir.mkdir(parents=True, exist_ok=True)\n\n        # Validate product exists in registry\n        self.product_spec = get_product_spec(self.agency, self.product_type)\n\n        super().__post_init__()\n\n    def get_interpolation_strategy(self) -&gt; Interpolator:\n        \"\"\"Get appropriate interpolation strategy for SP3 files.\"\"\"\n        config = Sp3Config(\n            use_velocities=self.add_velocities,\n            fallback_method=\"linear\",\n        )\n        return Sp3InterpolationStrategy(config=config)\n\n    def generate_filename_based_on_type(self) -&gt; Path:\n        \"\"\"Generate filename using product registry.\n\n        Pattern: {PREFIX}_{YYYYDOY}0000_{DURATION}_{SAMPLING}_ORB.SP3\n\n        Example: COD0MGXFIN_20240150000_01D_05M_ORB.SP3\n        \"\"\"\n        prefix = self.product_spec.prefix\n        duration = self.product_spec.duration\n        sampling = self.product_spec.sampling_rate\n\n        return Path(f\"{prefix}_{self.date}0000_{duration}_{sampling}_ORB.SP3\")\n\n    def download_aux_file(self) -&gt; None:\n        \"\"\"Download using product-specific path pattern.\n\n        Raises\n        ------\n        RuntimeError\n            If download fails from all servers.\n        ValueError\n            If GPS week calculation fails.\n        \"\"\"\n        orbit_file = self.generate_filename_based_on_type()\n        gps_week = get_gps_week_from_filename(orbit_file)\n\n        # Use product spec's path pattern\n        ftp_path = self.product_spec.ftp_path_pattern.format(\n            gps_week=gps_week,\n            file=f\"{orbit_file}.gz\",\n        )\n\n        full_url = f\"{self.ftp_server}{ftp_path}\"\n        destination = self.local_dir / orbit_file\n\n        file_info = {\n            \"gps_week\": gps_week,\n            \"filename\": orbit_file,\n            \"type\": \"orbit\",\n            \"agency\": self.agency,\n            \"latency\": self.product_spec.latency_hours,\n        }\n\n        try:\n            self.download_file(full_url, destination, file_info)\n            print(f\"Downloaded orbit file for {self.agency} on date {self.date}\")\n        except Exception as e:\n            raise RuntimeError(\n                f\"Failed to download SP3 file from all available servers: {str(e)}\"\n            )\n\n    def read_file(self) -&gt; xr.Dataset:\n        \"\"\"Read and validate SP3 file.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with satellite positions (X, Y, Z) in meters.\n\n        Raises\n        ------\n        FileNotFoundError\n            If file does not exist.\n        ValueError\n            If validation fails.\n        \"\"\"\n        # Use dedicated parser\n        parser = Sp3Parser(self.fpath, dimensionless=self.dimensionless)\n        dataset = parser.parse()\n\n        # Validate format\n        validator = Sp3Validator(dataset, self.fpath)\n        result = validator.validate()\n\n        if not result.is_valid:\n            raise ValueError(f\"SP3 validation failed:\\n{result.summary()}\")\n\n        # Add metadata\n        dataset = self._add_metadata(dataset)\n\n        # Compute velocities if requested\n        if self.add_velocities:\n            dataset = self.compute_velocity(dataset)\n\n        return dataset\n\n    def _add_metadata(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"Add file-level metadata to dataset.\"\"\"\n        ds.attrs = {\n            \"file\": str(self.fpath.name),\n            \"agency\": self.agency,\n            \"agency_name\": self.product_spec.agency_name,\n            \"product_type\": self.product_type,\n            \"ftp_server\": self.ftp_server,\n            \"date\": self.date,\n            \"sampling_rate\": self.product_spec.sampling_rate,\n            \"duration\": self.product_spec.duration,\n        }\n        return ds\n\n    def compute_velocity(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"Compute satellite velocities from position data.\n\n        Uses central differences for interior points, forward/backward\n        differences for endpoints.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset with X, Y, Z coordinates.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset augmented with Vx, Vy, Vz velocities.\n        \"\"\"\n        # Calculate time step\n        time_diffs = np.diff(ds[\"epoch\"].values)\n        dt = np.median(time_diffs).astype(\"timedelta64[s]\").astype(float)\n\n        # Initialize velocity arrays\n        vx = np.zeros_like(ds[\"X\"].values)\n        vy = np.zeros_like(ds[\"Y\"].values)\n        vz = np.zeros_like(ds[\"Z\"].values)\n\n        # Central difference for interior points\n        vx[1:-1] = (ds[\"X\"].values[2:] - ds[\"X\"].values[:-2]) / (2 * dt)\n        vy[1:-1] = (ds[\"Y\"].values[2:] - ds[\"Y\"].values[:-2]) / (2 * dt)\n        vz[1:-1] = (ds[\"Z\"].values[2:] - ds[\"Z\"].values[:-2]) / (2 * dt)\n\n        # Forward difference for first point\n        vx[0] = (ds[\"X\"].values[1] - ds[\"X\"].values[0]) / dt\n        vy[0] = (ds[\"Y\"].values[1] - ds[\"Y\"].values[0]) / dt\n        vz[0] = (ds[\"Z\"].values[1] - ds[\"Z\"].values[0]) / dt\n\n        # Backward difference for last point\n        vx[-1] = (ds[\"X\"].values[-1] - ds[\"X\"].values[-2]) / dt\n        vy[-1] = (ds[\"Y\"].values[-1] - ds[\"Y\"].values[-2]) / dt\n        vz[-1] = (ds[\"Z\"].values[-1] - ds[\"Z\"].values[-2]) / dt\n\n        # Add units if needed\n        if not self.dimensionless:\n            vx = vx * (UREG.meter / UREG.second)\n            vy = vy * (UREG.meter / UREG.second)\n            vz = vz * (UREG.meter / UREG.second)\n\n        # Add to dataset\n        ds = ds.assign(\n            Vx=((\"epoch\", \"sv\"), vx),\n            Vy=((\"epoch\", \"sv\"), vy),\n            Vz=((\"epoch\", \"sv\"), vz),\n        )\n\n        # Add velocity attributes\n        for var, attrs in self._get_velocity_attributes(dt).items():\n            if var in ds:\n                ds[var].attrs = attrs\n\n        return ds\n\n    def _get_velocity_attributes(\n        self,\n        dt: float,\n    ) -&gt; dict[str, dict[str, str | float]]:\n        \"\"\"Get standardized attributes for velocity variables.\"\"\"\n        base_attrs = {\n            \"units\": \"m/s\",\n            \"computation_method\": \"central_difference\",\n            \"time_step\": float(dt),\n            \"reference_frame\": \"ECEF\",\n        }\n\n        return {\n            \"Vx\": {\n                \"long_name\": \"x-component of velocity\",\n                \"standard_name\": \"v_x\",\n                \"short_name\": \"v_x\",\n                \"axis\": \"v_x\",\n                **base_attrs,\n            },\n            \"Vy\": {\n                \"long_name\": \"y-component of velocity\",\n                \"standard_name\": \"v_y\",\n                \"short_name\": \"v_y\",\n                \"axis\": \"v_y\",\n                **base_attrs,\n            },\n            \"Vz\": {\n                \"long_name\": \"z-component of velocity\",\n                \"standard_name\": \"v_z\",\n                \"short_name\": \"v_z\",\n                \"axis\": \"v_z\",\n                **base_attrs,\n            },\n        }\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File.__post_init__","level":3,"title":"<code>__post_init__()</code>","text":"<p>Initialize with product validation.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize with product validation.\"\"\"\n    self.file_type = [\"orbit\"]\n    self.local_dir = Path(self.local_dir)\n    self.local_dir.mkdir(parents=True, exist_ok=True)\n\n    # Validate product exists in registry\n    self.product_spec = get_product_spec(self.agency, self.product_type)\n\n    super().__post_init__()\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File.get_interpolation_strategy","level":3,"title":"<code>get_interpolation_strategy()</code>","text":"<p>Get appropriate interpolation strategy for SP3 files.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def get_interpolation_strategy(self) -&gt; Interpolator:\n    \"\"\"Get appropriate interpolation strategy for SP3 files.\"\"\"\n    config = Sp3Config(\n        use_velocities=self.add_velocities,\n        fallback_method=\"linear\",\n    )\n    return Sp3InterpolationStrategy(config=config)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File.generate_filename_based_on_type","level":3,"title":"<code>generate_filename_based_on_type()</code>","text":"<p>Generate filename using product registry.</p> <p>Pattern: {PREFIX}{YYYYDOY}0000_ORB.SP3}_{SAMPLING</p> <p>Example: COD0MGXFIN_20240150000_01D_05M_ORB.SP3</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def generate_filename_based_on_type(self) -&gt; Path:\n    \"\"\"Generate filename using product registry.\n\n    Pattern: {PREFIX}_{YYYYDOY}0000_{DURATION}_{SAMPLING}_ORB.SP3\n\n    Example: COD0MGXFIN_20240150000_01D_05M_ORB.SP3\n    \"\"\"\n    prefix = self.product_spec.prefix\n    duration = self.product_spec.duration\n    sampling = self.product_spec.sampling_rate\n\n    return Path(f\"{prefix}_{self.date}0000_{duration}_{sampling}_ORB.SP3\")\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File.download_aux_file","level":3,"title":"<code>download_aux_file()</code>","text":"<p>Download using product-specific path pattern.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File.download_aux_file--raises","level":5,"title":"Raises","text":"<p>RuntimeError     If download fails from all servers. ValueError     If GPS week calculation fails.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def download_aux_file(self) -&gt; None:\n    \"\"\"Download using product-specific path pattern.\n\n    Raises\n    ------\n    RuntimeError\n        If download fails from all servers.\n    ValueError\n        If GPS week calculation fails.\n    \"\"\"\n    orbit_file = self.generate_filename_based_on_type()\n    gps_week = get_gps_week_from_filename(orbit_file)\n\n    # Use product spec's path pattern\n    ftp_path = self.product_spec.ftp_path_pattern.format(\n        gps_week=gps_week,\n        file=f\"{orbit_file}.gz\",\n    )\n\n    full_url = f\"{self.ftp_server}{ftp_path}\"\n    destination = self.local_dir / orbit_file\n\n    file_info = {\n        \"gps_week\": gps_week,\n        \"filename\": orbit_file,\n        \"type\": \"orbit\",\n        \"agency\": self.agency,\n        \"latency\": self.product_spec.latency_hours,\n    }\n\n    try:\n        self.download_file(full_url, destination, file_info)\n        print(f\"Downloaded orbit file for {self.agency} on date {self.date}\")\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to download SP3 file from all available servers: {str(e)}\"\n        )\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File.read_file","level":3,"title":"<code>read_file()</code>","text":"<p>Read and validate SP3 file.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File.read_file--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with satellite positions (X, Y, Z) in meters.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File.read_file--raises","level":5,"title":"Raises","text":"<p>FileNotFoundError     If file does not exist. ValueError     If validation fails.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def read_file(self) -&gt; xr.Dataset:\n    \"\"\"Read and validate SP3 file.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with satellite positions (X, Y, Z) in meters.\n\n    Raises\n    ------\n    FileNotFoundError\n        If file does not exist.\n    ValueError\n        If validation fails.\n    \"\"\"\n    # Use dedicated parser\n    parser = Sp3Parser(self.fpath, dimensionless=self.dimensionless)\n    dataset = parser.parse()\n\n    # Validate format\n    validator = Sp3Validator(dataset, self.fpath)\n    result = validator.validate()\n\n    if not result.is_valid:\n        raise ValueError(f\"SP3 validation failed:\\n{result.summary()}\")\n\n    # Add metadata\n    dataset = self._add_metadata(dataset)\n\n    # Compute velocities if requested\n    if self.add_velocities:\n        dataset = self.compute_velocity(dataset)\n\n    return dataset\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File.compute_velocity","level":3,"title":"<code>compute_velocity(ds)</code>","text":"<p>Compute satellite velocities from position data.</p> <p>Uses central differences for interior points, forward/backward differences for endpoints.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File.compute_velocity--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with X, Y, Z coordinates.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Sp3File.compute_velocity--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset augmented with Vx, Vy, Vz velocities.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def compute_velocity(self, ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Compute satellite velocities from position data.\n\n    Uses central differences for interior points, forward/backward\n    differences for endpoints.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with X, Y, Z coordinates.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset augmented with Vx, Vy, Vz velocities.\n    \"\"\"\n    # Calculate time step\n    time_diffs = np.diff(ds[\"epoch\"].values)\n    dt = np.median(time_diffs).astype(\"timedelta64[s]\").astype(float)\n\n    # Initialize velocity arrays\n    vx = np.zeros_like(ds[\"X\"].values)\n    vy = np.zeros_like(ds[\"Y\"].values)\n    vz = np.zeros_like(ds[\"Z\"].values)\n\n    # Central difference for interior points\n    vx[1:-1] = (ds[\"X\"].values[2:] - ds[\"X\"].values[:-2]) / (2 * dt)\n    vy[1:-1] = (ds[\"Y\"].values[2:] - ds[\"Y\"].values[:-2]) / (2 * dt)\n    vz[1:-1] = (ds[\"Z\"].values[2:] - ds[\"Z\"].values[:-2]) / (2 * dt)\n\n    # Forward difference for first point\n    vx[0] = (ds[\"X\"].values[1] - ds[\"X\"].values[0]) / dt\n    vy[0] = (ds[\"Y\"].values[1] - ds[\"Y\"].values[0]) / dt\n    vz[0] = (ds[\"Z\"].values[1] - ds[\"Z\"].values[0]) / dt\n\n    # Backward difference for last point\n    vx[-1] = (ds[\"X\"].values[-1] - ds[\"X\"].values[-2]) / dt\n    vy[-1] = (ds[\"Y\"].values[-1] - ds[\"Y\"].values[-2]) / dt\n    vz[-1] = (ds[\"Z\"].values[-1] - ds[\"Z\"].values[-2]) / dt\n\n    # Add units if needed\n    if not self.dimensionless:\n        vx = vx * (UREG.meter / UREG.second)\n        vy = vy * (UREG.meter / UREG.second)\n        vz = vz * (UREG.meter / UREG.second)\n\n    # Add to dataset\n    ds = ds.assign(\n        Vx=((\"epoch\", \"sv\"), vx),\n        Vy=((\"epoch\", \"sv\"), vy),\n        Vz=((\"epoch\", \"sv\"), vz),\n    )\n\n    # Add velocity attributes\n    for var, attrs in self._get_velocity_attributes(dt).items():\n        if var in ds:\n            ds[var].attrs = attrs\n\n    return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile","level":2,"title":"<code>ClkFile</code>","text":"<p>               Bases: <code>AuxFile</code></p> <p>Handler for GNSS clock files in CLK format.</p> <p>This class reads and processes clock offset files containing satellite clock corrections. It handles the parsing of CLK format files and provides the data in xarray Dataset format.</p> <p>Supports multiple analysis centers via product registry with proper FTP paths and filename conventions.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic dataclass with <code>arbitrary_types_allowed=True</code>.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile--attributes","level":4,"title":"Attributes","text":"<p>date : str     String in YYYYDOY format representing the start date. agency : str     Analysis center identifier (e.g., \"COD\", \"GFZ\"). product_type : str     Product type (\"final\", \"rapid\", \"ultrarapid\"). ftp_server : str     Base URL for file downloads. local_dir : Path     Local storage directory. dimensionless : bool | None, default True     If True, outputs magnitude-only values (no units attached).</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass ClkFile(AuxFile):\n    \"\"\"Handler for GNSS clock files in CLK format.\n\n    This class reads and processes clock offset files containing satellite clock\n    corrections. It handles the parsing of CLK format files and provides the data\n    in xarray Dataset format.\n\n    Supports multiple analysis centers via product registry with proper FTP paths\n    and filename conventions.\n\n    Notes\n    -----\n    This is a Pydantic dataclass with `arbitrary_types_allowed=True`.\n\n    Attributes\n    ----------\n    date : str\n        String in YYYYDOY format representing the start date.\n    agency : str\n        Analysis center identifier (e.g., \"COD\", \"GFZ\").\n    product_type : str\n        Product type (\"final\", \"rapid\", \"ultrarapid\").\n    ftp_server : str\n        Base URL for file downloads.\n    local_dir : Path\n        Local storage directory.\n    dimensionless : bool | None, default True\n        If True, outputs magnitude-only values (no units attached).\n    \"\"\"\n\n    date: str\n    agency: str\n    product_type: str\n    ftp_server: str\n    local_dir: Path\n    dimensionless: bool | None = True\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize CLK file handler.\"\"\"\n        self.file_type = [\"clock\"]\n        self.local_dir = Path(self.local_dir)\n        self.local_dir.mkdir(parents=True, exist_ok=True)\n        super().__post_init__()\n\n    def get_interpolation_strategy(self) -&gt; Interpolator:\n        \"\"\"Get appropriate interpolation strategy for CLK files.\"\"\"\n        config = ClockConfig(\n            window_size=9,\n            jump_threshold=1e-6,\n        )\n        return ClockInterpolationStrategy(config=config)\n\n    def generate_filename_based_on_type(self) -&gt; Path:\n        \"\"\"Generate standard CLK filename using product registry.\n\n        Uses product registry to get correct prefix for the agency/product combination.\n        Filename format: {PREFIX}_{YYYYDOY}0000_01D_30S_CLK.CLK\n\n        Returns\n        -------\n        Path\n            Filename according to CLK conventions.\n\n        Raises\n        ------\n        ValueError\n            If agency/product combination not in registry.\n        \"\"\"\n        # Get product spec from registry\n        spec = get_product_spec(self.agency, self.product_type)\n\n        # CLK files use 30S sampling for most products\n        sampling = \"30S\"  # Could be made configurable via registry\n\n        return Path(f\"{spec.prefix}_{self.date}0000_01D_{sampling}_CLK.CLK\")\n\n    def download_aux_file(self) -&gt; None:\n        \"\"\"Download CLK file from FTP server with automatic fallback.\n\n        Constructs URL using product registry path pattern.\n        Uses GPS week for directory structure.\n\n        Raises\n        ------\n        RuntimeError\n            If file cannot be downloaded from any available server.\n        ValueError\n            If GPS week calculation fails.\n        \"\"\"\n        clock_file = self.generate_filename_based_on_type()\n        gps_week = get_gps_week_from_filename(clock_file)\n\n        # Get product spec from registry\n        spec = get_product_spec(self.agency, self.product_type)\n\n        # Use product spec's path pattern\n        ftp_path = spec.ftp_path_pattern.format(\n            gps_week=gps_week,\n            file=f\"{clock_file}.gz\",\n        )\n\n        full_url = f\"{self.ftp_server}{ftp_path}\"\n        destination = self.local_dir / clock_file\n\n        # File info for NASA CDDIS URL construction\n        file_info = {\n            \"gps_week\": gps_week,\n            \"filename\": clock_file,\n            \"type\": \"clock\",\n            \"agency\": self.agency,\n            \"latency\": spec.latency_hours,\n        }\n\n        try:\n            self.download_file(full_url, destination, file_info)\n            print(f\"Downloaded clock file for {self.agency} on date {self.date}\")\n        except Exception as e:\n            raise RuntimeError(\n                f\"Failed to download CLK file from all available servers: {str(e)}\"\n            )\n\n    def read_file(self) -&gt; xr.Dataset:\n        \"\"\"Read and parse CLK file into xarray Dataset.\n\n        Uses modular parser for data extraction and validator for quality checks.\n        Applies unit conversion from microseconds to seconds.\n\n        Returns\n        -------\n        xr.Dataset\n            Clock offsets with dimensions (epoch, sv). Values are in seconds\n            (or dimensionless if specified).\n        \"\"\"\n        # Parse file using modular parser\n        epochs, satellites, clock_offsets = parse_clk_file(self.fpath)\n\n        # Convert units (microseconds → seconds)\n        clock_offsets = (UREG.microsecond * clock_offsets).to(\"s\")\n\n        if self.dimensionless:\n            clock_offsets = clock_offsets.magnitude\n\n        # Create dataset\n        ds = xr.Dataset(\n            data_vars={\"clock_offset\": ((\"epoch\", \"sv\"), clock_offsets)},\n            coords={\n                \"epoch\": np.array(epochs, dtype=\"datetime64[ns]\"),\n                \"sv\": np.array(satellites),\n            },\n        )\n\n        # Validate and add metadata\n        validation = validate_clk_dataset(ds)\n        ds = self._prepare_dataset(ds, validation)\n\n        return ds\n\n    def _prepare_dataset(self, ds: xr.Dataset, validation: dict) -&gt; xr.Dataset:\n        \"\"\"Add metadata and attributes to dataset.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset from parsing.\n        validation : dict\n            Validation results dictionary.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with complete metadata.\n        \"\"\"\n        ds.attrs = {\n            \"file\": str(self.fpath.name),\n            \"agency\": self.agency,\n            \"product_type\": self.product_type,\n            \"ftp_server\": self.ftp_server,\n            \"date\": self.date,\n            \"valid_data_percent\": validation[\"valid_data_percent\"],\n            \"num_epochs\": validation[\"num_epochs\"],\n            \"num_satellites\": validation[\"num_satellites\"],\n        }\n\n        # Add variable attributes\n        ds.clock_offset.attrs = {\n            \"long_name\": \"Satellite clock offset\",\n            \"standard_name\": \"clock_offset\",\n            \"units\": \"seconds\",\n            \"description\": \"Clock correction for each satellite\",\n        }\n\n        return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile.__post_init__","level":3,"title":"<code>__post_init__()</code>","text":"<p>Initialize CLK file handler.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize CLK file handler.\"\"\"\n    self.file_type = [\"clock\"]\n    self.local_dir = Path(self.local_dir)\n    self.local_dir.mkdir(parents=True, exist_ok=True)\n    super().__post_init__()\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile.get_interpolation_strategy","level":3,"title":"<code>get_interpolation_strategy()</code>","text":"<p>Get appropriate interpolation strategy for CLK files.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>def get_interpolation_strategy(self) -&gt; Interpolator:\n    \"\"\"Get appropriate interpolation strategy for CLK files.\"\"\"\n    config = ClockConfig(\n        window_size=9,\n        jump_threshold=1e-6,\n    )\n    return ClockInterpolationStrategy(config=config)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile.generate_filename_based_on_type","level":3,"title":"<code>generate_filename_based_on_type()</code>","text":"<p>Generate standard CLK filename using product registry.</p> <p>Uses product registry to get correct prefix for the agency/product combination. Filename format: {PREFIX}_{YYYYDOY}0000_01D_30S_CLK.CLK</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile.generate_filename_based_on_type--returns","level":5,"title":"Returns","text":"<p>Path     Filename according to CLK conventions.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile.generate_filename_based_on_type--raises","level":5,"title":"Raises","text":"<p>ValueError     If agency/product combination not in registry.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>def generate_filename_based_on_type(self) -&gt; Path:\n    \"\"\"Generate standard CLK filename using product registry.\n\n    Uses product registry to get correct prefix for the agency/product combination.\n    Filename format: {PREFIX}_{YYYYDOY}0000_01D_30S_CLK.CLK\n\n    Returns\n    -------\n    Path\n        Filename according to CLK conventions.\n\n    Raises\n    ------\n    ValueError\n        If agency/product combination not in registry.\n    \"\"\"\n    # Get product spec from registry\n    spec = get_product_spec(self.agency, self.product_type)\n\n    # CLK files use 30S sampling for most products\n    sampling = \"30S\"  # Could be made configurable via registry\n\n    return Path(f\"{spec.prefix}_{self.date}0000_01D_{sampling}_CLK.CLK\")\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile.download_aux_file","level":3,"title":"<code>download_aux_file()</code>","text":"<p>Download CLK file from FTP server with automatic fallback.</p> <p>Constructs URL using product registry path pattern. Uses GPS week for directory structure.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile.download_aux_file--raises","level":5,"title":"Raises","text":"<p>RuntimeError     If file cannot be downloaded from any available server. ValueError     If GPS week calculation fails.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>def download_aux_file(self) -&gt; None:\n    \"\"\"Download CLK file from FTP server with automatic fallback.\n\n    Constructs URL using product registry path pattern.\n    Uses GPS week for directory structure.\n\n    Raises\n    ------\n    RuntimeError\n        If file cannot be downloaded from any available server.\n    ValueError\n        If GPS week calculation fails.\n    \"\"\"\n    clock_file = self.generate_filename_based_on_type()\n    gps_week = get_gps_week_from_filename(clock_file)\n\n    # Get product spec from registry\n    spec = get_product_spec(self.agency, self.product_type)\n\n    # Use product spec's path pattern\n    ftp_path = spec.ftp_path_pattern.format(\n        gps_week=gps_week,\n        file=f\"{clock_file}.gz\",\n    )\n\n    full_url = f\"{self.ftp_server}{ftp_path}\"\n    destination = self.local_dir / clock_file\n\n    # File info for NASA CDDIS URL construction\n    file_info = {\n        \"gps_week\": gps_week,\n        \"filename\": clock_file,\n        \"type\": \"clock\",\n        \"agency\": self.agency,\n        \"latency\": spec.latency_hours,\n    }\n\n    try:\n        self.download_file(full_url, destination, file_info)\n        print(f\"Downloaded clock file for {self.agency} on date {self.date}\")\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to download CLK file from all available servers: {str(e)}\"\n        )\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile.read_file","level":3,"title":"<code>read_file()</code>","text":"<p>Read and parse CLK file into xarray Dataset.</p> <p>Uses modular parser for data extraction and validator for quality checks. Applies unit conversion from microseconds to seconds.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ClkFile.read_file--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Clock offsets with dimensions (epoch, sv). Values are in seconds     (or dimensionless if specified).</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>def read_file(self) -&gt; xr.Dataset:\n    \"\"\"Read and parse CLK file into xarray Dataset.\n\n    Uses modular parser for data extraction and validator for quality checks.\n    Applies unit conversion from microseconds to seconds.\n\n    Returns\n    -------\n    xr.Dataset\n        Clock offsets with dimensions (epoch, sv). Values are in seconds\n        (or dimensionless if specified).\n    \"\"\"\n    # Parse file using modular parser\n    epochs, satellites, clock_offsets = parse_clk_file(self.fpath)\n\n    # Convert units (microseconds → seconds)\n    clock_offsets = (UREG.microsecond * clock_offsets).to(\"s\")\n\n    if self.dimensionless:\n        clock_offsets = clock_offsets.magnitude\n\n    # Create dataset\n    ds = xr.Dataset(\n        data_vars={\"clock_offset\": ((\"epoch\", \"sv\"), clock_offsets)},\n        coords={\n            \"epoch\": np.array(epochs, dtype=\"datetime64[ns]\"),\n            \"sv\": np.array(satellites),\n        },\n    )\n\n    # Validate and add metadata\n    validation = validate_clk_dataset(ds)\n    ds = self._prepare_dataset(ds, validation)\n\n    return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile","level":2,"title":"<code>AuxFile</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for GNSS auxiliary files (SP3, CLK, IONEX, etc.).</p> <p>This class provides two ways to create instances: 1. from_datetime_date(): Create from a datetime.date object and metadata 2. from_file(): Create directly from an existing file path</p> <p>The class handles both newly downloaded files and existing local files, maintaining consistent behavior regardless of how the instance is created.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile--ftp-server-configuration","level":4,"title":"FTP Server Configuration:","text":"<ul> <li>user_email: Optional email for NASA CDDIS authentication</li> <li>If None: Uses ESA FTP server exclusively (no authentication required)</li> <li>If provided: Enables NASA CDDIS as fallback server (requires registration)</li> <li>To enable CDDIS, set nasa_earthdata_acc_mail in config/processing.yaml</li> </ul>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic dataclass with <code>arbitrary_types_allowed=True</code>, and it uses <code>ABC</code> to define required subclass hooks.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/core/base.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass AuxFile(ABC):\n    \"\"\"Abstract base class for GNSS auxiliary files (SP3, CLK, IONEX, etc.).\n\n    This class provides two ways to create instances:\n    1. from_datetime_date(): Create from a datetime.date object and metadata\n    2. from_file(): Create directly from an existing file path\n\n    The class handles both newly downloaded files and existing local files,\n    maintaining consistent behavior regardless of how the instance is created.\n\n    FTP Server Configuration:\n    ------------------------\n    - user_email: Optional email for NASA CDDIS authentication\n      - If None: Uses ESA FTP server exclusively (no authentication required)\n      - If provided: Enables NASA CDDIS as fallback server (requires registration)\n      - To enable CDDIS, set nasa_earthdata_acc_mail in config/processing.yaml\n\n    Notes\n    -----\n    This is a Pydantic dataclass with `arbitrary_types_allowed=True`, and\n    it uses `ABC` to define required subclass hooks.\n    \"\"\"\n\n    date: str\n    agency: str\n    product_type: str\n    ftp_server: str\n    local_dir: Path\n    file_type: list[str] = Field(default_factory=list)\n    fpath: Path | None = None\n    user_email: str | None = None\n    downloader: FileDownloader | None = None\n    _data: xr.Dataset | None = Field(default=None, init=False)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize after dataclass creation.\n\n        Sets up paths, downloader, and verifies local file existence.\n        \"\"\"\n        if not self.file_type:\n            self.file_type = [\"unknown\"]\n        self.local_dir = Path(self.local_dir)\n        self.local_dir.mkdir(parents=True, exist_ok=True)\n\n        if self.downloader is None:\n            self.downloader = FtpDownloader(user_email=self.user_email)\n\n        self.fpath = self.check_file_exists()\n\n    @classmethod\n    def from_datetime_date(\n        cls,\n        date: datetime.date,\n        agency: str,\n        product_type: str,\n        ftp_server: str,\n        local_dir: Path,\n        **kwargs: Any,\n    ) -&gt; \"AuxFile\":\n        \"\"\"Create an AuxFile instance from a datetime.date.\n\n        Parameters\n        ----------\n        date : datetime.date\n            Date for the desired auxiliary file.\n        agency : str\n            Agency providing the data (e.g., \"COD\", \"IGS\").\n        product_type : str\n            Product type (\"final\", \"rapid\", \"ultrarapid\").\n        ftp_server : str\n            Base URL for file downloads.\n        local_dir : Path\n            Directory for storing files locally.\n        **kwargs : Any\n            Extra keyword arguments for subclass construction.\n\n        Returns\n        -------\n        AuxFile\n            A new instance of the AuxFile subclass.\n        \"\"\"\n        yyyydoy = YYYYDOY.from_date(date=date).to_str()\n        return cls(\n            date=yyyydoy,\n            agency=agency,\n            product_type=product_type,\n            ftp_server=ftp_server,\n            local_dir=local_dir,\n            **kwargs,\n        )\n\n    @classmethod\n    def from_file(cls, fpath: Path, **kwargs: Any) -&gt; \"AuxFile\":\n        \"\"\"Create an AuxFile instance from an existing file path.\n\n        Parameters\n        ----------\n        fpath : Path\n            Path to the existing GNSS file.\n        **kwargs : Any\n            Extra keyword arguments for subclass construction.\n\n        Returns\n        -------\n        AuxFile\n            A new instance of the AuxFile subclass.\n\n        Raises\n        ------\n        FileNotFoundError\n            If the specified file does not exist.\n        \"\"\"\n        if not fpath.exists():\n            raise FileNotFoundError(f\"File not found: {fpath}\")\n\n        fname = fpath.name\n        agency = fname[0:3]\n        yyyydoy = fname.split(\"_\")[1][0:7]\n\n        return cls(\n            date=yyyydoy,\n            agency=agency,\n            product_type=\"final\",\n            ftp_server=\"N/A\",\n            local_dir=fpath.parent,\n            fpath=fpath,\n            **kwargs,\n        )\n\n    def download_file(\n        self,\n        url: str,\n        destination: Path,\n        file_info: dict | None = None,\n    ) -&gt; Path:\n        \"\"\"Download a file using the configured downloader.\n\n        Parameters\n        ----------\n        url : str\n            Download URL.\n        destination : Path\n            Local file destination.\n        file_info : dict, optional\n            Extra info passed to the downloader.\n\n        Returns\n        -------\n        Path\n            Path to the downloaded file.\n        \"\"\"\n        if self.downloader is None:\n            raise RuntimeError(\"No downloader is configured\")\n        return self.downloader.download(url, destination, file_info)\n\n    @abstractmethod\n    def read_file(self) -&gt; xr.Dataset:\n        \"\"\"Read and parse the auxiliary file.\n\n        Returns\n        -------\n        xr.Dataset\n            Parsed dataset representation of the file.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_interpolation_strategy(self) -&gt; Interpolator:\n        \"\"\"Get the interpolation strategy for this file type.\"\"\"\n        pass\n\n    @property\n    def data(self) -&gt; xr.Dataset:\n        \"\"\"Access the file's data, loading it if necessary.\"\"\"\n        if self._data is None:\n            self._data = self.read_file()\n            strategy = self.get_interpolation_strategy()\n            self._data.attrs[\"interpolator_config\"] = strategy.to_attrs()\n        return self._data\n\n    def check_file_exists(self) -&gt; Path:\n        \"\"\"Verify file exists locally or download it if needed.\n\n        Returns\n        -------\n        Path\n            Local file path.\n        \"\"\"\n        filename = self.generate_filename_based_on_type()\n        file_path = self.local_dir / filename\n        if not file_path.exists():\n            print(f\"File {file_path} does not exist. Downloading...\")\n            self.download_aux_file()\n        else:\n            print(f\"File {file_path} exists.\")\n        return file_path\n\n    @abstractmethod\n    def generate_filename_based_on_type(self) -&gt; Path:\n        \"\"\"Generate the appropriate filename for this type of auxiliary file.\"\"\"\n        pass\n\n    @abstractmethod\n    def download_aux_file(self) -&gt; None:\n        \"\"\"Download the auxiliary file from the specified FTP server.\"\"\"\n        pass\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.data","level":3,"title":"<code>data</code>  <code>property</code>","text":"<p>Access the file's data, loading it if necessary.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.__post_init__","level":3,"title":"<code>__post_init__()</code>","text":"<p>Initialize after dataclass creation.</p> <p>Sets up paths, downloader, and verifies local file existence.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/core/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize after dataclass creation.\n\n    Sets up paths, downloader, and verifies local file existence.\n    \"\"\"\n    if not self.file_type:\n        self.file_type = [\"unknown\"]\n    self.local_dir = Path(self.local_dir)\n    self.local_dir.mkdir(parents=True, exist_ok=True)\n\n    if self.downloader is None:\n        self.downloader = FtpDownloader(user_email=self.user_email)\n\n    self.fpath = self.check_file_exists()\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.from_datetime_date","level":3,"title":"<code>from_datetime_date(date, agency, product_type, ftp_server, local_dir, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an AuxFile instance from a datetime.date.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.from_datetime_date--parameters","level":5,"title":"Parameters","text":"<p>date : datetime.date     Date for the desired auxiliary file. agency : str     Agency providing the data (e.g., \"COD\", \"IGS\"). product_type : str     Product type (\"final\", \"rapid\", \"ultrarapid\"). ftp_server : str     Base URL for file downloads. local_dir : Path     Directory for storing files locally. **kwargs : Any     Extra keyword arguments for subclass construction.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.from_datetime_date--returns","level":5,"title":"Returns","text":"<p>AuxFile     A new instance of the AuxFile subclass.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/core/base.py</code> <pre><code>@classmethod\ndef from_datetime_date(\n    cls,\n    date: datetime.date,\n    agency: str,\n    product_type: str,\n    ftp_server: str,\n    local_dir: Path,\n    **kwargs: Any,\n) -&gt; \"AuxFile\":\n    \"\"\"Create an AuxFile instance from a datetime.date.\n\n    Parameters\n    ----------\n    date : datetime.date\n        Date for the desired auxiliary file.\n    agency : str\n        Agency providing the data (e.g., \"COD\", \"IGS\").\n    product_type : str\n        Product type (\"final\", \"rapid\", \"ultrarapid\").\n    ftp_server : str\n        Base URL for file downloads.\n    local_dir : Path\n        Directory for storing files locally.\n    **kwargs : Any\n        Extra keyword arguments for subclass construction.\n\n    Returns\n    -------\n    AuxFile\n        A new instance of the AuxFile subclass.\n    \"\"\"\n    yyyydoy = YYYYDOY.from_date(date=date).to_str()\n    return cls(\n        date=yyyydoy,\n        agency=agency,\n        product_type=product_type,\n        ftp_server=ftp_server,\n        local_dir=local_dir,\n        **kwargs,\n    )\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.from_file","level":3,"title":"<code>from_file(fpath, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an AuxFile instance from an existing file path.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.from_file--parameters","level":5,"title":"Parameters","text":"<p>fpath : Path     Path to the existing GNSS file. **kwargs : Any     Extra keyword arguments for subclass construction.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.from_file--returns","level":5,"title":"Returns","text":"<p>AuxFile     A new instance of the AuxFile subclass.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.from_file--raises","level":5,"title":"Raises","text":"<p>FileNotFoundError     If the specified file does not exist.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/core/base.py</code> <pre><code>@classmethod\ndef from_file(cls, fpath: Path, **kwargs: Any) -&gt; \"AuxFile\":\n    \"\"\"Create an AuxFile instance from an existing file path.\n\n    Parameters\n    ----------\n    fpath : Path\n        Path to the existing GNSS file.\n    **kwargs : Any\n        Extra keyword arguments for subclass construction.\n\n    Returns\n    -------\n    AuxFile\n        A new instance of the AuxFile subclass.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the specified file does not exist.\n    \"\"\"\n    if not fpath.exists():\n        raise FileNotFoundError(f\"File not found: {fpath}\")\n\n    fname = fpath.name\n    agency = fname[0:3]\n    yyyydoy = fname.split(\"_\")[1][0:7]\n\n    return cls(\n        date=yyyydoy,\n        agency=agency,\n        product_type=\"final\",\n        ftp_server=\"N/A\",\n        local_dir=fpath.parent,\n        fpath=fpath,\n        **kwargs,\n    )\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.download_file","level":3,"title":"<code>download_file(url, destination, file_info=None)</code>","text":"<p>Download a file using the configured downloader.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.download_file--parameters","level":5,"title":"Parameters","text":"<p>url : str     Download URL. destination : Path     Local file destination. file_info : dict, optional     Extra info passed to the downloader.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.download_file--returns","level":5,"title":"Returns","text":"<p>Path     Path to the downloaded file.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/core/base.py</code> <pre><code>def download_file(\n    self,\n    url: str,\n    destination: Path,\n    file_info: dict | None = None,\n) -&gt; Path:\n    \"\"\"Download a file using the configured downloader.\n\n    Parameters\n    ----------\n    url : str\n        Download URL.\n    destination : Path\n        Local file destination.\n    file_info : dict, optional\n        Extra info passed to the downloader.\n\n    Returns\n    -------\n    Path\n        Path to the downloaded file.\n    \"\"\"\n    if self.downloader is None:\n        raise RuntimeError(\"No downloader is configured\")\n    return self.downloader.download(url, destination, file_info)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.read_file","level":3,"title":"<code>read_file()</code>  <code>abstractmethod</code>","text":"<p>Read and parse the auxiliary file.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.read_file--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Parsed dataset representation of the file.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/core/base.py</code> <pre><code>@abstractmethod\ndef read_file(self) -&gt; xr.Dataset:\n    \"\"\"Read and parse the auxiliary file.\n\n    Returns\n    -------\n    xr.Dataset\n        Parsed dataset representation of the file.\n    \"\"\"\n    pass\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.get_interpolation_strategy","level":3,"title":"<code>get_interpolation_strategy()</code>  <code>abstractmethod</code>","text":"<p>Get the interpolation strategy for this file type.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/core/base.py</code> <pre><code>@abstractmethod\ndef get_interpolation_strategy(self) -&gt; Interpolator:\n    \"\"\"Get the interpolation strategy for this file type.\"\"\"\n    pass\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.check_file_exists","level":3,"title":"<code>check_file_exists()</code>","text":"<p>Verify file exists locally or download it if needed.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.check_file_exists--returns","level":5,"title":"Returns","text":"<p>Path     Local file path.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/core/base.py</code> <pre><code>def check_file_exists(self) -&gt; Path:\n    \"\"\"Verify file exists locally or download it if needed.\n\n    Returns\n    -------\n    Path\n        Local file path.\n    \"\"\"\n    filename = self.generate_filename_based_on_type()\n    file_path = self.local_dir / filename\n    if not file_path.exists():\n        print(f\"File {file_path} does not exist. Downloading...\")\n        self.download_aux_file()\n    else:\n        print(f\"File {file_path} exists.\")\n    return file_path\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.generate_filename_based_on_type","level":3,"title":"<code>generate_filename_based_on_type()</code>  <code>abstractmethod</code>","text":"<p>Generate the appropriate filename for this type of auxiliary file.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/core/base.py</code> <pre><code>@abstractmethod\ndef generate_filename_based_on_type(self) -&gt; Path:\n    \"\"\"Generate the appropriate filename for this type of auxiliary file.\"\"\"\n    pass\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.AuxFile.download_aux_file","level":3,"title":"<code>download_aux_file()</code>  <code>abstractmethod</code>","text":"<p>Download the auxiliary file from the specified FTP server.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/core/base.py</code> <pre><code>@abstractmethod\ndef download_aux_file(self) -&gt; None:\n    \"\"\"Download the auxiliary file from the specified FTP server.\"\"\"\n    pass\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Interpolator","level":2,"title":"<code>Interpolator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for interpolation strategies.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Interpolator--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic dataclass with <code>arbitrary_types_allowed=True</code> and uses <code>ABC</code> to define required interpolation hooks.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass Interpolator(ABC):\n    \"\"\"Abstract base class for interpolation strategies.\n\n    Notes\n    -----\n    This is a Pydantic dataclass with `arbitrary_types_allowed=True` and\n    uses `ABC` to define required interpolation hooks.\n    \"\"\"\n\n    config: InterpolatorConfig\n\n    @abstractmethod\n    def interpolate(self, ds: xr.Dataset, target_epochs: np.ndarray) -&gt; xr.Dataset:\n        \"\"\"Interpolate dataset to match target epochs.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Source dataset with (epoch, sid) dimensions.\n        target_epochs : np.ndarray\n            Target epoch grid (datetime64).\n\n        Returns\n        -------\n        xr.Dataset\n            Interpolated dataset at target epochs.\n        \"\"\"\n        pass\n\n    def to_attrs(self) -&gt; dict[str, Any]:\n        \"\"\"Convert interpolator to attrs-compatible dictionary.\"\"\"\n        return {\n            \"interpolator_type\": self.__class__.__name__,\n            \"config\": self.config.to_dict(),\n        }\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Interpolator.interpolate","level":3,"title":"<code>interpolate(ds, target_epochs)</code>  <code>abstractmethod</code>","text":"<p>Interpolate dataset to match target epochs.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Interpolator.interpolate--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Source dataset with (epoch, sid) dimensions. target_epochs : np.ndarray     Target epoch grid (datetime64).</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Interpolator.interpolate--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Interpolated dataset at target epochs.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>@abstractmethod\ndef interpolate(self, ds: xr.Dataset, target_epochs: np.ndarray) -&gt; xr.Dataset:\n    \"\"\"Interpolate dataset to match target epochs.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Source dataset with (epoch, sid) dimensions.\n    target_epochs : np.ndarray\n        Target epoch grid (datetime64).\n\n    Returns\n    -------\n    xr.Dataset\n        Interpolated dataset at target epochs.\n    \"\"\"\n    pass\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.Interpolator.to_attrs","level":3,"title":"<code>to_attrs()</code>","text":"<p>Convert interpolator to attrs-compatible dictionary.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>def to_attrs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert interpolator to attrs-compatible dictionary.\"\"\"\n    return {\n        \"interpolator_type\": self.__class__.__name__,\n        \"config\": self.config.to_dict(),\n    }\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.InterpolatorConfig","level":2,"title":"<code>InterpolatorConfig</code>","text":"<p>Base class for interpolator configuration.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>class InterpolatorConfig:\n    \"\"\"Base class for interpolator configuration.\"\"\"\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert config to dictionary for attrs storage.\"\"\"\n        return asdict(self)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.InterpolatorConfig.to_dict","level":3,"title":"<code>to_dict()</code>","text":"<p>Convert config to dictionary for attrs storage.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert config to dictionary for attrs storage.\"\"\"\n    return asdict(self)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.DatasetMatcher","level":2,"title":"<code>DatasetMatcher</code>","text":"<p>Match auxiliary datasets to a reference RINEX dataset temporally.</p> <p>Handles temporal alignment of datasets with different sampling rates using appropriate interpolation strategies. The reference dataset (typically RINEX observations) remains unchanged while auxiliary datasets are interpolated to match its epochs.</p> <p>The matcher: 1. Validates all datasets have required dimensions (epoch, sid) 2. Determines relative temporal resolutions 3. Applies appropriate interpolation:    - Higher resolution aux → nearest neighbor    - Lower resolution aux → specialized interpolator from metadata</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.DatasetMatcher--examples","level":4,"title":"Examples","text":"<p>from canvod.auxiliary.matching import DatasetMatcher</p> <p>matcher = DatasetMatcher() matched = matcher.match_datasets( ...     rinex_ds, ...     ephemerides=sp3_data, ...     clock=clk_data ... )</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.DatasetMatcher--auxiliary-datasets-now-aligned-to-rinex-epochs","level":3,"title":"Auxiliary datasets now aligned to RINEX epochs","text":"<p>len(matched['ephemerides'].epoch) == len(rinex_ds.epoch) True len(matched['clock'].epoch) == len(rinex_ds.epoch) True</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.DatasetMatcher--notes","level":4,"title":"Notes","text":"<ul> <li>Reference dataset should be the RINEX observations</li> <li>Auxiliary datasets should have 'interpolator_config' in attrs</li> <li>If no interpolator config, falls back to nearest neighbor</li> <li>Temporal distance is tracked for quality assessment</li> </ul> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/matching/dataset_matcher.py</code> <pre><code>class DatasetMatcher:\n    \"\"\"Match auxiliary datasets to a reference RINEX dataset temporally.\n\n    Handles temporal alignment of datasets with different sampling rates\n    using appropriate interpolation strategies. The reference dataset\n    (typically RINEX observations) remains unchanged while auxiliary\n    datasets are interpolated to match its epochs.\n\n    The matcher:\n    1. Validates all datasets have required dimensions (epoch, sid)\n    2. Determines relative temporal resolutions\n    3. Applies appropriate interpolation:\n       - Higher resolution aux → nearest neighbor\n       - Lower resolution aux → specialized interpolator from metadata\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.auxiliary.matching import DatasetMatcher\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; matcher = DatasetMatcher()\n    &gt;&gt;&gt; matched = matcher.match_datasets(\n    ...     rinex_ds,\n    ...     ephemerides=sp3_data,\n    ...     clock=clk_data\n    ... )\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Auxiliary datasets now aligned to RINEX epochs\n    &gt;&gt;&gt; len(matched['ephemerides'].epoch) == len(rinex_ds.epoch)\n    True\n    &gt;&gt;&gt; len(matched['clock'].epoch) == len(rinex_ds.epoch)\n    True\n\n    Notes\n    -----\n    - Reference dataset should be the RINEX observations\n    - Auxiliary datasets should have 'interpolator_config' in attrs\n    - If no interpolator config, falls back to nearest neighbor\n    - Temporal distance is tracked for quality assessment\n    \"\"\"\n\n    def match_datasets(\n        self, reference_ds: xr.Dataset, **aux_datasets: xr.Dataset\n    ) -&gt; dict[str, xr.Dataset]:\n        \"\"\"Match auxiliary datasets to reference dataset epochs.\n\n        Parameters\n        ----------\n        reference_ds : xr.Dataset\n            Primary dataset (usually RINEX observations) that defines\n            the target epoch timeline. This dataset remains unchanged.\n        **aux_datasets : dict[str, xr.Dataset]\n            Named auxiliary datasets to align to reference epochs.\n            Keys become the names in the returned dict.\n            Example: ephemerides=sp3_data, clock=clk_data\n\n        Returns\n        -------\n        dict[str, xr.Dataset]\n            Dictionary of matched auxiliary datasets, all aligned to\n            reference_ds.epoch. Keys match the input **aux_datasets keys.\n\n        Raises\n        ------\n        ValueError\n            - If no auxiliary datasets provided\n            - If datasets missing required dimensions\n            - If interpolation config missing (warns, doesn't raise)\n\n        Examples\n        --------\n        &gt;&gt;&gt; matcher = DatasetMatcher()\n        &gt;&gt;&gt; matched = matcher.match_datasets(\n        ...     rinex_ds,\n        ...     ephemerides=sp3_data,\n        ...     clock=clk_data\n        ... )\n        &gt;&gt;&gt; matched.keys()\n        dict_keys(['ephemerides', 'clock'])\n        \"\"\"\n        self._validate_inputs(reference_ds, aux_datasets)\n\n        ref_interval = self._get_temporal_interval(reference_ds)\n        matched = self._match_temporal_resolution(\n            reference_ds, ref_interval, aux_datasets\n        )\n\n        return matched\n\n    def _validate_inputs(\n        self,\n        reference_ds: xr.Dataset,\n        aux_datasets: dict[str, xr.Dataset],\n    ) -&gt; None:\n        \"\"\"Validate input datasets.\n\n        Checks:\n        1. At least one auxiliary dataset provided\n        2. All datasets have required dimensions (epoch, sid)\n        3. Interpolation configuration available (warns if missing)\n\n        Parameters\n        ----------\n        reference_ds : xr.Dataset\n            Reference dataset.\n        aux_datasets : dict[str, xr.Dataset]\n            Auxiliary datasets.\n\n        Raises\n        ------\n        ValueError\n            If validation fails.\n        \"\"\"\n        if not aux_datasets:\n            raise ValueError(\"At least one auxiliary dataset required\")\n\n        self._validate_dimensions(reference_ds, \"Reference dataset\")\n\n        for name, ds in aux_datasets.items():\n            self._validate_dimensions(ds, f\"Dataset '{name}'\")\n            self._validate_interpolation_config(ds, name)\n\n    def _validate_dimensions(self, ds: xr.Dataset, name: str) -&gt; None:\n        \"\"\"Check dataset has required dimensions.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset to validate.\n        name : str\n            Dataset name for error messages.\n\n        Raises\n        ------\n        ValueError\n            If missing 'epoch' or 'sid' dimension.\n        \"\"\"\n        if \"epoch\" not in ds.dims or \"sid\" not in ds.dims:\n            raise ValueError(f\"{name} missing required dimension 'epoch' or 'sid'\")\n\n    def _validate_interpolation_config(\n        self,\n        ds: xr.Dataset,\n        name: str,\n    ) -&gt; None:\n        \"\"\"Verify dataset has interpolation configuration.\n\n        Issues warning if missing - interpolation will still work\n        via fallback to nearest neighbor.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset to check.\n        name : str\n            Dataset name for warning message.\n        \"\"\"\n        if \"interpolator_config\" not in ds.attrs:\n            warnings.warn(\n                f\"Dataset '{name}' missing interpolation configuration. \"\n                \"Will use nearest-neighbor interpolation.\",\n                UserWarning,\n            )\n\n    def _get_temporal_interval(self, ds: xr.Dataset) -&gt; float:\n        \"\"\"Calculate temporal interval of dataset in seconds.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset with epoch dimension.\n\n        Returns\n        -------\n        float\n            Interval between first two epochs in seconds.\n        \"\"\"\n        time_diff = ds[\"epoch\"][1] - ds[\"epoch\"][0]\n        return float(time_diff.values)\n\n    def _match_temporal_resolution(\n        self,\n        reference_ds: xr.Dataset,\n        ref_interval: float,\n        aux_datasets: dict[str, xr.Dataset],\n    ) -&gt; dict[str, xr.Dataset]:\n        \"\"\"Match temporal resolution of auxiliary datasets to reference.\n\n        Strategy depends on relative sampling rates:\n        - Higher resolution aux (finer sampling) → nearest neighbor\n        - Lower resolution aux (coarser sampling) → specialized interpolator\n\n        Parameters\n        ----------\n        reference_ds : xr.Dataset\n            Reference dataset defining target epochs.\n        ref_interval : float\n            Reference dataset temporal interval in seconds.\n        aux_datasets : dict[str, xr.Dataset]\n            Auxiliary datasets to interpolate.\n\n        Returns\n        -------\n        dict[str, xr.Dataset]\n            Matched datasets aligned to reference epochs.\n        \"\"\"\n        matched_datasets = {}\n\n        for name, ds in aux_datasets.items():\n            ds_interval = self._get_temporal_interval(ds)\n\n            if ds_interval &lt; ref_interval:\n                # Higher resolution → simple nearest neighbor\n                matched_datasets[name] = ds.interp(\n                    epoch=reference_ds.epoch, method=\"nearest\"\n                )\n            else:\n                # Lower resolution → use specialized interpolator\n                if \"interpolator_config\" in ds.attrs:\n                    interpolator = create_interpolator_from_attrs(ds.attrs)\n                    matched_datasets[name] = interpolator.interpolate(\n                        ds, reference_ds.epoch\n                    )\n                else:\n                    # Fallback to nearest neighbor\n                    matched_datasets[name] = ds.interp(\n                        epoch=reference_ds.epoch, method=\"nearest\"\n                    )\n\n                # Track temporal distance for quality assessment\n                temporal_distance = abs(matched_datasets[name][\"epoch\"] - ds[\"epoch\"])\n                matched_datasets[name][f\"{name.lower()}_temporal_distance\"] = (\n                    temporal_distance\n                )\n\n        return matched_datasets\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.DatasetMatcher.match_datasets","level":3,"title":"<code>match_datasets(reference_ds, **aux_datasets)</code>","text":"<p>Match auxiliary datasets to reference dataset epochs.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.DatasetMatcher.match_datasets--parameters","level":5,"title":"Parameters","text":"<p>reference_ds : xr.Dataset     Primary dataset (usually RINEX observations) that defines     the target epoch timeline. This dataset remains unchanged. **aux_datasets : dict[str, xr.Dataset]     Named auxiliary datasets to align to reference epochs.     Keys become the names in the returned dict.     Example: ephemerides=sp3_data, clock=clk_data</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.DatasetMatcher.match_datasets--returns","level":5,"title":"Returns","text":"<p>dict[str, xr.Dataset]     Dictionary of matched auxiliary datasets, all aligned to     reference_ds.epoch. Keys match the input **aux_datasets keys.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.DatasetMatcher.match_datasets--raises","level":5,"title":"Raises","text":"<p>ValueError     - If no auxiliary datasets provided     - If datasets missing required dimensions     - If interpolation config missing (warns, doesn't raise)</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.DatasetMatcher.match_datasets--examples","level":5,"title":"Examples","text":"<p>matcher = DatasetMatcher() matched = matcher.match_datasets( ...     rinex_ds, ...     ephemerides=sp3_data, ...     clock=clk_data ... ) matched.keys() dict_keys(['ephemerides', 'clock'])</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/matching/dataset_matcher.py</code> <pre><code>def match_datasets(\n    self, reference_ds: xr.Dataset, **aux_datasets: xr.Dataset\n) -&gt; dict[str, xr.Dataset]:\n    \"\"\"Match auxiliary datasets to reference dataset epochs.\n\n    Parameters\n    ----------\n    reference_ds : xr.Dataset\n        Primary dataset (usually RINEX observations) that defines\n        the target epoch timeline. This dataset remains unchanged.\n    **aux_datasets : dict[str, xr.Dataset]\n        Named auxiliary datasets to align to reference epochs.\n        Keys become the names in the returned dict.\n        Example: ephemerides=sp3_data, clock=clk_data\n\n    Returns\n    -------\n    dict[str, xr.Dataset]\n        Dictionary of matched auxiliary datasets, all aligned to\n        reference_ds.epoch. Keys match the input **aux_datasets keys.\n\n    Raises\n    ------\n    ValueError\n        - If no auxiliary datasets provided\n        - If datasets missing required dimensions\n        - If interpolation config missing (warns, doesn't raise)\n\n    Examples\n    --------\n    &gt;&gt;&gt; matcher = DatasetMatcher()\n    &gt;&gt;&gt; matched = matcher.match_datasets(\n    ...     rinex_ds,\n    ...     ephemerides=sp3_data,\n    ...     clock=clk_data\n    ... )\n    &gt;&gt;&gt; matched.keys()\n    dict_keys(['ephemerides', 'clock'])\n    \"\"\"\n    self._validate_inputs(reference_ds, aux_datasets)\n\n    ref_interval = self._get_temporal_interval(reference_ds)\n    matched = self._match_temporal_resolution(\n        reference_ds, ref_interval, aux_datasets\n    )\n\n    return matched\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition","level":2,"title":"<code>ECEFPosition</code>  <code>dataclass</code>","text":"<p>Earth-Centered, Earth-Fixed (ECEF) position in meters.</p> <p>ECEF is a Cartesian coordinate system with: - Origin at Earth's center of mass - X-axis pointing to 0° latitude, 0° longitude (Prime Meridian at Equator) - Y-axis pointing to 0° latitude, 90° East longitude - Z-axis pointing to North Pole</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition--parameters","level":4,"title":"Parameters","text":"<p>x : float     X coordinate in meters. y : float     Y coordinate in meters. z : float     Z coordinate in meters.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition--examples","level":4,"title":"Examples","text":"Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/position.py</code> <pre><code>@dataclass(frozen=True)\nclass ECEFPosition:\n    \"\"\"Earth-Centered, Earth-Fixed (ECEF) position in meters.\n\n    ECEF is a Cartesian coordinate system with:\n    - Origin at Earth's center of mass\n    - X-axis pointing to 0° latitude, 0° longitude (Prime Meridian at Equator)\n    - Y-axis pointing to 0° latitude, 90° East longitude\n    - Z-axis pointing to North Pole\n\n    Parameters\n    ----------\n    x : float\n        X coordinate in meters.\n    y : float\n        Y coordinate in meters.\n    z : float\n        Z coordinate in meters.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # From RINEX dataset metadata\n    &gt;&gt;&gt; pos = ECEFPosition.from_ds_metadata(rinex_ds)\n    &gt;&gt;&gt; print(f\"X: {pos.x:.3f} m\")\n\n    &gt;&gt;&gt; # Manual creation\n    &gt;&gt;&gt; pos = ECEFPosition(x=4194304.123, y=176481.234, z=4780013.456)\n    &gt;&gt;&gt; lat, lon, alt = pos.to_geodetic()\n    \"\"\"\n\n    x: float  # meters\n    y: float  # meters\n    z: float  # meters\n\n    def to_geodetic(self) -&gt; tuple[float, float, float]:\n        \"\"\"Convert ECEF to geodetic coordinates.\n\n        Returns\n        -------\n        tuple[float, float, float]\n            (latitude, longitude, altitude) where:\n            - latitude: degrees [-90, 90]\n            - longitude: degrees [-180, 180]\n            - altitude: meters above WGS84 ellipsoid\n        \"\"\"\n        lat, lon, alt = pm.ecef2geodetic(self.x, self.y, self.z)\n        return lat, lon, alt\n\n    @classmethod\n    def from_ds_metadata(cls, ds: xr.Dataset) -&gt; Self:\n        \"\"\"Extract ECEF position from RINEX dataset metadata.\n\n        Reads from standard RINEX header attributes.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            RINEX dataset with position in attributes.\n\n        Returns\n        -------\n        ECEFPosition\n            Receiver position in ECEF.\n\n        Raises\n        ------\n        KeyError\n            If position attributes not found in dataset.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from canvod.readers import Rnxv3Obs\n        &gt;&gt;&gt; rnx = Rnxv3Obs(fpath=\"station.24o\")\n        &gt;&gt;&gt; ds = rnx.to_ds()\n        &gt;&gt;&gt; pos = ECEFPosition.from_ds_metadata(ds)\n        \"\"\"\n        # Try different attribute names\n        if \"APPROX POSITION X\" in ds.attrs:\n            # Standard RINEX v3 format\n            x = ds.attrs[\"APPROX POSITION X\"]\n            y = ds.attrs[\"APPROX POSITION Y\"]\n            z = ds.attrs[\"APPROX POSITION Z\"]\n        elif \"Approximate Position\" in ds.attrs:\n            # Alternative format: \"X=..., Y=..., Z=...\"\n            pos = ds.attrs[\"Approximate Position\"]\n            pos_parts = pos.split(\",\")\n\n            def sanitize(s: str) -&gt; float:\n                return float(s.split(\"=\")[1].strip().split()[0])\n\n            x = sanitize(pos_parts[0])\n            y = sanitize(pos_parts[1])\n            z = sanitize(pos_parts[2])\n        else:\n            raise KeyError(\n                \"Position not found in dataset attributes. \"\n                \"Expected 'APPROX POSITION X/Y/Z' or 'Approximate Position'\"\n            )\n\n        return cls(x=x, y=y, z=z)\n\n    def __repr__(self) -&gt; str:\n        return f\"ECEFPosition(x={self.x:.3f}, y={self.y:.3f}, z={self.z:.3f})\"\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition--from-rinex-dataset-metadata","level":3,"title":"From RINEX dataset metadata","text":"<p>pos = ECEFPosition.from_ds_metadata(rinex_ds) print(f\"X: {pos.x:.3f} m\")</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition--manual-creation","level":3,"title":"Manual creation","text":"<p>pos = ECEFPosition(x=4194304.123, y=176481.234, z=4780013.456) lat, lon, alt = pos.to_geodetic()</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition.to_geodetic","level":3,"title":"<code>to_geodetic()</code>","text":"<p>Convert ECEF to geodetic coordinates.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition.to_geodetic--returns","level":5,"title":"Returns","text":"<p>tuple[float, float, float]     (latitude, longitude, altitude) where:     - latitude: degrees [-90, 90]     - longitude: degrees [-180, 180]     - altitude: meters above WGS84 ellipsoid</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/position.py</code> <pre><code>def to_geodetic(self) -&gt; tuple[float, float, float]:\n    \"\"\"Convert ECEF to geodetic coordinates.\n\n    Returns\n    -------\n    tuple[float, float, float]\n        (latitude, longitude, altitude) where:\n        - latitude: degrees [-90, 90]\n        - longitude: degrees [-180, 180]\n        - altitude: meters above WGS84 ellipsoid\n    \"\"\"\n    lat, lon, alt = pm.ecef2geodetic(self.x, self.y, self.z)\n    return lat, lon, alt\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition.from_ds_metadata","level":3,"title":"<code>from_ds_metadata(ds)</code>  <code>classmethod</code>","text":"<p>Extract ECEF position from RINEX dataset metadata.</p> <p>Reads from standard RINEX header attributes.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition.from_ds_metadata--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     RINEX dataset with position in attributes.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition.from_ds_metadata--returns","level":5,"title":"Returns","text":"<p>ECEFPosition     Receiver position in ECEF.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition.from_ds_metadata--raises","level":5,"title":"Raises","text":"<p>KeyError     If position attributes not found in dataset.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ECEFPosition.from_ds_metadata--examples","level":5,"title":"Examples","text":"<p>from canvod.readers import Rnxv3Obs rnx = Rnxv3Obs(fpath=\"station.24o\") ds = rnx.to_ds() pos = ECEFPosition.from_ds_metadata(ds)</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/position.py</code> <pre><code>@classmethod\ndef from_ds_metadata(cls, ds: xr.Dataset) -&gt; Self:\n    \"\"\"Extract ECEF position from RINEX dataset metadata.\n\n    Reads from standard RINEX header attributes.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        RINEX dataset with position in attributes.\n\n    Returns\n    -------\n    ECEFPosition\n        Receiver position in ECEF.\n\n    Raises\n    ------\n    KeyError\n        If position attributes not found in dataset.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.readers import Rnxv3Obs\n    &gt;&gt;&gt; rnx = Rnxv3Obs(fpath=\"station.24o\")\n    &gt;&gt;&gt; ds = rnx.to_ds()\n    &gt;&gt;&gt; pos = ECEFPosition.from_ds_metadata(ds)\n    \"\"\"\n    # Try different attribute names\n    if \"APPROX POSITION X\" in ds.attrs:\n        # Standard RINEX v3 format\n        x = ds.attrs[\"APPROX POSITION X\"]\n        y = ds.attrs[\"APPROX POSITION Y\"]\n        z = ds.attrs[\"APPROX POSITION Z\"]\n    elif \"Approximate Position\" in ds.attrs:\n        # Alternative format: \"X=..., Y=..., Z=...\"\n        pos = ds.attrs[\"Approximate Position\"]\n        pos_parts = pos.split(\",\")\n\n        def sanitize(s: str) -&gt; float:\n            return float(s.split(\"=\")[1].strip().split()[0])\n\n        x = sanitize(pos_parts[0])\n        y = sanitize(pos_parts[1])\n        z = sanitize(pos_parts[2])\n    else:\n        raise KeyError(\n            \"Position not found in dataset attributes. \"\n            \"Expected 'APPROX POSITION X/Y/Z' or 'Approximate Position'\"\n        )\n\n    return cls(x=x, y=y, z=z)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.GeodeticPosition","level":2,"title":"<code>GeodeticPosition</code>  <code>dataclass</code>","text":"<p>Geodetic (WGS84) position.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.GeodeticPosition--parameters","level":4,"title":"Parameters","text":"<p>lat : float     Latitude in degrees [-90, 90]. lon : float     Longitude in degrees [-180, 180]. alt : float     Altitude in meters above WGS84 ellipsoid.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.GeodeticPosition--examples","level":4,"title":"Examples","text":"<p>pos = GeodeticPosition(lat=48.208, lon=16.373, alt=200.0) print(f\"Vienna: {pos.lat}°N, {pos.lon}°E, {pos.alt}m\")</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/position.py</code> <pre><code>@dataclass(frozen=True)\nclass GeodeticPosition:\n    \"\"\"Geodetic (WGS84) position.\n\n    Parameters\n    ----------\n    lat : float\n        Latitude in degrees [-90, 90].\n    lon : float\n        Longitude in degrees [-180, 180].\n    alt : float\n        Altitude in meters above WGS84 ellipsoid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pos = GeodeticPosition(lat=48.208, lon=16.373, alt=200.0)\n    &gt;&gt;&gt; print(f\"Vienna: {pos.lat}°N, {pos.lon}°E, {pos.alt}m\")\n    \"\"\"\n\n    lat: float  # degrees\n    lon: float  # degrees\n    alt: float  # meters\n\n    def to_ecef(self) -&gt; ECEFPosition:\n        \"\"\"Convert geodetic to ECEF coordinates.\n\n        Returns\n        -------\n        ECEFPosition\n            Position in ECEF frame.\n        \"\"\"\n        x, y, z = pm.geodetic2ecef(self.lat, self.lon, self.alt)\n        return ECEFPosition(x=x, y=y, z=z)\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"GeodeticPosition(lat={self.lat:.6f}°, \"\n            f\"lon={self.lon:.6f}°, alt={self.alt:.1f}m)\"\n        )\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.GeodeticPosition.to_ecef","level":3,"title":"<code>to_ecef()</code>","text":"<p>Convert geodetic to ECEF coordinates.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.GeodeticPosition.to_ecef--returns","level":5,"title":"Returns","text":"<p>ECEFPosition     Position in ECEF frame.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/position.py</code> <pre><code>def to_ecef(self) -&gt; ECEFPosition:\n    \"\"\"Convert geodetic to ECEF coordinates.\n\n    Returns\n    -------\n    ECEFPosition\n        Position in ECEF frame.\n    \"\"\"\n    x, y, z = pm.geodetic2ecef(self.lat, self.lon, self.alt)\n    return ECEFPosition(x=x, y=y, z=z)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#preprocessing","level":2,"title":"Preprocessing","text":"<p>Preprocessing utilities for auxiliary GNSS data.</p> <p>Handles conversion of raw auxiliary data (SP3, CLK) from satellite vehicle (sv) dimension to signal ID (sid) dimension required for matching with RINEX data.</p> <p>Matches gnssvodpy.icechunk_manager.preprocessing.IcechunkPreprocessor exactly.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.create_sv_to_sid_mapping","level":2,"title":"<code>create_sv_to_sid_mapping(svs, aggregate_glonass_fdma=True)</code>","text":"<p>Build mapping from each SV to its possible SIDs.</p> <p>Adds <code>X1|X</code> placeholder SIDs as well.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.create_sv_to_sid_mapping--parameters","level":4,"title":"Parameters","text":"<p>svs : list[str]     List of space vehicles (e.g., [\"G01\", \"E02\"]). aggregate_glonass_fdma : bool, default True     Whether to aggregate GLONASS FDMA bands.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.create_sv_to_sid_mapping--returns","level":4,"title":"Returns","text":"<p>dict[str, list[str]]     Mapping from sv → list of SIDs.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/preprocessing.py</code> <pre><code>def create_sv_to_sid_mapping(\n    svs: list[str], aggregate_glonass_fdma: bool = True\n) -&gt; dict[str, list[str]]:\n    \"\"\"Build mapping from each SV to its possible SIDs.\n\n    Adds ``X1|X`` placeholder SIDs as well.\n\n    Parameters\n    ----------\n    svs : list[str]\n        List of space vehicles (e.g., [\"G01\", \"E02\"]).\n    aggregate_glonass_fdma : bool, default True\n        Whether to aggregate GLONASS FDMA bands.\n\n    Returns\n    -------\n    dict[str, list[str]]\n        Mapping from sv → list of SIDs.\n    \"\"\"\n    mapper = SignalIDMapper(aggregate_glonass_fdma=aggregate_glonass_fdma)\n    systems = {\n        \"G\": GPS(),\n        \"E\": GALILEO(),\n        \"R\": GLONASS(aggregate_fdma=aggregate_glonass_fdma),\n        \"C\": BEIDOU(),\n        \"I\": IRNSS(),\n        \"S\": SBAS(),\n        \"J\": QZSS(),\n    }\n\n    sv_to_sids: dict[str, list[str]] = {}\n    for sv in svs:\n        sys_letter = sv[0]\n        if sys_letter not in systems:\n            continue\n\n        system = systems[sys_letter]\n        sids = []\n        if sys_letter in mapper.SYSTEM_BANDS:\n            for _, band in mapper.SYSTEM_BANDS[sys_letter].items():\n                codes = system.BAND_CODES.get(band, [\"X\"])\n                sids.extend(f\"{sv}|{band}|{code}\" for code in codes)\n        sids.append(f\"{sv}|X1|X\")  # aux observation\n\n        sv_to_sids[sv] = sorted(sids)\n\n    return sv_to_sids\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.map_aux_sv_to_sid","level":2,"title":"<code>map_aux_sv_to_sid(aux_ds, fill_value=np.nan, aggregate_glonass_fdma=True)</code>","text":"<p>Transform auxiliary dataset from sv → sid dimension.</p> <p>Each sv in the dataset is expanded to all its possible SIDs. Values are replicated across SIDs for the same satellite.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.map_aux_sv_to_sid--parameters","level":4,"title":"Parameters","text":"<p>aux_ds : xr.Dataset     Dataset with 'sv' dimension. fill_value : float, default np.nan     Fill value for missing entries. aggregate_glonass_fdma : bool, default True     Whether to aggregate GLONASS FDMA bands.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.map_aux_sv_to_sid--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Dataset with 'sid' dimension replacing 'sv'.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/preprocessing.py</code> <pre><code>def map_aux_sv_to_sid(\n    aux_ds: xr.Dataset,\n    fill_value: float = np.nan,\n    aggregate_glonass_fdma: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Transform auxiliary dataset from sv → sid dimension.\n\n    Each sv in the dataset is expanded to all its possible SIDs.\n    Values are replicated across SIDs for the same satellite.\n\n    Parameters\n    ----------\n    aux_ds : xr.Dataset\n        Dataset with 'sv' dimension.\n    fill_value : float, default np.nan\n        Fill value for missing entries.\n    aggregate_glonass_fdma : bool, default True\n        Whether to aggregate GLONASS FDMA bands.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with 'sid' dimension replacing 'sv'.\n    \"\"\"\n    svs = aux_ds[\"sv\"].values.tolist()\n    sv_to_sids = create_sv_to_sid_mapping(svs, aggregate_glonass_fdma)\n    all_sids = sorted({sid for sv in svs for sid in sv_to_sids.get(sv, [])})\n\n    new_data_vars = {}\n    for name, arr in aux_ds.data_vars.items():\n        if \"sv\" in arr.dims:\n            sv_dim = arr.dims.index(\"sv\")\n            new_shape = list(arr.shape)\n            new_shape[sv_dim] = len(all_sids)\n            expanded = np.full(new_shape, fill_value, dtype=arr.dtype)\n\n            for sv_idx, sv in enumerate(svs):\n                for sid in sv_to_sids.get(sv, []):\n                    sid_idx = all_sids.index(sid)\n                    if sv_dim == 0:\n                        expanded[sid_idx, ...] = arr.values[sv_idx, ...]\n                    elif sv_dim == 1:\n                        expanded[..., sid_idx] = arr.values[..., sv_idx]\n                    else:\n                        slices_new = [slice(None)] * len(new_shape)\n                        slices_old = [slice(None)] * len(arr.shape)\n                        slices_new[sv_dim] = sid_idx\n                        slices_old[sv_dim] = sv_idx\n                        expanded[tuple(slices_new)] = arr.values[tuple(slices_old)]\n\n            new_dims = list(arr.dims)\n            new_dims[sv_dim] = \"sid\"\n            new_data_vars[name] = (tuple(new_dims), expanded, arr.attrs)\n        else:\n            new_data_vars[name] = arr\n\n    # Coordinates\n    new_coords = {\n        **{k: v for k, v in aux_ds.coords.items() if k != \"sv\"},\n        \"sid\": (\"sid\", all_sids),\n    }\n\n    return xr.Dataset(new_data_vars, coords=new_coords, attrs=aux_ds.attrs.copy())\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.pad_to_global_sid","level":2,"title":"<code>pad_to_global_sid(ds, keep_sids=None, aggregate_glonass_fdma=True)</code>","text":"<p>Pad dataset so it has all possible SIDs across all constellations. Ensures consistent sid dimension for appending to Icechunk.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.pad_to_global_sid--parameters","level":4,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with 'sid' dimension. keep_sids : list[str] | None     Optional list of specific SIDs to keep. If None, keeps all. aggregate_glonass_fdma : bool, default True     Whether to aggregate GLONASS FDMA bands.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.pad_to_global_sid--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Dataset padded with NaN for missing SIDs.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/preprocessing.py</code> <pre><code>def pad_to_global_sid(\n    ds: xr.Dataset,\n    keep_sids: list[str] | None = None,\n    aggregate_glonass_fdma: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Pad dataset so it has all possible SIDs across all constellations.\n    Ensures consistent sid dimension for appending to Icechunk.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with 'sid' dimension.\n    keep_sids : list[str] | None\n        Optional list of specific SIDs to keep. If None, keeps all.\n    aggregate_glonass_fdma : bool, default True\n        Whether to aggregate GLONASS FDMA bands.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset padded with NaN for missing SIDs.\n    \"\"\"\n    mapper = SignalIDMapper(aggregate_glonass_fdma=aggregate_glonass_fdma)\n    systems = {\n        \"G\": GPS(),\n        \"E\": GALILEO(),\n        \"R\": GLONASS(aggregate_fdma=aggregate_glonass_fdma),\n        \"C\": BEIDOU(),\n        \"I\": IRNSS(),\n        \"S\": SBAS(),\n        \"J\": QZSS(),\n    }\n\n    # Generate all possible SIDs\n    sids = [\n        f\"{sv}|{band}|{code}\"\n        for sys_letter, bands in mapper.SYSTEM_BANDS.items()\n        for _, band in bands.items()\n        for sv in systems[sys_letter].svs\n        for code in systems[sys_letter].BAND_CODES.get(band, [\"X\"])\n    ]\n    sids = sorted(sids)\n\n    # Filter to keep_sids if provided\n    if keep_sids is not None and len(keep_sids) &gt; 0:\n        sids = sorted(set(sids).intersection(set(keep_sids)))\n\n    return ds.reindex({\"sid\": sids}, fill_value=np.nan)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.normalize_sid_dtype","level":2,"title":"<code>normalize_sid_dtype(ds)</code>","text":"<p>Ensure sid coordinate uses object dtype.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.normalize_sid_dtype--parameters","level":4,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with 'sid' coordinate.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.normalize_sid_dtype--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Dataset with sid as object dtype.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/preprocessing.py</code> <pre><code>def normalize_sid_dtype(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Ensure sid coordinate uses object dtype.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with 'sid' coordinate.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with sid as object dtype.\n    \"\"\"\n    if ds is None:\n        return ds\n    if \"sid\" in ds.coords and ds.sid.dtype.kind == \"U\":\n        ds = ds.assign_coords(\n            sid=xr.Variable(\"sid\", ds.sid.values.astype(object), ds.sid.attrs)\n        )\n    return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.strip_fillvalue","level":2,"title":"<code>strip_fillvalue(ds)</code>","text":"<p>Remove _FillValue attrs/encodings.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.strip_fillvalue--parameters","level":4,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset to clean.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.strip_fillvalue--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Dataset with _FillValue attributes removed.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/preprocessing.py</code> <pre><code>def strip_fillvalue(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Remove _FillValue attrs/encodings.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset to clean.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with _FillValue attributes removed.\n    \"\"\"\n    if ds is None:\n        return ds\n    for v in ds.data_vars:\n        ds[v].attrs.pop(\"_FillValue\", None)\n        ds[v].encoding.pop(\"_FillValue\", None)\n    return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.add_future_datavars","level":2,"title":"<code>add_future_datavars(ds, var_config)</code>","text":"<p>Add placeholder data variables from a configuration dictionary.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.add_future_datavars--parameters","level":4,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset to add variables to. var_config : dict[str, dict[str, Any]]     Configuration dict with structure:     {         \"var_name\": {             \"fill_value\": value,             \"dtype\": numpy dtype,             \"attrs\": {attribute dict}         }     }</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.add_future_datavars--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Dataset with new variables added.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/preprocessing.py</code> <pre><code>def add_future_datavars(\n    ds: xr.Dataset, var_config: dict[str, dict[str, Any]]\n) -&gt; xr.Dataset:\n    \"\"\"Add placeholder data variables from a configuration dictionary.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset to add variables to.\n    var_config : dict[str, dict[str, Any]]\n        Configuration dict with structure:\n        {\n            \"var_name\": {\n                \"fill_value\": value,\n                \"dtype\": numpy dtype,\n                \"attrs\": {attribute dict}\n            }\n        }\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with new variables added.\n    \"\"\"\n    n_epochs, n_sids = ds.sizes[\"epoch\"], ds.sizes[\"sid\"]\n    for name, cfg in var_config.items():\n        if name not in ds:\n            arr = np.full((n_epochs, n_sids), cfg[\"fill_value\"], dtype=cfg[\"dtype\"])\n            ds[name] = ((\"epoch\", \"sid\"), arr, cfg[\"attrs\"])\n    return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.prep_aux_ds","level":2,"title":"<code>prep_aux_ds(aux_ds, fill_value=np.nan, aggregate_glonass_fdma=True, keep_sids=None)</code>","text":"<p>Preprocess auxiliary dataset before writing to Icechunk.</p> <p>Performs complete 4-step preprocessing: 1. Convert sv → sid dimension 2. Pad to global sid list (all constellations) or filter to keep_sids 3. Normalize sid dtype to object 4. Strip _FillValue attributes</p> <p>This matches gnssvodpy.icechunk_manager.preprocessing.IcechunkPreprocessor.prep_aux_ds().</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.prep_aux_ds--parameters","level":4,"title":"Parameters","text":"<p>aux_ds : xr.Dataset     Dataset with 'sv' dimension. fill_value : float, default np.nan     Fill value for missing entries. aggregate_glonass_fdma : bool, default True     Whether to aggregate GLONASS FDMA bands. keep_sids : list[str] | None, default None     List of specific SIDs to keep. If None, keeps all possible SIDs.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.prep_aux_ds--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Fully preprocessed dataset ready for Icechunk or interpolation.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/preprocessing.py</code> <pre><code>def prep_aux_ds(\n    aux_ds: xr.Dataset,\n    fill_value: float = np.nan,\n    aggregate_glonass_fdma: bool = True,\n    keep_sids: list[str] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess auxiliary dataset before writing to Icechunk.\n\n    Performs complete 4-step preprocessing:\n    1. Convert sv → sid dimension\n    2. Pad to global sid list (all constellations) or filter to keep_sids\n    3. Normalize sid dtype to object\n    4. Strip _FillValue attributes\n\n    This matches\n    gnssvodpy.icechunk_manager.preprocessing.IcechunkPreprocessor.prep_aux_ds().\n\n    Parameters\n    ----------\n    aux_ds : xr.Dataset\n        Dataset with 'sv' dimension.\n    fill_value : float, default np.nan\n        Fill value for missing entries.\n    aggregate_glonass_fdma : bool, default True\n        Whether to aggregate GLONASS FDMA bands.\n    keep_sids : list[str] | None, default None\n        List of specific SIDs to keep. If None, keeps all possible SIDs.\n\n    Returns\n    -------\n    xr.Dataset\n        Fully preprocessed dataset ready for Icechunk or interpolation.\n    \"\"\"\n    ds = map_aux_sv_to_sid(aux_ds, fill_value, aggregate_glonass_fdma)\n    ds = pad_to_global_sid(\n        ds, keep_sids=keep_sids, aggregate_glonass_fdma=aggregate_glonass_fdma\n    )\n    ds = normalize_sid_dtype(ds)\n    ds = strip_fillvalue(ds)\n    return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.preprocess_aux_for_interpolation","level":2,"title":"<code>preprocess_aux_for_interpolation(aux_ds, fill_value=np.nan, full_preprocessing=False, aggregate_glonass_fdma=True)</code>","text":"<p>Preprocess auxiliary dataset before interpolation.</p> <p>Converts satellite vehicle (sv) dimension to Signal ID (sid) dimension, which is required for matching with RINEX observations after interpolation.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.preprocess_aux_for_interpolation--parameters","level":4,"title":"Parameters","text":"<p>aux_ds : xr.Dataset     Raw auxiliary dataset with 'sv' dimension. fill_value : float, default np.nan     Fill value for missing entries. full_preprocessing : bool, default False     If True, applies full 4-step preprocessing (pad_to_global_sid,     normalize_sid_dtype, strip_fillvalue). If False, only converts     sv → sid (sufficient for interpolation). aggregate_glonass_fdma : bool, default True     Whether to aggregate GLONASS FDMA bands.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.preprocess_aux_for_interpolation--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Preprocessed dataset with 'sid' dimension.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.preprocess_aux_for_interpolation--notes","level":4,"title":"Notes","text":"<p>This must be called BEFORE interpolation. The workflow is: 1. Load raw SP3/CLK data (sv dimension) 2. Convert sv → sid (this function) 3. Interpolate to target epochs 4. Match with RINEX data (sid dimension)</p> <p>For most interpolation use cases, <code>full_preprocessing=False</code> is sufficient. Use <code>full_preprocessing=True</code> when preparing data for Icechunk storage.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.preprocess_aux_for_interpolation--examples","level":4,"title":"Examples","text":"Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/preprocessing.py</code> <pre><code>def preprocess_aux_for_interpolation(\n    aux_ds: xr.Dataset,\n    fill_value: float = np.nan,\n    full_preprocessing: bool = False,\n    aggregate_glonass_fdma: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess auxiliary dataset before interpolation.\n\n    Converts satellite vehicle (sv) dimension to Signal ID (sid) dimension,\n    which is required for matching with RINEX observations after interpolation.\n\n    Parameters\n    ----------\n    aux_ds : xr.Dataset\n        Raw auxiliary dataset with 'sv' dimension.\n    fill_value : float, default np.nan\n        Fill value for missing entries.\n    full_preprocessing : bool, default False\n        If True, applies full 4-step preprocessing (pad_to_global_sid,\n        normalize_sid_dtype, strip_fillvalue). If False, only converts\n        sv → sid (sufficient for interpolation).\n    aggregate_glonass_fdma : bool, default True\n        Whether to aggregate GLONASS FDMA bands.\n\n    Returns\n    -------\n    xr.Dataset\n        Preprocessed dataset with 'sid' dimension.\n\n    Notes\n    -----\n    This must be called BEFORE interpolation. The workflow is:\n    1. Load raw SP3/CLK data (sv dimension)\n    2. Convert sv → sid (this function)\n    3. Interpolate to target epochs\n    4. Match with RINEX data (sid dimension)\n\n    For most interpolation use cases, `full_preprocessing=False` is sufficient.\n    Use `full_preprocessing=True` when preparing data for Icechunk storage.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Load raw SP3 data\n    &gt;&gt;&gt; sp3_data = Sp3File(...).to_dataset()\n    &gt;&gt;&gt; sp3_data.dims\n    {'epoch': 96, 'sv': 32}\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Preprocess before interpolation (minimal)\n    &gt;&gt;&gt; sp3_preprocessed = preprocess_aux_for_interpolation(sp3_data)\n    &gt;&gt;&gt; sp3_preprocessed.dims\n    {'epoch': 96, 'sid': 384}\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Preprocess before Icechunk (full)\n    &gt;&gt;&gt; sp3_preprocessed = preprocess_aux_for_interpolation(\n    ...     sp3_data,\n    ...     full_preprocessing=True,\n    ... )\n    &gt;&gt;&gt; sp3_preprocessed.dims\n    {'epoch': 96, 'sid': ~2000}  # Padded to all possible sids\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Now interpolate\n    &gt;&gt;&gt; sp3_interp = interpolator.interpolate(sp3_preprocessed, target_epochs)\n    \"\"\"\n    if full_preprocessing:\n        return prep_aux_ds(aux_ds, fill_value, aggregate_glonass_fdma)\n    else:\n        return map_aux_sv_to_sid(aux_ds, fill_value, aggregate_glonass_fdma)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.preprocess_aux_for_interpolation--load-raw-sp3-data","level":3,"title":"Load raw SP3 data","text":"<p>sp3_data = Sp3File(...).to_dataset() sp3_data.dims</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.preprocess_aux_for_interpolation--preprocess-before-interpolation-minimal","level":3,"title":"Preprocess before interpolation (minimal)","text":"<p>sp3_preprocessed = preprocess_aux_for_interpolation(sp3_data) sp3_preprocessed.dims</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.preprocess_aux_for_interpolation--preprocess-before-icechunk-full","level":3,"title":"Preprocess before Icechunk (full)","text":"<p>sp3_preprocessed = preprocess_aux_for_interpolation( ...     sp3_data, ...     full_preprocessing=True, ... ) sp3_preprocessed.dims {'epoch': 96, 'sid': ~2000}  # Padded to all possible sids</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.preprocessing.preprocess_aux_for_interpolation--now-interpolate","level":3,"title":"Now interpolate","text":"<p>sp3_interp = interpolator.interpolate(sp3_preprocessed, target_epochs)</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#interpolation","level":2,"title":"Interpolation","text":"<p>Interpolation strategies for GNSS auxiliary data.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.ClockConfig","level":2,"title":"<code>ClockConfig</code>","text":"<p>               Bases: <code>InterpolatorConfig</code></p> <p>Configuration for clock correction interpolation.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.ClockConfig--attributes","level":4,"title":"Attributes","text":"<p>window_size : int, default 9     Window size for discontinuity detection. jump_threshold : float, default 1e-6     Threshold for detecting clock jumps (seconds).</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>@dataclass\nclass ClockConfig(InterpolatorConfig):\n    \"\"\"Configuration for clock correction interpolation.\n\n    Attributes\n    ----------\n    window_size : int, default 9\n        Window size for discontinuity detection.\n    jump_threshold : float, default 1e-6\n        Threshold for detecting clock jumps (seconds).\n    \"\"\"\n\n    window_size: int = 9\n    jump_threshold: float = 1e-6\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.ClockInterpolationStrategy","level":2,"title":"<code>ClockInterpolationStrategy</code>","text":"<p>               Bases: <code>Interpolator</code></p> <p>Piecewise linear interpolation for clock corrections.</p> <p>Detects and handles discontinuities (clock jumps) properly.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.ClockInterpolationStrategy--examples","level":4,"title":"Examples","text":"<p>from canvod.auxiliary.interpolation import ClockInterpolationStrategy, ClockConfig</p> <p>config = ClockConfig(window_size=9, jump_threshold=1e-6) interpolator = ClockInterpolationStrategy(config=config)</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass ClockInterpolationStrategy(Interpolator):\n    \"\"\"Piecewise linear interpolation for clock corrections.\n\n    Detects and handles discontinuities (clock jumps) properly.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.auxiliary.interpolation import ClockInterpolationStrategy, ClockConfig\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; config = ClockConfig(window_size=9, jump_threshold=1e-6)\n    &gt;&gt;&gt; interpolator = ClockInterpolationStrategy(config=config)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Interpolate to RINEX epochs\n    &gt;&gt;&gt; clk_interp = interpolator.interpolate(clk_data, rinex_epochs)\n    \"\"\"\n\n    config: ClockConfig\n\n    def interpolate(self, ds: xr.Dataset, target_epochs: np.ndarray) -&gt; xr.Dataset:\n        \"\"\"Interpolate clock corrections with discontinuity handling.\"\"\"\n        # Determine satellite dimension (sv or sid)\n        sat_dim = \"sv\" if \"sv\" in ds.dims else \"sid\"\n        sat_values = ds[sat_dim].values\n\n        result_ds = xr.Dataset(coords={\"epoch\": target_epochs, sat_dim: sat_values})\n\n        # Convert epochs to seconds\n        t_source = (\n            (ds[\"epoch\"] - ds[\"epoch\"].values[0])\n            .values.astype(\"timedelta64[s]\")\n            .astype(float)\n        )\n\n        t_target = (\n            (target_epochs - ds[\"epoch\"].values[0])\n            .astype(\"timedelta64[s]\")\n            .astype(float)\n        )\n\n        # Find clock variables\n        clock_vars = [\n            var\n            for var in ds.data_vars\n            if any(c in var for c in [\"clock\", \"clk\", \"Clock\", \"CLK\", \"clock_offset\"])\n        ]\n\n        if not clock_vars:\n            raise ValueError(\"No clock variables found in dataset\")\n\n        # Process each clock variable\n        for var in clock_vars:\n            data = ds[var].values\n            output = np.full((len(target_epochs), len(sat_values)), np.nan)\n\n            # Parallel processing per satellite\n            with ThreadPoolExecutor() as executor:\n                futures = []\n                for sat_idx in range(len(sat_values)):\n                    futures.append(\n                        executor.submit(\n                            self._interpolate_sat_clock,\n                            data[:, sat_idx],\n                            t_source,\n                            t_target,\n                            self.config.jump_threshold,\n                        )\n                    )\n\n                # Collect results\n                for sat_idx, future in enumerate(futures):\n                    output[:, sat_idx] = future.result()\n\n            result_ds[var] = ((\"epoch\", sat_dim), output)\n            if var in ds:\n                result_ds[var].attrs = ds[var].attrs\n\n        return result_ds\n\n    def _interpolate_sat_clock(\n        self,\n        data: np.ndarray,\n        t_source: np.ndarray,\n        t_target: np.ndarray,\n        threshold: float,\n    ) -&gt; np.ndarray:\n        \"\"\"Interpolate clock data for a single satellite.\n\n        Handles discontinuities by splitting into segments.\n\n        Parameters\n        ----------\n        data : np.ndarray\n            Source clock data for one satellite.\n        t_source : np.ndarray\n            Source epochs in seconds from start.\n        t_target : np.ndarray\n            Target epochs in seconds from start.\n        threshold : float\n            Jump threshold for discontinuity detection.\n\n        Returns\n        -------\n        np.ndarray\n            Interpolated values aligned to `t_target`.\n        \"\"\"\n        output = np.full_like(t_target, np.nan)\n\n        # Skip if all data is NaN\n        if np.all(np.isnan(data)):\n            return output\n\n        # Find valid data points\n        valid_mask = ~np.isnan(data)\n        if not np.any(valid_mask):\n            return output\n\n        valid_data = data[valid_mask]\n        valid_time = t_source[valid_mask]\n\n        # Detect discontinuities (clock jumps)\n        jumps = np.where(np.abs(np.diff(valid_data)) &gt; threshold)[0]\n        segments = np.split(np.arange(len(valid_time)), jumps + 1)\n\n        # Interpolate each continuous segment\n        for seg in segments:\n            if len(seg) &lt; 2:\n                continue\n\n            seg_time = valid_time[seg]\n            seg_data = valid_data[seg]\n\n            # Find target points in this segment\n            mask = (t_target &gt;= seg_time[0]) &amp; (t_target &lt;= seg_time[-1])\n            if not np.any(mask):\n                continue\n\n            # Linear interpolation within segment\n            interpolator = interp1d(\n                seg_time, seg_data, bounds_error=False, fill_value=np.nan\n            )\n            output[mask] = interpolator(t_target[mask])\n\n        return output\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.ClockInterpolationStrategy--interpolate-to-rinex-epochs","level":3,"title":"Interpolate to RINEX epochs","text":"<p>clk_interp = interpolator.interpolate(clk_data, rinex_epochs)</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.ClockInterpolationStrategy.interpolate","level":3,"title":"<code>interpolate(ds, target_epochs)</code>","text":"<p>Interpolate clock corrections with discontinuity handling.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>def interpolate(self, ds: xr.Dataset, target_epochs: np.ndarray) -&gt; xr.Dataset:\n    \"\"\"Interpolate clock corrections with discontinuity handling.\"\"\"\n    # Determine satellite dimension (sv or sid)\n    sat_dim = \"sv\" if \"sv\" in ds.dims else \"sid\"\n    sat_values = ds[sat_dim].values\n\n    result_ds = xr.Dataset(coords={\"epoch\": target_epochs, sat_dim: sat_values})\n\n    # Convert epochs to seconds\n    t_source = (\n        (ds[\"epoch\"] - ds[\"epoch\"].values[0])\n        .values.astype(\"timedelta64[s]\")\n        .astype(float)\n    )\n\n    t_target = (\n        (target_epochs - ds[\"epoch\"].values[0])\n        .astype(\"timedelta64[s]\")\n        .astype(float)\n    )\n\n    # Find clock variables\n    clock_vars = [\n        var\n        for var in ds.data_vars\n        if any(c in var for c in [\"clock\", \"clk\", \"Clock\", \"CLK\", \"clock_offset\"])\n    ]\n\n    if not clock_vars:\n        raise ValueError(\"No clock variables found in dataset\")\n\n    # Process each clock variable\n    for var in clock_vars:\n        data = ds[var].values\n        output = np.full((len(target_epochs), len(sat_values)), np.nan)\n\n        # Parallel processing per satellite\n        with ThreadPoolExecutor() as executor:\n            futures = []\n            for sat_idx in range(len(sat_values)):\n                futures.append(\n                    executor.submit(\n                        self._interpolate_sat_clock,\n                        data[:, sat_idx],\n                        t_source,\n                        t_target,\n                        self.config.jump_threshold,\n                    )\n                )\n\n            # Collect results\n            for sat_idx, future in enumerate(futures):\n                output[:, sat_idx] = future.result()\n\n        result_ds[var] = ((\"epoch\", sat_dim), output)\n        if var in ds:\n            result_ds[var].attrs = ds[var].attrs\n\n    return result_ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Interpolator","level":2,"title":"<code>Interpolator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for interpolation strategies.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Interpolator--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic dataclass with <code>arbitrary_types_allowed=True</code> and uses <code>ABC</code> to define required interpolation hooks.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass Interpolator(ABC):\n    \"\"\"Abstract base class for interpolation strategies.\n\n    Notes\n    -----\n    This is a Pydantic dataclass with `arbitrary_types_allowed=True` and\n    uses `ABC` to define required interpolation hooks.\n    \"\"\"\n\n    config: InterpolatorConfig\n\n    @abstractmethod\n    def interpolate(self, ds: xr.Dataset, target_epochs: np.ndarray) -&gt; xr.Dataset:\n        \"\"\"Interpolate dataset to match target epochs.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Source dataset with (epoch, sid) dimensions.\n        target_epochs : np.ndarray\n            Target epoch grid (datetime64).\n\n        Returns\n        -------\n        xr.Dataset\n            Interpolated dataset at target epochs.\n        \"\"\"\n        pass\n\n    def to_attrs(self) -&gt; dict[str, Any]:\n        \"\"\"Convert interpolator to attrs-compatible dictionary.\"\"\"\n        return {\n            \"interpolator_type\": self.__class__.__name__,\n            \"config\": self.config.to_dict(),\n        }\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Interpolator.interpolate","level":3,"title":"<code>interpolate(ds, target_epochs)</code>  <code>abstractmethod</code>","text":"<p>Interpolate dataset to match target epochs.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Interpolator.interpolate--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Source dataset with (epoch, sid) dimensions. target_epochs : np.ndarray     Target epoch grid (datetime64).</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Interpolator.interpolate--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Interpolated dataset at target epochs.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>@abstractmethod\ndef interpolate(self, ds: xr.Dataset, target_epochs: np.ndarray) -&gt; xr.Dataset:\n    \"\"\"Interpolate dataset to match target epochs.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Source dataset with (epoch, sid) dimensions.\n    target_epochs : np.ndarray\n        Target epoch grid (datetime64).\n\n    Returns\n    -------\n    xr.Dataset\n        Interpolated dataset at target epochs.\n    \"\"\"\n    pass\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Interpolator.to_attrs","level":3,"title":"<code>to_attrs()</code>","text":"<p>Convert interpolator to attrs-compatible dictionary.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>def to_attrs(self) -&gt; dict[str, Any]:\n    \"\"\"Convert interpolator to attrs-compatible dictionary.\"\"\"\n    return {\n        \"interpolator_type\": self.__class__.__name__,\n        \"config\": self.config.to_dict(),\n    }\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.InterpolatorConfig","level":2,"title":"<code>InterpolatorConfig</code>","text":"<p>Base class for interpolator configuration.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>class InterpolatorConfig:\n    \"\"\"Base class for interpolator configuration.\"\"\"\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert config to dictionary for attrs storage.\"\"\"\n        return asdict(self)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.InterpolatorConfig.to_dict","level":3,"title":"<code>to_dict()</code>","text":"<p>Convert config to dictionary for attrs storage.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert config to dictionary for attrs storage.\"\"\"\n    return asdict(self)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Sp3Config","level":2,"title":"<code>Sp3Config</code>","text":"<p>               Bases: <code>InterpolatorConfig</code></p> <p>Configuration for SP3 ephemeris interpolation.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Sp3Config--attributes","level":4,"title":"Attributes","text":"<p>use_velocities : bool, default True     Use Hermite splines with satellite velocities if available. fallback_method : str, default 'linear'     Interpolation method when velocities are unavailable.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>@dataclass\nclass Sp3Config(InterpolatorConfig):\n    \"\"\"Configuration for SP3 ephemeris interpolation.\n\n    Attributes\n    ----------\n    use_velocities : bool, default True\n        Use Hermite splines with satellite velocities if available.\n    fallback_method : str, default 'linear'\n        Interpolation method when velocities are unavailable.\n    \"\"\"\n\n    use_velocities: bool = True\n    fallback_method: str = \"linear\"\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Sp3InterpolationStrategy","level":2,"title":"<code>Sp3InterpolationStrategy</code>","text":"<p>               Bases: <code>Interpolator</code></p> <p>Hermite cubic spline interpolation for SP3 ephemeris data.</p> <p>Uses satellite velocities (Vx, Vy, Vz) for higher accuracy. Falls back to linear interpolation if velocities unavailable.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Sp3InterpolationStrategy--examples","level":4,"title":"Examples","text":"<p>from canvod.auxiliary.interpolation import Sp3InterpolationStrategy, Sp3Config</p> <p>config = Sp3Config(use_velocities=True, fallback_method='linear') interpolator = Sp3InterpolationStrategy(config=config)</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass Sp3InterpolationStrategy(Interpolator):\n    \"\"\"Hermite cubic spline interpolation for SP3 ephemeris data.\n\n    Uses satellite velocities (Vx, Vy, Vz) for higher accuracy.\n    Falls back to linear interpolation if velocities unavailable.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.auxiliary.interpolation import Sp3InterpolationStrategy, Sp3Config\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; config = Sp3Config(use_velocities=True, fallback_method='linear')\n    &gt;&gt;&gt; interpolator = Sp3InterpolationStrategy(config=config)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Interpolate to RINEX epochs\n    &gt;&gt;&gt; sp3_interp = interpolator.interpolate(sp3_data, rinex_epochs)\n    \"\"\"\n\n    config: Sp3Config\n\n    def interpolate(self, ds: xr.Dataset, target_epochs: np.ndarray) -&gt; xr.Dataset:\n        \"\"\"Interpolate SP3 ephemeris to target epochs.\"\"\"\n        if self.config.use_velocities and all(v in ds for v in [\"Vx\", \"Vy\", \"Vz\"]):\n            return self._interpolate_with_velocities(ds, target_epochs)\n        return self._interpolate_positions_only(ds, target_epochs)\n\n    def _interpolate_with_velocities(\n        self, ds: xr.Dataset, target_epochs: np.ndarray\n    ) -&gt; xr.Dataset:\n        \"\"\"Hermite interpolation using satellite velocities.\"\"\"\n        # Determine satellite dimension (sv or sid)\n        sat_dim = \"sv\" if \"sv\" in ds.dims else \"sid\"\n        sat_values = ds[sat_dim].values\n\n        result_ds = xr.Dataset(coords={\"epoch\": target_epochs, sat_dim: sat_values})\n\n        # Convert epochs to seconds (relative to first epoch)\n        t_source = (\n            (ds[\"epoch\"] - ds[\"epoch\"].values[0])\n            .values.astype(\"timedelta64[s]\")\n            .astype(float)\n        )\n\n        t_target = (\n            (target_epochs - ds[\"epoch\"].values[0])\n            .astype(\"timedelta64[s]\")\n            .astype(float)\n        )\n\n        # Pre-allocate output arrays\n        n_targets = len(target_epochs)\n        n_sats = len(sat_values)\n        coords = {\n            \"X\": np.empty((n_targets, n_sats)),\n            \"Y\": np.empty((n_targets, n_sats)),\n            \"Z\": np.empty((n_targets, n_sats)),\n        }\n\n        vels = {\n            \"Vx\": np.empty((n_targets, n_sats)),\n            \"Vy\": np.empty((n_targets, n_sats)),\n            \"Vz\": np.empty((n_targets, n_sats)),\n        }\n\n        # Parallel interpolation per satellite\n        with ThreadPoolExecutor() as executor:\n            futures = []\n            for i, sat in enumerate(sat_values):\n                futures.append(\n                    executor.submit(\n                        self._interpolate_sat,\n                        ds,\n                        sat,\n                        sat_dim,\n                        t_source,\n                        t_target,\n                        i,\n                        coords,\n                        vels,\n                    )\n                )\n\n            # Wait for completion\n            for future in futures:\n                future.result()\n\n        # Assign results to dataset\n        for coord, data in coords.items():\n            result_ds[coord] = ((\"epoch\", sat_dim), data)\n            if coord in ds:\n                result_ds[coord].attrs = ds[coord].attrs\n\n        for vel, data in vels.items():\n            result_ds[vel] = ((\"epoch\", sat_dim), data)\n            if vel in ds:\n                result_ds[vel].attrs = ds[vel].attrs\n\n        return result_ds\n\n    def _interpolate_sat(\n        self,\n        ds: xr.Dataset,\n        sat: str,\n        sat_dim: str,\n        t_source: np.ndarray,\n        t_target: np.ndarray,\n        idx: int,\n        coords: dict[str, np.ndarray],\n        vels: dict[str, np.ndarray],\n    ) -&gt; None:\n        \"\"\"Interpolate a single satellite using Hermite splines.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Source dataset.\n        sat : str\n            Satellite identifier.\n        sat_dim : str\n            Dimension name for satellites (\"sv\" or \"sid\").\n        t_source : np.ndarray\n            Source epochs in seconds from start.\n        t_target : np.ndarray\n            Target epochs in seconds from start.\n        idx : int\n            Output index for this satellite.\n        coords : dict[str, np.ndarray]\n            Output position arrays to fill.\n        vels : dict[str, np.ndarray]\n            Output velocity arrays to fill.\n        \"\"\"\n        for coord, vel in [(\"X\", \"Vx\"), (\"Y\", \"Vy\"), (\"Z\", \"Vz\")]:\n            # Select by satellite dimension name\n            pos = ds[coord].sel({sat_dim: sat}).values\n            vel_data = ds[vel].sel({sat_dim: sat}).values\n\n            # Skip if data is all NaN\n            if not np.isfinite(pos).any() or not np.isfinite(vel_data).any():\n                coords[coord][:, idx] = np.nan\n                if vels is not None:\n                    vels[vel][:, idx] = np.nan\n                continue\n\n            # Hermite cubic spline interpolation\n            interpolator = CubicHermiteSpline(t_source, pos, vel_data)\n            coords[coord][:, idx] = interpolator(t_target)\n            if vels is not None:\n                vels[vel][:, idx] = interpolator.derivative()(t_target)\n\n    def _interpolate_positions_only(\n        self, ds: xr.Dataset, target_epochs: np.ndarray\n    ) -&gt; xr.Dataset:\n        \"\"\"Fallback: simple linear interpolation for positions only.\"\"\"\n        return ds.interp(epoch=target_epochs, method=self.config.fallback_method)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Sp3InterpolationStrategy--interpolate-to-rinex-epochs","level":3,"title":"Interpolate to RINEX epochs","text":"<p>sp3_interp = interpolator.interpolate(sp3_data, rinex_epochs)</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.Sp3InterpolationStrategy.interpolate","level":3,"title":"<code>interpolate(ds, target_epochs)</code>","text":"<p>Interpolate SP3 ephemeris to target epochs.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>def interpolate(self, ds: xr.Dataset, target_epochs: np.ndarray) -&gt; xr.Dataset:\n    \"\"\"Interpolate SP3 ephemeris to target epochs.\"\"\"\n    if self.config.use_velocities and all(v in ds for v in [\"Vx\", \"Vy\", \"Vz\"]):\n        return self._interpolate_with_velocities(ds, target_epochs)\n    return self._interpolate_positions_only(ds, target_epochs)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.create_interpolator_from_attrs","level":2,"title":"<code>create_interpolator_from_attrs(attrs)</code>","text":"<p>Recreate interpolator instance from dataset attributes.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.create_interpolator_from_attrs--parameters","level":4,"title":"Parameters","text":"<p>attrs : dict     Dataset attributes containing interpolator_config.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.create_interpolator_from_attrs--returns","level":4,"title":"Returns","text":"<p>Interpolator     Reconstructed interpolator instance.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.create_interpolator_from_attrs--examples","level":4,"title":"Examples","text":"Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/interpolation/interpolator.py</code> <pre><code>def create_interpolator_from_attrs(attrs: dict[str, Any]) -&gt; Interpolator:\n    \"\"\"Recreate interpolator instance from dataset attributes.\n\n    Parameters\n    ----------\n    attrs : dict\n        Dataset attributes containing interpolator_config.\n\n    Returns\n    -------\n    Interpolator\n        Reconstructed interpolator instance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Save interpolator config in dataset\n    &gt;&gt;&gt; ds.attrs['interpolator_config'] = interpolator.to_attrs()\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Later, recreate interpolator\n    &gt;&gt;&gt; interpolator = create_interpolator_from_attrs(ds.attrs)\n    \"\"\"\n    interpolator_type = attrs[\"interpolator_config\"][\"interpolator_type\"]\n    config_dict = attrs[\"interpolator_config\"][\"config\"]\n\n    if interpolator_type == \"Sp3InterpolationStrategy\":\n        config = Sp3Config(**config_dict)\n        return Sp3InterpolationStrategy(config=config)\n    elif interpolator_type == \"ClockInterpolationStrategy\":\n        config = ClockConfig(**config_dict)\n        return ClockInterpolationStrategy(config=config)\n    else:\n        raise ValueError(f\"Unknown interpolator type: {interpolator_type}\")\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.create_interpolator_from_attrs--save-interpolator-config-in-dataset","level":3,"title":"Save interpolator config in dataset","text":"<p>ds.attrs['interpolator_config'] = interpolator.to_attrs()</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.interpolation.create_interpolator_from_attrs--later-recreate-interpolator","level":3,"title":"Later, recreate interpolator","text":"<p>interpolator = create_interpolator_from_attrs(ds.attrs)</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#ephemeris-sp3","level":2,"title":"Ephemeris (SP3)","text":"<p>Ephemeris (satellite orbit) data handling.</p> <p>This module provides tools for reading, parsing, and validating satellite ephemeris data from SP3 format files.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Parser","level":2,"title":"<code>Sp3Parser</code>","text":"<p>Parser for SP3 (Standard Product #3) orbit files.</p> <p>Handles parsing of SP3 format files containing precise satellite orbit data. Implements optimized single-pass reading for performance.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Parser--parameters","level":4,"title":"Parameters","text":"<p>fpath : Path     Path to SP3 file. dimensionless : bool, default True     If True, strip units from output.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/parser.py</code> <pre><code>class Sp3Parser:\n    \"\"\"Parser for SP3 (Standard Product #3) orbit files.\n\n    Handles parsing of SP3 format files containing precise satellite orbit data.\n    Implements optimized single-pass reading for performance.\n\n    Parameters\n    ----------\n    fpath : Path\n        Path to SP3 file.\n    dimensionless : bool, default True\n        If True, strip units from output.\n    \"\"\"\n\n    def __init__(self, fpath: Path, dimensionless: bool = True) -&gt; None:\n        \"\"\"Initialize SP3 parser.\"\"\"\n        self.fpath = Path(fpath)\n        self.dimensionless = dimensionless\n\n    def parse(self) -&gt; xr.Dataset:\n        \"\"\"Parse SP3 file to xarray Dataset.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with satellite positions (X, Y, Z) in meters.\n\n        Raises\n        ------\n        FileNotFoundError\n            If file does not exist.\n        ValueError\n            If file format is invalid.\n        \"\"\"\n        if not self.fpath.exists():\n            raise FileNotFoundError(f\"SP3 file not found: {self.fpath}\")\n\n        epochs: list[datetime.datetime] = []\n        epoch_data: list[tuple[Any]] = []\n        svs: set[str] = set()\n        current_epoch_idx: int = -1\n\n        with open(self.fpath) as f:\n            # Skip header until first epoch marker\n            for line in f:\n                if line.startswith(\"*\"):\n                    current_epoch = self._parse_epoch_line(line)\n                    epochs.append(current_epoch)\n                    current_epoch_idx += 1\n                    break\n\n            # Single pass through file\n            for line in f:\n                if line.startswith(\"*\"):\n                    current_epoch = self._parse_epoch_line(line)\n                    epochs.append(current_epoch)\n                    current_epoch_idx += 1\n\n                elif line.startswith(\"P\"):\n                    sv_code = line[1:4].strip()\n                    svs.add(sv_code)\n\n                    x = self._parse_coordinate(line[4:18])\n                    y = self._parse_coordinate(line[18:32])\n                    z = self._parse_coordinate(line[32:46])\n\n                    epoch_data.append(\n                        (\n                            current_epoch_idx,\n                            sv_code,\n                            x if x is not None else np.nan,\n                            y if y is not None else np.nan,\n                            z if z is not None else np.nan,\n                        )\n                    )\n\n                elif line.startswith(\"EOF\"):\n                    break\n\n        # Build arrays\n        sv_list = sorted(svs)\n        sv_idx = {sv: i for i, sv in enumerate(sv_list)}\n        shape = (len(epochs), len(sv_list))\n\n        x_data = np.full(shape, np.nan)\n        y_data = np.full(shape, np.nan)\n        z_data = np.full(shape, np.nan)\n\n        for epoch_idx, sv, x, y, z in epoch_data:\n            j = sv_idx[sv]\n            x_data[epoch_idx, j] = x\n            y_data[epoch_idx, j] = y\n            z_data[epoch_idx, j] = z\n\n        # Convert to meters\n        x_data = (x_data * UREG.kilometer).to(UREG.meter)\n        y_data = (y_data * UREG.kilometer).to(UREG.meter)\n        z_data = (z_data * UREG.kilometer).to(UREG.meter)\n\n        if self.dimensionless:\n            x_data = x_data.magnitude\n            y_data = y_data.magnitude\n            z_data = z_data.magnitude\n\n        # Create dataset\n        ds = xr.Dataset(\n            data_vars={\n                \"X\": ((\"epoch\", \"sv\"), x_data),\n                \"Y\": ((\"epoch\", \"sv\"), y_data),\n                \"Z\": ((\"epoch\", \"sv\"), z_data),\n            },\n            coords={\n                \"epoch\": np.array(epochs, dtype=\"datetime64[ns]\"),\n                \"sv\": np.array(sv_list),\n            },\n        )\n\n        # Add variable attributes\n        for var, attrs in self._get_variable_attributes().items():\n            if var in ds:\n                ds[var].attrs = attrs\n\n        return ds\n\n    def _parse_epoch_line(self, line: str) -&gt; datetime.datetime:\n        \"\"\"Parse epoch marker line.\n\n        Parameters\n        ----------\n        line : str\n            Line starting with \"*\" containing timestamp.\n\n        Returns\n        -------\n        datetime.datetime\n            Parsed datetime.\n\n        Examples\n        --------\n        &gt;&gt;&gt; _parse_epoch_line(\"* 2024 1 15 0 0 0.00000000\")\n        datetime.datetime(2024, 1, 15, 0, 0)\n        \"\"\"\n        parts = line.split()\n        return datetime.datetime(\n            year=int(parts[1]),\n            month=int(parts[2]),\n            day=int(parts[3]),\n            hour=int(parts[4]),\n            minute=int(parts[5]),\n            second=0,\n        )\n\n    def _parse_coordinate(self, coord_str: str) -&gt; float | None:\n        \"\"\"Parse coordinate field from SP3 file.\n\n        Handles edge cases like missing spaces and invalid data markers.\n\n        Parameters\n        ----------\n        coord_str : str\n            14-character coordinate string.\n\n        Returns\n        -------\n        float | None\n            Coordinate value in kilometers, or None if missing.\n\n        Raises\n        ------\n        ValueError\n            If coordinate cannot be parsed.\n        \"\"\"\n        coord_str = coord_str.strip()\n        if not coord_str or coord_str == \"999999.999999\":\n            return None\n\n        try:\n            return float(coord_str)\n        except ValueError:\n            # Handle missing spaces between fields\n            if \".\" in coord_str:\n                parts = coord_str.split(\".\")\n                if len(parts) == 2:\n                    integer_part = parts[0].replace(\" \", \"\")\n                    return float(f\"{integer_part}.{parts[1]}\")\n            raise\n\n    def _get_variable_attributes(self) -&gt; dict[str, dict[str, str]]:\n        \"\"\"Get standardized attributes for position variables.\"\"\"\n        return {\n            \"X\": {\n                \"long_name\": \"x-coordinate in ECEF\",\n                \"standard_name\": r\"x_{ECEF}\",\n                \"short_name\": \"x\",\n                \"units\": \"m\",\n                \"axis\": \"x\",\n                \"description\": \"x-coordinate in ECEF (Earth-Centered, Earth-Fixed) frame\",\n            },\n            \"Y\": {\n                \"long_name\": \"y-coordinate in ECEF\",\n                \"standard_name\": r\"y_{ECEF}\",\n                \"short_name\": \"y\",\n                \"units\": \"m\",\n                \"axis\": \"y\",\n                \"description\": \"y-coordinate in ECEF (Earth-Centered, Earth-Fixed) frame\",\n            },\n            \"Z\": {\n                \"long_name\": \"z-coordinate in ECEF\",\n                \"standard_name\": r\"z_{ECEF}\",\n                \"short_name\": \"z\",\n                \"units\": \"m\",\n                \"axis\": \"z\",\n                \"description\": \"z-coordinate in ECEF (Earth-Centered, Earth-Fixed) frame\",\n            },\n        }\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Parser.__init__","level":3,"title":"<code>__init__(fpath, dimensionless=True)</code>","text":"<p>Initialize SP3 parser.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/parser.py</code> <pre><code>def __init__(self, fpath: Path, dimensionless: bool = True) -&gt; None:\n    \"\"\"Initialize SP3 parser.\"\"\"\n    self.fpath = Path(fpath)\n    self.dimensionless = dimensionless\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Parser.parse","level":3,"title":"<code>parse()</code>","text":"<p>Parse SP3 file to xarray Dataset.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Parser.parse--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with satellite positions (X, Y, Z) in meters.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Parser.parse--raises","level":5,"title":"Raises","text":"<p>FileNotFoundError     If file does not exist. ValueError     If file format is invalid.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/parser.py</code> <pre><code>def parse(self) -&gt; xr.Dataset:\n    \"\"\"Parse SP3 file to xarray Dataset.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with satellite positions (X, Y, Z) in meters.\n\n    Raises\n    ------\n    FileNotFoundError\n        If file does not exist.\n    ValueError\n        If file format is invalid.\n    \"\"\"\n    if not self.fpath.exists():\n        raise FileNotFoundError(f\"SP3 file not found: {self.fpath}\")\n\n    epochs: list[datetime.datetime] = []\n    epoch_data: list[tuple[Any]] = []\n    svs: set[str] = set()\n    current_epoch_idx: int = -1\n\n    with open(self.fpath) as f:\n        # Skip header until first epoch marker\n        for line in f:\n            if line.startswith(\"*\"):\n                current_epoch = self._parse_epoch_line(line)\n                epochs.append(current_epoch)\n                current_epoch_idx += 1\n                break\n\n        # Single pass through file\n        for line in f:\n            if line.startswith(\"*\"):\n                current_epoch = self._parse_epoch_line(line)\n                epochs.append(current_epoch)\n                current_epoch_idx += 1\n\n            elif line.startswith(\"P\"):\n                sv_code = line[1:4].strip()\n                svs.add(sv_code)\n\n                x = self._parse_coordinate(line[4:18])\n                y = self._parse_coordinate(line[18:32])\n                z = self._parse_coordinate(line[32:46])\n\n                epoch_data.append(\n                    (\n                        current_epoch_idx,\n                        sv_code,\n                        x if x is not None else np.nan,\n                        y if y is not None else np.nan,\n                        z if z is not None else np.nan,\n                    )\n                )\n\n            elif line.startswith(\"EOF\"):\n                break\n\n    # Build arrays\n    sv_list = sorted(svs)\n    sv_idx = {sv: i for i, sv in enumerate(sv_list)}\n    shape = (len(epochs), len(sv_list))\n\n    x_data = np.full(shape, np.nan)\n    y_data = np.full(shape, np.nan)\n    z_data = np.full(shape, np.nan)\n\n    for epoch_idx, sv, x, y, z in epoch_data:\n        j = sv_idx[sv]\n        x_data[epoch_idx, j] = x\n        y_data[epoch_idx, j] = y\n        z_data[epoch_idx, j] = z\n\n    # Convert to meters\n    x_data = (x_data * UREG.kilometer).to(UREG.meter)\n    y_data = (y_data * UREG.kilometer).to(UREG.meter)\n    z_data = (z_data * UREG.kilometer).to(UREG.meter)\n\n    if self.dimensionless:\n        x_data = x_data.magnitude\n        y_data = y_data.magnitude\n        z_data = z_data.magnitude\n\n    # Create dataset\n    ds = xr.Dataset(\n        data_vars={\n            \"X\": ((\"epoch\", \"sv\"), x_data),\n            \"Y\": ((\"epoch\", \"sv\"), y_data),\n            \"Z\": ((\"epoch\", \"sv\"), z_data),\n        },\n        coords={\n            \"epoch\": np.array(epochs, dtype=\"datetime64[ns]\"),\n            \"sv\": np.array(sv_list),\n        },\n    )\n\n    # Add variable attributes\n    for var, attrs in self._get_variable_attributes().items():\n        if var in ds:\n            ds[var].attrs = attrs\n\n    return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File","level":2,"title":"<code>Sp3File</code>","text":"<p>               Bases: <code>AuxFile</code></p> <p>Handler for SP3 orbit files with multi-product support.</p> <p>Now supports all IGS analysis centers via product registry: COD, GFZ, ESA, JPL, IGS, WHU, GRG, SHA</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic dataclass with <code>arbitrary_types_allowed=True</code>.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File--attributes","level":4,"title":"Attributes","text":"<p>date : str     String in YYYYDOY format. agency : str     Agency code (e.g., \"COD\", \"GFZ\", \"ESA\"). product_type : str     Product type (\"final\", \"rapid\"). ftp_server : str     Base URL for downloads. local_dir : Path     Local storage directory. add_velocities : bool | None, default True     Whether to compute velocities. dimensionless : bool | None, default True     Whether to strip units (store magnitudes only). product_spec : ProductSpec | None, optional     Product specification resolved from the registry.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass Sp3File(AuxFile):\n    \"\"\"Handler for SP3 orbit files with multi-product support.\n\n    Now supports all IGS analysis centers via product registry:\n    COD, GFZ, ESA, JPL, IGS, WHU, GRG, SHA\n\n    Notes\n    -----\n    This is a Pydantic dataclass with `arbitrary_types_allowed=True`.\n\n    Attributes\n    ----------\n    date : str\n        String in YYYYDOY format.\n    agency : str\n        Agency code (e.g., \"COD\", \"GFZ\", \"ESA\").\n    product_type : str\n        Product type (\"final\", \"rapid\").\n    ftp_server : str\n        Base URL for downloads.\n    local_dir : Path\n        Local storage directory.\n    add_velocities : bool | None, default True\n        Whether to compute velocities.\n    dimensionless : bool | None, default True\n        Whether to strip units (store magnitudes only).\n    product_spec : ProductSpec | None, optional\n        Product specification resolved from the registry.\n    \"\"\"\n\n    date: str\n    agency: str\n    product_type: str\n    ftp_server: str\n    local_dir: Path\n    add_velocities: bool | None = True\n    dimensionless: bool | None = True\n    product_spec: ProductSpec | None = None\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize with product validation.\"\"\"\n        self.file_type = [\"orbit\"]\n        self.local_dir = Path(self.local_dir)\n        self.local_dir.mkdir(parents=True, exist_ok=True)\n\n        # Validate product exists in registry\n        self.product_spec = get_product_spec(self.agency, self.product_type)\n\n        super().__post_init__()\n\n    def get_interpolation_strategy(self) -&gt; Interpolator:\n        \"\"\"Get appropriate interpolation strategy for SP3 files.\"\"\"\n        config = Sp3Config(\n            use_velocities=self.add_velocities,\n            fallback_method=\"linear\",\n        )\n        return Sp3InterpolationStrategy(config=config)\n\n    def generate_filename_based_on_type(self) -&gt; Path:\n        \"\"\"Generate filename using product registry.\n\n        Pattern: {PREFIX}_{YYYYDOY}0000_{DURATION}_{SAMPLING}_ORB.SP3\n\n        Example: COD0MGXFIN_20240150000_01D_05M_ORB.SP3\n        \"\"\"\n        prefix = self.product_spec.prefix\n        duration = self.product_spec.duration\n        sampling = self.product_spec.sampling_rate\n\n        return Path(f\"{prefix}_{self.date}0000_{duration}_{sampling}_ORB.SP3\")\n\n    def download_aux_file(self) -&gt; None:\n        \"\"\"Download using product-specific path pattern.\n\n        Raises\n        ------\n        RuntimeError\n            If download fails from all servers.\n        ValueError\n            If GPS week calculation fails.\n        \"\"\"\n        orbit_file = self.generate_filename_based_on_type()\n        gps_week = get_gps_week_from_filename(orbit_file)\n\n        # Use product spec's path pattern\n        ftp_path = self.product_spec.ftp_path_pattern.format(\n            gps_week=gps_week,\n            file=f\"{orbit_file}.gz\",\n        )\n\n        full_url = f\"{self.ftp_server}{ftp_path}\"\n        destination = self.local_dir / orbit_file\n\n        file_info = {\n            \"gps_week\": gps_week,\n            \"filename\": orbit_file,\n            \"type\": \"orbit\",\n            \"agency\": self.agency,\n            \"latency\": self.product_spec.latency_hours,\n        }\n\n        try:\n            self.download_file(full_url, destination, file_info)\n            print(f\"Downloaded orbit file for {self.agency} on date {self.date}\")\n        except Exception as e:\n            raise RuntimeError(\n                f\"Failed to download SP3 file from all available servers: {str(e)}\"\n            )\n\n    def read_file(self) -&gt; xr.Dataset:\n        \"\"\"Read and validate SP3 file.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with satellite positions (X, Y, Z) in meters.\n\n        Raises\n        ------\n        FileNotFoundError\n            If file does not exist.\n        ValueError\n            If validation fails.\n        \"\"\"\n        # Use dedicated parser\n        parser = Sp3Parser(self.fpath, dimensionless=self.dimensionless)\n        dataset = parser.parse()\n\n        # Validate format\n        validator = Sp3Validator(dataset, self.fpath)\n        result = validator.validate()\n\n        if not result.is_valid:\n            raise ValueError(f\"SP3 validation failed:\\n{result.summary()}\")\n\n        # Add metadata\n        dataset = self._add_metadata(dataset)\n\n        # Compute velocities if requested\n        if self.add_velocities:\n            dataset = self.compute_velocity(dataset)\n\n        return dataset\n\n    def _add_metadata(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"Add file-level metadata to dataset.\"\"\"\n        ds.attrs = {\n            \"file\": str(self.fpath.name),\n            \"agency\": self.agency,\n            \"agency_name\": self.product_spec.agency_name,\n            \"product_type\": self.product_type,\n            \"ftp_server\": self.ftp_server,\n            \"date\": self.date,\n            \"sampling_rate\": self.product_spec.sampling_rate,\n            \"duration\": self.product_spec.duration,\n        }\n        return ds\n\n    def compute_velocity(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"Compute satellite velocities from position data.\n\n        Uses central differences for interior points, forward/backward\n        differences for endpoints.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset with X, Y, Z coordinates.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset augmented with Vx, Vy, Vz velocities.\n        \"\"\"\n        # Calculate time step\n        time_diffs = np.diff(ds[\"epoch\"].values)\n        dt = np.median(time_diffs).astype(\"timedelta64[s]\").astype(float)\n\n        # Initialize velocity arrays\n        vx = np.zeros_like(ds[\"X\"].values)\n        vy = np.zeros_like(ds[\"Y\"].values)\n        vz = np.zeros_like(ds[\"Z\"].values)\n\n        # Central difference for interior points\n        vx[1:-1] = (ds[\"X\"].values[2:] - ds[\"X\"].values[:-2]) / (2 * dt)\n        vy[1:-1] = (ds[\"Y\"].values[2:] - ds[\"Y\"].values[:-2]) / (2 * dt)\n        vz[1:-1] = (ds[\"Z\"].values[2:] - ds[\"Z\"].values[:-2]) / (2 * dt)\n\n        # Forward difference for first point\n        vx[0] = (ds[\"X\"].values[1] - ds[\"X\"].values[0]) / dt\n        vy[0] = (ds[\"Y\"].values[1] - ds[\"Y\"].values[0]) / dt\n        vz[0] = (ds[\"Z\"].values[1] - ds[\"Z\"].values[0]) / dt\n\n        # Backward difference for last point\n        vx[-1] = (ds[\"X\"].values[-1] - ds[\"X\"].values[-2]) / dt\n        vy[-1] = (ds[\"Y\"].values[-1] - ds[\"Y\"].values[-2]) / dt\n        vz[-1] = (ds[\"Z\"].values[-1] - ds[\"Z\"].values[-2]) / dt\n\n        # Add units if needed\n        if not self.dimensionless:\n            vx = vx * (UREG.meter / UREG.second)\n            vy = vy * (UREG.meter / UREG.second)\n            vz = vz * (UREG.meter / UREG.second)\n\n        # Add to dataset\n        ds = ds.assign(\n            Vx=((\"epoch\", \"sv\"), vx),\n            Vy=((\"epoch\", \"sv\"), vy),\n            Vz=((\"epoch\", \"sv\"), vz),\n        )\n\n        # Add velocity attributes\n        for var, attrs in self._get_velocity_attributes(dt).items():\n            if var in ds:\n                ds[var].attrs = attrs\n\n        return ds\n\n    def _get_velocity_attributes(\n        self,\n        dt: float,\n    ) -&gt; dict[str, dict[str, str | float]]:\n        \"\"\"Get standardized attributes for velocity variables.\"\"\"\n        base_attrs = {\n            \"units\": \"m/s\",\n            \"computation_method\": \"central_difference\",\n            \"time_step\": float(dt),\n            \"reference_frame\": \"ECEF\",\n        }\n\n        return {\n            \"Vx\": {\n                \"long_name\": \"x-component of velocity\",\n                \"standard_name\": \"v_x\",\n                \"short_name\": \"v_x\",\n                \"axis\": \"v_x\",\n                **base_attrs,\n            },\n            \"Vy\": {\n                \"long_name\": \"y-component of velocity\",\n                \"standard_name\": \"v_y\",\n                \"short_name\": \"v_y\",\n                \"axis\": \"v_y\",\n                **base_attrs,\n            },\n            \"Vz\": {\n                \"long_name\": \"z-component of velocity\",\n                \"standard_name\": \"v_z\",\n                \"short_name\": \"v_z\",\n                \"axis\": \"v_z\",\n                **base_attrs,\n            },\n        }\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File.__post_init__","level":3,"title":"<code>__post_init__()</code>","text":"<p>Initialize with product validation.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize with product validation.\"\"\"\n    self.file_type = [\"orbit\"]\n    self.local_dir = Path(self.local_dir)\n    self.local_dir.mkdir(parents=True, exist_ok=True)\n\n    # Validate product exists in registry\n    self.product_spec = get_product_spec(self.agency, self.product_type)\n\n    super().__post_init__()\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File.get_interpolation_strategy","level":3,"title":"<code>get_interpolation_strategy()</code>","text":"<p>Get appropriate interpolation strategy for SP3 files.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def get_interpolation_strategy(self) -&gt; Interpolator:\n    \"\"\"Get appropriate interpolation strategy for SP3 files.\"\"\"\n    config = Sp3Config(\n        use_velocities=self.add_velocities,\n        fallback_method=\"linear\",\n    )\n    return Sp3InterpolationStrategy(config=config)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File.generate_filename_based_on_type","level":3,"title":"<code>generate_filename_based_on_type()</code>","text":"<p>Generate filename using product registry.</p> <p>Pattern: {PREFIX}{YYYYDOY}0000_ORB.SP3}_{SAMPLING</p> <p>Example: COD0MGXFIN_20240150000_01D_05M_ORB.SP3</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def generate_filename_based_on_type(self) -&gt; Path:\n    \"\"\"Generate filename using product registry.\n\n    Pattern: {PREFIX}_{YYYYDOY}0000_{DURATION}_{SAMPLING}_ORB.SP3\n\n    Example: COD0MGXFIN_20240150000_01D_05M_ORB.SP3\n    \"\"\"\n    prefix = self.product_spec.prefix\n    duration = self.product_spec.duration\n    sampling = self.product_spec.sampling_rate\n\n    return Path(f\"{prefix}_{self.date}0000_{duration}_{sampling}_ORB.SP3\")\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File.download_aux_file","level":3,"title":"<code>download_aux_file()</code>","text":"<p>Download using product-specific path pattern.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File.download_aux_file--raises","level":5,"title":"Raises","text":"<p>RuntimeError     If download fails from all servers. ValueError     If GPS week calculation fails.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def download_aux_file(self) -&gt; None:\n    \"\"\"Download using product-specific path pattern.\n\n    Raises\n    ------\n    RuntimeError\n        If download fails from all servers.\n    ValueError\n        If GPS week calculation fails.\n    \"\"\"\n    orbit_file = self.generate_filename_based_on_type()\n    gps_week = get_gps_week_from_filename(orbit_file)\n\n    # Use product spec's path pattern\n    ftp_path = self.product_spec.ftp_path_pattern.format(\n        gps_week=gps_week,\n        file=f\"{orbit_file}.gz\",\n    )\n\n    full_url = f\"{self.ftp_server}{ftp_path}\"\n    destination = self.local_dir / orbit_file\n\n    file_info = {\n        \"gps_week\": gps_week,\n        \"filename\": orbit_file,\n        \"type\": \"orbit\",\n        \"agency\": self.agency,\n        \"latency\": self.product_spec.latency_hours,\n    }\n\n    try:\n        self.download_file(full_url, destination, file_info)\n        print(f\"Downloaded orbit file for {self.agency} on date {self.date}\")\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to download SP3 file from all available servers: {str(e)}\"\n        )\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File.read_file","level":3,"title":"<code>read_file()</code>","text":"<p>Read and validate SP3 file.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File.read_file--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with satellite positions (X, Y, Z) in meters.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File.read_file--raises","level":5,"title":"Raises","text":"<p>FileNotFoundError     If file does not exist. ValueError     If validation fails.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def read_file(self) -&gt; xr.Dataset:\n    \"\"\"Read and validate SP3 file.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with satellite positions (X, Y, Z) in meters.\n\n    Raises\n    ------\n    FileNotFoundError\n        If file does not exist.\n    ValueError\n        If validation fails.\n    \"\"\"\n    # Use dedicated parser\n    parser = Sp3Parser(self.fpath, dimensionless=self.dimensionless)\n    dataset = parser.parse()\n\n    # Validate format\n    validator = Sp3Validator(dataset, self.fpath)\n    result = validator.validate()\n\n    if not result.is_valid:\n        raise ValueError(f\"SP3 validation failed:\\n{result.summary()}\")\n\n    # Add metadata\n    dataset = self._add_metadata(dataset)\n\n    # Compute velocities if requested\n    if self.add_velocities:\n        dataset = self.compute_velocity(dataset)\n\n    return dataset\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File.compute_velocity","level":3,"title":"<code>compute_velocity(ds)</code>","text":"<p>Compute satellite velocities from position data.</p> <p>Uses central differences for interior points, forward/backward differences for endpoints.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File.compute_velocity--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with X, Y, Z coordinates.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3File.compute_velocity--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset augmented with Vx, Vy, Vz velocities.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/reader.py</code> <pre><code>def compute_velocity(self, ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Compute satellite velocities from position data.\n\n    Uses central differences for interior points, forward/backward\n    differences for endpoints.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with X, Y, Z coordinates.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset augmented with Vx, Vy, Vz velocities.\n    \"\"\"\n    # Calculate time step\n    time_diffs = np.diff(ds[\"epoch\"].values)\n    dt = np.median(time_diffs).astype(\"timedelta64[s]\").astype(float)\n\n    # Initialize velocity arrays\n    vx = np.zeros_like(ds[\"X\"].values)\n    vy = np.zeros_like(ds[\"Y\"].values)\n    vz = np.zeros_like(ds[\"Z\"].values)\n\n    # Central difference for interior points\n    vx[1:-1] = (ds[\"X\"].values[2:] - ds[\"X\"].values[:-2]) / (2 * dt)\n    vy[1:-1] = (ds[\"Y\"].values[2:] - ds[\"Y\"].values[:-2]) / (2 * dt)\n    vz[1:-1] = (ds[\"Z\"].values[2:] - ds[\"Z\"].values[:-2]) / (2 * dt)\n\n    # Forward difference for first point\n    vx[0] = (ds[\"X\"].values[1] - ds[\"X\"].values[0]) / dt\n    vy[0] = (ds[\"Y\"].values[1] - ds[\"Y\"].values[0]) / dt\n    vz[0] = (ds[\"Z\"].values[1] - ds[\"Z\"].values[0]) / dt\n\n    # Backward difference for last point\n    vx[-1] = (ds[\"X\"].values[-1] - ds[\"X\"].values[-2]) / dt\n    vy[-1] = (ds[\"Y\"].values[-1] - ds[\"Y\"].values[-2]) / dt\n    vz[-1] = (ds[\"Z\"].values[-1] - ds[\"Z\"].values[-2]) / dt\n\n    # Add units if needed\n    if not self.dimensionless:\n        vx = vx * (UREG.meter / UREG.second)\n        vy = vy * (UREG.meter / UREG.second)\n        vz = vz * (UREG.meter / UREG.second)\n\n    # Add to dataset\n    ds = ds.assign(\n        Vx=((\"epoch\", \"sv\"), vx),\n        Vy=((\"epoch\", \"sv\"), vy),\n        Vz=((\"epoch\", \"sv\"), vz),\n    )\n\n    # Add velocity attributes\n    for var, attrs in self._get_velocity_attributes(dt).items():\n        if var in ds:\n            ds[var].attrs = attrs\n\n    return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Validator","level":2,"title":"<code>Sp3Validator</code>","text":"<p>Validator for SP3 orbit files.</p> <p>Performs format and data quality checks on parsed SP3 datasets.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Validator--parameters","level":4,"title":"Parameters","text":"<p>dataset : xr.Dataset     Parsed SP3 dataset. fpath : Path     Path to the original file.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/validator.py</code> <pre><code>class Sp3Validator:\n    \"\"\"Validator for SP3 orbit files.\n\n    Performs format and data quality checks on parsed SP3 datasets.\n\n    Parameters\n    ----------\n    dataset : xr.Dataset\n        Parsed SP3 dataset.\n    fpath : Path\n        Path to the original file.\n    \"\"\"\n\n    def __init__(self, dataset: xr.Dataset, fpath: Path) -&gt; None:\n        \"\"\"Initialize validator.\"\"\"\n        self.dataset = dataset\n        self.fpath = Path(fpath)\n        self.result = FileValidationResult(\n            is_valid=True,\n            errors=[],\n            warnings=[],\n            file_path=self.fpath,\n            file_type=\"SP3\",\n        )\n\n    def validate(self) -&gt; FileValidationResult:\n        \"\"\"Run all validation checks.\n\n        Returns\n        -------\n        FileValidationResult\n            Validation result with errors and warnings.\n        \"\"\"\n        self._check_required_variables()\n        self._check_required_coordinates()\n        self._check_data_quality()\n\n        return self.result\n\n    def _check_required_variables(self) -&gt; None:\n        \"\"\"Check that required variables are present.\"\"\"\n        required = [\"X\", \"Y\", \"Z\"]\n        missing = [var for var in required if var not in self.dataset.data_vars]\n\n        if missing:\n            self.result.add_error(f\"Missing required variables: {missing}\")\n\n    def _check_required_coordinates(self) -&gt; None:\n        \"\"\"Check that required coordinates are present.\"\"\"\n        required = [\"epoch\", \"sv\"]\n        missing = [coord for coord in required if coord not in self.dataset.coords]\n\n        if missing:\n            self.result.add_error(f\"Missing required coordinates: {missing}\")\n\n    def _check_data_quality(self) -&gt; None:\n        \"\"\"Check data quality metrics.\"\"\"\n        if \"X\" not in self.dataset:\n            return\n\n        # Check for excessive NaN values\n        for var in [\"X\", \"Y\", \"Z\"]:\n            nan_count = self.dataset[var].isnull().sum().item()\n            total_count = self.dataset[var].size\n            nan_percentage = (nan_count / total_count) * 100\n\n            if nan_percentage &gt; 50:\n                self.result.add_error(\n                    f\"Variable {var} has {nan_percentage:.1f}% NaN values \"\n                    f\"(&gt;50% threshold)\"\n                )\n            elif nan_percentage &gt; 10:\n                self.result.add_warning(\n                    f\"Variable {var} has {nan_percentage:.1f}% NaN values\"\n                )\n\n    def get_summary(self) -&gt; str:\n        \"\"\"Get validation summary.\"\"\"\n        return self.result.summary()\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Validator.__init__","level":3,"title":"<code>__init__(dataset, fpath)</code>","text":"<p>Initialize validator.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/validator.py</code> <pre><code>def __init__(self, dataset: xr.Dataset, fpath: Path) -&gt; None:\n    \"\"\"Initialize validator.\"\"\"\n    self.dataset = dataset\n    self.fpath = Path(fpath)\n    self.result = FileValidationResult(\n        is_valid=True,\n        errors=[],\n        warnings=[],\n        file_path=self.fpath,\n        file_type=\"SP3\",\n    )\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Validator.validate","level":3,"title":"<code>validate()</code>","text":"<p>Run all validation checks.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Validator.validate--returns","level":5,"title":"Returns","text":"<p>FileValidationResult     Validation result with errors and warnings.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/validator.py</code> <pre><code>def validate(self) -&gt; FileValidationResult:\n    \"\"\"Run all validation checks.\n\n    Returns\n    -------\n    FileValidationResult\n        Validation result with errors and warnings.\n    \"\"\"\n    self._check_required_variables()\n    self._check_required_coordinates()\n    self._check_data_quality()\n\n    return self.result\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.ephemeris.Sp3Validator.get_summary","level":3,"title":"<code>get_summary()</code>","text":"<p>Get validation summary.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/ephemeris/validator.py</code> <pre><code>def get_summary(self) -&gt; str:\n    \"\"\"Get validation summary.\"\"\"\n    return self.result.summary()\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#clock-clk","level":2,"title":"Clock (CLK)","text":"<p>Clock correction data handling.</p> <p>This module provides tools for reading, parsing, and validating satellite clock correction data from RINEX CLK format files.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile","level":2,"title":"<code>ClkFile</code>","text":"<p>               Bases: <code>AuxFile</code></p> <p>Handler for GNSS clock files in CLK format.</p> <p>This class reads and processes clock offset files containing satellite clock corrections. It handles the parsing of CLK format files and provides the data in xarray Dataset format.</p> <p>Supports multiple analysis centers via product registry with proper FTP paths and filename conventions.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic dataclass with <code>arbitrary_types_allowed=True</code>.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile--attributes","level":4,"title":"Attributes","text":"<p>date : str     String in YYYYDOY format representing the start date. agency : str     Analysis center identifier (e.g., \"COD\", \"GFZ\"). product_type : str     Product type (\"final\", \"rapid\", \"ultrarapid\"). ftp_server : str     Base URL for file downloads. local_dir : Path     Local storage directory. dimensionless : bool | None, default True     If True, outputs magnitude-only values (no units attached).</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass ClkFile(AuxFile):\n    \"\"\"Handler for GNSS clock files in CLK format.\n\n    This class reads and processes clock offset files containing satellite clock\n    corrections. It handles the parsing of CLK format files and provides the data\n    in xarray Dataset format.\n\n    Supports multiple analysis centers via product registry with proper FTP paths\n    and filename conventions.\n\n    Notes\n    -----\n    This is a Pydantic dataclass with `arbitrary_types_allowed=True`.\n\n    Attributes\n    ----------\n    date : str\n        String in YYYYDOY format representing the start date.\n    agency : str\n        Analysis center identifier (e.g., \"COD\", \"GFZ\").\n    product_type : str\n        Product type (\"final\", \"rapid\", \"ultrarapid\").\n    ftp_server : str\n        Base URL for file downloads.\n    local_dir : Path\n        Local storage directory.\n    dimensionless : bool | None, default True\n        If True, outputs magnitude-only values (no units attached).\n    \"\"\"\n\n    date: str\n    agency: str\n    product_type: str\n    ftp_server: str\n    local_dir: Path\n    dimensionless: bool | None = True\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize CLK file handler.\"\"\"\n        self.file_type = [\"clock\"]\n        self.local_dir = Path(self.local_dir)\n        self.local_dir.mkdir(parents=True, exist_ok=True)\n        super().__post_init__()\n\n    def get_interpolation_strategy(self) -&gt; Interpolator:\n        \"\"\"Get appropriate interpolation strategy for CLK files.\"\"\"\n        config = ClockConfig(\n            window_size=9,\n            jump_threshold=1e-6,\n        )\n        return ClockInterpolationStrategy(config=config)\n\n    def generate_filename_based_on_type(self) -&gt; Path:\n        \"\"\"Generate standard CLK filename using product registry.\n\n        Uses product registry to get correct prefix for the agency/product combination.\n        Filename format: {PREFIX}_{YYYYDOY}0000_01D_30S_CLK.CLK\n\n        Returns\n        -------\n        Path\n            Filename according to CLK conventions.\n\n        Raises\n        ------\n        ValueError\n            If agency/product combination not in registry.\n        \"\"\"\n        # Get product spec from registry\n        spec = get_product_spec(self.agency, self.product_type)\n\n        # CLK files use 30S sampling for most products\n        sampling = \"30S\"  # Could be made configurable via registry\n\n        return Path(f\"{spec.prefix}_{self.date}0000_01D_{sampling}_CLK.CLK\")\n\n    def download_aux_file(self) -&gt; None:\n        \"\"\"Download CLK file from FTP server with automatic fallback.\n\n        Constructs URL using product registry path pattern.\n        Uses GPS week for directory structure.\n\n        Raises\n        ------\n        RuntimeError\n            If file cannot be downloaded from any available server.\n        ValueError\n            If GPS week calculation fails.\n        \"\"\"\n        clock_file = self.generate_filename_based_on_type()\n        gps_week = get_gps_week_from_filename(clock_file)\n\n        # Get product spec from registry\n        spec = get_product_spec(self.agency, self.product_type)\n\n        # Use product spec's path pattern\n        ftp_path = spec.ftp_path_pattern.format(\n            gps_week=gps_week,\n            file=f\"{clock_file}.gz\",\n        )\n\n        full_url = f\"{self.ftp_server}{ftp_path}\"\n        destination = self.local_dir / clock_file\n\n        # File info for NASA CDDIS URL construction\n        file_info = {\n            \"gps_week\": gps_week,\n            \"filename\": clock_file,\n            \"type\": \"clock\",\n            \"agency\": self.agency,\n            \"latency\": spec.latency_hours,\n        }\n\n        try:\n            self.download_file(full_url, destination, file_info)\n            print(f\"Downloaded clock file for {self.agency} on date {self.date}\")\n        except Exception as e:\n            raise RuntimeError(\n                f\"Failed to download CLK file from all available servers: {str(e)}\"\n            )\n\n    def read_file(self) -&gt; xr.Dataset:\n        \"\"\"Read and parse CLK file into xarray Dataset.\n\n        Uses modular parser for data extraction and validator for quality checks.\n        Applies unit conversion from microseconds to seconds.\n\n        Returns\n        -------\n        xr.Dataset\n            Clock offsets with dimensions (epoch, sv). Values are in seconds\n            (or dimensionless if specified).\n        \"\"\"\n        # Parse file using modular parser\n        epochs, satellites, clock_offsets = parse_clk_file(self.fpath)\n\n        # Convert units (microseconds → seconds)\n        clock_offsets = (UREG.microsecond * clock_offsets).to(\"s\")\n\n        if self.dimensionless:\n            clock_offsets = clock_offsets.magnitude\n\n        # Create dataset\n        ds = xr.Dataset(\n            data_vars={\"clock_offset\": ((\"epoch\", \"sv\"), clock_offsets)},\n            coords={\n                \"epoch\": np.array(epochs, dtype=\"datetime64[ns]\"),\n                \"sv\": np.array(satellites),\n            },\n        )\n\n        # Validate and add metadata\n        validation = validate_clk_dataset(ds)\n        ds = self._prepare_dataset(ds, validation)\n\n        return ds\n\n    def _prepare_dataset(self, ds: xr.Dataset, validation: dict) -&gt; xr.Dataset:\n        \"\"\"Add metadata and attributes to dataset.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset from parsing.\n        validation : dict\n            Validation results dictionary.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with complete metadata.\n        \"\"\"\n        ds.attrs = {\n            \"file\": str(self.fpath.name),\n            \"agency\": self.agency,\n            \"product_type\": self.product_type,\n            \"ftp_server\": self.ftp_server,\n            \"date\": self.date,\n            \"valid_data_percent\": validation[\"valid_data_percent\"],\n            \"num_epochs\": validation[\"num_epochs\"],\n            \"num_satellites\": validation[\"num_satellites\"],\n        }\n\n        # Add variable attributes\n        ds.clock_offset.attrs = {\n            \"long_name\": \"Satellite clock offset\",\n            \"standard_name\": \"clock_offset\",\n            \"units\": \"seconds\",\n            \"description\": \"Clock correction for each satellite\",\n        }\n\n        return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile.__post_init__","level":3,"title":"<code>__post_init__()</code>","text":"<p>Initialize CLK file handler.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize CLK file handler.\"\"\"\n    self.file_type = [\"clock\"]\n    self.local_dir = Path(self.local_dir)\n    self.local_dir.mkdir(parents=True, exist_ok=True)\n    super().__post_init__()\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile.get_interpolation_strategy","level":3,"title":"<code>get_interpolation_strategy()</code>","text":"<p>Get appropriate interpolation strategy for CLK files.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>def get_interpolation_strategy(self) -&gt; Interpolator:\n    \"\"\"Get appropriate interpolation strategy for CLK files.\"\"\"\n    config = ClockConfig(\n        window_size=9,\n        jump_threshold=1e-6,\n    )\n    return ClockInterpolationStrategy(config=config)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile.generate_filename_based_on_type","level":3,"title":"<code>generate_filename_based_on_type()</code>","text":"<p>Generate standard CLK filename using product registry.</p> <p>Uses product registry to get correct prefix for the agency/product combination. Filename format: {PREFIX}_{YYYYDOY}0000_01D_30S_CLK.CLK</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile.generate_filename_based_on_type--returns","level":5,"title":"Returns","text":"<p>Path     Filename according to CLK conventions.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile.generate_filename_based_on_type--raises","level":5,"title":"Raises","text":"<p>ValueError     If agency/product combination not in registry.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>def generate_filename_based_on_type(self) -&gt; Path:\n    \"\"\"Generate standard CLK filename using product registry.\n\n    Uses product registry to get correct prefix for the agency/product combination.\n    Filename format: {PREFIX}_{YYYYDOY}0000_01D_30S_CLK.CLK\n\n    Returns\n    -------\n    Path\n        Filename according to CLK conventions.\n\n    Raises\n    ------\n    ValueError\n        If agency/product combination not in registry.\n    \"\"\"\n    # Get product spec from registry\n    spec = get_product_spec(self.agency, self.product_type)\n\n    # CLK files use 30S sampling for most products\n    sampling = \"30S\"  # Could be made configurable via registry\n\n    return Path(f\"{spec.prefix}_{self.date}0000_01D_{sampling}_CLK.CLK\")\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile.download_aux_file","level":3,"title":"<code>download_aux_file()</code>","text":"<p>Download CLK file from FTP server with automatic fallback.</p> <p>Constructs URL using product registry path pattern. Uses GPS week for directory structure.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile.download_aux_file--raises","level":5,"title":"Raises","text":"<p>RuntimeError     If file cannot be downloaded from any available server. ValueError     If GPS week calculation fails.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>def download_aux_file(self) -&gt; None:\n    \"\"\"Download CLK file from FTP server with automatic fallback.\n\n    Constructs URL using product registry path pattern.\n    Uses GPS week for directory structure.\n\n    Raises\n    ------\n    RuntimeError\n        If file cannot be downloaded from any available server.\n    ValueError\n        If GPS week calculation fails.\n    \"\"\"\n    clock_file = self.generate_filename_based_on_type()\n    gps_week = get_gps_week_from_filename(clock_file)\n\n    # Get product spec from registry\n    spec = get_product_spec(self.agency, self.product_type)\n\n    # Use product spec's path pattern\n    ftp_path = spec.ftp_path_pattern.format(\n        gps_week=gps_week,\n        file=f\"{clock_file}.gz\",\n    )\n\n    full_url = f\"{self.ftp_server}{ftp_path}\"\n    destination = self.local_dir / clock_file\n\n    # File info for NASA CDDIS URL construction\n    file_info = {\n        \"gps_week\": gps_week,\n        \"filename\": clock_file,\n        \"type\": \"clock\",\n        \"agency\": self.agency,\n        \"latency\": spec.latency_hours,\n    }\n\n    try:\n        self.download_file(full_url, destination, file_info)\n        print(f\"Downloaded clock file for {self.agency} on date {self.date}\")\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to download CLK file from all available servers: {str(e)}\"\n        )\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile.read_file","level":3,"title":"<code>read_file()</code>","text":"<p>Read and parse CLK file into xarray Dataset.</p> <p>Uses modular parser for data extraction and validator for quality checks. Applies unit conversion from microseconds to seconds.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.ClkFile.read_file--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Clock offsets with dimensions (epoch, sv). Values are in seconds     (or dimensionless if specified).</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/reader.py</code> <pre><code>def read_file(self) -&gt; xr.Dataset:\n    \"\"\"Read and parse CLK file into xarray Dataset.\n\n    Uses modular parser for data extraction and validator for quality checks.\n    Applies unit conversion from microseconds to seconds.\n\n    Returns\n    -------\n    xr.Dataset\n        Clock offsets with dimensions (epoch, sv). Values are in seconds\n        (or dimensionless if specified).\n    \"\"\"\n    # Parse file using modular parser\n    epochs, satellites, clock_offsets = parse_clk_file(self.fpath)\n\n    # Convert units (microseconds → seconds)\n    clock_offsets = (UREG.microsecond * clock_offsets).to(\"s\")\n\n    if self.dimensionless:\n        clock_offsets = clock_offsets.magnitude\n\n    # Create dataset\n    ds = xr.Dataset(\n        data_vars={\"clock_offset\": ((\"epoch\", \"sv\"), clock_offsets)},\n        coords={\n            \"epoch\": np.array(epochs, dtype=\"datetime64[ns]\"),\n            \"sv\": np.array(satellites),\n        },\n    )\n\n    # Validate and add metadata\n    validation = validate_clk_dataset(ds)\n    ds = self._prepare_dataset(ds, validation)\n\n    return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.parse_clk_data","level":2,"title":"<code>parse_clk_data(file_handle)</code>","text":"<p>Parse CLK data records using two-pass strategy.</p> Two-Pass Strategy <p>Pass 1: Collect all unique epochs and satellites Pass 2: Fill data arrays with clock offsets</p> <p>This ensures we capture all satellites, even if they're not in the header, and handles satellites appearing/disappearing during the time period.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.parse_clk_data--parameters","level":4,"title":"Parameters","text":"<p>file_handle : TextIO     Open file object positioned after header.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.parse_clk_data--returns","level":4,"title":"Returns","text":"<p>tuple[list[datetime.datetime], list[str], np.ndarray]     (epochs, satellites, clock_offsets) where:     - epochs: list of datetime objects     - satellites: sorted list of satellite codes     - clock_offsets: 2D array (epochs × satellites) in microseconds</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/parser.py</code> <pre><code>def parse_clk_data(\n    file_handle: TextIO,\n) -&gt; tuple[list[datetime.datetime], list[str], np.ndarray]:\n    \"\"\"Parse CLK data records using two-pass strategy.\n\n    Two-Pass Strategy:\n        Pass 1: Collect all unique epochs and satellites\n        Pass 2: Fill data arrays with clock offsets\n\n    This ensures we capture all satellites, even if they're not in the header,\n    and handles satellites appearing/disappearing during the time period.\n\n    Parameters\n    ----------\n    file_handle : TextIO\n        Open file object positioned after header.\n\n    Returns\n    -------\n    tuple[list[datetime.datetime], list[str], np.ndarray]\n        (epochs, satellites, clock_offsets) where:\n        - epochs: list of datetime objects\n        - satellites: sorted list of satellite codes\n        - clock_offsets: 2D array (epochs × satellites) in microseconds\n    \"\"\"\n    # First pass: collect all epochs and satellites\n    epochs = []\n    satellites = set()\n    clock_records = []\n    current_epoch = None\n\n    for line in file_handle:\n        if line.startswith(\"AS\"):\n            parts = line.split()\n\n            # Parse epoch\n            epoch = datetime.datetime(\n                year=int(parts[2]),\n                month=int(parts[3]),\n                day=int(parts[4]),\n                hour=int(parts[5]),\n                minute=int(parts[6]),\n                second=int(float(parts[7])),\n            )\n\n            # Store unique epochs and satellites\n            if epoch != current_epoch:\n                current_epoch = epoch\n                epochs.append(epoch)\n\n            sv_code = parts[1]\n            satellites.add(sv_code)\n\n            # Store complete record\n            clock_offset = float(parts[9])  # microseconds\n            clock_records.append((epoch, sv_code, clock_offset))\n\n    # Create lookup dictionaries for efficient indexing\n    epoch_idx = {epoch: i for i, epoch in enumerate(epochs)}\n    sv_list = sorted(satellites)\n    sv_idx = {sv: i for i, sv in enumerate(sv_list)}\n\n    # Pre-allocate array with NaN\n    shape = (len(epochs), len(sv_list))\n    clock_offsets = np.full(shape, np.nan)\n\n    # Fill array using collected records\n    for epoch, sv, offset in clock_records:\n        i = epoch_idx[epoch]\n        j = sv_idx[sv]\n        clock_offsets[i, j] = offset\n\n    return epochs, sv_list, clock_offsets\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.parse_clk_file","level":2,"title":"<code>parse_clk_file(filepath)</code>","text":"<p>Parse complete CLK file.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.parse_clk_file--parameters","level":4,"title":"Parameters","text":"<p>filepath : Path     Path to CLK file.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.parse_clk_file--returns","level":4,"title":"Returns","text":"<p>tuple[list[datetime.datetime], list[str], np.ndarray]     (epochs, satellites, clock_offsets).</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/parser.py</code> <pre><code>def parse_clk_file(\n    filepath: Path,\n) -&gt; tuple[list[datetime.datetime], list[str], np.ndarray]:\n    \"\"\"Parse complete CLK file.\n\n    Parameters\n    ----------\n    filepath : Path\n        Path to CLK file.\n\n    Returns\n    -------\n    tuple[list[datetime.datetime], list[str], np.ndarray]\n        (epochs, satellites, clock_offsets).\n    \"\"\"\n    with filepath.open() as f:\n        # Parse header (skip it, we get satellites from data anyway)\n        _ = parse_clk_header(f)\n\n        # Parse data\n        epochs, satellites, clock_offsets = parse_clk_data(f)\n\n    return epochs, satellites, clock_offsets\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.parse_clk_header","level":2,"title":"<code>parse_clk_header(file_handle)</code>","text":"<p>Parse CLK file header to extract satellite list.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.parse_clk_header--parameters","level":4,"title":"Parameters","text":"<p>file_handle : TextIO     Open file object positioned at start.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.parse_clk_header--returns","level":4,"title":"Returns","text":"<p>set[str]     Satellite identifiers (e.g., \"G01\", \"R01\").</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.parse_clk_header--raises","level":4,"title":"Raises","text":"<p>ValueError     If header format is invalid.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/parser.py</code> <pre><code>def parse_clk_header(file_handle: TextIO) -&gt; set[str]:\n    \"\"\"Parse CLK file header to extract satellite list.\n\n    Parameters\n    ----------\n    file_handle : TextIO\n        Open file object positioned at start.\n\n    Returns\n    -------\n    set[str]\n        Satellite identifiers (e.g., \"G01\", \"R01\").\n\n    Raises\n    ------\n    ValueError\n        If header format is invalid.\n    \"\"\"\n    satellites = set()\n\n    for line in file_handle:\n        if \"OF SOLN SATS\" in line:\n            _ = int(line[4:6])\n        elif \"PRN LIST\" in line:\n            # Parse PRN list line(s)\n            parts = line.split()[2:]  # Skip 'PRN' and 'LIST'\n            for prn in parts:\n                if len(prn) == 3:  # Valid PRN format (e.g., G01)\n                    satellites.add(prn)\n        elif \"END OF HEADER\" in line:\n            break\n\n    return satellites\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.check_clk_data_quality","level":2,"title":"<code>check_clk_data_quality(ds, min_coverage=80.0)</code>","text":"<p>Check if CLK data meets minimum quality requirements.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.check_clk_data_quality--parameters","level":4,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset from a CLK file. min_coverage : float, default 80.0     Minimum required data coverage percentage.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.check_clk_data_quality--returns","level":4,"title":"Returns","text":"<p>bool     True if data quality is acceptable.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/validator.py</code> <pre><code>def check_clk_data_quality(ds: xr.Dataset, min_coverage: float = 80.0) -&gt; bool:\n    \"\"\"Check if CLK data meets minimum quality requirements.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset from a CLK file.\n    min_coverage : float, default 80.0\n        Minimum required data coverage percentage.\n\n    Returns\n    -------\n    bool\n        True if data quality is acceptable.\n    \"\"\"\n    results = validate_clk_dataset(ds)\n\n    # Must have all required components\n    if not (results[\"has_clock_offset\"] and results[\"has_epoch\"] and results[\"has_sv\"]):\n        return False\n\n    # Must have monotonic epochs\n    if not results[\"epochs_monotonic\"]:\n        return False\n\n    # Must meet minimum coverage requirement\n    if results[\"valid_data_percent\"] &lt; min_coverage:\n        return False\n\n    return True\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.validate_clk_dataset","level":2,"title":"<code>validate_clk_dataset(ds)</code>","text":"<p>Validate CLK dataset structure and data quality.</p> Checks <ul> <li>Required variable exists (clock_offset)</li> <li>Required coordinates exist (epoch, sv)</li> <li>Data completeness (percentage of valid values)</li> <li>Temporal consistency (monotonic epochs)</li> </ul>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.validate_clk_dataset--parameters","level":4,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset from a CLK file.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.clock.validate_clk_dataset--returns","level":4,"title":"Returns","text":"<p>dict[str, bool | float | int]     Validation results with keys:     - has_clock_offset     - has_epoch     - has_sv     - valid_data_percent     - epochs_monotonic     - num_epochs     - num_satellites</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/clock/validator.py</code> <pre><code>def validate_clk_dataset(ds: xr.Dataset) -&gt; dict[str, bool | float | int]:\n    \"\"\"Validate CLK dataset structure and data quality.\n\n    Checks:\n        - Required variable exists (clock_offset)\n        - Required coordinates exist (epoch, sv)\n        - Data completeness (percentage of valid values)\n        - Temporal consistency (monotonic epochs)\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset from a CLK file.\n\n    Returns\n    -------\n    dict[str, bool | float | int]\n        Validation results with keys:\n        - has_clock_offset\n        - has_epoch\n        - has_sv\n        - valid_data_percent\n        - epochs_monotonic\n        - num_epochs\n        - num_satellites\n    \"\"\"\n    results = {}\n\n    # Check required variable\n    results[\"has_clock_offset\"] = \"clock_offset\" in ds.data_vars\n\n    # Check required coordinates\n    results[\"has_epoch\"] = \"epoch\" in ds.coords\n    results[\"has_sv\"] = \"sv\" in ds.coords\n\n    if results[\"has_clock_offset\"]:\n        # Calculate data completeness\n        clock_data = ds[\"clock_offset\"].values\n        total_values = clock_data.size\n        valid_values = np.sum(~np.isnan(clock_data))\n        results[\"valid_data_percent\"] = (valid_values / total_values) * 100\n    else:\n        results[\"valid_data_percent\"] = 0.0\n\n    if results[\"has_epoch\"]:\n        # Check temporal consistency\n        epochs = ds[\"epoch\"].values\n        results[\"epochs_monotonic\"] = np.all(epochs[:-1] &lt;= epochs[1:])\n        results[\"num_epochs\"] = len(epochs)\n    else:\n        results[\"epochs_monotonic\"] = False\n        results[\"num_epochs\"] = 0\n\n    if results[\"has_sv\"]:\n        results[\"num_satellites\"] = len(ds[\"sv\"])\n    else:\n        results[\"num_satellites\"] = 0\n\n    return results\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#position-and-coordinates","level":2,"title":"Position and Coordinates","text":"<p>Position and coordinate transformations for GNSS data.</p> <p>Provides ECEF/geodetic position representations and spherical coordinate computation for satellite-receiver geometry analysis.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition","level":2,"title":"<code>ECEFPosition</code>  <code>dataclass</code>","text":"<p>Earth-Centered, Earth-Fixed (ECEF) position in meters.</p> <p>ECEF is a Cartesian coordinate system with: - Origin at Earth's center of mass - X-axis pointing to 0° latitude, 0° longitude (Prime Meridian at Equator) - Y-axis pointing to 0° latitude, 90° East longitude - Z-axis pointing to North Pole</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition--parameters","level":4,"title":"Parameters","text":"<p>x : float     X coordinate in meters. y : float     Y coordinate in meters. z : float     Z coordinate in meters.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition--examples","level":4,"title":"Examples","text":"Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/position.py</code> <pre><code>@dataclass(frozen=True)\nclass ECEFPosition:\n    \"\"\"Earth-Centered, Earth-Fixed (ECEF) position in meters.\n\n    ECEF is a Cartesian coordinate system with:\n    - Origin at Earth's center of mass\n    - X-axis pointing to 0° latitude, 0° longitude (Prime Meridian at Equator)\n    - Y-axis pointing to 0° latitude, 90° East longitude\n    - Z-axis pointing to North Pole\n\n    Parameters\n    ----------\n    x : float\n        X coordinate in meters.\n    y : float\n        Y coordinate in meters.\n    z : float\n        Z coordinate in meters.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # From RINEX dataset metadata\n    &gt;&gt;&gt; pos = ECEFPosition.from_ds_metadata(rinex_ds)\n    &gt;&gt;&gt; print(f\"X: {pos.x:.3f} m\")\n\n    &gt;&gt;&gt; # Manual creation\n    &gt;&gt;&gt; pos = ECEFPosition(x=4194304.123, y=176481.234, z=4780013.456)\n    &gt;&gt;&gt; lat, lon, alt = pos.to_geodetic()\n    \"\"\"\n\n    x: float  # meters\n    y: float  # meters\n    z: float  # meters\n\n    def to_geodetic(self) -&gt; tuple[float, float, float]:\n        \"\"\"Convert ECEF to geodetic coordinates.\n\n        Returns\n        -------\n        tuple[float, float, float]\n            (latitude, longitude, altitude) where:\n            - latitude: degrees [-90, 90]\n            - longitude: degrees [-180, 180]\n            - altitude: meters above WGS84 ellipsoid\n        \"\"\"\n        lat, lon, alt = pm.ecef2geodetic(self.x, self.y, self.z)\n        return lat, lon, alt\n\n    @classmethod\n    def from_ds_metadata(cls, ds: xr.Dataset) -&gt; Self:\n        \"\"\"Extract ECEF position from RINEX dataset metadata.\n\n        Reads from standard RINEX header attributes.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            RINEX dataset with position in attributes.\n\n        Returns\n        -------\n        ECEFPosition\n            Receiver position in ECEF.\n\n        Raises\n        ------\n        KeyError\n            If position attributes not found in dataset.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from canvod.readers import Rnxv3Obs\n        &gt;&gt;&gt; rnx = Rnxv3Obs(fpath=\"station.24o\")\n        &gt;&gt;&gt; ds = rnx.to_ds()\n        &gt;&gt;&gt; pos = ECEFPosition.from_ds_metadata(ds)\n        \"\"\"\n        # Try different attribute names\n        if \"APPROX POSITION X\" in ds.attrs:\n            # Standard RINEX v3 format\n            x = ds.attrs[\"APPROX POSITION X\"]\n            y = ds.attrs[\"APPROX POSITION Y\"]\n            z = ds.attrs[\"APPROX POSITION Z\"]\n        elif \"Approximate Position\" in ds.attrs:\n            # Alternative format: \"X=..., Y=..., Z=...\"\n            pos = ds.attrs[\"Approximate Position\"]\n            pos_parts = pos.split(\",\")\n\n            def sanitize(s: str) -&gt; float:\n                return float(s.split(\"=\")[1].strip().split()[0])\n\n            x = sanitize(pos_parts[0])\n            y = sanitize(pos_parts[1])\n            z = sanitize(pos_parts[2])\n        else:\n            raise KeyError(\n                \"Position not found in dataset attributes. \"\n                \"Expected 'APPROX POSITION X/Y/Z' or 'Approximate Position'\"\n            )\n\n        return cls(x=x, y=y, z=z)\n\n    def __repr__(self) -&gt; str:\n        return f\"ECEFPosition(x={self.x:.3f}, y={self.y:.3f}, z={self.z:.3f})\"\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition--from-rinex-dataset-metadata","level":3,"title":"From RINEX dataset metadata","text":"<p>pos = ECEFPosition.from_ds_metadata(rinex_ds) print(f\"X: {pos.x:.3f} m\")</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition--manual-creation","level":3,"title":"Manual creation","text":"<p>pos = ECEFPosition(x=4194304.123, y=176481.234, z=4780013.456) lat, lon, alt = pos.to_geodetic()</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition.to_geodetic","level":3,"title":"<code>to_geodetic()</code>","text":"<p>Convert ECEF to geodetic coordinates.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition.to_geodetic--returns","level":5,"title":"Returns","text":"<p>tuple[float, float, float]     (latitude, longitude, altitude) where:     - latitude: degrees [-90, 90]     - longitude: degrees [-180, 180]     - altitude: meters above WGS84 ellipsoid</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/position.py</code> <pre><code>def to_geodetic(self) -&gt; tuple[float, float, float]:\n    \"\"\"Convert ECEF to geodetic coordinates.\n\n    Returns\n    -------\n    tuple[float, float, float]\n        (latitude, longitude, altitude) where:\n        - latitude: degrees [-90, 90]\n        - longitude: degrees [-180, 180]\n        - altitude: meters above WGS84 ellipsoid\n    \"\"\"\n    lat, lon, alt = pm.ecef2geodetic(self.x, self.y, self.z)\n    return lat, lon, alt\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition.from_ds_metadata","level":3,"title":"<code>from_ds_metadata(ds)</code>  <code>classmethod</code>","text":"<p>Extract ECEF position from RINEX dataset metadata.</p> <p>Reads from standard RINEX header attributes.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition.from_ds_metadata--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     RINEX dataset with position in attributes.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition.from_ds_metadata--returns","level":5,"title":"Returns","text":"<p>ECEFPosition     Receiver position in ECEF.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition.from_ds_metadata--raises","level":5,"title":"Raises","text":"<p>KeyError     If position attributes not found in dataset.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.ECEFPosition.from_ds_metadata--examples","level":5,"title":"Examples","text":"<p>from canvod.readers import Rnxv3Obs rnx = Rnxv3Obs(fpath=\"station.24o\") ds = rnx.to_ds() pos = ECEFPosition.from_ds_metadata(ds)</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/position.py</code> <pre><code>@classmethod\ndef from_ds_metadata(cls, ds: xr.Dataset) -&gt; Self:\n    \"\"\"Extract ECEF position from RINEX dataset metadata.\n\n    Reads from standard RINEX header attributes.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        RINEX dataset with position in attributes.\n\n    Returns\n    -------\n    ECEFPosition\n        Receiver position in ECEF.\n\n    Raises\n    ------\n    KeyError\n        If position attributes not found in dataset.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.readers import Rnxv3Obs\n    &gt;&gt;&gt; rnx = Rnxv3Obs(fpath=\"station.24o\")\n    &gt;&gt;&gt; ds = rnx.to_ds()\n    &gt;&gt;&gt; pos = ECEFPosition.from_ds_metadata(ds)\n    \"\"\"\n    # Try different attribute names\n    if \"APPROX POSITION X\" in ds.attrs:\n        # Standard RINEX v3 format\n        x = ds.attrs[\"APPROX POSITION X\"]\n        y = ds.attrs[\"APPROX POSITION Y\"]\n        z = ds.attrs[\"APPROX POSITION Z\"]\n    elif \"Approximate Position\" in ds.attrs:\n        # Alternative format: \"X=..., Y=..., Z=...\"\n        pos = ds.attrs[\"Approximate Position\"]\n        pos_parts = pos.split(\",\")\n\n        def sanitize(s: str) -&gt; float:\n            return float(s.split(\"=\")[1].strip().split()[0])\n\n        x = sanitize(pos_parts[0])\n        y = sanitize(pos_parts[1])\n        z = sanitize(pos_parts[2])\n    else:\n        raise KeyError(\n            \"Position not found in dataset attributes. \"\n            \"Expected 'APPROX POSITION X/Y/Z' or 'Approximate Position'\"\n        )\n\n    return cls(x=x, y=y, z=z)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.GeodeticPosition","level":2,"title":"<code>GeodeticPosition</code>  <code>dataclass</code>","text":"<p>Geodetic (WGS84) position.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.GeodeticPosition--parameters","level":4,"title":"Parameters","text":"<p>lat : float     Latitude in degrees [-90, 90]. lon : float     Longitude in degrees [-180, 180]. alt : float     Altitude in meters above WGS84 ellipsoid.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.GeodeticPosition--examples","level":4,"title":"Examples","text":"<p>pos = GeodeticPosition(lat=48.208, lon=16.373, alt=200.0) print(f\"Vienna: {pos.lat}°N, {pos.lon}°E, {pos.alt}m\")</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/position.py</code> <pre><code>@dataclass(frozen=True)\nclass GeodeticPosition:\n    \"\"\"Geodetic (WGS84) position.\n\n    Parameters\n    ----------\n    lat : float\n        Latitude in degrees [-90, 90].\n    lon : float\n        Longitude in degrees [-180, 180].\n    alt : float\n        Altitude in meters above WGS84 ellipsoid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pos = GeodeticPosition(lat=48.208, lon=16.373, alt=200.0)\n    &gt;&gt;&gt; print(f\"Vienna: {pos.lat}°N, {pos.lon}°E, {pos.alt}m\")\n    \"\"\"\n\n    lat: float  # degrees\n    lon: float  # degrees\n    alt: float  # meters\n\n    def to_ecef(self) -&gt; ECEFPosition:\n        \"\"\"Convert geodetic to ECEF coordinates.\n\n        Returns\n        -------\n        ECEFPosition\n            Position in ECEF frame.\n        \"\"\"\n        x, y, z = pm.geodetic2ecef(self.lat, self.lon, self.alt)\n        return ECEFPosition(x=x, y=y, z=z)\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"GeodeticPosition(lat={self.lat:.6f}°, \"\n            f\"lon={self.lon:.6f}°, alt={self.alt:.1f}m)\"\n        )\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.GeodeticPosition.to_ecef","level":3,"title":"<code>to_ecef()</code>","text":"<p>Convert geodetic to ECEF coordinates.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.GeodeticPosition.to_ecef--returns","level":5,"title":"Returns","text":"<p>ECEFPosition     Position in ECEF frame.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/position.py</code> <pre><code>def to_ecef(self) -&gt; ECEFPosition:\n    \"\"\"Convert geodetic to ECEF coordinates.\n\n    Returns\n    -------\n    ECEFPosition\n        Position in ECEF frame.\n    \"\"\"\n    x, y, z = pm.geodetic2ecef(self.lat, self.lon, self.alt)\n    return ECEFPosition(x=x, y=y, z=z)\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.add_spherical_coords_to_dataset","level":2,"title":"<code>add_spherical_coords_to_dataset(ds, r, theta, phi)</code>","text":"<p>Add spherical coordinates to xarray Dataset with proper metadata.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.add_spherical_coords_to_dataset--parameters","level":4,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with 'epoch' and 'sid' dimensions. r : np.ndarray     Radial distances in meters. theta : np.ndarray     Polar angles in radians [0, π]. phi : np.ndarray     Azimuthal angles in radians [0, 2π).</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.add_spherical_coords_to_dataset--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Dataset with phi, theta, r variables added.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.add_spherical_coords_to_dataset--notes","level":4,"title":"Notes","text":"<p>Variables are added with CF-compliant attributes following physics convention.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.add_spherical_coords_to_dataset--examples","level":4,"title":"Examples","text":"Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/spherical_coords.py</code> <pre><code>def add_spherical_coords_to_dataset(\n    ds: xr.Dataset,\n    r: np.ndarray,\n    theta: np.ndarray,\n    phi: np.ndarray,\n) -&gt; xr.Dataset:\n    \"\"\"Add spherical coordinates to xarray Dataset with proper metadata.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with 'epoch' and 'sid' dimensions.\n    r : np.ndarray\n        Radial distances in meters.\n    theta : np.ndarray\n        Polar angles in radians [0, π].\n    phi : np.ndarray\n        Azimuthal angles in radians [0, 2π).\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with phi, theta, r variables added.\n\n    Notes\n    -----\n    Variables are added with CF-compliant attributes following physics\n    convention.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # After computing spherical coordinates\n    &gt;&gt;&gt; r, theta, phi = compute_spherical_coordinates(sat_x, sat_y, sat_z, rx_pos)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Add to RINEX dataset\n    &gt;&gt;&gt; augmented_ds = add_spherical_coords_to_dataset(rinex_ds, r, theta, phi)\n    &gt;&gt;&gt; print(augmented_ds.phi.attrs['description'])\n    \"\"\"\n    ds = ds.assign(\n        {\n            \"phi\": xr.DataArray(\n                phi,\n                coords=[ds[\"epoch\"], ds[\"sid\"]],\n                dims=[\"epoch\", \"sid\"],\n                attrs={\n                    \"long_name\": \"Azimuthal angle (navigation convention)\",\n                    \"short_name\": \"φ\",\n                    \"units\": \"rad\",\n                    \"description\": (\n                        \"Azimuthal angle from North in ENU frame, clockwise\"\n                    ),\n                    \"valid_range\": [0.0, 2 * np.pi],\n                    \"convention\": \"navigation (0=North, π/2=East, π=South, 3π/2=West)\",\n                },\n            ),\n            \"theta\": xr.DataArray(\n                theta,\n                coords=[ds[\"epoch\"], ds[\"sid\"]],\n                dims=[\"epoch\", \"sid\"],\n                attrs={\n                    \"long_name\": \"Polar angle (physics convention)\",\n                    \"short_name\": \"θ\",\n                    \"units\": \"rad\",\n                    \"description\": \"Polar angle from zenith (+z/Up)\",\n                    \"valid_range\": [0.0, np.pi / 2],\n                    \"convention\": \"physics (0=zenith, π/2=horizon)\",\n                },\n            ),\n            \"r\": xr.DataArray(\n                r,\n                coords=[ds[\"epoch\"], ds[\"sid\"]],\n                dims=[\"epoch\", \"sid\"],\n                attrs={\n                    \"long_name\": \"Distance\",\n                    \"short_name\": \"r\",\n                    \"units\": \"m\",\n                    \"description\": \"Distance between satellite and receiver\",\n                },\n            ),\n        }\n    )\n    return ds\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.add_spherical_coords_to_dataset--after-computing-spherical-coordinates","level":3,"title":"After computing spherical coordinates","text":"<p>r, theta, phi = compute_spherical_coordinates(sat_x, sat_y, sat_z, rx_pos)</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.add_spherical_coords_to_dataset--add-to-rinex-dataset","level":3,"title":"Add to RINEX dataset","text":"<p>augmented_ds = add_spherical_coords_to_dataset(rinex_ds, r, theta, phi) print(augmented_ds.phi.attrs['description'])</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.compute_spherical_coordinates","level":2,"title":"<code>compute_spherical_coordinates(sat_x, sat_y, sat_z, rx_pos)</code>","text":"<p>Compute spherical coordinates (r, theta, phi) in navigation convention.</p> <p>Uses local ENU (East-North-Up) topocentric frame centered at receiver.</p> <p>Navigation Convention: - theta: Polar angle from +z axis (zenith), [0, π] radians   * theta = 0 → zenith (straight up)   * theta = π/2 → horizon   * theta &gt; π/2 → below horizon (set to NaN) - phi: Azimuthal angle from North, clockwise, [0, 2π) radians   * phi = 0 → North   * phi = π/2 → East   * phi = π → South   * phi = 3π/2 → West - r: Radial distance in meters</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.compute_spherical_coordinates--note","level":4,"title":"Note","text":"<p>The phi convention follows geographic/navigation standards where: - 0° points North (positive Y in ENU frame) - Angles increase clockwise when viewed from above - This is computed as arctan2(East, North), giving North=0° reference - Differs from physics convention which uses East=0° reference</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.compute_spherical_coordinates--parameters","level":4,"title":"Parameters","text":"<p>sat_x : np.ndarray     Satellite X coordinates in ECEF (meters). sat_y : np.ndarray     Satellite Y coordinates in ECEF (meters). sat_z : np.ndarray     Satellite Z coordinates in ECEF (meters). rx_pos : ECEFPosition     Receiver position in ECEF.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.compute_spherical_coordinates--returns","level":4,"title":"Returns","text":"<p>tuple[np.ndarray, np.ndarray, np.ndarray]     (r, theta, phi) where:     - r: distances in meters     - theta: polar angles in radians [0, π]     - phi: azimuthal angles in radians [0, 2π)</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.compute_spherical_coordinates--notes","level":4,"title":"Notes","text":"<p>Satellites below horizon (theta &gt; π/2) are set to NaN.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.compute_spherical_coordinates--examples","level":4,"title":"Examples","text":"<p>from canvod.auxiliary.position import ECEFPosition, compute_spherical_coordinates</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/position/spherical_coords.py</code> <pre><code>def compute_spherical_coordinates(\n    sat_x: np.ndarray,\n    sat_y: np.ndarray,\n    sat_z: np.ndarray,\n    rx_pos: ECEFPosition,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Compute spherical coordinates (r, theta, phi) in navigation convention.\n\n    Uses local ENU (East-North-Up) topocentric frame centered at receiver.\n\n    Navigation Convention:\n    - theta: Polar angle from +z axis (zenith), [0, π] radians\n      * theta = 0 → zenith (straight up)\n      * theta = π/2 → horizon\n      * theta &gt; π/2 → below horizon (set to NaN)\n    - phi: Azimuthal angle from North, clockwise, [0, 2π) radians\n      * phi = 0 → North\n      * phi = π/2 → East\n      * phi = π → South\n      * phi = 3π/2 → West\n    - r: Radial distance in meters\n\n    Note\n    ----\n    The phi convention follows geographic/navigation standards where:\n    - 0° points North (positive Y in ENU frame)\n    - Angles increase clockwise when viewed from above\n    - This is computed as arctan2(East, North), giving North=0° reference\n    - Differs from physics convention which uses East=0° reference\n\n    Parameters\n    ----------\n    sat_x : np.ndarray\n        Satellite X coordinates in ECEF (meters).\n    sat_y : np.ndarray\n        Satellite Y coordinates in ECEF (meters).\n    sat_z : np.ndarray\n        Satellite Z coordinates in ECEF (meters).\n    rx_pos : ECEFPosition\n        Receiver position in ECEF.\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray, np.ndarray]\n        (r, theta, phi) where:\n        - r: distances in meters\n        - theta: polar angles in radians [0, π]\n        - phi: azimuthal angles in radians [0, 2π)\n\n    Notes\n    -----\n    Satellites below horizon (theta &gt; π/2) are set to NaN.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.auxiliary.position import ECEFPosition, compute_spherical_coordinates\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Receiver position\n    &gt;&gt;&gt; rx = ECEFPosition(x=4194304.0, y=176481.0, z=4780013.0)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Satellite positions (example)\n    &gt;&gt;&gt; sat_x = np.array([16364123.0, 10205789.0])\n    &gt;&gt;&gt; sat_y = np.array([12123456.0, -8901234.0])\n    &gt;&gt;&gt; sat_z = np.array([18456789.0, 21234567.0])\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; r, theta, phi = compute_spherical_coordinates(sat_x, sat_y, sat_z, rx)\n    &gt;&gt;&gt; print(f\"Distance: {r[0]/1e6:.2f} Mm\")\n    &gt;&gt;&gt; print(f\"Polar angle: {np.degrees(theta[0]):.1f}°\")\n    &gt;&gt;&gt; print(f\"Azimuth: {np.degrees(phi[0]):.1f}°\")\n    \"\"\"\n    # Receiver ECEF coordinates\n    rx_x = rx_pos.x\n    rx_y = rx_pos.y\n    rx_z = rx_pos.z\n\n    # Convert receiver ECEF to geodetic (lat, lon, alt)\n    lat, lon, alt = pm.ecef2geodetic(rx_x, rx_y, rx_z)\n\n    # Convert satellite ECEF to ENU (East-North-Up) relative to receiver\n    e, n, u = pm.ecef2enu(sat_x, sat_y, sat_z, lat, lon, alt)\n\n    # Compute radial distance\n    r = np.sqrt(e**2 + n**2 + u**2)\n\n    # Compute theta: polar angle from +z (Up) axis\n    # Clamp u/r to [-1, 1] to handle numerical errors\n    cos_theta = np.clip(u / r, -1.0, 1.0)\n    theta = np.arccos(cos_theta)\n\n    # Mask satellites below horizon (u &lt; 0 means below horizon)\n    below_horizon = u &lt; 0\n\n    # Compute phi: azimuthal angle from North, clockwise (navigation convention)\n    # phi = arctan2(East, North) gives North=0°, East=90°, South=180°, West=270°\n    phi = np.arctan2(e, n)\n    phi = np.mod(phi, 2 * np.pi)  # Wrap to [0, 2π)\n\n    # Set below-horizon satellites to NaN\n    r = np.where(below_horizon, np.nan, r)\n    theta = np.where(below_horizon, np.nan, theta)\n    phi = np.where(below_horizon, np.nan, phi)\n\n    return r, theta, phi\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.compute_spherical_coordinates--receiver-position","level":3,"title":"Receiver position","text":"<p>rx = ECEFPosition(x=4194304.0, y=176481.0, z=4780013.0)</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.position.compute_spherical_coordinates--satellite-positions-example","level":3,"title":"Satellite positions (example)","text":"<p>sat_x = np.array([16364123.0, 10205789.0]) sat_y = np.array([12123456.0, -8901234.0]) sat_z = np.array([18456789.0, 21234567.0])</p> <p>r, theta, phi = compute_spherical_coordinates(sat_x, sat_y, sat_z, rx) print(f\"Distance: {r[0]/1e6:.2f} Mm\") print(f\"Polar angle: {np.degrees(theta[0]):.1f}°\") print(f\"Azimuth: {np.degrees(phi[0]):.1f}°\")</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#product-registry","level":2,"title":"Product Registry","text":"<p>Product registry and specifications for IGS analysis centers.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.products.PRODUCT_REGISTRY","level":2,"title":"<code>PRODUCT_REGISTRY = {('COD', 'final'): ProductSpec(agency_code='COD', agency_name='Center for Orbit Determination in Europe', product_type='final', prefix='COD0MGXFIN', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=168, description='CODE final multi-GNSS orbits and clocks'), ('COD', 'rapid'): ProductSpec(agency_code='COD', agency_name='Center for Orbit Determination in Europe', product_type='rapid', prefix='COD0OPSRAP', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=17, description='CODE rapid multi-GNSS orbits and clocks'), ('GFZ', 'final'): ProductSpec(agency_code='GFZ', agency_name='GeoForschungsZentrum Potsdam', product_type='final', prefix='GFZ0MGXFIN', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=336, description='GFZ final multi-GNSS orbits and clocks'), ('GFZ', 'rapid'): ProductSpec(agency_code='GFZ', agency_name='GeoForschungsZentrum Potsdam', product_type='rapid', prefix='GFZ0MGXRAP', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=17, description='GFZ rapid multi-GNSS orbits and clocks'), ('ESA', 'final'): ProductSpec(agency_code='ESA', agency_name='European Space Agency', product_type='final', prefix='ESA0MGXFIN', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=168, description='ESA final multi-GNSS orbits and clocks'), ('ESA', 'rapid'): ProductSpec(agency_code='ESA', agency_name='European Space Agency', product_type='rapid', prefix='ESA0MGXRAP', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=17, description='ESA rapid multi-GNSS orbits and clocks'), ('JPL', 'final'): ProductSpec(agency_code='JPL', agency_name='Jet Propulsion Laboratory', product_type='final', prefix='JPL0MGXFIN', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=240, description='JPL final multi-GNSS orbits and clocks'), ('JPL', 'rapid'): ProductSpec(agency_code='JPL', agency_name='Jet Propulsion Laboratory', product_type='rapid', prefix='JPL0MGXRAP', sampling_rate='15M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=24, description='JPL rapid multi-GNSS orbits and clocks'), ('IGS', 'final'): ProductSpec(agency_code='IGS', agency_name='International GNSS Service', product_type='final', prefix='IGS0MGXFIN', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=336, description='IGS combined final multi-GNSS orbits and clocks'), ('IGS', 'rapid'): ProductSpec(agency_code='IGS', agency_name='International GNSS Service', product_type='rapid', prefix='IGS0MGXRAP', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=17, description='IGS combined rapid multi-GNSS orbits and clocks'), ('WHU', 'final'): ProductSpec(agency_code='WHU', agency_name='Wuhan University', product_type='final', prefix='WHU0MGXFIN', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=168, description='WHU final multi-GNSS orbits and clocks'), ('GRG', 'final'): ProductSpec(agency_code='GRG', agency_name=\"Centre National d'Études Spatiales\", product_type='final', prefix='GRG0MGXFIN', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=192, description='CNES/GRG final multi-GNSS orbits and clocks'), ('SHA', 'final'): ProductSpec(agency_code='SHA', agency_name='Shanghai Observatory', product_type='final', prefix='SHA0MGXFIN', sampling_rate='05M', duration='01D', available_formats=['SP3', 'CLK'], ftp_path_pattern='/products/{gps_week}/{file}', latency_hours=168, description='SHA final multi-GNSS orbits and clocks')}</code>  <code>module-attribute</code>","text":"","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.products.ProductSpec","level":2,"title":"<code>ProductSpec</code>  <code>dataclass</code>","text":"<p>Specification for an IGS product.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.products.ProductSpec--notes","level":4,"title":"Notes","text":"<p>This is a standard dataclass.</p>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.products.ProductSpec--parameters","level":4,"title":"Parameters","text":"<p>agency_code : str     3-letter code (COD, GFZ, ESA, JPL, IGS, etc.). agency_name : str     Full name of the analysis center. product_type : str     Product type (final, rapid, ultrarapid, predicted). prefix : str     Filename prefix following IGS conventions. sampling_rate : str     Temporal resolution (05M, 15M, 30S, etc.). duration : str     File duration (01D, 03D, etc.). available_formats : list[str]     List of available file formats. ftp_path_pattern : str     URL pattern with placeholders. latency_hours : int     Typical latency in hours from epoch. description : str     Optional description.</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/products/registry.py</code> <pre><code>@dataclass\nclass ProductSpec:\n    \"\"\"Specification for an IGS product.\n\n    Notes\n    -----\n    This is a standard dataclass.\n\n    Parameters\n    ----------\n    agency_code : str\n        3-letter code (COD, GFZ, ESA, JPL, IGS, etc.).\n    agency_name : str\n        Full name of the analysis center.\n    product_type : str\n        Product type (final, rapid, ultrarapid, predicted).\n    prefix : str\n        Filename prefix following IGS conventions.\n    sampling_rate : str\n        Temporal resolution (05M, 15M, 30S, etc.).\n    duration : str\n        File duration (01D, 03D, etc.).\n    available_formats : list[str]\n        List of available file formats.\n    ftp_path_pattern : str\n        URL pattern with placeholders.\n    latency_hours : int\n        Typical latency in hours from epoch.\n    description : str\n        Optional description.\n    \"\"\"\n\n    agency_code: str\n    agency_name: str\n    product_type: str\n    prefix: str\n    sampling_rate: str\n    duration: str\n    available_formats: list[str]\n    ftp_path_pattern: str\n    latency_hours: int\n    description: str = \"\"\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.products.get_product_spec","level":2,"title":"<code>get_product_spec(agency, product_type)</code>","text":"<p>Get product specification for a given agency and product type.</p> <p>Parameters:</p> Name Type Description Default <code>agency</code> <code>str</code> <p>Agency code (e.g., 'COD', 'GFZ', 'ESA')</p> required <code>product_type</code> <code>str</code> <p>Product type (e.g., 'final', 'rapid')</p> required <p>Returns:</p> Type Description <code>ProductSpec</code> <p>ProductSpec object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If product is not supported</p> Example <p>spec = get_product_spec('COD', 'final') print(spec.prefix) 'COD0MGXFIN'</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/products/registry.py</code> <pre><code>def get_product_spec(agency: str, product_type: str) -&gt; ProductSpec:\n    \"\"\"\n    Get product specification for a given agency and product type.\n\n    Args:\n        agency: Agency code (e.g., 'COD', 'GFZ', 'ESA')\n        product_type: Product type (e.g., 'final', 'rapid')\n\n    Returns:\n        ProductSpec object\n\n    Raises:\n        ValueError: If product is not supported\n\n    Example:\n        &gt;&gt;&gt; spec = get_product_spec('COD', 'final')\n        &gt;&gt;&gt; print(spec.prefix)\n        'COD0MGXFIN'\n    \"\"\"\n    key = (agency.upper(), product_type.lower())\n    if key not in PRODUCT_REGISTRY:\n        available = list(PRODUCT_REGISTRY.keys())\n        raise ValueError(\n            f\"Product {agency}/{product_type} not supported.\\n\"\n            f\"Available products: {available}\"\n        )\n    return PRODUCT_REGISTRY[key]\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.products.list_available_products","level":2,"title":"<code>list_available_products()</code>","text":"<p>List all available products in the registry.</p> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>List of (agency_code, product_type) tuples</p> Example <p>products = list_available_products() print(products[:3]) [('COD', 'final'), ('COD', 'rapid'), ('GFZ', 'final')]</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/products/registry.py</code> <pre><code>def list_available_products() -&gt; list[tuple[str, str]]:\n    \"\"\"\n    List all available products in the registry.\n\n    Returns:\n        List of (agency_code, product_type) tuples\n\n    Example:\n        &gt;&gt;&gt; products = list_available_products()\n        &gt;&gt;&gt; print(products[:3])\n        [('COD', 'final'), ('COD', 'rapid'), ('GFZ', 'final')]\n    \"\"\"\n    return list(PRODUCT_REGISTRY.keys())\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.products.list_agencies","level":2,"title":"<code>list_agencies()</code>","text":"<p>List all available analysis centers.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of unique agency codes</p> Example <p>agencies = list_agencies() print(agencies) ['COD', 'ESA', 'GFZ', 'GRG', 'IGS', 'JPL', 'SHA', 'WHU']</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/products/registry.py</code> <pre><code>def list_agencies() -&gt; list[str]:\n    \"\"\"\n    List all available analysis centers.\n\n    Returns:\n        Sorted list of unique agency codes\n\n    Example:\n        &gt;&gt;&gt; agencies = list_agencies()\n        &gt;&gt;&gt; print(agencies)\n        ['COD', 'ESA', 'GFZ', 'GRG', 'IGS', 'JPL', 'SHA', 'WHU']\n    \"\"\"\n    return sorted(set(agency for agency, _ in PRODUCT_REGISTRY.keys()))\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-auxiliary/#canvod.auxiliary.products.get_products_for_agency","level":2,"title":"<code>get_products_for_agency(agency)</code>","text":"<p>Get all product types available for a specific agency.</p> <p>Parameters:</p> Name Type Description Default <code>agency</code> <code>str</code> <p>Agency code</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of product types</p> Example <p>products = get_products_for_agency('COD') print(products) ['final', 'rapid']</p> Source code in <code>packages/canvod-auxiliary/src/canvod/auxiliary/products/registry.py</code> <pre><code>def get_products_for_agency(agency: str) -&gt; list[str]:\n    \"\"\"\n    Get all product types available for a specific agency.\n\n    Args:\n        agency: Agency code\n\n    Returns:\n        List of product types\n\n    Example:\n        &gt;&gt;&gt; products = get_products_for_agency('COD')\n        &gt;&gt;&gt; print(products)\n        ['final', 'rapid']\n    \"\"\"\n    agency_upper = agency.upper()\n    return [\n        product_type\n        for (ag, product_type) in PRODUCT_REGISTRY.keys()\n        if ag == agency_upper\n    ]\n</code></pre>","path":["API Reference","canvod.auxiliary API Reference"],"tags":[]},{"location":"api/canvod-grids/","level":1,"title":"canvod.grids API Reference","text":"<p>Hemispheric grid implementations and spatial analysis tools.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#package","level":2,"title":"Package","text":"<p>HEALPix and hemispheric grid operations.</p> <p>Provides hemisphere grid structures for GNSS signal observation analysis.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.BaseGridBuilder","level":2,"title":"<code>BaseGridBuilder</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for hemispherical grid builders.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.BaseGridBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Angular resolution in degrees cutoff_theta : float     Maximum polar angle cutoff in degrees phi_rotation : float     Rotation angle in degrees (applied to all phi values)</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_builder.py</code> <pre><code>class BaseGridBuilder(ABC):\n    \"\"\"Abstract base for hemispherical grid builders.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Angular resolution in degrees\n    cutoff_theta : float\n        Maximum polar angle cutoff in degrees\n    phi_rotation : float\n        Rotation angle in degrees (applied to all phi values)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        angular_resolution: float = 2,\n        cutoff_theta: float = 0,\n        phi_rotation: float = 0,\n    ) -&gt; None:\n        \"\"\"Initialize the grid builder.\n\n        Parameters\n        ----------\n        angular_resolution : float, default 2\n            Angular resolution in degrees.\n        cutoff_theta : float, default 0\n            Maximum polar angle cutoff in degrees.\n        phi_rotation : float, default 0\n            Rotation angle in degrees.\n\n        \"\"\"\n        self.angular_resolution = angular_resolution\n        self.angular_resolution_rad = np.deg2rad(angular_resolution)\n        self.cutoff_theta = cutoff_theta\n        self.cutoff_theta_rad = np.deg2rad(cutoff_theta)\n        self.phi_rotation = phi_rotation\n        self.phi_rotation_rad = np.deg2rad(phi_rotation)\n        self._logger = _get_logger()\n\n    @abstractmethod\n    def _build_grid(\n        self,\n    ) -&gt; (\n        tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]\n        | tuple[\n            pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray], dict[str, Any]\n        ]\n    ):\n        \"\"\"Build grid.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            Grid cells\n        theta_lims : np.ndarray\n            Theta band limits\n        phi_lims : list[np.ndarray]\n            Phi limits per band\n        cell_ids : list[np.ndarray]\n            Cell IDs per band\n        extra_kwargs : dict, optional\n            Additional metadata\n\n        \"\"\"\n\n    @abstractmethod\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Get grid type identifier.\"\"\"\n\n    def build(self) -&gt; GridData:\n        \"\"\"Build hemisphere grid.\n\n        Returns\n        -------\n        GridData\n            Complete grid data structure\n\n        \"\"\"\n        self._logger.info(\n            \"grid_build_started\",\n            grid_type=self.get_grid_type(),\n            angular_resolution=self.angular_resolution,\n        )\n\n        result = self._build_grid()\n\n        if len(result) == 4:\n            grid, theta_lims, phi_lims, cell_ids = result\n            extra_kwargs = {}\n        elif len(result) == 5:\n            grid, theta_lims, phi_lims, cell_ids, extra_kwargs = result\n        else:\n            raise ValueError(f\"Invalid grid builder result: {len(result)} elements\")\n\n        # Apply phi rotation if specified (vectorized operations)\n        if self.phi_rotation_rad != 0:\n            grid = grid.with_columns(\n                [(pl.col(\"phi\") + self.phi_rotation_rad) % (2 * np.pi)]\n            )\n\n            if \"phi_min\" in grid.columns:\n                grid = grid.with_columns(\n                    [\n                        (\n                            (pl.col(\"phi_min\") + self.phi_rotation_rad) % (2 * np.pi)\n                        ).alias(\"phi_min\"),\n                        (\n                            (pl.col(\"phi_max\") + self.phi_rotation_rad) % (2 * np.pi)\n                        ).alias(\"phi_max\"),\n                    ]\n                )\n\n        self._logger.info(\"grid_build_complete\", ncells=len(grid))\n\n        # Merge builder metadata into any extra_kwargs metadata\n        builder_meta = {\n            \"angular_resolution\": self.angular_resolution,\n            \"cutoff_theta\": self.cutoff_theta,\n        }\n        if \"metadata\" in extra_kwargs and extra_kwargs[\"metadata\"]:\n            extra_kwargs[\"metadata\"] = {**builder_meta, **extra_kwargs[\"metadata\"]}\n        else:\n            extra_kwargs[\"metadata\"] = builder_meta\n\n        return GridData(\n            grid=grid,\n            theta_lims=theta_lims,\n            phi_lims=phi_lims,\n            cell_ids=cell_ids,\n            grid_type=self.get_grid_type(),\n            **extra_kwargs,\n        )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.BaseGridBuilder.__init__","level":3,"title":"<code>__init__(angular_resolution=2, cutoff_theta=0, phi_rotation=0)</code>","text":"<p>Initialize the grid builder.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.BaseGridBuilder.__init__--parameters","level":5,"title":"Parameters","text":"<p>angular_resolution : float, default 2     Angular resolution in degrees. cutoff_theta : float, default 0     Maximum polar angle cutoff in degrees. phi_rotation : float, default 0     Rotation angle in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_builder.py</code> <pre><code>def __init__(\n    self,\n    angular_resolution: float = 2,\n    cutoff_theta: float = 0,\n    phi_rotation: float = 0,\n) -&gt; None:\n    \"\"\"Initialize the grid builder.\n\n    Parameters\n    ----------\n    angular_resolution : float, default 2\n        Angular resolution in degrees.\n    cutoff_theta : float, default 0\n        Maximum polar angle cutoff in degrees.\n    phi_rotation : float, default 0\n        Rotation angle in degrees.\n\n    \"\"\"\n    self.angular_resolution = angular_resolution\n    self.angular_resolution_rad = np.deg2rad(angular_resolution)\n    self.cutoff_theta = cutoff_theta\n    self.cutoff_theta_rad = np.deg2rad(cutoff_theta)\n    self.phi_rotation = phi_rotation\n    self.phi_rotation_rad = np.deg2rad(phi_rotation)\n    self._logger = _get_logger()\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.BaseGridBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>  <code>abstractmethod</code>","text":"<p>Get grid type identifier.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_builder.py</code> <pre><code>@abstractmethod\ndef get_grid_type(self) -&gt; str:\n    \"\"\"Get grid type identifier.\"\"\"\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.BaseGridBuilder.build","level":3,"title":"<code>build()</code>","text":"<p>Build hemisphere grid.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.BaseGridBuilder.build--returns","level":5,"title":"Returns","text":"<p>GridData     Complete grid data structure</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_builder.py</code> <pre><code>def build(self) -&gt; GridData:\n    \"\"\"Build hemisphere grid.\n\n    Returns\n    -------\n    GridData\n        Complete grid data structure\n\n    \"\"\"\n    self._logger.info(\n        \"grid_build_started\",\n        grid_type=self.get_grid_type(),\n        angular_resolution=self.angular_resolution,\n    )\n\n    result = self._build_grid()\n\n    if len(result) == 4:\n        grid, theta_lims, phi_lims, cell_ids = result\n        extra_kwargs = {}\n    elif len(result) == 5:\n        grid, theta_lims, phi_lims, cell_ids, extra_kwargs = result\n    else:\n        raise ValueError(f\"Invalid grid builder result: {len(result)} elements\")\n\n    # Apply phi rotation if specified (vectorized operations)\n    if self.phi_rotation_rad != 0:\n        grid = grid.with_columns(\n            [(pl.col(\"phi\") + self.phi_rotation_rad) % (2 * np.pi)]\n        )\n\n        if \"phi_min\" in grid.columns:\n            grid = grid.with_columns(\n                [\n                    (\n                        (pl.col(\"phi_min\") + self.phi_rotation_rad) % (2 * np.pi)\n                    ).alias(\"phi_min\"),\n                    (\n                        (pl.col(\"phi_max\") + self.phi_rotation_rad) % (2 * np.pi)\n                    ).alias(\"phi_max\"),\n                ]\n            )\n\n    self._logger.info(\"grid_build_complete\", ncells=len(grid))\n\n    # Merge builder metadata into any extra_kwargs metadata\n    builder_meta = {\n        \"angular_resolution\": self.angular_resolution,\n        \"cutoff_theta\": self.cutoff_theta,\n    }\n    if \"metadata\" in extra_kwargs and extra_kwargs[\"metadata\"]:\n        extra_kwargs[\"metadata\"] = {**builder_meta, **extra_kwargs[\"metadata\"]}\n    else:\n        extra_kwargs[\"metadata\"] = builder_meta\n\n    return GridData(\n        grid=grid,\n        theta_lims=theta_lims,\n        phi_lims=phi_lims,\n        cell_ids=cell_ids,\n        grid_type=self.get_grid_type(),\n        **extra_kwargs,\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GridData","level":2,"title":"<code>GridData</code>  <code>dataclass</code>","text":"<p>Immutable container for hemispherical grid structure.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GridData--parameters","level":4,"title":"Parameters","text":"<p>grid : pl.DataFrame     Grid cells with phi, theta, and bounds theta_lims : np.ndarray     Theta band limits phi_lims : list[np.ndarray]     Phi limits per theta band cell_ids : list[np.ndarray]     Cell IDs per theta band grid_type : str     Grid type identifier solid_angles : np.ndarray, optional     Solid angles per cell [steradians] metadata : dict, optional     Additional grid metadata voronoi : Any, optional     Voronoi tessellation object (for Fibonacci grids) vertices : np.ndarray, optional     3D vertices (for triangular grids) points_xyz : np.ndarray, optional     3D point cloud (for Fibonacci grids) vertex_phi : np.ndarray, optional     Vertex phi coordinates vertex_theta : np.ndarray, optional     Vertex theta coordinates</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_data.py</code> <pre><code>@dataclass(frozen=True)\nclass GridData:\n    \"\"\"Immutable container for hemispherical grid structure.\n\n    Parameters\n    ----------\n    grid : pl.DataFrame\n        Grid cells with phi, theta, and bounds\n    theta_lims : np.ndarray\n        Theta band limits\n    phi_lims : list[np.ndarray]\n        Phi limits per theta band\n    cell_ids : list[np.ndarray]\n        Cell IDs per theta band\n    grid_type : str\n        Grid type identifier\n    solid_angles : np.ndarray, optional\n        Solid angles per cell [steradians]\n    metadata : dict, optional\n        Additional grid metadata\n    voronoi : Any, optional\n        Voronoi tessellation object (for Fibonacci grids)\n    vertices : np.ndarray, optional\n        3D vertices (for triangular grids)\n    points_xyz : np.ndarray, optional\n        3D point cloud (for Fibonacci grids)\n    vertex_phi : np.ndarray, optional\n        Vertex phi coordinates\n    vertex_theta : np.ndarray, optional\n        Vertex theta coordinates\n\n    \"\"\"\n\n    grid: pl.DataFrame\n    theta_lims: np.ndarray\n    phi_lims: list[np.ndarray]\n    cell_ids: list[np.ndarray]\n    grid_type: str\n    solid_angles: np.ndarray | None = None\n    metadata: dict | None = None\n    voronoi: Any | None = None\n    vertices: np.ndarray | None = None\n    points_xyz: np.ndarray | None = None\n    vertex_phi: np.ndarray | None = None\n    vertex_theta: np.ndarray | None = None\n\n    @property\n    def coords(self) -&gt; pl.DataFrame:\n        \"\"\"Get cell coordinates.\"\"\"\n        return self.grid.select([\"phi\", \"theta\"])\n\n    @property\n    def ncells(self) -&gt; int:\n        \"\"\"Number of cells in grid.\"\"\"\n        return len(self.grid)\n\n    def get_patches(self) -&gt; pl.Series:\n        \"\"\"Create matplotlib patches for polar visualization.\"\"\"\n        patches = [\n            Rectangle(\n                (row[\"phi_min\"], row[\"theta_min\"]),\n                row[\"phi_max\"] - row[\"phi_min\"],\n                row[\"theta_max\"] - row[\"theta_min\"],\n                fill=True,\n            )\n            for row in self.grid.iter_rows(named=True)\n        ]\n        return pl.Series(\"Patches\", patches)\n\n    def get_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Calculate solid angle for each cell [steradians].\"\"\"\n        if self.solid_angles is not None:\n            return self.solid_angles\n\n        # HEALPix\n        if self.grid_type == \"healpix\" and \"healpix_nside\" in self.grid.columns:\n            try:\n                import healpy as hp\n\n                nside = int(self.grid[\"healpix_nside\"][0])\n                return np.full(\n                    len(self.grid), hp.nside2pixarea(nside), dtype=np.float64\n                )\n            except ImportError:\n                pass\n\n        # Geodesic\n        if self.grid_type == \"geodesic\" and \"geodesic_vertices\" in self.grid.columns:\n            return self._compute_geodesic_solid_angles()\n\n        # HTM\n        if self.grid_type == \"htm\" and \"htm_vertex_0\" in self.grid.columns:\n            return self._compute_htm_solid_angles()\n\n        # Fibonacci\n        if self.grid_type == \"fibonacci\" and \"voronoi_region\" in self.grid.columns:\n            return self._compute_voronoi_solid_angles()\n\n        # Default\n        return self._geometric_solid_angles()\n\n    def _compute_htm_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Compute solid angles for HTM triangular cells.\"\"\"\n        solid_angles = []\n\n        for row in self.grid.iter_rows(named=True):\n            v0 = np.array(row[\"htm_vertex_0\"])\n            v1 = np.array(row[\"htm_vertex_1\"])\n            v2 = np.array(row[\"htm_vertex_2\"])\n\n            # Spherical excess formula\n            a = np.arccos(np.clip(np.dot(v1, v2), -1, 1))\n            b = np.arccos(np.clip(np.dot(v0, v2), -1, 1))\n            c = np.arccos(np.clip(np.dot(v0, v1), -1, 1))\n\n            s = (a + b + c) / 2\n            tan_E_4 = np.sqrt(\n                np.tan(s / 2)\n                * np.tan((s - a) / 2)\n                * np.tan((s - b) / 2)\n                * np.tan((s - c) / 2)\n            )\n            E = 4 * np.arctan(tan_E_4)\n\n            solid_angles.append(E)\n\n        return np.array(solid_angles)\n\n    def _compute_geodesic_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Compute solid angles for geodesic triangular cells.\"\"\"\n        vertices = self.vertices\n        if vertices is None:\n            return self._geometric_solid_angles()\n\n        solid_angles = []\n        for row in self.grid.iter_rows(named=True):\n            v_indices = row[\"geodesic_vertices\"]\n            v0, v1, v2 = vertices[v_indices]\n\n            a = np.arccos(np.clip(np.dot(v1, v2), -1, 1))\n            b = np.arccos(np.clip(np.dot(v0, v2), -1, 1))\n            c = np.arccos(np.clip(np.dot(v0, v1), -1, 1))\n\n            s = (a + b + c) / 2\n            tan_E_4 = np.sqrt(\n                np.tan(s / 2)\n                * np.tan((s - a) / 2)\n                * np.tan((s - b) / 2)\n                * np.tan((s - c) / 2)\n            )\n            E = 4 * np.arctan(tan_E_4)\n\n            solid_angles.append(E)\n\n        return np.array(solid_angles)\n\n    def _compute_voronoi_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Compute solid angles for Voronoi cells.\"\"\"\n        if self.voronoi is None:\n            return self._geometric_solid_angles()\n\n        sv = self.voronoi\n        solid_angles = []\n        for row in self.grid.iter_rows(named=True):\n            region = row[\"voronoi_region\"]\n            if len(region) &lt; 3:\n                solid_angles.append(np.nan)\n                continue\n\n            vertices = sv.vertices[region]\n            center = np.array(\n                [\n                    np.sin(row[\"theta\"]) * np.cos(row[\"phi\"]),\n                    np.sin(row[\"theta\"]) * np.sin(row[\"phi\"]),\n                    np.cos(row[\"theta\"]),\n                ]\n            )\n\n            total_angle = 0\n            n = len(vertices)\n            for i in range(n):\n                v1 = vertices[i]\n                v2 = vertices[(i + 1) % n]\n                a = np.arccos(np.clip(np.dot(center, v1), -1, 1))\n                b = np.arccos(np.clip(np.dot(center, v2), -1, 1))\n                c = np.arccos(np.clip(np.dot(v1, v2), -1, 1))\n                s = (a + b + c) / 2\n                tan_E_4 = np.sqrt(\n                    np.tan(s / 2)\n                    * np.tan((s - a) / 2)\n                    * np.tan((s - b) / 2)\n                    * np.tan((s - c) / 2)\n                )\n                E = 4 * np.arctan(tan_E_4)\n                total_angle += E\n            solid_angles.append(total_angle)\n\n        return np.array(solid_angles)\n\n    def _geometric_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Fallback geometric calculation.\"\"\"\n        solid_angles = []\n        for row in self.grid.iter_rows(named=True):\n            delta_phi = row[\"phi_max\"] - row[\"phi_min\"]\n            cos_diff = np.cos(row[\"theta_min\"]) - np.cos(row[\"theta_max\"])\n            omega = delta_phi * cos_diff\n            solid_angles.append(omega)\n        return np.array(solid_angles)\n\n    def get_grid_stats(self) -&gt; dict:\n        \"\"\"Get grid statistics including solid angle uniformity.\"\"\"\n        solid_angles = self.get_solid_angles()\n\n        stats = {\n            \"total_cells\": self.ncells,\n            \"grid_type\": self.grid_type,\n            \"theta_bands\": len(self.theta_lims),\n            \"cells_per_band\": [len(ids) for ids in self.cell_ids],\n            \"solid_angle_mean_sr\": float(np.mean(solid_angles)),\n            \"solid_angle_std_sr\": float(np.std(solid_angles)),\n            \"solid_angle_cv_percent\": float(\n                np.std(solid_angles) / np.mean(solid_angles) * 100\n            ),\n            \"total_solid_angle_sr\": float(np.sum(solid_angles)),\n            \"hemisphere_solid_angle_sr\": 2 * np.pi,\n        }\n\n        # Add HEALPix-specific info\n        if self.grid_type == \"healpix\" and \"healpix_nside\" in self.grid.columns:\n            try:\n                import healpy as hp\n\n                nside = int(self.grid[\"healpix_nside\"][0])\n                stats[\"healpix_nside\"] = nside\n                stats[\"healpix_npix_total\"] = hp.nside2npix(nside)\n                stats[\"healpix_pixel_area_sr\"] = hp.nside2pixarea(nside)\n                stats[\"healpix_resolution_arcmin\"] = hp.nside2resol(nside, arcmin=True)\n            except ImportError:\n                pass\n\n        return stats\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GridData.coords","level":3,"title":"<code>coords</code>  <code>property</code>","text":"<p>Get cell coordinates.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GridData.ncells","level":3,"title":"<code>ncells</code>  <code>property</code>","text":"<p>Number of cells in grid.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GridData.get_patches","level":3,"title":"<code>get_patches()</code>","text":"<p>Create matplotlib patches for polar visualization.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_data.py</code> <pre><code>def get_patches(self) -&gt; pl.Series:\n    \"\"\"Create matplotlib patches for polar visualization.\"\"\"\n    patches = [\n        Rectangle(\n            (row[\"phi_min\"], row[\"theta_min\"]),\n            row[\"phi_max\"] - row[\"phi_min\"],\n            row[\"theta_max\"] - row[\"theta_min\"],\n            fill=True,\n        )\n        for row in self.grid.iter_rows(named=True)\n    ]\n    return pl.Series(\"Patches\", patches)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GridData.get_solid_angles","level":3,"title":"<code>get_solid_angles()</code>","text":"<p>Calculate solid angle for each cell [steradians].</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_data.py</code> <pre><code>def get_solid_angles(self) -&gt; np.ndarray:\n    \"\"\"Calculate solid angle for each cell [steradians].\"\"\"\n    if self.solid_angles is not None:\n        return self.solid_angles\n\n    # HEALPix\n    if self.grid_type == \"healpix\" and \"healpix_nside\" in self.grid.columns:\n        try:\n            import healpy as hp\n\n            nside = int(self.grid[\"healpix_nside\"][0])\n            return np.full(\n                len(self.grid), hp.nside2pixarea(nside), dtype=np.float64\n            )\n        except ImportError:\n            pass\n\n    # Geodesic\n    if self.grid_type == \"geodesic\" and \"geodesic_vertices\" in self.grid.columns:\n        return self._compute_geodesic_solid_angles()\n\n    # HTM\n    if self.grid_type == \"htm\" and \"htm_vertex_0\" in self.grid.columns:\n        return self._compute_htm_solid_angles()\n\n    # Fibonacci\n    if self.grid_type == \"fibonacci\" and \"voronoi_region\" in self.grid.columns:\n        return self._compute_voronoi_solid_angles()\n\n    # Default\n    return self._geometric_solid_angles()\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GridData.get_grid_stats","level":3,"title":"<code>get_grid_stats()</code>","text":"<p>Get grid statistics including solid angle uniformity.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_data.py</code> <pre><code>def get_grid_stats(self) -&gt; dict:\n    \"\"\"Get grid statistics including solid angle uniformity.\"\"\"\n    solid_angles = self.get_solid_angles()\n\n    stats = {\n        \"total_cells\": self.ncells,\n        \"grid_type\": self.grid_type,\n        \"theta_bands\": len(self.theta_lims),\n        \"cells_per_band\": [len(ids) for ids in self.cell_ids],\n        \"solid_angle_mean_sr\": float(np.mean(solid_angles)),\n        \"solid_angle_std_sr\": float(np.std(solid_angles)),\n        \"solid_angle_cv_percent\": float(\n            np.std(solid_angles) / np.mean(solid_angles) * 100\n        ),\n        \"total_solid_angle_sr\": float(np.sum(solid_angles)),\n        \"hemisphere_solid_angle_sr\": 2 * np.pi,\n    }\n\n    # Add HEALPix-specific info\n    if self.grid_type == \"healpix\" and \"healpix_nside\" in self.grid.columns:\n        try:\n            import healpy as hp\n\n            nside = int(self.grid[\"healpix_nside\"][0])\n            stats[\"healpix_nside\"] = nside\n            stats[\"healpix_npix_total\"] = hp.nside2npix(nside)\n            stats[\"healpix_pixel_area_sr\"] = hp.nside2pixarea(nside)\n            stats[\"healpix_resolution_arcmin\"] = hp.nside2resol(nside, arcmin=True)\n        except ImportError:\n            pass\n\n    return stats\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GridType","level":2,"title":"<code>GridType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Available grid projection types for hemispherical tessellation.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_types.py</code> <pre><code>class GridType(Enum):\n    \"\"\"Available grid projection types for hemispherical tessellation.\"\"\"\n\n    EQUAL_AREA = \"equal_area\"  # Equal solid angle (ring-based)\n    EQUAL_ANGLE = \"equal_angle\"  # Equal angular spacing\n    EQUIRECTANGULAR = \"equirectangular\"  # Simple rectangular\n    HEALPIX = \"healpix\"  # Hierarchical equal area\n    GEODESIC = \"geodesic\"  # Icosahedral triangular\n    FIBONACCI = \"fibonacci\"  # Golden spiral + Voronoi\n    HTM = \"htm\"  # Hierarchical Triangular Mesh\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAreaBuilder","level":2,"title":"<code>EqualAreaBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Equal solid angle tessellation using concentric theta bands.</p> <p>The hemisphere is divided into annular bands of constant width in theta. Within each band the number of azimuthal (phi) sectors is chosen so that every cell subtends approximately the same solid angle.  This is the only grid type that has been validated for scientific use in this codebase.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAreaBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle measured from zenith (0 = straight up,   π/2 = horizon)</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAreaBuilder--what-angular_resolution-means","level":4,"title":"What <code>angular_resolution</code> means","text":"<p><code>angular_resolution</code> (degrees) sets the width of each theta band. All bands have this same width Δθ.  The azimuthal width of cells varies by band: near the zenith cells are wide in phi; near the horizon they are narrow, so that the solid angle stays constant.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAreaBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li> <p>Target solid angle per cell is chosen equal to the solid angle of a    cap of half-angle Δθ/2::</p> <p>Ω_target = 2π (1 − cos(Δθ/2))</p> </li> <li> <p>Zenith cap – a single cell covers [0, Δθ/2] in theta and the full    azimuth [0, 2π).</p> </li> <li> <p>Theta bands – edges are placed at Δθ/2, 3Δθ/2, 5Δθ/2, … up to    π/2 − cutoff_theta.  For each band [θ_inner, θ_outer] the band's    total solid angle is::</p> <p>Ω_band = 2π (cos θ_inner − cos θ_outer)</p> </li> <li> <p>Phi divisions – the number of sectors in the band is::</p> <p>n_phi = round(Ω_band / Ω_target)</p> </li> </ol> <p>Each sector spans Δφ = 2π / n_phi.  The cell centre is placed at the    geometric midpoint of its (phi, theta) rectangle.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAreaBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Theta-band width in degrees.  Controls both the radial resolution and     (indirectly, via the equal-area constraint) the azimuthal resolution. cutoff_theta : float     Minimum elevation above the horizon in degrees.  Bands whose outer     edge is at or below this cutoff are omitted.  In GNSS terms this is     the satellite elevation mask angle. phi_rotation : float     Rigid rotation applied to all phi coordinates after grid construction,     in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equal_area_grid.py</code> <pre><code>class EqualAreaBuilder(BaseGridBuilder):\n    \"\"\"Equal solid angle tessellation using concentric theta bands.\n\n    The hemisphere is divided into annular bands of constant width in theta.\n    Within each band the number of azimuthal (phi) sectors is chosen so that\n    every cell subtends approximately the same solid angle.  This is the only\n    grid type that has been validated for scientific use in this codebase.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle measured from zenith (0 = straight up,\n      π/2 = horizon)\n\n    What ``angular_resolution`` means\n    ----------------------------------\n    ``angular_resolution`` (degrees) sets the **width of each theta band**.\n    All bands have this same width Δθ.  The *azimuthal* width of cells varies\n    by band: near the zenith cells are wide in phi; near the horizon they are\n    narrow, so that the solid angle stays constant.\n\n    Mathematical construction\n    -------------------------\n    1. **Target solid angle** per cell is chosen equal to the solid angle of a\n       cap of half-angle Δθ/2::\n\n           Ω_target = 2π (1 − cos(Δθ/2))\n\n    2. **Zenith cap** – a single cell covers [0, Δθ/2] in theta and the full\n       azimuth [0, 2π).\n\n    3. **Theta bands** – edges are placed at Δθ/2, 3Δθ/2, 5Δθ/2, … up to\n       π/2 − cutoff_theta.  For each band [θ_inner, θ_outer] the band's\n       total solid angle is::\n\n           Ω_band = 2π (cos θ_inner − cos θ_outer)\n\n    4. **Phi divisions** – the number of sectors in the band is::\n\n           n_phi = round(Ω_band / Ω_target)\n\n       Each sector spans Δφ = 2π / n_phi.  The cell centre is placed at the\n       geometric midpoint of its (phi, theta) rectangle.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Theta-band width in degrees.  Controls both the radial resolution and\n        (indirectly, via the equal-area constraint) the azimuthal resolution.\n    cutoff_theta : float\n        Minimum elevation above the horizon in degrees.  Bands whose outer\n        edge is at or below this cutoff are omitted.  In GNSS terms this is\n        the satellite elevation mask angle.\n    phi_rotation : float\n        Rigid rotation applied to all phi coordinates after grid construction,\n        in degrees.\n\n    \"\"\"\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"equal_area\"``\n\n        \"\"\"\n        return GridType.EQUAL_AREA.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]:\n        \"\"\"Construct the equal-area hemisphere grid.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per cell with columns: phi, theta, phi_min, phi_max,\n            theta_min, theta_max, cell_id.\n        theta_lims : np.ndarray\n            Outer theta edge of each band (radians).\n        phi_lims : list[np.ndarray]\n            Array of phi_min values for each band.\n        cell_ids : list[np.ndarray]\n            Cell-id arrays, one per band.\n\n        \"\"\"\n        # Theta band edges (from zenith to horizon)\n        max_theta = np.pi / 2  # horizon\n        theta_edges = np.arange(\n            self.angular_resolution_rad / 2,\n            max_theta - self.cutoff_theta_rad,\n            self.angular_resolution_rad,\n        )\n\n        # Target solid angle per cell\n        target_omega = 2 * np.pi * (1 - np.cos(self.angular_resolution_rad / 2))\n\n        cells = []\n        theta_lims = []\n        phi_lims = []\n        cell_ids = []\n\n        # Zenith cell (special case) - only if cutoff allows\n        next_cell_id = 0\n        zenith_theta_max = self.angular_resolution_rad / 2\n\n        if self.cutoff_theta_rad &lt; zenith_theta_max:\n            cells.append(\n                pl.DataFrame(\n                    {\n                        \"phi\": [0.0],\n                        \"theta\": [0.0],\n                        \"phi_min\": [0.0],\n                        \"phi_max\": [2 * np.pi],\n                        \"theta_min\": [max(0.0, self.cutoff_theta_rad)],\n                        \"theta_max\": [zenith_theta_max],\n                    }\n                )\n            )\n            theta_lims.append(zenith_theta_max)\n            phi_lims.append(np.array([0.0]))\n            cell_ids.append(np.array([0]))\n            next_cell_id = 1\n\n        # Build theta bands\n        for iband, theta_outer in enumerate(theta_edges[1:]):\n            theta_inner = theta_edges[iband]\n\n            # Skip bands below cutoff\n            if theta_outer &lt;= self.cutoff_theta_rad:\n                continue\n\n            # Solid angle of this band\n            band_omega = 2 * np.pi * (np.cos(theta_inner) - np.cos(theta_outer))\n\n            # Number of phi divisions\n            n_phi = max(1, round(band_omega / target_omega))\n            phi_span = 2 * np.pi / n_phi\n\n            cell_id_list = list(range(next_cell_id, next_cell_id + n_phi))\n            next_cell_id = cell_id_list[-1] + 1\n\n            # Use arange for better precision than linspace\n            phi_min_arr = np.arange(n_phi) * phi_span\n            phi_max_arr = (np.arange(n_phi) + 1) * phi_span\n            phi_max_arr[-1] = 2 * np.pi  # Force exact closure\n\n            cells.append(\n                pl.DataFrame(\n                    {\n                        \"phi\": (phi_min_arr + phi_max_arr) / 2,\n                        \"theta\": np.full(n_phi, (theta_inner + theta_outer) / 2),\n                        \"phi_min\": phi_min_arr,\n                        \"phi_max\": phi_max_arr,\n                        \"theta_min\": np.full(n_phi, theta_inner),\n                        \"theta_max\": np.full(n_phi, theta_outer),\n                    }\n                )\n            )\n\n            theta_lims.append(theta_outer)\n            phi_lims.append(phi_min_arr)\n            cell_ids.append(np.array(cell_id_list))\n\n        if len(cells) == 0:\n            raise ValueError(\n                \"No cells generated - check cutoff_theta and angular_resolution\"\n            )\n\n        grid = pl.concat(cells).with_columns(pl.int_range(0, pl.len()).alias(\"cell_id\"))\n\n        return grid, np.array(theta_lims), phi_lims, cell_ids\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAreaBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAreaBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"equal_area\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equal_area_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"equal_area\"``\n\n    \"\"\"\n    return GridType.EQUAL_AREA.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAngleBuilder","level":2,"title":"<code>EqualAngleBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Equal angular spacing in both theta and phi (NOT equal area).</p> <p>Every cell is a rectangle of the same angular size Δθ × Δφ in the (theta, phi) parameter space.  Because solid angle depends on cos(theta), cells near the zenith subtend more solid angle than cells near the horizon.  This makes the grid biased toward the zenith for any solid-angle-weighted statistic.  Not recommended for scientific analysis – use <code>EqualAreaBuilder</code> instead.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAngleBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle from zenith</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAngleBuilder--what-angular_resolution-means","level":4,"title":"What <code>angular_resolution</code> means","text":"<p><code>angular_resolution</code> (degrees) is used as both the theta-band width and the phi-sector width.  The number of phi divisions is constant across all bands::</p> <pre><code>n_phi = round(2π / Δθ)\n</code></pre> <p>and does not change with latitude.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAngleBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li>A zenith cap cell covers [0, Δθ/2] × [0, 2π).</li> <li>Theta band edges are placed at Δθ/2, 3Δθ/2, … up to π/2.</li> <li>Within every band, the full azimuth is split into <code>n_phi</code> sectors of    equal width Δφ = 2π / n_phi.</li> <li>Cell centres are at the midpoint of each (phi, theta) rectangle.</li> </ol>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAngleBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Angular spacing in degrees, applied identically in both theta and phi. cutoff_theta : float     Elevation mask angle in degrees (bands below this are omitted). phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equal_angle_grid.py</code> <pre><code>class EqualAngleBuilder(BaseGridBuilder):\n    \"\"\"Equal angular spacing in both theta and phi (NOT equal area).\n\n    Every cell is a rectangle of the same angular size Δθ × Δφ in the\n    (theta, phi) parameter space.  Because solid angle depends on cos(theta),\n    cells near the zenith subtend *more* solid angle than cells near the\n    horizon.  This makes the grid biased toward the zenith for any\n    solid-angle-weighted statistic.  **Not recommended for scientific\n    analysis** – use ``EqualAreaBuilder`` instead.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle from zenith\n\n    What ``angular_resolution`` means\n    ----------------------------------\n    ``angular_resolution`` (degrees) is used as **both** the theta-band width\n    and the phi-sector width.  The number of phi divisions is constant across\n    all bands::\n\n        n_phi = round(2π / Δθ)\n\n    and does not change with latitude.\n\n    Mathematical construction\n    -------------------------\n    1. A zenith cap cell covers [0, Δθ/2] × [0, 2π).\n    2. Theta band edges are placed at Δθ/2, 3Δθ/2, … up to π/2.\n    3. Within every band, the full azimuth is split into ``n_phi`` sectors of\n       equal width Δφ = 2π / n_phi.\n    4. Cell centres are at the midpoint of each (phi, theta) rectangle.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Angular spacing in degrees, applied identically in both theta and phi.\n    cutoff_theta : float\n        Elevation mask angle in degrees (bands below this are omitted).\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    \"\"\"\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"equal_angle\"``\n\n        \"\"\"\n        return GridType.EQUAL_ANGLE.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]:\n        \"\"\"Construct the equal-angle hemisphere grid.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per cell.\n        theta_lims : np.ndarray\n            Outer theta edge of each band (radians).\n        phi_lims : list[np.ndarray]\n            Array of phi_min values for each band (identical across bands).\n        cell_ids : list[np.ndarray]\n            Cell-id arrays, one per band.\n\n        \"\"\"\n        max_theta = np.pi / 2\n        theta_edges = np.arange(\n            self.angular_resolution_rad / 2,\n            max_theta - self.cutoff_theta_rad,\n            self.angular_resolution_rad,\n        )\n\n        n_phi_divisions = int(2 * np.pi / self.angular_resolution_rad)\n\n        cells = []\n        theta_lims = []\n        phi_lims = []\n        cell_ids = []\n\n        # Zenith\n        cells.append(\n            pl.DataFrame(\n                {\n                    \"phi\": [0.0],\n                    \"theta\": [0.0],\n                    \"phi_min\": [0.0],\n                    \"phi_max\": [2 * np.pi],\n                    \"theta_min\": [0.0],\n                    \"theta_max\": [self.angular_resolution_rad / 2],\n                }\n            )\n        )\n        theta_lims.append(self.angular_resolution_rad / 2)\n        phi_lims.append(np.array([0.0]))\n        cell_ids.append(np.array([0]))\n        next_cell_id = 1\n\n        for iband, theta_outer in enumerate(theta_edges[1:]):\n            theta_inner = theta_edges[iband]\n            phi_span = 2 * np.pi / n_phi_divisions\n\n            cell_id_list = list(range(next_cell_id, next_cell_id + n_phi_divisions))\n            next_cell_id = cell_id_list[-1] + 1\n\n            phi_min_arr = np.linspace(0, 2 * np.pi - phi_span, n_phi_divisions)\n            phi_max_arr = np.concatenate((phi_min_arr[1:], [2 * np.pi]))\n\n            cells.append(\n                pl.DataFrame(\n                    {\n                        \"phi\": (phi_min_arr + phi_max_arr) / 2,\n                        \"theta\": np.full(\n                            n_phi_divisions,\n                            (theta_inner + theta_outer) / 2,\n                        ),\n                        \"phi_min\": phi_min_arr,\n                        \"phi_max\": phi_max_arr,\n                        \"theta_min\": np.full(n_phi_divisions, theta_inner),\n                        \"theta_max\": np.full(n_phi_divisions, theta_outer),\n                    }\n                )\n            )\n\n            theta_lims.append(theta_outer)\n            phi_lims.append(phi_min_arr)\n            cell_ids.append(np.array(cell_id_list))\n\n        grid = pl.concat(cells).with_row_index(\"cell_id\")\n        return grid, np.array(theta_lims), phi_lims, cell_ids\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAngleBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EqualAngleBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"equal_angle\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equal_angle_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"equal_angle\"``\n\n    \"\"\"\n    return GridType.EQUAL_ANGLE.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EquirectangularBuilder","level":2,"title":"<code>EquirectangularBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Simple rectangular grid in (theta, phi) space.</p> <p>The hemisphere is divided into a regular rectangular array: a constant number of theta bands, each containing the same constant number of phi sectors.  Every cell is an identical rectangle in angular coordinates. This is structurally identical to <code>EqualAngleBuilder</code> except for one difference in the zenith treatment: <code>EqualAngleBuilder</code> collapses the first band into a single zenith cap, while this builder does not — every band has the same number of sectors.</p> <p>Because solid angle depends on cos(theta), cells near the zenith subtend more solid angle than cells near the horizon.  This makes the grid biased toward the zenith for any solid-angle-weighted statistic. Not recommended for scientific analysis – use <code>EqualAreaBuilder</code> instead.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EquirectangularBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,   π/2 = horizon)</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EquirectangularBuilder--what-angular_resolution-means","level":4,"title":"What <code>angular_resolution</code> means","text":"<p><code>angular_resolution</code> (degrees) is used as both the theta-band width and the phi-sector width.  The grid is therefore square in angular coordinates::</p> <pre><code>n_theta = round((π/2 − cutoff) / Δθ)\nn_phi   = round(2π / Δθ)\ntotal cells = n_theta × n_phi\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EquirectangularBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li>Theta edges are placed at <code>cutoff_theta</code>, <code>cutoff_theta + Δθ</code>,    <code>cutoff_theta + 2Δθ</code>, … up to π/2.</li> <li>Phi edges are placed at 0, Δθ, 2Δθ, … up to 2π.</li> <li>Every (theta_band, phi_sector) combination produces one cell.  The    cell centre is the midpoint of the rectangle.</li> <li>No special zenith cap is created; the band nearest the zenith has    the same number of phi sectors as all other bands.</li> </ol>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EquirectangularBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Angular spacing in degrees, applied identically in both theta and phi. cutoff_theta : float     Elevation mask angle in degrees.  Bands whose inner edge is at or     below <code>π/2 − cutoff_theta</code> are omitted. phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equirectangular_grid.py</code> <pre><code>class EquirectangularBuilder(BaseGridBuilder):\n    \"\"\"Simple rectangular grid in (theta, phi) space.\n\n    The hemisphere is divided into a regular rectangular array: a constant\n    number of theta bands, each containing the same constant number of phi\n    sectors.  Every cell is an identical rectangle in angular coordinates.\n    This is *structurally* identical to ``EqualAngleBuilder`` except for one\n    difference in the zenith treatment: ``EqualAngleBuilder`` collapses the\n    first band into a single zenith cap, while this builder does not — every\n    band has the same number of sectors.\n\n    Because solid angle depends on cos(theta), cells near the zenith subtend\n    *more* solid angle than cells near the horizon.  This makes the grid\n    biased toward the zenith for any solid-angle-weighted statistic.\n    **Not recommended for scientific analysis** – use ``EqualAreaBuilder``\n    instead.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,\n      π/2 = horizon)\n\n    What ``angular_resolution`` means\n    ----------------------------------\n    ``angular_resolution`` (degrees) is used as **both** the theta-band width\n    *and* the phi-sector width.  The grid is therefore square in angular\n    coordinates::\n\n        n_theta = round((π/2 − cutoff) / Δθ)\n        n_phi   = round(2π / Δθ)\n        total cells = n_theta × n_phi\n\n    Mathematical construction\n    -------------------------\n    1. Theta edges are placed at ``cutoff_theta``, ``cutoff_theta + Δθ``,\n       ``cutoff_theta + 2Δθ``, … up to π/2.\n    2. Phi edges are placed at 0, Δθ, 2Δθ, … up to 2π.\n    3. Every (theta_band, phi_sector) combination produces one cell.  The\n       cell centre is the midpoint of the rectangle.\n    4. No special zenith cap is created; the band nearest the zenith has\n       the same number of phi sectors as all other bands.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Angular spacing in degrees, applied identically in both theta and phi.\n    cutoff_theta : float\n        Elevation mask angle in degrees.  Bands whose *inner* edge is at or\n        below ``π/2 − cutoff_theta`` are omitted.\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    \"\"\"\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"equirectangular\"``\n\n        \"\"\"\n        return GridType.EQUIRECTANGULAR.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]:\n        \"\"\"Construct the equirectangular hemisphere grid.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per cell with columns: phi, theta, phi_min, phi_max,\n            theta_min, theta_max, cell_id.\n        theta_lims : np.ndarray\n            Inner theta edge of each band (radians).\n        phi_lims : list[np.ndarray]\n            Array of phi_min values for each band (identical across bands).\n        cell_ids : list[np.ndarray]\n            Cell-id arrays, one per band.\n\n        \"\"\"\n        max_theta = np.pi / 2\n\n        theta_edges = np.arange(\n            self.cutoff_theta_rad,\n            max_theta + self.angular_resolution_rad,\n            self.angular_resolution_rad,\n        )\n        phi_edges = np.arange(\n            0, 2 * np.pi + self.angular_resolution_rad, self.angular_resolution_rad\n        )\n\n        cells = []\n\n        for i in range(len(theta_edges) - 1):\n            theta_min, theta_max = theta_edges[i], theta_edges[i + 1]\n\n            for j in range(len(phi_edges) - 1):\n                phi_min, phi_max = phi_edges[j], phi_edges[j + 1]\n\n                cells.append(\n                    {\n                        \"phi\": (phi_min + phi_max) / 2,\n                        \"theta\": (theta_min + theta_max) / 2,\n                        \"phi_min\": phi_min,\n                        \"phi_max\": min(2 * np.pi, phi_max),\n                        \"theta_min\": theta_min,\n                        \"theta_max\": theta_max,\n                    }\n                )\n\n        grid = pl.DataFrame(cells).with_columns(\n            pl.int_range(0, pl.len()).alias(\"cell_id\")\n        )\n\n        theta_lims = theta_edges[:-1]\n        phi_lims = [phi_edges[:-1] for _ in range(len(theta_edges) - 1)]\n        cell_ids_list = [\n            np.arange(i * (len(phi_edges) - 1), (i + 1) * (len(phi_edges) - 1))\n            for i in range(len(theta_edges) - 1)\n        ]\n\n        return grid, theta_lims, phi_lims, cell_ids_list\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EquirectangularBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.EquirectangularBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"equirectangular\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equirectangular_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"equirectangular\"``\n\n    \"\"\"\n    return GridType.EQUIRECTANGULAR.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder","level":2,"title":"<code>HEALPixBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>HEALPix tessellation (Hierarchical Equal Area isoLatitude Pixelization).</p> <p>HEALPix partitions the sphere into 12 base pixels arranged at equal latitudes.  Each base pixel is recursively subdivided into 4 children, producing <code>12 × nside²</code> pixels on the full sphere, all with exactly the same solid angle.  This strict equal-area property makes HEALPix the gold standard for pixelisations that must be unbiased under solid-angle weighting.</p> <p>This builder delegates the pixel geometry entirely to the <code>healpy</code> library.  It filters the full-sphere pixelisation down to the northern hemisphere and stores approximate bounding boxes (<code>phi_min/max</code>, <code>theta_min/max</code>) derived from the pixel resolution.  The bounding boxes are not the true pixel boundaries (which are curvilinear); they are only approximations suitable for quick spatial queries.  For exact pixel membership use <code>healpy.ang2pix</code> directly.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder--coordinate-convention","level":4,"title":"Coordinate convention","text":"<p>HEALPix natively uses colatitude <code>theta ∈ [0, π]</code> (0 = North Pole) and longitude <code>phi ∈ [0, 2π)</code>.  This matches the GNSS convention used elsewhere in canvodpy: theta = 0 is the zenith, theta = π/2 is the horizon.  No coordinate transform is applied.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder--what-nside-resolution-means","level":4,"title":"What <code>nside</code> (resolution) means","text":"<p><code>nside</code> is the single resolution parameter of HEALPix.  It must be a power of 2.  The key derived quantities are::</p> <pre><code>n_pixels   = 12 × nside²           (full sphere)\npixel_area = 4π / n_pixels          (steradians, exact)\nresolution ≈ √(pixel_area)          (approximate angular diameter)\n           ≈ 58.6° / nside         (degrees)\n</code></pre> nside Pixels (full) Approx resolution Pixel area (sr) 1 12 58.6° 1.049 2 48 29.3° 0.262 4 192 14.7° 0.065 8 768 7.3° 0.016 16 3 072 3.7° 0.004 32 12 288 1.8° 0.001 <p>When <code>nside</code> is not provided, it is estimated from <code>angular_resolution</code> and rounded to the nearest power of 2::</p> <pre><code>nside_estimate = round_to_pow2( √(3/π) × 60 / angular_resolution )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<p>HEALPix construction is performed entirely by <code>healpy</code>.  At a high level:</p> <ol> <li>The sphere is divided into 12 congruent base pixels (a curvilinear    quadrilateral arrangement at three latitude zones: polar caps and    equatorial belt).</li> <li>Each base pixel is subdivided into <code>nside²</code> equal-area children    using a hierarchical quadtree.</li> <li>Pixel centres are returned by <code>healpy.pix2ang(nside, ipix)</code> in    RING ordering (pixels ordered by increasing colatitude).</li> <li>This builder keeps only pixels with <code>theta ≤ π/2 − cutoff_theta</code>    (northern hemisphere above the elevation mask).</li> </ol>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Approximate angular resolution in degrees.  Used only to derive     <code>nside</code> when that parameter is not given explicitly. cutoff_theta : float     Elevation mask angle in degrees.  Pixels with colatitude     <code>theta &gt; π/2 − cutoff_theta</code> (i.e. below the mask) are excluded. nside : int or None     HEALPix resolution parameter.  Must be a power of 2.  If <code>None</code>,     estimated from <code>angular_resolution</code>. phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder--raises","level":4,"title":"Raises","text":"<p>ImportError     If <code>healpy</code> is not installed. ValueError     If <code>nside</code> is not a power of 2.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/healpix_grid.py</code> <pre><code>class HEALPixBuilder(BaseGridBuilder):\n    \"\"\"HEALPix tessellation (Hierarchical Equal Area isoLatitude Pixelization).\n\n    HEALPix partitions the sphere into 12 base pixels arranged at equal\n    latitudes.  Each base pixel is recursively subdivided into 4 children,\n    producing ``12 × nside²`` pixels on the full sphere, all with *exactly*\n    the same solid angle.  This strict equal-area property makes HEALPix\n    the gold standard for pixelisations that must be unbiased under\n    solid-angle weighting.\n\n    This builder delegates the pixel geometry entirely to the ``healpy``\n    library.  It filters the full-sphere pixelisation down to the northern\n    hemisphere and stores approximate bounding boxes (``phi_min/max``,\n    ``theta_min/max``) derived from the pixel resolution.  The bounding\n    boxes are **not** the true pixel boundaries (which are curvilinear);\n    they are only approximations suitable for quick spatial queries.  For\n    exact pixel membership use ``healpy.ang2pix`` directly.\n\n    Coordinate convention\n    ---------------------\n    HEALPix natively uses colatitude ``theta ∈ [0, π]`` (0 = North Pole)\n    and longitude ``phi ∈ [0, 2π)``.  This matches the GNSS convention used\n    elsewhere in canvodpy: theta = 0 is the zenith, theta = π/2 is the\n    horizon.  **No coordinate transform is applied.**\n\n    What ``nside`` (resolution) means\n    ----------------------------------\n    ``nside`` is the single resolution parameter of HEALPix.  It must be a\n    power of 2.  The key derived quantities are::\n\n        n_pixels   = 12 × nside²           (full sphere)\n        pixel_area = 4π / n_pixels          (steradians, exact)\n        resolution ≈ √(pixel_area)          (approximate angular diameter)\n                   ≈ 58.6° / nside         (degrees)\n\n    | nside | Pixels (full) | Approx resolution | Pixel area (sr) |\n    |-------|---------------|-------------------|-----------------|\n    | 1     | 12            | 58.6°             | 1.049           |\n    | 2     | 48            | 29.3°             | 0.262           |\n    | 4     | 192           | 14.7°             | 0.065           |\n    | 8     | 768           | 7.3°              | 0.016           |\n    | 16    | 3 072         | 3.7°              | 0.004           |\n    | 32    | 12 288        | 1.8°              | 0.001           |\n\n    When ``nside`` is not provided, it is estimated from ``angular_resolution``\n    and rounded to the nearest power of 2::\n\n        nside_estimate = round_to_pow2( √(3/π) × 60 / angular_resolution )\n\n    Mathematical construction\n    -------------------------\n    HEALPix construction is performed entirely by ``healpy``.  At a high\n    level:\n\n    1. The sphere is divided into 12 congruent base pixels (a curvilinear\n       quadrilateral arrangement at three latitude zones: polar caps and\n       equatorial belt).\n    2. Each base pixel is subdivided into ``nside²`` equal-area children\n       using a hierarchical quadtree.\n    3. Pixel centres are returned by ``healpy.pix2ang(nside, ipix)`` in\n       RING ordering (pixels ordered by increasing colatitude).\n    4. This builder keeps only pixels with ``theta ≤ π/2 − cutoff_theta``\n       (northern hemisphere above the elevation mask).\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Approximate angular resolution in degrees.  Used only to derive\n        ``nside`` when that parameter is not given explicitly.\n    cutoff_theta : float\n        Elevation mask angle in degrees.  Pixels with colatitude\n        ``theta &gt; π/2 − cutoff_theta`` (i.e. below the mask) are excluded.\n    nside : int or None\n        HEALPix resolution parameter.  Must be a power of 2.  If ``None``,\n        estimated from ``angular_resolution``.\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    Raises\n    ------\n    ImportError\n        If ``healpy`` is not installed.\n    ValueError\n        If ``nside`` is not a power of 2.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        angular_resolution: float = 2,\n        cutoff_theta: float = 0,\n        nside: int | None = None,\n        phi_rotation: float = 0,\n    ) -&gt; None:\n        \"\"\"Initialize the HEALPix grid builder.\n\n        Parameters\n        ----------\n        angular_resolution : float, default 2\n            Angular resolution in degrees.\n        cutoff_theta : float, default 0\n            Maximum polar angle cutoff in degrees.\n        nside : int | None, optional\n            HEALPix nside parameter.\n        phi_rotation : float, default 0\n            Rotation angle in degrees.\n\n        \"\"\"\n        super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n        # Determine nside\n        if nside is None:\n            nside_estimate = int(np.sqrt(3 / np.pi) * 60 / angular_resolution)\n            self.nside = 2 ** max(0, int(np.round(np.log2(nside_estimate))))\n        else:\n            if nside &lt; 1 or (nside &amp; (nside - 1)) != 0:\n                raise ValueError(f\"nside must be a power of 2, got {nside}\")\n            self.nside = nside\n\n        # Import healpy\n        try:\n            import healpy as hp\n\n            self.hp = hp\n        except ImportError:\n            raise ImportError(\n                \"healpy is required for HEALPix grid. Install with: pip install healpy\"\n            )\n\n        pixel_size_arcmin = self.hp.nside2resol(self.nside, arcmin=True)\n        self.actual_angular_resolution = pixel_size_arcmin / 60.0\n\n        self._logger.info(\n            f\"HEALPix: nside={self.nside}, \"\n            f\"requested_res={angular_resolution:.2f}°, \"\n            f\"actual_res={self.actual_angular_resolution:.2f}°\"\n        )\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"healpix\"``\n\n        \"\"\"\n        return GridType.HEALPIX.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]:\n        \"\"\"Build HEALPix grid for the northern hemisphere.\n\n        Iterates over all ``12 × nside²`` pixels, retains those with\n        ``theta ≤ π/2 − cutoff_theta``, and constructs approximate\n        bounding boxes from the pixel resolution.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per pixel.  Contains phi, theta (centre), approximate\n            bounding-box limits, ``healpix_ipix`` (RING-ordered pixel index),\n            and ``healpix_nside``.\n        theta_lims : np.ndarray\n            Synthetic evenly-spaced theta limits (interface compatibility only).\n        phi_lims : list[np.ndarray]\n            Synthetic evenly-spaced phi limits (interface compatibility only).\n        cell_ids : list[np.ndarray]\n            Single-element list containing the valid pixel indices.\n\n        \"\"\"\n        npix = self.hp.nside2npix(self.nside)\n\n        cells = []\n        valid_pixels = []\n\n        for ipix in range(npix):\n            theta, phi = self.hp.pix2ang(self.nside, ipix)\n\n            # Keep only northern hemisphere above the elevation mask\n            if theta &gt; (np.pi / 2 - self.cutoff_theta_rad):\n                continue\n\n            pixel_radius = self.hp.nside2resol(self.nside)\n\n            cells.append(\n                {\n                    \"phi\": float(phi),\n                    \"theta\": float(theta),\n                    \"phi_min\": float(max(0, phi - pixel_radius / 2)),\n                    \"phi_max\": float(min(2 * np.pi, phi + pixel_radius / 2)),\n                    \"theta_min\": float(max(0, theta - pixel_radius / 2)),\n                    \"theta_max\": float(min(np.pi / 2, theta + pixel_radius / 2)),\n                    \"healpix_ipix\": int(ipix),\n                    \"healpix_nside\": int(self.nside),\n                }\n            )\n            valid_pixels.append(int(ipix))\n\n        if len(cells) == 0:\n            raise ValueError(\"No valid HEALPix pixels found in hemisphere\")\n\n        grid = pl.DataFrame(cells)\n\n        grid = grid.with_columns(\n            [\n                pl.col(\"healpix_ipix\").cast(pl.Int64),\n                pl.col(\"healpix_nside\").cast(pl.Int64),\n            ]\n        )\n\n        theta_unique = sorted(grid[\"theta\"].unique())\n        n_theta_bands = len(theta_unique)\n\n        # NOTE: These limits are SYNTHETIC and do NOT correspond to actual\n        # HEALPix pixel boundaries. They exist only for interface\n        # compatibility with ring-based grids. For spatial queries, use the\n        # per-pixel theta_min/max and phi_min/max columns instead.\n        theta_lims = np.linspace(0, np.pi / 2, min(n_theta_bands, 20))\n        phi_lims = [np.linspace(0, 2 * np.pi, 20) for _ in range(len(theta_lims))]\n\n        cell_ids_list = [np.array(valid_pixels, dtype=np.int64)]\n\n        return grid, theta_lims, phi_lims, cell_ids_list\n\n    def get_healpix_info(self) -&gt; dict:\n        \"\"\"Get HEALPix-specific information.\n\n        Returns\n        -------\n        info : dict\n            Keys: ``nside``, ``npix_total``, ``pixel_area_sr``,\n            ``pixel_area_arcmin2``, ``resolution_arcmin``,\n            ``resolution_deg``, ``max_pixel_radius_deg``.\n\n        \"\"\"\n        return {\n            \"nside\": self.nside,\n            \"npix_total\": self.hp.nside2npix(self.nside),\n            \"pixel_area_sr\": self.hp.nside2pixarea(self.nside),\n            \"pixel_area_arcmin2\": (\n                self.hp.nside2pixarea(self.nside, degrees=True) * 3600\n            ),\n            \"resolution_arcmin\": self.hp.nside2resol(self.nside, arcmin=True),\n            \"resolution_deg\": self.actual_angular_resolution,\n            \"max_pixel_radius_deg\": np.rad2deg(self.hp.max_pixrad(self.nside)),\n        }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder.__init__","level":3,"title":"<code>__init__(angular_resolution=2, cutoff_theta=0, nside=None, phi_rotation=0)</code>","text":"<p>Initialize the HEALPix grid builder.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder.__init__--parameters","level":5,"title":"Parameters","text":"<p>angular_resolution : float, default 2     Angular resolution in degrees. cutoff_theta : float, default 0     Maximum polar angle cutoff in degrees. nside : int | None, optional     HEALPix nside parameter. phi_rotation : float, default 0     Rotation angle in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/healpix_grid.py</code> <pre><code>def __init__(\n    self,\n    angular_resolution: float = 2,\n    cutoff_theta: float = 0,\n    nside: int | None = None,\n    phi_rotation: float = 0,\n) -&gt; None:\n    \"\"\"Initialize the HEALPix grid builder.\n\n    Parameters\n    ----------\n    angular_resolution : float, default 2\n        Angular resolution in degrees.\n    cutoff_theta : float, default 0\n        Maximum polar angle cutoff in degrees.\n    nside : int | None, optional\n        HEALPix nside parameter.\n    phi_rotation : float, default 0\n        Rotation angle in degrees.\n\n    \"\"\"\n    super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n    # Determine nside\n    if nside is None:\n        nside_estimate = int(np.sqrt(3 / np.pi) * 60 / angular_resolution)\n        self.nside = 2 ** max(0, int(np.round(np.log2(nside_estimate))))\n    else:\n        if nside &lt; 1 or (nside &amp; (nside - 1)) != 0:\n            raise ValueError(f\"nside must be a power of 2, got {nside}\")\n        self.nside = nside\n\n    # Import healpy\n    try:\n        import healpy as hp\n\n        self.hp = hp\n    except ImportError:\n        raise ImportError(\n            \"healpy is required for HEALPix grid. Install with: pip install healpy\"\n        )\n\n    pixel_size_arcmin = self.hp.nside2resol(self.nside, arcmin=True)\n    self.actual_angular_resolution = pixel_size_arcmin / 60.0\n\n    self._logger.info(\n        f\"HEALPix: nside={self.nside}, \"\n        f\"requested_res={angular_resolution:.2f}°, \"\n        f\"actual_res={self.actual_angular_resolution:.2f}°\"\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"healpix\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/healpix_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"healpix\"``\n\n    \"\"\"\n    return GridType.HEALPIX.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder.get_healpix_info","level":3,"title":"<code>get_healpix_info()</code>","text":"<p>Get HEALPix-specific information.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HEALPixBuilder.get_healpix_info--returns","level":5,"title":"Returns","text":"<p>info : dict     Keys: <code>nside</code>, <code>npix_total</code>, <code>pixel_area_sr</code>,     <code>pixel_area_arcmin2</code>, <code>resolution_arcmin</code>,     <code>resolution_deg</code>, <code>max_pixel_radius_deg</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/healpix_grid.py</code> <pre><code>def get_healpix_info(self) -&gt; dict:\n    \"\"\"Get HEALPix-specific information.\n\n    Returns\n    -------\n    info : dict\n        Keys: ``nside``, ``npix_total``, ``pixel_area_sr``,\n        ``pixel_area_arcmin2``, ``resolution_arcmin``,\n        ``resolution_deg``, ``max_pixel_radius_deg``.\n\n    \"\"\"\n    return {\n        \"nside\": self.nside,\n        \"npix_total\": self.hp.nside2npix(self.nside),\n        \"pixel_area_sr\": self.hp.nside2pixarea(self.nside),\n        \"pixel_area_arcmin2\": (\n            self.hp.nside2pixarea(self.nside, degrees=True) * 3600\n        ),\n        \"resolution_arcmin\": self.hp.nside2resol(self.nside, arcmin=True),\n        \"resolution_deg\": self.actual_angular_resolution,\n        \"max_pixel_radius_deg\": np.rad2deg(self.hp.max_pixrad(self.nside)),\n    }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder","level":2,"title":"<code>GeodesicBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Geodesic grid based on a subdivided icosahedron.</p> <p>The sphere is tessellated into triangular cells by starting with an icosahedron (20 equilateral triangles) and recursively subdividing each triangle into four smaller triangles.  All vertices are projected back onto the unit sphere after each subdivision step, so the final cells are spherical triangles.  The grid has no polar singularity and provides near-uniform cell areas, though strict equal-area is not guaranteed — cell areas vary by a few percent depending on how they inherit the icosahedral symmetry axes.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,   π/2 = horizon)</li> </ul> <p>Cell centres are computed as the 3D Cartesian mean of the three vertices, re-normalised onto the unit sphere.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder--what-angular_resolution-means","level":4,"title":"What <code>angular_resolution</code> means","text":"<p><code>angular_resolution</code> is not used directly as a cell size.  Instead it is used only when <code>subdivision_level</code> is not explicitly supplied, to estimate an appropriate subdivision level.  The heuristic targets an approximate triangle edge length of <code>2 × angular_resolution</code>::</p> <pre><code>target_edge ≈ 2 × angular_resolution   (degrees)\nsubdivision_level = ceil(log₂(63.4 / target_edge))\n</code></pre> <p>The number 63.4° is the edge length of a regular icosahedron inscribed in a unit sphere.  Each subdivision halves the edge length, so the actual edge length at level n is approximately::</p> <pre><code>edge ≈ 63.4° / 2ⁿ   (degrees)\n</code></pre> <p>The total number of triangles on the full sphere is <code>20 × 4ⁿ</code>. Roughly half fall in the northern hemisphere (exact count depends on the hemisphere boundary).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li>Icosahedron – 12 vertices placed at the intersections of three    mutually perpendicular golden-ratio rectangles, normalised to the    unit sphere.  20 triangular faces connect them.</li> <li>Subdivision – each triangle is split into 4 by inserting edge    midpoints.  Each midpoint is projected onto the unit sphere    (re-normalised) before the next subdivision.  This is repeated    <code>subdivision_level</code> times.</li> <li>Hemisphere filter – faces are kept if any of their three    vertices satisfies <code>theta ≤ π/2 − cutoff_theta</code>.  Consequently,    boundary triangles that straddle the horizon are included and    extend slightly below it.</li> <li>Phi wrapping – for triangles that straddle the 0/2π azimuthal    boundary, vertex phis below π are shifted by +2π before computing    bounding-box limits, then wrapped back.</li> </ol>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Approximate angular resolution in degrees.  Used only to derive     <code>subdivision_level</code> when that parameter is not given explicitly. cutoff_theta : float     Elevation mask angle in degrees.  Triangles are excluded only if     all their vertices are below this elevation. subdivision_level : int or None     Number of icosahedral subdivisions.  If <code>None</code>, estimated from     <code>angular_resolution</code>.  Typical range 0–5. phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder--notes","level":4,"title":"Notes","text":"<p>The <code>theta_lims</code>, <code>phi_lims</code>, and <code>cell_ids</code> fields of the returned <code>GridData</code> are synthetic evenly-spaced arrays kept only for interface compatibility with ring-based grids.  They do not describe the actual triangular cell layout.  Use the <code>geodesic_vertices</code> column and the <code>vertices</code> array in <code>GridData.vertices</code> for the true geometry.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/geodesic_grid.py</code> <pre><code>class GeodesicBuilder(BaseGridBuilder):\n    \"\"\"Geodesic grid based on a subdivided icosahedron.\n\n    The sphere is tessellated into triangular cells by starting with an\n    icosahedron (20 equilateral triangles) and recursively subdividing each\n    triangle into four smaller triangles.  All vertices are projected back\n    onto the unit sphere after each subdivision step, so the final cells are\n    *spherical* triangles.  The grid has no polar singularity and provides\n    near-uniform cell areas, though strict equal-area is *not* guaranteed —\n    cell areas vary by a few percent depending on how they inherit the\n    icosahedral symmetry axes.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,\n      π/2 = horizon)\n\n    Cell centres are computed as the 3D Cartesian mean of the three vertices,\n    re-normalised onto the unit sphere.\n\n    What ``angular_resolution`` means\n    ----------------------------------\n    ``angular_resolution`` is **not** used directly as a cell size.  Instead it\n    is used only when ``subdivision_level`` is *not* explicitly supplied, to\n    *estimate* an appropriate subdivision level.  The heuristic targets an\n    approximate triangle edge length of ``2 × angular_resolution``::\n\n        target_edge ≈ 2 × angular_resolution   (degrees)\n        subdivision_level = ceil(log₂(63.4 / target_edge))\n\n    The number 63.4° is the edge length of a regular icosahedron inscribed in\n    a unit sphere.  Each subdivision halves the edge length, so the actual\n    edge length at level *n* is approximately::\n\n        edge ≈ 63.4° / 2ⁿ   (degrees)\n\n    The total number of triangles on the **full sphere** is ``20 × 4ⁿ``.\n    Roughly half fall in the northern hemisphere (exact count depends on\n    the hemisphere boundary).\n\n    Mathematical construction\n    -------------------------\n    1. **Icosahedron** – 12 vertices placed at the intersections of three\n       mutually perpendicular golden-ratio rectangles, normalised to the\n       unit sphere.  20 triangular faces connect them.\n    2. **Subdivision** – each triangle is split into 4 by inserting edge\n       midpoints.  Each midpoint is projected onto the unit sphere\n       (re-normalised) before the next subdivision.  This is repeated\n       ``subdivision_level`` times.\n    3. **Hemisphere filter** – faces are kept if *any* of their three\n       vertices satisfies ``theta ≤ π/2 − cutoff_theta``.  Consequently,\n       boundary triangles that straddle the horizon *are* included and\n       extend slightly below it.\n    4. **Phi wrapping** – for triangles that straddle the 0/2π azimuthal\n       boundary, vertex phis below π are shifted by +2π before computing\n       bounding-box limits, then wrapped back.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Approximate angular resolution in degrees.  Used only to derive\n        ``subdivision_level`` when that parameter is not given explicitly.\n    cutoff_theta : float\n        Elevation mask angle in degrees.  Triangles are excluded only if\n        *all* their vertices are below this elevation.\n    subdivision_level : int or None\n        Number of icosahedral subdivisions.  If ``None``, estimated from\n        ``angular_resolution``.  Typical range 0–5.\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    Notes\n    -----\n    The ``theta_lims``, ``phi_lims``, and ``cell_ids`` fields of the returned\n    ``GridData`` are *synthetic* evenly-spaced arrays kept only for interface\n    compatibility with ring-based grids.  They do **not** describe the actual\n    triangular cell layout.  Use the ``geodesic_vertices`` column and the\n    ``vertices`` array in ``GridData.vertices`` for the true geometry.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        angular_resolution: float = 2,\n        cutoff_theta: float = 0,\n        subdivision_level: int | None = None,\n        phi_rotation: float = 0,\n    ) -&gt; None:\n        \"\"\"Initialize the geodesic grid builder.\n\n        Parameters\n        ----------\n        angular_resolution : float, default 2\n            Angular resolution in degrees.\n        cutoff_theta : float, default 0\n            Maximum polar angle cutoff in degrees.\n        subdivision_level : int | None, optional\n            Subdivision level override.\n        phi_rotation : float, default 0\n            Rotation angle in degrees.\n\n        \"\"\"\n        super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n        self._triangles: np.ndarray | None = None\n\n        if subdivision_level is None:\n            target_edge_deg = angular_resolution * 2\n            self.subdivision_level = max(\n                0,\n                int(np.ceil(np.log2(63.4 / target_edge_deg))),\n            )\n        else:\n            self.subdivision_level = subdivision_level\n\n        self._logger.info(\n            f\"Geodesic: subdivision_level={self.subdivision_level}, \"\n            f\"~{20 * 4**self.subdivision_level} triangles\"\n        )\n\n    def get_triangles(self) -&gt; np.ndarray | None:\n        \"\"\"Return triangle vertex coordinates for visualization.\n\n        Returns\n        -------\n        triangles : np.ndarray or None\n            Array of shape ``(n_faces, 3, 3)`` where ``triangles[i]`` contains\n            the three 3D unit-sphere vertices of triangle *i*.  ``None`` if\n            the grid has not been built yet.\n\n        \"\"\"\n        return self._triangles\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"geodesic\"``\n\n        \"\"\"\n        return GridType.GEODESIC.value\n\n    def _extract_triangle_vertices(\n        self, vertices: np.ndarray, faces: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Extract 3D vertex coordinates for each face.\n\n        Parameters\n        ----------\n        vertices : np.ndarray\n            All sphere vertices, shape ``(n_vertices, 3)``.\n        faces : np.ndarray\n            Face index array, shape ``(n_faces, 3)``.\n\n        Returns\n        -------\n        triangles : np.ndarray\n            Shape ``(n_faces, 3, 3)`` – three 3D vertices per face.\n\n        \"\"\"\n        # Vectorized: use NumPy advanced indexing instead of loop\n        return vertices[faces]\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[\n        pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray], dict[str, Any]\n    ]:\n        \"\"\"Build geodesic grid from subdivided icosahedron.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per triangular cell.  Columns include phi, theta (centre),\n            bounding-box limits, ``geodesic_vertices`` (3 vertex indices into\n            the ``vertices`` array), and ``geodesic_subdivision``.\n        theta_lims : np.ndarray\n            Synthetic evenly-spaced theta limits (interface compatibility only).\n        phi_lims : list[np.ndarray]\n            Synthetic evenly-spaced phi limits (interface compatibility only).\n        cell_ids : list[np.ndarray]\n            Single-element list containing all cell ids.\n        extra_kwargs : dict\n            Contains ``vertices`` (shape ``(n_vertices, 3)``),\n            ``vertex_phi``, and ``vertex_theta`` arrays for the full\n            subdivided icosahedron.\n\n        \"\"\"\n        vertices, faces = self._create_icosahedron()\n\n        # Subdivide\n        for _ in range(self.subdivision_level):\n            vertices, faces = self._subdivide_mesh(vertices, faces)\n\n        # Project to unit sphere\n        vertices = vertices / np.linalg.norm(vertices, axis=1, keepdims=True)\n\n        # Convert to spherical\n        x, y, z = vertices[:, 0], vertices[:, 1], vertices[:, 2]\n        theta = np.arccos(np.clip(z, -1, 1))\n        phi = np.arctan2(y, x)\n        phi = np.mod(phi, 2 * np.pi)\n\n        # Filter to northern hemisphere\n        hemisphere_mask = theta &lt;= (np.pi / 2 - self.cutoff_theta_rad)\n\n        # Filter faces\n        valid_faces = []\n        for face in faces:\n            if any(hemisphere_mask[v] for v in face):\n                valid_faces.append(face)\n\n        if len(valid_faces) == 0:\n            raise ValueError(\"No valid faces in hemisphere\")\n\n        valid_faces = np.array(valid_faces)\n\n        # Create cells\n        cells = []\n        for face in valid_faces:\n            v_indices = face\n            face_phi = phi[v_indices]\n            face_theta = theta[v_indices]\n\n            # Handle phi wrapping for triangles crossing 0/2π boundary\n            phi_range = np.ptp(face_phi)\n            if phi_range &gt; np.pi:\n                # Triangle crosses the wraparound - unwrap relative to median\n                ref_phi = np.median(face_phi)\n                face_phi_unwrapped = face_phi.copy()\n                # Unwrap angles that are &gt; π away from reference\n                mask_low = (ref_phi - face_phi_unwrapped) &gt; np.pi\n                mask_high = (face_phi_unwrapped - ref_phi) &gt; np.pi\n                face_phi_unwrapped[mask_low] += 2 * np.pi\n                face_phi_unwrapped[mask_high] -= 2 * np.pi\n                phi_min = float(np.min(face_phi_unwrapped) % (2 * np.pi))\n                phi_max = float(np.max(face_phi_unwrapped) % (2 * np.pi))\n            else:\n                phi_min = float(np.min(face_phi))\n                phi_max = float(np.max(face_phi))\n\n            # Cell center - 3D Cartesian mean\n            face_vertices_3d = vertices[v_indices]\n            center_3d = np.mean(face_vertices_3d, axis=0)\n            center_3d = center_3d / np.linalg.norm(center_3d)\n\n            center_theta = np.arccos(np.clip(center_3d[2], -1, 1))\n            center_phi = np.arctan2(center_3d[1], center_3d[0])\n            center_phi = np.mod(center_phi, 2 * np.pi)\n\n            # Cell bounds (theta from vertices, phi already computed above)\n            theta_min = float(np.min(face_theta))\n            theta_max = float(np.max(face_theta))\n\n            cells.append(\n                {\n                    \"phi\": center_phi,\n                    \"theta\": center_theta,\n                    \"phi_min\": phi_min,\n                    \"phi_max\": phi_max,\n                    \"theta_min\": theta_min,\n                    \"theta_max\": theta_max,\n                    \"geodesic_vertices\": v_indices.tolist(),\n                    \"geodesic_subdivision\": self.subdivision_level,\n                }\n            )\n\n        grid = pl.DataFrame(cells).with_columns(\n            pl.int_range(0, pl.len()).alias(\"cell_id\")\n        )\n\n        extra_kwargs: dict[str, Any] = {\n            \"vertices\": vertices,\n            \"vertex_phi\": phi,\n            \"vertex_theta\": theta,\n        }\n\n        theta_lims = np.linspace(0, np.pi / 2, 10)\n        phi_lims = [np.linspace(0, 2 * np.pi, 20) for _ in range(len(theta_lims))]\n        cell_ids_list = [np.arange(grid.height)]\n\n        self._triangles = self._extract_triangle_vertices(vertices, faces)\n\n        return grid, theta_lims, phi_lims, cell_ids_list, extra_kwargs\n\n    def _create_icosahedron(self) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Create a unit-sphere icosahedron.\n\n        Returns\n        -------\n        vertices : np.ndarray\n            Shape ``(12, 3)`` – vertices on the unit sphere.\n        faces : np.ndarray\n            Shape ``(20, 3)`` – integer vertex indices per triangular face.\n\n        \"\"\"\n        phi_golden = (1 + np.sqrt(5)) / 2\n\n        vertices = np.array(\n            [\n                [-1, phi_golden, 0],\n                [1, phi_golden, 0],\n                [-1, -phi_golden, 0],\n                [1, -phi_golden, 0],\n                [0, -1, phi_golden],\n                [0, 1, phi_golden],\n                [0, -1, -phi_golden],\n                [0, 1, -phi_golden],\n                [phi_golden, 0, -1],\n                [phi_golden, 0, 1],\n                [-phi_golden, 0, -1],\n                [-phi_golden, 0, 1],\n            ],\n            dtype=np.float64,\n        )\n\n        vertices = vertices / np.linalg.norm(vertices, axis=1, keepdims=True)\n\n        faces = np.array(\n            [\n                [0, 11, 5],\n                [0, 5, 1],\n                [0, 1, 7],\n                [0, 7, 10],\n                [0, 10, 11],\n                [1, 5, 9],\n                [5, 11, 4],\n                [11, 10, 2],\n                [10, 7, 6],\n                [7, 1, 8],\n                [3, 9, 4],\n                [3, 4, 2],\n                [3, 2, 6],\n                [3, 6, 8],\n                [3, 8, 9],\n                [4, 9, 5],\n                [2, 4, 11],\n                [6, 2, 10],\n                [8, 6, 7],\n                [9, 8, 1],\n            ],\n            dtype=np.int64,\n        )\n\n        return vertices, faces\n\n    def _subdivide_mesh(\n        self, vertices: np.ndarray, faces: np.ndarray\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Subdivide each triangle into 4 smaller triangles.\n\n        Each edge midpoint is computed, normalised onto the unit sphere, and\n        cached so that shared edges produce only one new vertex.\n\n        Parameters\n        ----------\n        vertices : np.ndarray\n            Current vertex array, shape ``(n_vertices, 3)``.\n        faces : np.ndarray\n            Current face array, shape ``(n_faces, 3)``.\n\n        Returns\n        -------\n        new_vertices : np.ndarray\n            Expanded vertex array, shape ``(n_vertices + n_new_midpoints, 3)``.\n        new_faces : np.ndarray\n            New face array, shape ``(4 × n_faces, 3)``.\n\n        \"\"\"\n        new_faces = []\n        edge_midpoints: dict[tuple[int, int], int] = {}\n\n        def get_midpoint(v1: int, v2: int) -&gt; int:\n            \"\"\"Return midpoint vertex index for an edge.\n\n            Parameters\n            ----------\n            v1 : int\n                First vertex index.\n            v2 : int\n                Second vertex index.\n\n            Returns\n            -------\n            int\n                Index of the midpoint vertex.\n\n            \"\"\"\n            edge = tuple(sorted([v1, v2]))\n            if edge not in edge_midpoints:\n                edge_midpoints[edge] = len(vertices) + len(edge_midpoints)\n            return edge_midpoints[edge]\n\n        for face in faces:\n            v0, v1, v2 = face\n\n            m01 = get_midpoint(v0, v1)\n            m12 = get_midpoint(v1, v2)\n            m20 = get_midpoint(v2, v0)\n\n            new_faces.extend(\n                [\n                    [v0, m01, m20],\n                    [v1, m12, m01],\n                    [v2, m20, m12],\n                    [m01, m12, m20],\n                ]\n            )\n\n        n_original = len(vertices)\n        n_new = len(edge_midpoints)\n        final_vertices = np.zeros((n_original + n_new, 3))\n        final_vertices[:n_original] = vertices\n\n        for edge, idx in edge_midpoints.items():\n            v1, v2 = edge\n            midpoint = (vertices[v1] + vertices[v2]) / 2\n            midpoint = midpoint / np.linalg.norm(midpoint)\n            final_vertices[idx] = midpoint\n\n        return final_vertices, np.array(new_faces)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder.__init__","level":3,"title":"<code>__init__(angular_resolution=2, cutoff_theta=0, subdivision_level=None, phi_rotation=0)</code>","text":"<p>Initialize the geodesic grid builder.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder.__init__--parameters","level":5,"title":"Parameters","text":"<p>angular_resolution : float, default 2     Angular resolution in degrees. cutoff_theta : float, default 0     Maximum polar angle cutoff in degrees. subdivision_level : int | None, optional     Subdivision level override. phi_rotation : float, default 0     Rotation angle in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/geodesic_grid.py</code> <pre><code>def __init__(\n    self,\n    angular_resolution: float = 2,\n    cutoff_theta: float = 0,\n    subdivision_level: int | None = None,\n    phi_rotation: float = 0,\n) -&gt; None:\n    \"\"\"Initialize the geodesic grid builder.\n\n    Parameters\n    ----------\n    angular_resolution : float, default 2\n        Angular resolution in degrees.\n    cutoff_theta : float, default 0\n        Maximum polar angle cutoff in degrees.\n    subdivision_level : int | None, optional\n        Subdivision level override.\n    phi_rotation : float, default 0\n        Rotation angle in degrees.\n\n    \"\"\"\n    super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n    self._triangles: np.ndarray | None = None\n\n    if subdivision_level is None:\n        target_edge_deg = angular_resolution * 2\n        self.subdivision_level = max(\n            0,\n            int(np.ceil(np.log2(63.4 / target_edge_deg))),\n        )\n    else:\n        self.subdivision_level = subdivision_level\n\n    self._logger.info(\n        f\"Geodesic: subdivision_level={self.subdivision_level}, \"\n        f\"~{20 * 4**self.subdivision_level} triangles\"\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder.get_triangles","level":3,"title":"<code>get_triangles()</code>","text":"<p>Return triangle vertex coordinates for visualization.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder.get_triangles--returns","level":5,"title":"Returns","text":"<p>triangles : np.ndarray or None     Array of shape <code>(n_faces, 3, 3)</code> where <code>triangles[i]</code> contains     the three 3D unit-sphere vertices of triangle i.  <code>None</code> if     the grid has not been built yet.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/geodesic_grid.py</code> <pre><code>def get_triangles(self) -&gt; np.ndarray | None:\n    \"\"\"Return triangle vertex coordinates for visualization.\n\n    Returns\n    -------\n    triangles : np.ndarray or None\n        Array of shape ``(n_faces, 3, 3)`` where ``triangles[i]`` contains\n        the three 3D unit-sphere vertices of triangle *i*.  ``None`` if\n        the grid has not been built yet.\n\n    \"\"\"\n    return self._triangles\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.GeodesicBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"geodesic\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/geodesic_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"geodesic\"``\n\n    \"\"\"\n    return GridType.GEODESIC.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.FibonacciBuilder","level":2,"title":"<code>FibonacciBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Fibonacci sphere grid with spherical Voronoi tessellation.</p> <p>Points are distributed on the sphere using the Fibonacci lattice (golden-spiral method), which provides one of the most uniform point distributions achievable on a sphere without iterative optimisation.  Each point then becomes the centre of a spherical Voronoi cell — the region of the sphere closer to that point than to any other.  The resulting tessellation has no polar singularities and near-uniform cell areas.</p> <p>The tessellation is computed by <code>scipy.spatial.SphericalVoronoi</code>. Because Voronoi cells have curvilinear boundaries, the <code>phi_min/max</code> and <code>theta_min/max</code> columns in the grid are axis-aligned bounding boxes, not the true cell boundaries.  They are unreliable for spatial queries — use the <code>voronoi_region</code> column (vertex indices into the <code>SphericalVoronoi.vertices</code> array) for exact geometry.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.FibonacciBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,   π/2 = horizon)</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.FibonacciBuilder--what-n_points-resolution-means","level":4,"title":"What <code>n_points</code> (resolution) means","text":"<p>Resolution is controlled by <code>n_points</code>, the number of Voronoi cells in the hemisphere.  When <code>n_points</code> is not supplied it is estimated from <code>angular_resolution</code> via::</p> <pre><code>cell_area  ≈ angular_resolution²   (radians²)\nn_points   = max(10, round(2π / cell_area))\n</code></pre> <p>The approximate cell \"diameter\" (assuming a circular cell of equal area) is::</p> <pre><code>d ≈ 2 √(2π / n_points)   (radians)\n  ≈ 2 × angular_resolution\n</code></pre> <p><code>angular_resolution</code> therefore has no direct geometric meaning for this grid type — it is only a convenience for the <code>n_points</code> estimator.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.FibonacciBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li> <p>Full-sphere Fibonacci lattice – <code>2 × n_points</code> points are    generated on the unit sphere.  Point i has::</p> <p>θᵢ = arccos(1 − 2(i + 0.5) / N)    φᵢ = 2π (i + 0.5) / φ_golden   (mod 2π)</p> </li> </ol> <p>where <code>N = 2 × n_points</code> and <code>φ_golden = (1+√5)/2</code>.  The    <code>+0.5</code> offset avoids placing points exactly at the poles. 2. Hemisphere filter – points with <code>θ &gt; π/2 − cutoff_theta</code>    are discarded. 3. Spherical Voronoi tessellation –    <code>scipy.spatial.SphericalVoronoi</code> computes the Voronoi diagram    on the unit sphere.  Regions are sorted so that vertices appear    in counter-clockwise order around each cell. 4. Bounding boxes – axis-aligned bounding boxes in (phi, theta)    are computed from the Voronoi vertex coordinates.  These are    approximations only (see caveat above).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.FibonacciBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Approximate angular resolution in degrees.  Used only to estimate     <code>n_points</code> when that parameter is not given explicitly. cutoff_theta : float     Elevation mask angle in degrees.  Points below this elevation are     excluded before tessellation. n_points : int or None     Target number of Voronoi cells in the hemisphere.  If <code>None</code>,     estimated from <code>angular_resolution</code>. phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.FibonacciBuilder--raises","level":4,"title":"Raises","text":"<p>ImportError     If <code>scipy</code> is not installed. ValueError     If fewer than 4 points survive the hemisphere filter.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.FibonacciBuilder--notes","level":4,"title":"Notes","text":"<p>The <code>theta_lims</code>, <code>phi_lims</code>, and <code>cell_ids</code> fields of the returned <code>GridData</code> are synthetic evenly-spaced arrays kept only for interface compatibility with ring-based grids.  They do not describe the actual Voronoi cell layout.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/fibonacci_grid.py</code> <pre><code>class FibonacciBuilder(BaseGridBuilder):\n    \"\"\"Fibonacci sphere grid with spherical Voronoi tessellation.\n\n    Points are distributed on the sphere using the *Fibonacci lattice*\n    (golden-spiral method), which provides one of the most uniform\n    point distributions achievable on a sphere without iterative\n    optimisation.  Each point then becomes the centre of a *spherical\n    Voronoi cell* — the region of the sphere closer to that point than\n    to any other.  The resulting tessellation has no polar singularities\n    and near-uniform cell areas.\n\n    The tessellation is computed by ``scipy.spatial.SphericalVoronoi``.\n    Because Voronoi cells have curvilinear boundaries, the ``phi_min/max``\n    and ``theta_min/max`` columns in the grid are axis-aligned *bounding\n    boxes*, **not** the true cell boundaries.  They are unreliable for\n    spatial queries — use the ``voronoi_region`` column (vertex indices\n    into the ``SphericalVoronoi.vertices`` array) for exact geometry.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,\n      π/2 = horizon)\n\n    What ``n_points`` (resolution) means\n    -------------------------------------\n    Resolution is controlled by ``n_points``, the number of Voronoi cells\n    in the hemisphere.  When ``n_points`` is not supplied it is estimated\n    from ``angular_resolution`` via::\n\n        cell_area  ≈ angular_resolution²   (radians²)\n        n_points   = max(10, round(2π / cell_area))\n\n    The approximate cell \"diameter\" (assuming a circular cell of equal area)\n    is::\n\n        d ≈ 2 √(2π / n_points)   (radians)\n          ≈ 2 × angular_resolution\n\n    ``angular_resolution`` therefore has **no direct geometric meaning** for\n    this grid type — it is only a convenience for the ``n_points`` estimator.\n\n    Mathematical construction\n    -------------------------\n    1. **Full-sphere Fibonacci lattice** – ``2 × n_points`` points are\n       generated on the unit sphere.  Point *i* has::\n\n           θᵢ = arccos(1 − 2(i + 0.5) / N)\n           φᵢ = 2π (i + 0.5) / φ_golden   (mod 2π)\n\n       where ``N = 2 × n_points`` and ``φ_golden = (1+√5)/2``.  The\n       ``+0.5`` offset avoids placing points exactly at the poles.\n    2. **Hemisphere filter** – points with ``θ &gt; π/2 − cutoff_theta``\n       are discarded.\n    3. **Spherical Voronoi tessellation** –\n       ``scipy.spatial.SphericalVoronoi`` computes the Voronoi diagram\n       on the unit sphere.  Regions are sorted so that vertices appear\n       in counter-clockwise order around each cell.\n    4. **Bounding boxes** – axis-aligned bounding boxes in (phi, theta)\n       are computed from the Voronoi vertex coordinates.  These are\n       approximations only (see caveat above).\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Approximate angular resolution in degrees.  Used only to estimate\n        ``n_points`` when that parameter is not given explicitly.\n    cutoff_theta : float\n        Elevation mask angle in degrees.  Points below this elevation are\n        excluded before tessellation.\n    n_points : int or None\n        Target number of Voronoi cells in the hemisphere.  If ``None``,\n        estimated from ``angular_resolution``.\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    Raises\n    ------\n    ImportError\n        If ``scipy`` is not installed.\n    ValueError\n        If fewer than 4 points survive the hemisphere filter.\n\n    Notes\n    -----\n    The ``theta_lims``, ``phi_lims``, and ``cell_ids`` fields of the returned\n    ``GridData`` are *synthetic* evenly-spaced arrays kept only for interface\n    compatibility with ring-based grids.  They do **not** describe the actual\n    Voronoi cell layout.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        angular_resolution: float = 2,\n        cutoff_theta: float = 0,\n        n_points: int | None = None,\n        phi_rotation: float = 0,\n    ) -&gt; None:\n        \"\"\"Initialize the Fibonacci grid builder.\n\n        Parameters\n        ----------\n        angular_resolution : float, default 2\n            Angular resolution in degrees.\n        cutoff_theta : float, default 0\n            Maximum polar angle cutoff in degrees.\n        n_points : int | None, optional\n            Number of points to generate.\n        phi_rotation : float, default 0\n            Rotation angle in degrees.\n\n        \"\"\"\n        super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n        if n_points is None:\n            cell_area = self.angular_resolution_rad**2\n            hemisphere_area = 2 * np.pi\n            self.n_points = max(10, int(hemisphere_area / cell_area))\n        else:\n            self.n_points = n_points\n\n        self._logger.info(f\"Fibonacci: generating {self.n_points} points\")\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"fibonacci\"``\n\n        \"\"\"\n        return GridType.FIBONACCI.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[\n        pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray], dict[str, Any]\n    ]:\n        \"\"\"Build Fibonacci sphere grid with Voronoi tessellation.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per Voronoi cell.  Contains phi, theta (centre),\n            bounding-box limits, ``voronoi_region`` (list of vertex indices\n            into the Voronoi vertex array), and ``n_vertices``.\n        theta_lims : np.ndarray\n            Synthetic evenly-spaced theta limits (interface compatibility only).\n        phi_lims : list[np.ndarray]\n            Synthetic evenly-spaced phi limits (interface compatibility only).\n        cell_ids : list[np.ndarray]\n            Single-element list containing all cell ids.\n        extra_kwargs : dict\n            Contains ``voronoi`` (the ``SphericalVoronoi`` object) and\n            ``points_xyz`` (the hemisphere point cloud, shape\n            ``(n_points, 3)``).\n\n        \"\"\"\n        points_xyz = self._generate_fibonacci_sphere(self.n_points * 2)\n\n        # Convert to spherical\n        x, y, z = points_xyz[:, 0], points_xyz[:, 1], points_xyz[:, 2]\n        theta = np.arccos(np.clip(z, -1, 1))\n        phi = np.arctan2(y, x)\n        phi = np.mod(phi, 2 * np.pi)\n\n        # Filter to northern hemisphere\n        mask = (theta &lt;= (np.pi / 2 - self.cutoff_theta_rad)) &amp; (theta &gt;= 0)\n\n        phi = phi[mask]\n        theta = theta[mask]\n        points_xyz = points_xyz[mask]\n\n        if len(points_xyz) &lt; 4:\n            raise ValueError(\"Not enough points in hemisphere for Voronoi tessellation\")\n\n        # Compute spherical Voronoi tessellation\n        try:\n            from scipy.spatial import SphericalVoronoi\n\n            sv = SphericalVoronoi(points_xyz, radius=1, threshold=1e-10)\n            sv.sort_vertices_of_regions()\n\n        except ImportError:\n            raise ImportError(\n                \"scipy required for Fibonacci grid. Install: pip install scipy\"\n            )\n\n        # Create cells\n        cells = []\n        for point_idx, (p_phi, p_theta) in enumerate(zip(phi, theta)):\n            region_vertices = sv.regions[point_idx]\n\n            if -1 in region_vertices:\n                continue\n\n            region_coords = sv.vertices[region_vertices]\n\n            # Convert region vertices to spherical\n            rv_x, rv_y, rv_z = (\n                region_coords[:, 0],\n                region_coords[:, 1],\n                region_coords[:, 2],\n            )\n            rv_theta = np.arccos(np.clip(rv_z, -1, 1))\n            rv_phi = np.arctan2(rv_y, rv_x)\n            rv_phi = np.mod(rv_phi, 2 * np.pi)\n\n            cells.append(\n                {\n                    \"phi\": p_phi,\n                    \"theta\": p_theta,\n                    \"phi_min\": np.min(rv_phi),\n                    \"phi_max\": np.max(rv_phi),\n                    \"theta_min\": np.min(rv_theta),\n                    \"theta_max\": np.max(rv_theta),\n                    \"voronoi_region\": (\n                        region_vertices\n                        if isinstance(region_vertices, list)\n                        else region_vertices.tolist()\n                    ),\n                    \"n_vertices\": len(region_vertices),\n                }\n            )\n\n        grid = pl.DataFrame(cells).with_columns(\n            pl.int_range(0, pl.len()).alias(\"cell_id\")\n        )\n\n        extra_kwargs: dict[str, Any] = {\n            \"voronoi\": sv,\n            \"points_xyz\": points_xyz,\n        }\n\n        theta_lims = np.linspace(0, np.pi / 2, 10)\n        phi_lims = [np.linspace(0, 2 * np.pi, 20) for _ in range(len(theta_lims))]\n        cell_ids_list = [np.arange(grid.height)]\n\n        return grid, theta_lims, phi_lims, cell_ids_list, extra_kwargs\n\n    def _generate_fibonacci_sphere(self, n: int) -&gt; np.ndarray:\n        \"\"\"Generate points on the unit sphere using the golden-spiral lattice.\n\n        Parameters\n        ----------\n        n : int\n            Total number of points on the full sphere.\n\n        Returns\n        -------\n        points : np.ndarray\n            Shape ``(n, 3)`` – Cartesian (x, y, z) coordinates on the unit\n            sphere.\n\n        \"\"\"\n        golden_ratio = (1 + np.sqrt(5)) / 2\n\n        indices = np.arange(0, n, dtype=np.float64) + 0.5\n\n        # Polar angle\n        theta = np.arccos(1 - 2 * indices / n)\n\n        # Azimuthal angle\n        phi = 2 * np.pi * indices / golden_ratio\n        phi = np.mod(phi, 2 * np.pi)\n\n        # Convert to Cartesian\n        x = np.sin(theta) * np.cos(phi)\n        y = np.sin(theta) * np.sin(phi)\n        z = np.cos(theta)\n\n        return np.column_stack([x, y, z])\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.FibonacciBuilder.__init__","level":3,"title":"<code>__init__(angular_resolution=2, cutoff_theta=0, n_points=None, phi_rotation=0)</code>","text":"<p>Initialize the Fibonacci grid builder.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.FibonacciBuilder.__init__--parameters","level":5,"title":"Parameters","text":"<p>angular_resolution : float, default 2     Angular resolution in degrees. cutoff_theta : float, default 0     Maximum polar angle cutoff in degrees. n_points : int | None, optional     Number of points to generate. phi_rotation : float, default 0     Rotation angle in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/fibonacci_grid.py</code> <pre><code>def __init__(\n    self,\n    angular_resolution: float = 2,\n    cutoff_theta: float = 0,\n    n_points: int | None = None,\n    phi_rotation: float = 0,\n) -&gt; None:\n    \"\"\"Initialize the Fibonacci grid builder.\n\n    Parameters\n    ----------\n    angular_resolution : float, default 2\n        Angular resolution in degrees.\n    cutoff_theta : float, default 0\n        Maximum polar angle cutoff in degrees.\n    n_points : int | None, optional\n        Number of points to generate.\n    phi_rotation : float, default 0\n        Rotation angle in degrees.\n\n    \"\"\"\n    super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n    if n_points is None:\n        cell_area = self.angular_resolution_rad**2\n        hemisphere_area = 2 * np.pi\n        self.n_points = max(10, int(hemisphere_area / cell_area))\n    else:\n        self.n_points = n_points\n\n    self._logger.info(f\"Fibonacci: generating {self.n_points} points\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.FibonacciBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.FibonacciBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"fibonacci\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/fibonacci_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"fibonacci\"``\n\n    \"\"\"\n    return GridType.FIBONACCI.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder","level":2,"title":"<code>HTMBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Hierarchical Triangular Mesh (HTM) grid.</p> <p>HTM divides the sphere into an octahedron (8 triangular faces), then recursively subdivides each face into 4 smaller triangles by inserting edge-midpoint vertices projected onto the unit sphere.  The recursion depth is controlled by <code>htm_level</code>.  This produces a strictly hierarchical triangulation: every triangle at level n is the union of exactly 4 triangles at level n + 1.</p> <p>Cell areas are approximately equal but not strictly so — area uniformity improves with level because the icosahedral edge-length asymmetry averages out over many subdivisions.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,   π/2 = horizon)</li> </ul> <p>Cell centres are the 3D Cartesian mean of the three triangle vertices, re-normalised onto the unit sphere.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder--what-htm_level-resolution-means","level":4,"title":"What <code>htm_level</code> (resolution) means","text":"<p>The resolution is set by <code>htm_level</code>, not by <code>angular_resolution</code>. <code>angular_resolution</code> is used only to estimate an appropriate level when <code>htm_level</code> is not supplied explicitly.  The heuristic is::</p> <pre><code>target_edge ≈ 2 × angular_resolution   (degrees)\nhtm_level   = min(15, ceil(log₂(90 / target_edge)))\n</code></pre> <p>The approximate triangle edge length at level n is::</p> <pre><code>edge ≈ 90° / 2ⁿ\n</code></pre> Level Triangles (full sphere) Approx edge 0 8 90° 1 32 45° 2 128 22.5° 3 512 11.25° 4 2 048 5.6° n 8 × 4ⁿ 90° / 2ⁿ","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li>Octahedron – 6 vertices at ±x, ±y, ±z on the unit sphere, forming    8 triangular faces (4 northern, 4 southern).</li> <li> <p>Subdivision – for each triangle [v₀, v₁, v₂], three edge    midpoints are computed and projected onto the unit sphere::</p> <p>m₀ = normalise((v₀ + v₁) / 2)    m₁ = normalise((v₁ + v₂) / 2)    m₂ = normalise((v₂ + v₀) / 2)</p> </li> </ol> <p>The four children are [v₀, m₀, m₂], [v₁, m₁, m₀], [v₂, m₂, m₁],    and [m₀, m₁, m₂].  This is repeated <code>htm_level</code> times. 3. Hemisphere filter – a triangle is kept if any of its three    vertices satisfies <code>theta ≤ π/2 − cutoff_theta</code>.  Boundary    triangles that straddle the horizon are therefore included and may    extend slightly below it. 4. Each leaf triangle becomes one cell; its centre, bounding box, and    three vertex coordinates are stored.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Approximate angular resolution in degrees.  Used only to derive     <code>htm_level</code> when that parameter is not given explicitly. cutoff_theta : float     Elevation mask angle in degrees.  Triangles are excluded only when     all their vertices are below this elevation. htm_level : int or None     HTM subdivision depth.  If <code>None</code>, estimated from     <code>angular_resolution</code>.  Practical range 0–15. phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder--notes","level":4,"title":"Notes","text":"<p>The <code>theta_lims</code>, <code>phi_lims</code>, and <code>cell_ids</code> fields of the returned <code>GridData</code> are synthetic evenly-spaced arrays kept only for interface compatibility with ring-based grids.  They do not describe the actual triangular cell layout.</p> <p>HTM IDs in this implementation use a decimal-digit scheme (<code>parent_id × 10 + child_index</code>) which diverges from the original SDSS HTM binary-coded ID scheme.  This is adequate for indexing but should not be compared with external HTM catalogues.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder--references","level":4,"title":"References","text":"<p>Kunszt et al. (2001): \"The Hierarchical Triangular Mesh\" https://www.sdss.org/dr12/algorithms/htm/</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/htm_grid.py</code> <pre><code>class HTMBuilder(BaseGridBuilder):\n    \"\"\"Hierarchical Triangular Mesh (HTM) grid.\n\n    HTM divides the sphere into an octahedron (8 triangular faces), then\n    recursively subdivides each face into 4 smaller triangles by inserting\n    edge-midpoint vertices projected onto the unit sphere.  The recursion\n    depth is controlled by ``htm_level``.  This produces a strictly\n    hierarchical triangulation: every triangle at level *n* is the union of\n    exactly 4 triangles at level *n* + 1.\n\n    Cell areas are *approximately* equal but not strictly so — area\n    uniformity improves with level because the icosahedral edge-length\n    asymmetry averages out over many subdivisions.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,\n      π/2 = horizon)\n\n    Cell centres are the 3D Cartesian mean of the three triangle vertices,\n    re-normalised onto the unit sphere.\n\n    What ``htm_level`` (resolution) means\n    --------------------------------------\n    The resolution is set by ``htm_level``, **not** by ``angular_resolution``.\n    ``angular_resolution`` is used only to *estimate* an appropriate level\n    when ``htm_level`` is not supplied explicitly.  The heuristic is::\n\n        target_edge ≈ 2 × angular_resolution   (degrees)\n        htm_level   = min(15, ceil(log₂(90 / target_edge)))\n\n    The approximate triangle edge length at level *n* is::\n\n        edge ≈ 90° / 2ⁿ\n\n    | Level | Triangles (full sphere) | Approx edge |\n    |-------|-------------------------|-------------|\n    | 0     | 8                       | 90°         |\n    | 1     | 32                      | 45°         |\n    | 2     | 128                     | 22.5°       |\n    | 3     | 512                     | 11.25°      |\n    | 4     | 2 048                   | 5.6°        |\n    | n     | 8 × 4ⁿ                  | 90° / 2ⁿ   |\n\n    Mathematical construction\n    -------------------------\n    1. **Octahedron** – 6 vertices at ±x, ±y, ±z on the unit sphere, forming\n       8 triangular faces (4 northern, 4 southern).\n    2. **Subdivision** – for each triangle [v₀, v₁, v₂], three edge\n       midpoints are computed and projected onto the unit sphere::\n\n           m₀ = normalise((v₀ + v₁) / 2)\n           m₁ = normalise((v₁ + v₂) / 2)\n           m₂ = normalise((v₂ + v₀) / 2)\n\n       The four children are [v₀, m₀, m₂], [v₁, m₁, m₀], [v₂, m₂, m₁],\n       and [m₀, m₁, m₂].  This is repeated ``htm_level`` times.\n    3. **Hemisphere filter** – a triangle is kept if *any* of its three\n       vertices satisfies ``theta ≤ π/2 − cutoff_theta``.  Boundary\n       triangles that straddle the horizon are therefore included and may\n       extend slightly below it.\n    4. Each leaf triangle becomes one cell; its centre, bounding box, and\n       three vertex coordinates are stored.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Approximate angular resolution in degrees.  Used only to derive\n        ``htm_level`` when that parameter is not given explicitly.\n    cutoff_theta : float\n        Elevation mask angle in degrees.  Triangles are excluded only when\n        *all* their vertices are below this elevation.\n    htm_level : int or None\n        HTM subdivision depth.  If ``None``, estimated from\n        ``angular_resolution``.  Practical range 0–15.\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    Notes\n    -----\n    The ``theta_lims``, ``phi_lims``, and ``cell_ids`` fields of the returned\n    ``GridData`` are *synthetic* evenly-spaced arrays kept only for interface\n    compatibility with ring-based grids.  They do **not** describe the actual\n    triangular cell layout.\n\n    HTM IDs in this implementation use a decimal-digit scheme\n    (``parent_id × 10 + child_index``) which diverges from the original\n    SDSS HTM binary-coded ID scheme.  This is adequate for indexing but\n    should not be compared with external HTM catalogues.\n\n    References\n    ----------\n    Kunszt et al. (2001): \"The Hierarchical Triangular Mesh\"\n    https://www.sdss.org/dr12/algorithms/htm/\n\n    \"\"\"\n\n    def __init__(\n        self,\n        angular_resolution: float = 2,\n        cutoff_theta: float = 0,\n        htm_level: int | None = None,\n        phi_rotation: float = 0,\n    ) -&gt; None:\n        \"\"\"Initialize the HTM grid builder.\n\n        Parameters\n        ----------\n        angular_resolution : float, default 2\n            Angular resolution in degrees.\n        cutoff_theta : float, default 0\n            Maximum polar angle cutoff in degrees.\n        htm_level : int | None, optional\n            HTM subdivision level.\n        phi_rotation : float, default 0\n            Rotation angle in degrees.\n\n        \"\"\"\n        super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n        if htm_level is None:\n            target_edge_deg = angular_resolution * 2\n            self.htm_level = max(\n                0,\n                int(np.ceil(np.log2(90 / target_edge_deg))),\n            )\n            self.htm_level = min(self.htm_level, 15)\n        else:\n            self.htm_level = htm_level\n\n        self._logger.info(\n            f\"HTM: level={self.htm_level}, ~{8 * 4**self.htm_level} triangles\"\n        )\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"htm\"``\n\n        \"\"\"\n        return GridType.HTM.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]:\n        \"\"\"Build HTM grid by recursive octahedron subdivision.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per triangular cell.  Contains phi, theta (centre),\n            bounding-box limits, ``htm_id``, ``htm_level``, and the three\n            vertex coordinate columns ``htm_vertex_0/1/2`` (each a list of\n            3 floats in Cartesian xyz).\n        theta_lims : np.ndarray\n            Synthetic evenly-spaced theta limits (interface compatibility only).\n        phi_lims : list[np.ndarray]\n            Synthetic evenly-spaced phi limits (interface compatibility only).\n        cell_ids : list[np.ndarray]\n            Single-element list containing all cell ids.\n\n        \"\"\"\n        base_vertices = np.array(\n            [\n                [0, 0, 1],  # 0: North pole\n                [1, 0, 0],  # 1: +X\n                [0, 1, 0],  # 2: +Y\n                [-1, 0, 0],  # 3: -X\n                [0, -1, 0],  # 4: -Y\n                [0, 0, -1],  # 5: South pole\n            ],\n            dtype=np.float64,\n        )\n\n        base_faces = [\n            [0, 1, 2],\n            [0, 2, 3],\n            [0, 3, 4],\n            [0, 4, 1],  # Northern\n            [5, 2, 1],\n            [5, 3, 2],\n            [5, 4, 3],\n            [5, 1, 4],  # Southern\n        ]\n\n        all_triangles = []\n        all_htm_ids = []\n\n        for base_idx, base_face in enumerate(base_faces):\n            v0 = base_vertices[base_face[0]]\n            v1 = base_vertices[base_face[1]]\n            v2 = base_vertices[base_face[2]]\n\n            triangles, ids = self._subdivide_htm([v0, v1, v2], base_idx, self.htm_level)\n            all_triangles.extend(triangles)\n            all_htm_ids.extend(ids)\n\n        # Convert to cells\n        cells = []\n        for tri, htm_id in zip(all_triangles, all_htm_ids):\n            v0, v1, v2 = tri\n\n            # Center\n            center = (v0 + v1 + v2) / 3\n            center = center / np.linalg.norm(center)\n\n            theta_center = np.arccos(np.clip(center[2], -1, 1))\n            phi_center = np.arctan2(center[1], center[0])\n            phi_center = np.mod(phi_center, 2 * np.pi)\n\n            # Filter hemisphere\n            vertex_thetas = [np.arccos(np.clip(v[2], -1, 1)) for v in [v0, v1, v2]]\n            if all(t &gt; (np.pi / 2 - self.cutoff_theta_rad) for t in vertex_thetas):\n                continue\n\n            # Vertex coords\n            thetas, phis = [], []\n            for v in [v0, v1, v2]:\n                t = np.arccos(np.clip(v[2], -1, 1))\n                p = np.arctan2(v[1], v[0])\n                p = np.mod(p, 2 * np.pi)\n                thetas.append(t)\n                phis.append(p)\n\n            cells.append(\n                {\n                    \"phi\": phi_center,\n                    \"theta\": theta_center,\n                    \"phi_min\": min(phis),\n                    \"phi_max\": max(phis),\n                    \"theta_min\": min(thetas),\n                    \"theta_max\": max(thetas),\n                    \"htm_id\": htm_id,\n                    \"htm_level\": self.htm_level,\n                    \"htm_vertex_0\": v0.tolist(),\n                    \"htm_vertex_1\": v1.tolist(),\n                    \"htm_vertex_2\": v2.tolist(),\n                }\n            )\n\n        grid = pl.DataFrame(cells).with_columns(\n            pl.int_range(0, pl.len()).alias(\"cell_id\")\n        )\n\n        theta_lims = np.linspace(0, np.pi / 2, 10)\n        phi_lims = [np.linspace(0, 2 * np.pi, 20) for _ in range(len(theta_lims))]\n        cell_ids_list = [grid[\"cell_id\"].to_numpy()]\n\n        return grid, theta_lims, phi_lims, cell_ids_list\n\n    def _subdivide_htm(\n        self,\n        tri: list,\n        htm_id: int,\n        target_level: int,\n        current_level: int = 0,\n    ) -&gt; tuple[list, list]:\n        \"\"\"Recursively subdivide a single triangle.\n\n        Parameters\n        ----------\n        tri : list of np.ndarray\n            Three vertex arrays [v₀, v₁, v₂], each shape ``(3,)``.\n        htm_id : int\n            Current HTM identifier for this triangle.\n        target_level : int\n            Recursion depth to reach.\n        current_level : int\n            Current recursion depth.\n\n        Returns\n        -------\n        triangles : list of list\n            Leaf triangles at ``target_level``.\n        ids : list of int\n            Corresponding HTM identifiers.\n\n        \"\"\"\n        if current_level == target_level:\n            return [tri], [htm_id]\n\n        v0, v1, v2 = tri\n\n        # Midpoints on sphere\n        m0 = (v0 + v1) / 2\n        m0 = m0 / np.linalg.norm(m0)\n        m1 = (v1 + v2) / 2\n        m1 = m1 / np.linalg.norm(m1)\n        m2 = (v2 + v0) / 2\n        m2 = m2 / np.linalg.norm(m2)\n\n        # 4 children\n        children = [[v0, m0, m2], [v1, m1, m0], [v2, m2, m1], [m0, m1, m2]]\n\n        all_tris = []\n        all_ids = []\n\n        for child_idx, child in enumerate(children):\n            child_id = htm_id * 10 + child_idx\n            tris, ids = self._subdivide_htm(\n                child,\n                child_id,\n                target_level,\n                current_level + 1,\n            )\n            all_tris.extend(tris)\n            all_ids.extend(ids)\n\n        return all_tris, all_ids\n\n    def get_htm_info(self) -&gt; dict:\n        \"\"\"Get HTM-specific information.\n\n        Returns\n        -------\n        info : dict\n            Keys: ``htm_level``, ``n_triangles_full_sphere``,\n            ``approx_edge_length_deg``, ``approx_edge_length_arcmin``.\n\n        \"\"\"\n        n_triangles = 8 * 4**self.htm_level\n        approx_edge_deg = 90 / (2**self.htm_level)\n\n        return {\n            \"htm_level\": self.htm_level,\n            \"n_triangles_full_sphere\": n_triangles,\n            \"approx_edge_length_deg\": approx_edge_deg,\n            \"approx_edge_length_arcmin\": approx_edge_deg * 60,\n        }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder.__init__","level":3,"title":"<code>__init__(angular_resolution=2, cutoff_theta=0, htm_level=None, phi_rotation=0)</code>","text":"<p>Initialize the HTM grid builder.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder.__init__--parameters","level":5,"title":"Parameters","text":"<p>angular_resolution : float, default 2     Angular resolution in degrees. cutoff_theta : float, default 0     Maximum polar angle cutoff in degrees. htm_level : int | None, optional     HTM subdivision level. phi_rotation : float, default 0     Rotation angle in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/htm_grid.py</code> <pre><code>def __init__(\n    self,\n    angular_resolution: float = 2,\n    cutoff_theta: float = 0,\n    htm_level: int | None = None,\n    phi_rotation: float = 0,\n) -&gt; None:\n    \"\"\"Initialize the HTM grid builder.\n\n    Parameters\n    ----------\n    angular_resolution : float, default 2\n        Angular resolution in degrees.\n    cutoff_theta : float, default 0\n        Maximum polar angle cutoff in degrees.\n    htm_level : int | None, optional\n        HTM subdivision level.\n    phi_rotation : float, default 0\n        Rotation angle in degrees.\n\n    \"\"\"\n    super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n    if htm_level is None:\n        target_edge_deg = angular_resolution * 2\n        self.htm_level = max(\n            0,\n            int(np.ceil(np.log2(90 / target_edge_deg))),\n        )\n        self.htm_level = min(self.htm_level, 15)\n    else:\n        self.htm_level = htm_level\n\n    self._logger.info(\n        f\"HTM: level={self.htm_level}, ~{8 * 4**self.htm_level} triangles\"\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"htm\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/htm_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"htm\"``\n\n    \"\"\"\n    return GridType.HTM.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder.get_htm_info","level":3,"title":"<code>get_htm_info()</code>","text":"<p>Get HTM-specific information.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.HTMBuilder.get_htm_info--returns","level":5,"title":"Returns","text":"<p>info : dict     Keys: <code>htm_level</code>, <code>n_triangles_full_sphere</code>,     <code>approx_edge_length_deg</code>, <code>approx_edge_length_arcmin</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/htm_grid.py</code> <pre><code>def get_htm_info(self) -&gt; dict:\n    \"\"\"Get HTM-specific information.\n\n    Returns\n    -------\n    info : dict\n        Keys: ``htm_level``, ``n_triangles_full_sphere``,\n        ``approx_edge_length_deg``, ``approx_edge_length_arcmin``.\n\n    \"\"\"\n    n_triangles = 8 * 4**self.htm_level\n    approx_edge_deg = 90 / (2**self.htm_level)\n\n    return {\n        \"htm_level\": self.htm_level,\n        \"n_triangles_full_sphere\": n_triangles,\n        \"approx_edge_length_deg\": approx_edge_deg,\n        \"approx_edge_length_arcmin\": approx_edge_deg * 60,\n    }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.CellAggregator","level":2,"title":"<code>CellAggregator</code>","text":"<p>Polars-based per-cell aggregation helpers.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>class CellAggregator:\n    \"\"\"Polars-based per-cell aggregation helpers.\"\"\"\n\n    @staticmethod\n    def aggregate_by_cell(\n        df: pl.DataFrame,\n        value_var: str = \"VOD\",\n        method: str = \"mean\",\n    ) -&gt; pl.DataFrame:\n        \"\"\"Aggregate values by ``cell_id``.\n\n        Parameters\n        ----------\n        df : pl.DataFrame\n            Must contain ``cell_id`` and *value_var* columns.\n        value_var : str\n            Column to aggregate.\n        method : {'mean', 'median', 'std', 'count'}\n            Aggregation method.\n\n        Returns\n        -------\n        pl.DataFrame\n            Two-column DataFrame: ``cell_id``, *value_var*.\n\n        \"\"\"\n        if \"cell_id\" not in df.columns:\n            raise ValueError(\"pl.DataFrame must have 'cell_id' column\")\n        if value_var not in df.columns:\n            raise ValueError(f\"pl.DataFrame must have '{value_var}' column\")\n\n        agg_map = {\n            \"mean\": pl.col(value_var).mean(),\n            \"median\": pl.col(value_var).median(),\n            \"std\": pl.col(value_var).std(),\n            \"count\": pl.col(value_var).count(),\n        }\n        if method not in agg_map:\n            raise ValueError(f\"Unknown method: {method}\")\n\n        return (\n            df.group_by(\"cell_id\").agg(agg_map[method].alias(value_var)).sort(\"cell_id\")\n        )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.CellAggregator.aggregate_by_cell","level":3,"title":"<code>aggregate_by_cell(df, value_var='VOD', method='mean')</code>  <code>staticmethod</code>","text":"<p>Aggregate values by <code>cell_id</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.CellAggregator.aggregate_by_cell--parameters","level":5,"title":"Parameters","text":"<p>df : pl.DataFrame     Must contain <code>cell_id</code> and value_var columns. value_var : str     Column to aggregate. method : {'mean', 'median', 'std', 'count'}     Aggregation method.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.CellAggregator.aggregate_by_cell--returns","level":5,"title":"Returns","text":"<p>pl.DataFrame     Two-column DataFrame: <code>cell_id</code>, value_var.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>@staticmethod\ndef aggregate_by_cell(\n    df: pl.DataFrame,\n    value_var: str = \"VOD\",\n    method: str = \"mean\",\n) -&gt; pl.DataFrame:\n    \"\"\"Aggregate values by ``cell_id``.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Must contain ``cell_id`` and *value_var* columns.\n    value_var : str\n        Column to aggregate.\n    method : {'mean', 'median', 'std', 'count'}\n        Aggregation method.\n\n    Returns\n    -------\n    pl.DataFrame\n        Two-column DataFrame: ``cell_id``, *value_var*.\n\n    \"\"\"\n    if \"cell_id\" not in df.columns:\n        raise ValueError(\"pl.DataFrame must have 'cell_id' column\")\n    if value_var not in df.columns:\n        raise ValueError(f\"pl.DataFrame must have '{value_var}' column\")\n\n    agg_map = {\n        \"mean\": pl.col(value_var).mean(),\n        \"median\": pl.col(value_var).median(),\n        \"std\": pl.col(value_var).std(),\n        \"count\": pl.col(value_var).count(),\n    }\n    if method not in agg_map:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    return (\n        df.group_by(\"cell_id\").agg(agg_map[method].alias(value_var)).sort(\"cell_id\")\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow","level":2,"title":"<code>AdaptedVODWorkflow</code>","text":"<p>Core VOD analysis workflow with polars-optimised loading and refined temporal matching.</p> <p>All heavy lifting (filtering, grid operations) is delegated to <code>canvod.grids.analysis</code>.  This class is responsible only for Icechunk I/O and orchestration.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow--parameters","level":4,"title":"Parameters","text":"<p>vod_store_path : Path or str     Path to the VOD Icechunk store directory.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>class AdaptedVODWorkflow:\n    \"\"\"Core VOD analysis workflow with polars-optimised loading and refined\n    temporal matching.\n\n    All heavy lifting (filtering, grid operations) is delegated to\n    ``canvod.grids.analysis``.  This class is responsible only for\n    Icechunk I/O and orchestration.\n\n    Parameters\n    ----------\n    vod_store_path : Path or str\n        Path to the VOD Icechunk store directory.\n\n    \"\"\"\n\n    def __init__(self, vod_store_path: Path | str) -&gt; None:\n        \"\"\"Initialize the workflow.\n\n        Parameters\n        ----------\n        vod_store_path : Path | str\n            Path to the VOD Icechunk store directory.\n\n        \"\"\"\n        self.vod_store_path = Path(vod_store_path)\n        self.vod_store: MyIcechunkStore = _get_store(self.vod_store_path)\n\n    # ------------------------------------------------------------------\n    # Data loading\n    # ------------------------------------------------------------------\n\n    def load_vod_data(\n        self,\n        group_name: str = \"reference_01_canopy_01\",\n        branch: str = \"main\",\n    ) -&gt; xr.Dataset:\n        \"\"\"Load a VOD dataset from the store.\n\n        Parameters\n        ----------\n        group_name : str\n            Zarr group path inside the store.\n        branch : str\n            Icechunk branch to read from.\n\n        Returns\n        -------\n        xr.Dataset\n            Lazy-loaded VOD dataset.\n\n        \"\"\"\n        logger.info(\"Loading VOD data from branch=%s group=%s\", branch, group_name)\n        with self.vod_store.readonly_session(branch=branch) as session:\n            vod_ds = xr.open_zarr(session.store, group=group_name, consolidated=False)\n        logger.info(\"Loaded VOD dataset: %s\", dict(vod_ds.sizes))\n        return vod_ds\n\n    # ------------------------------------------------------------------\n    # Temporal coverage checks\n    # ------------------------------------------------------------------\n\n    def check_temporal_coverage_compatibility(\n        self,\n        main_ds: xr.Dataset,\n        processed_ds: xr.Dataset,\n        requested_time_range: tuple[datetime.date, datetime.date] | None = None,\n    ) -&gt; tuple[bool, dict[str, Any]]:\n        \"\"\"Check whether *processed_ds* adequately covers a time range.\n\n        When *requested_time_range* is ``None`` the method checks that the\n        processed dataset covers at least 70 % of the main dataset's span.\n        When a range is given it verifies that both endpoints fall within the\n        processed dataset (with a 1-day tolerance).\n\n        Parameters\n        ----------\n        main_ds : xr.Dataset\n            Reference (unfiltered) dataset.\n        processed_ds : xr.Dataset\n            Filtered dataset to validate.\n        requested_time_range : tuple of date, optional\n            ``(start, end)`` to check against.\n\n        Returns\n        -------\n        compatible : bool\n        coverage_info : dict\n            Diagnostic information with ``main_range``, ``processed_range``,\n            and ``requested_range``.\n\n        \"\"\"\n\n        def _date_range(ds: xr.Dataset) -&gt; tuple[datetime.date, datetime.date]:\n            \"\"\"Return the date range for a dataset.\n\n            Parameters\n            ----------\n            ds : xr.Dataset\n                Dataset with an epoch coordinate.\n\n            Returns\n            -------\n            tuple[datetime.date, datetime.date]\n                Start and end dates.\n\n            \"\"\"\n            return (\n                pd.to_datetime(ds.epoch.min().values).date(),\n                pd.to_datetime(ds.epoch.max().values).date(),\n            )\n\n        main_start, main_end = _date_range(main_ds)\n        proc_start, proc_end = _date_range(processed_ds)\n\n        coverage_info: dict[str, Any] = {\n            \"main_range\": (main_start, main_end),\n            \"processed_range\": (proc_start, proc_end),\n            \"requested_range\": requested_time_range,\n        }\n\n        if requested_time_range is None:\n            main_days = (main_end - main_start).days\n            proc_days = (proc_end - proc_start).days\n            ratio = proc_days / main_days if main_days &gt; 0 else 0.0\n            logger.info(\n                \"Coverage check: main=%d days, processed=%d days, ratio=%.1f%%\",\n                main_days,\n                proc_days,\n                ratio * 100,\n            )\n            return ratio &gt;= 0.7, coverage_info\n\n        req_start, req_end = requested_time_range\n        one_day = datetime.timedelta(days=1)\n        start_ok = proc_start &lt;= req_start &lt;= proc_end + one_day\n        end_ok = proc_start - one_day &lt;= req_end &lt;= proc_end\n        compatible = start_ok and end_ok\n\n        if not compatible:\n            logger.warning(\n                \"Temporal coverage mismatch: processed=%s→%s, requested=%s→%s\",\n                proc_start,\n                proc_end,\n                req_start,\n                req_end,\n            )\n        return compatible, coverage_info\n\n    # ------------------------------------------------------------------\n    # Filtering entry-points\n    # ------------------------------------------------------------------\n\n    def create_processed_data_fast_hampel_complete(\n        self,\n        start_date: datetime.date | datetime.datetime,\n        end_date: datetime.date | datetime.datetime,\n        force_recreate: bool = False,\n        window_hours: float = 1.0,\n        sigma_threshold: float = 3.0,\n        min_points: int = 5,\n        ultra_fast_mode: bool = False,\n        cell_batch_size: int = 200,\n        n_workers: int | None = None,\n    ) -&gt; str | None:\n        \"\"\"Run the vectorised / ultra-fast Hampel pipeline end-to-end.\n\n        Delegates the actual filtering to\n        :func:`canvod.grids.analysis.sigma_clip_filter.astropy_hampel_vectorized_fast`\n        (or its ultra-fast variant) and persists the result on a\n        ``processing`` branch.\n\n        Parameters\n        ----------\n        start_date, end_date : date or datetime\n            Temporal extent to process.\n        force_recreate : bool\n            Overwrite existing filtered data.\n        window_hours : float\n            Hampel temporal window in hours.\n        sigma_threshold : float\n            MAD-based outlier threshold.\n        min_points : int\n            Minimum observations required per window.\n        ultra_fast_mode : bool\n            Use the pure-NumPy sigma-clip path (faster, less precise).\n        cell_batch_size : int\n            Number of cells per spatial batch.\n        n_workers : int, optional\n            Parallel workers.  ``None`` → auto-detect.\n\n        Returns\n        -------\n        str or None\n            Icechunk snapshot ID, or ``None`` if existing data was kept.\n\n        \"\"\"\n        return _create_processed_data_fast_hampel(\n            workflow_instance=self,\n            start_date=start_date,\n            end_date=end_date,\n            force_recreate=force_recreate,\n            window_hours=window_hours,\n            sigma_threshold=sigma_threshold,\n            min_points=min_points,\n            ultra_fast_mode=ultra_fast_mode,\n            cell_batch_size=cell_batch_size,\n            n_workers=n_workers,\n        )\n\n    def create_processed_data_hampel_parallel_complete(\n        self,\n        start_date: datetime.date | datetime.datetime,\n        end_date: datetime.date | datetime.datetime,\n        force_recreate: bool = False,\n        threshold: float = 3.0,\n        min_obs_per_sid: int = 20,\n        spatial_batch_size: int = 500,\n        n_workers: int | None = None,\n        temporal_agg: str | None = None,\n        agg_method: str | None = None,\n    ) -&gt; str | None:\n        \"\"\"Run the parallelised cell-SID Hampel pipeline end-to-end.\n\n        Loads the complete requested time range (no temporal chunking) and\n        applies\n        :func:`canvod.grids.analysis.hampel_filtering.aggr_hampel_cell_sid_parallelized`\n        with spatial batching.\n\n        Parameters\n        ----------\n        start_date, end_date : date or datetime\n            Temporal extent to process.\n        force_recreate : bool\n            Overwrite existing filtered data.\n        threshold : float\n            MAD-based outlier threshold.\n        min_obs_per_sid : int\n            Minimum observations per cell-SID combination.\n        spatial_batch_size : int\n            Cells per spatial batch.\n        n_workers : int, optional\n            Parallel workers.  ``None`` → auto-detect.\n        temporal_agg : str, optional\n            Post-filtering aggregation frequency (e.g. ``'1H'``, ``'1D'``).\n        agg_method : str, optional\n            Aggregation method (e.g. ``'mean'``).\n\n        Returns\n        -------\n        str or None\n            Icechunk snapshot ID, or ``None`` if existing data was kept.\n\n        \"\"\"\n        from canvod.grids import create_hemigrid\n        from canvod.grids.analysis.hampel_filtering import (\n            aggr_hampel_cell_sid_parallelized,\n        )\n        from canvod.grids.operations import add_cell_ids_to_ds_fast\n\n        logger.info(\"=\" * 60)\n        logger.info(\"PARALLEL HAMPEL — complete temporal coverage\")\n        logger.info(\n            \"Range: %s → %s | threshold=%.1f | min_obs=%d | batch=%d | workers=%s\",\n            start_date,\n            end_date,\n            threshold,\n            min_obs_per_sid,\n            spatial_batch_size,\n            n_workers or \"auto\",\n        )\n\n        # --- guard: existing data ---\n        if not self._force_or_skip(\"processing\", force_recreate):\n            return None\n\n        # --- load complete time range ---\n        logger.info(\"Loading complete time range for parallel processing\")\n        with self.vod_store.readonly_session(branch=\"main\") as session:\n            vod_ds = xr.open_zarr(session.store, group=\"reference_01_canopy_01\")\n\n        vod_ds_complete = vod_ds.sel(epoch=slice(start_date, end_date))\n\n        if \"cell_id_equal_area_2deg\" not in vod_ds_complete:\n            grid = create_hemigrid(grid_type=\"equal_area\", angular_resolution=2)\n            vod_ds_complete = add_cell_ids_to_ds_fast(\n                vod_ds_complete, grid, \"equal_area_2deg\", data_var=\"VOD\"\n            )\n\n        logger.info(\"Dataset loaded: %s\", dict(vod_ds_complete.sizes))\n\n        # --- filter ---\n        t0 = time.time()\n        vod_ds_filtered = aggr_hampel_cell_sid_parallelized(\n            vod_ds_complete,\n            threshold=threshold,\n            min_obs_per_sid=min_obs_per_sid,\n            spatial_batch_size=spatial_batch_size,\n            n_workers=n_workers,\n            temporal_agg=temporal_agg,\n            agg_method=agg_method,\n        )\n        logger.info(\"Parallel filtering completed in %.1f s\", time.time() - t0)\n\n        # --- persist ---\n        snapshot_id = self._persist_filtered(\n            vod_ds_filtered,\n            \"parallel Cell-SID Hampel\",\n        )\n        logger.info(\"Parallel Hampel complete. Snapshot: %s\", snapshot_id)\n        return snapshot_id\n\n    # ------------------------------------------------------------------\n    # High-level orchestration\n    # ------------------------------------------------------------------\n\n    def run_complete_workflow(\n        self,\n        group_name: str = \"reference_01_canopy_01\",\n        branch: str = \"auto\",\n        time_range: tuple[datetime.date, datetime.date] | None = None,\n        **kwargs: Any,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Orchestrate a complete analysis run.\n\n        Auto-detection logic (``branch='auto'``) looks for Hampel-filtered\n        data on the ``processing`` branch first.  If found and temporally\n        compatible it is used directly; otherwise raw data from ``main`` is\n        returned.\n\n        Parameters\n        ----------\n        group_name : str\n            Zarr group for the raw VOD data.\n        branch : str\n            ``'auto'`` for detection, or an explicit branch name.\n        time_range : tuple of date, optional\n            ``(start, end)`` to select.\n\n        Returns\n        -------\n        dict\n            Keys: ``final_data`` (Dataset), ``source_branch``,\n            ``pre_filtered`` (bool), ``filter_type``.\n\n        \"\"\"\n        logger.info(\"=\" * 60)\n        logger.info(\"HAMPEL-FILTERED VOD ANALYSIS WORKFLOW\")\n        logger.info(\"branch=%s group=%s time_range=%s\", branch, group_name, time_range)\n\n        results: dict[str, Any] = {}\n\n        if branch == \"auto\":\n            hampel_ds = self._try_load_hampel()\n\n            if hampel_ds is not None:\n                # Validate temporal coverage when a range is requested\n                if time_range is not None:\n                    dataset_start = pd.to_datetime(hampel_ds.epoch.min().values).date()\n                    dataset_end = pd.to_datetime(hampel_ds.epoch.max().values).date()\n\n                    start_ok = normalize_datetime_for_comparison(\n                        time_range[0]\n                    ) &gt;= normalize_datetime_for_comparison(dataset_start)\n                    end_ok = normalize_datetime_for_comparison(\n                        time_range[1]\n                    ) &lt;= normalize_datetime_for_comparison(dataset_end)\n\n                    if start_ok and end_ok:\n                        hampel_ds = hampel_ds.sel(\n                            epoch=slice(time_range[0], time_range[1])\n                        )\n                    else:\n                        logger.warning(\n                            \"Hampel data (%s→%s) does not cover requested \"\n                            \"range (%s→%s); falling back to main branch\",\n                            dataset_start,\n                            dataset_end,\n                            time_range[0],\n                            time_range[1],\n                        )\n                        hampel_ds = None\n\n                if hampel_ds is not None:\n                    logger.info(\"Using Hampel filtered data from processing branch\")\n                    return {\n                        \"final_data\": hampel_ds,\n                        \"source_branch\": \"processing\",\n                        \"pre_filtered\": True,\n                        \"filter_type\": \"hampel\",\n                    }\n\n            logger.info(\"No usable Hampel data found; using raw data from main branch\")\n            branch = \"main\"\n\n        # --- main branch (raw) ---\n        logger.info(\"Loading raw data from branch=%s\", branch)\n        vod_ds = self.load_vod_data(group_name, branch)\n\n        if time_range is not None:\n            vod_ds = vod_ds.sel(epoch=slice(time_range[0], time_range[1]))\n\n        results = {\n            \"final_data\": vod_ds,\n            \"source_branch\": branch,\n            \"pre_filtered\": False,\n            \"filter_type\": \"none\",\n        }\n        logger.info(\"Workflow complete — source=%s\", branch)\n        return results\n\n    # ------------------------------------------------------------------\n    # Private helpers\n    # ------------------------------------------------------------------\n\n    def _try_load_hampel(self) -&gt; xr.Dataset | None:\n        \"\"\"Attempt to load Hampel-filtered data from the processing branch.\"\"\"\n        try:\n            with self.vod_store.readonly_session(branch=\"processing\") as session:\n                ds = xr.open_zarr(\n                    session.store,\n                    group=\"reference_01_canopy_01_hampel_filtered\",\n                    consolidated=False,\n                )\n            logger.info(\"Found Hampel filtered data on processing branch\")\n            return ds\n        except Exception:\n            logger.debug(\"No Hampel data on processing branch\", exc_info=True)\n            return None\n\n    def _force_or_skip(self, branch: str, force_recreate: bool) -&gt; bool:\n        \"\"\"Guard pattern: return ``True`` to proceed, ``False`` to skip.\n\n        If filtered data already exists and *force_recreate* is ``False``\n        the method logs a warning and returns ``False``.  When\n        *force_recreate* is ``True`` it deletes the branch first.\n        \"\"\"\n        exists = self._try_load_hampel() is not None\n        if exists and not force_recreate:\n            logger.warning(\n                \"Filtered data already exists. Pass force_recreate=True to overwrite.\"\n            )\n            return False\n        if exists and force_recreate:\n            try:\n                self.vod_store.delete_branch(branch)\n                logger.info(\"Deleted existing %s branch\", branch)\n            except Exception:\n                logger.warning(\"Could not delete branch %s\", branch, exc_info=True)\n        return True\n\n    def _persist_filtered(\n        self,\n        ds: xr.Dataset,\n        label: str,\n        target_group: str = \"reference_01_canopy_01_hampel_filtered\",\n    ) -&gt; str:\n        \"\"\"Write a filtered dataset to the ``processing`` branch.\n\n        Rechunks variables along the ``epoch`` dimension (max 50 000 epochs\n        per chunk) before writing.\n\n        Returns the Icechunk snapshot ID.\n        \"\"\"\n        from icechunk.xarray import to_icechunk\n\n        # Ensure processing branch exists\n        try:\n            current_snapshot = next(self.vod_store.repo.ancestry(branch=\"main\")).id\n            self.vod_store.repo.create_branch(\"processing\", current_snapshot)\n        except Exception:\n            pass  # branch may already exist\n\n        with self.vod_store.writable_session(\"processing\") as session:\n            logger.info(\"Persisting filtered data (%s)\", label)\n            for var_name in ds.data_vars:\n                if \"epoch\" in ds[var_name].dims:\n                    epoch_size = ds[var_name].sizes[\"epoch\"]\n                    ds[var_name] = ds[var_name].chunk(\n                        {\"epoch\": min(epoch_size, 50000), \"sid\": -1}\n                    )\n            to_icechunk(ds, session, group=target_group, mode=\"w\", safe_chunks=False)\n            snapshot_id: str = session.commit(label)\n\n        return snapshot_id\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.__init__","level":3,"title":"<code>__init__(vod_store_path)</code>","text":"<p>Initialize the workflow.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.__init__--parameters","level":5,"title":"Parameters","text":"<p>vod_store_path : Path | str     Path to the VOD Icechunk store directory.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def __init__(self, vod_store_path: Path | str) -&gt; None:\n    \"\"\"Initialize the workflow.\n\n    Parameters\n    ----------\n    vod_store_path : Path | str\n        Path to the VOD Icechunk store directory.\n\n    \"\"\"\n    self.vod_store_path = Path(vod_store_path)\n    self.vod_store: MyIcechunkStore = _get_store(self.vod_store_path)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.load_vod_data","level":3,"title":"<code>load_vod_data(group_name='reference_01_canopy_01', branch='main')</code>","text":"<p>Load a VOD dataset from the store.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.load_vod_data--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Zarr group path inside the store. branch : str     Icechunk branch to read from.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.load_vod_data--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Lazy-loaded VOD dataset.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def load_vod_data(\n    self,\n    group_name: str = \"reference_01_canopy_01\",\n    branch: str = \"main\",\n) -&gt; xr.Dataset:\n    \"\"\"Load a VOD dataset from the store.\n\n    Parameters\n    ----------\n    group_name : str\n        Zarr group path inside the store.\n    branch : str\n        Icechunk branch to read from.\n\n    Returns\n    -------\n    xr.Dataset\n        Lazy-loaded VOD dataset.\n\n    \"\"\"\n    logger.info(\"Loading VOD data from branch=%s group=%s\", branch, group_name)\n    with self.vod_store.readonly_session(branch=branch) as session:\n        vod_ds = xr.open_zarr(session.store, group=group_name, consolidated=False)\n    logger.info(\"Loaded VOD dataset: %s\", dict(vod_ds.sizes))\n    return vod_ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.check_temporal_coverage_compatibility","level":3,"title":"<code>check_temporal_coverage_compatibility(main_ds, processed_ds, requested_time_range=None)</code>","text":"<p>Check whether processed_ds adequately covers a time range.</p> <p>When requested_time_range is <code>None</code> the method checks that the processed dataset covers at least 70 % of the main dataset's span. When a range is given it verifies that both endpoints fall within the processed dataset (with a 1-day tolerance).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.check_temporal_coverage_compatibility--parameters","level":5,"title":"Parameters","text":"<p>main_ds : xr.Dataset     Reference (unfiltered) dataset. processed_ds : xr.Dataset     Filtered dataset to validate. requested_time_range : tuple of date, optional     <code>(start, end)</code> to check against.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.check_temporal_coverage_compatibility--returns","level":5,"title":"Returns","text":"<p>compatible : bool coverage_info : dict     Diagnostic information with <code>main_range</code>, <code>processed_range</code>,     and <code>requested_range</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def check_temporal_coverage_compatibility(\n    self,\n    main_ds: xr.Dataset,\n    processed_ds: xr.Dataset,\n    requested_time_range: tuple[datetime.date, datetime.date] | None = None,\n) -&gt; tuple[bool, dict[str, Any]]:\n    \"\"\"Check whether *processed_ds* adequately covers a time range.\n\n    When *requested_time_range* is ``None`` the method checks that the\n    processed dataset covers at least 70 % of the main dataset's span.\n    When a range is given it verifies that both endpoints fall within the\n    processed dataset (with a 1-day tolerance).\n\n    Parameters\n    ----------\n    main_ds : xr.Dataset\n        Reference (unfiltered) dataset.\n    processed_ds : xr.Dataset\n        Filtered dataset to validate.\n    requested_time_range : tuple of date, optional\n        ``(start, end)`` to check against.\n\n    Returns\n    -------\n    compatible : bool\n    coverage_info : dict\n        Diagnostic information with ``main_range``, ``processed_range``,\n        and ``requested_range``.\n\n    \"\"\"\n\n    def _date_range(ds: xr.Dataset) -&gt; tuple[datetime.date, datetime.date]:\n        \"\"\"Return the date range for a dataset.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset with an epoch coordinate.\n\n        Returns\n        -------\n        tuple[datetime.date, datetime.date]\n            Start and end dates.\n\n        \"\"\"\n        return (\n            pd.to_datetime(ds.epoch.min().values).date(),\n            pd.to_datetime(ds.epoch.max().values).date(),\n        )\n\n    main_start, main_end = _date_range(main_ds)\n    proc_start, proc_end = _date_range(processed_ds)\n\n    coverage_info: dict[str, Any] = {\n        \"main_range\": (main_start, main_end),\n        \"processed_range\": (proc_start, proc_end),\n        \"requested_range\": requested_time_range,\n    }\n\n    if requested_time_range is None:\n        main_days = (main_end - main_start).days\n        proc_days = (proc_end - proc_start).days\n        ratio = proc_days / main_days if main_days &gt; 0 else 0.0\n        logger.info(\n            \"Coverage check: main=%d days, processed=%d days, ratio=%.1f%%\",\n            main_days,\n            proc_days,\n            ratio * 100,\n        )\n        return ratio &gt;= 0.7, coverage_info\n\n    req_start, req_end = requested_time_range\n    one_day = datetime.timedelta(days=1)\n    start_ok = proc_start &lt;= req_start &lt;= proc_end + one_day\n    end_ok = proc_start - one_day &lt;= req_end &lt;= proc_end\n    compatible = start_ok and end_ok\n\n    if not compatible:\n        logger.warning(\n            \"Temporal coverage mismatch: processed=%s→%s, requested=%s→%s\",\n            proc_start,\n            proc_end,\n            req_start,\n            req_end,\n        )\n    return compatible, coverage_info\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.create_processed_data_fast_hampel_complete","level":3,"title":"<code>create_processed_data_fast_hampel_complete(start_date, end_date, force_recreate=False, window_hours=1.0, sigma_threshold=3.0, min_points=5, ultra_fast_mode=False, cell_batch_size=200, n_workers=None)</code>","text":"<p>Run the vectorised / ultra-fast Hampel pipeline end-to-end.</p> <p>Delegates the actual filtering to :func:<code>canvod.grids.analysis.sigma_clip_filter.astropy_hampel_vectorized_fast</code> (or its ultra-fast variant) and persists the result on a <code>processing</code> branch.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.create_processed_data_fast_hampel_complete--parameters","level":5,"title":"Parameters","text":"<p>start_date, end_date : date or datetime     Temporal extent to process. force_recreate : bool     Overwrite existing filtered data. window_hours : float     Hampel temporal window in hours. sigma_threshold : float     MAD-based outlier threshold. min_points : int     Minimum observations required per window. ultra_fast_mode : bool     Use the pure-NumPy sigma-clip path (faster, less precise). cell_batch_size : int     Number of cells per spatial batch. n_workers : int, optional     Parallel workers.  <code>None</code> → auto-detect.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.create_processed_data_fast_hampel_complete--returns","level":5,"title":"Returns","text":"<p>str or None     Icechunk snapshot ID, or <code>None</code> if existing data was kept.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def create_processed_data_fast_hampel_complete(\n    self,\n    start_date: datetime.date | datetime.datetime,\n    end_date: datetime.date | datetime.datetime,\n    force_recreate: bool = False,\n    window_hours: float = 1.0,\n    sigma_threshold: float = 3.0,\n    min_points: int = 5,\n    ultra_fast_mode: bool = False,\n    cell_batch_size: int = 200,\n    n_workers: int | None = None,\n) -&gt; str | None:\n    \"\"\"Run the vectorised / ultra-fast Hampel pipeline end-to-end.\n\n    Delegates the actual filtering to\n    :func:`canvod.grids.analysis.sigma_clip_filter.astropy_hampel_vectorized_fast`\n    (or its ultra-fast variant) and persists the result on a\n    ``processing`` branch.\n\n    Parameters\n    ----------\n    start_date, end_date : date or datetime\n        Temporal extent to process.\n    force_recreate : bool\n        Overwrite existing filtered data.\n    window_hours : float\n        Hampel temporal window in hours.\n    sigma_threshold : float\n        MAD-based outlier threshold.\n    min_points : int\n        Minimum observations required per window.\n    ultra_fast_mode : bool\n        Use the pure-NumPy sigma-clip path (faster, less precise).\n    cell_batch_size : int\n        Number of cells per spatial batch.\n    n_workers : int, optional\n        Parallel workers.  ``None`` → auto-detect.\n\n    Returns\n    -------\n    str or None\n        Icechunk snapshot ID, or ``None`` if existing data was kept.\n\n    \"\"\"\n    return _create_processed_data_fast_hampel(\n        workflow_instance=self,\n        start_date=start_date,\n        end_date=end_date,\n        force_recreate=force_recreate,\n        window_hours=window_hours,\n        sigma_threshold=sigma_threshold,\n        min_points=min_points,\n        ultra_fast_mode=ultra_fast_mode,\n        cell_batch_size=cell_batch_size,\n        n_workers=n_workers,\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.create_processed_data_hampel_parallel_complete","level":3,"title":"<code>create_processed_data_hampel_parallel_complete(start_date, end_date, force_recreate=False, threshold=3.0, min_obs_per_sid=20, spatial_batch_size=500, n_workers=None, temporal_agg=None, agg_method=None)</code>","text":"<p>Run the parallelised cell-SID Hampel pipeline end-to-end.</p> <p>Loads the complete requested time range (no temporal chunking) and applies :func:<code>canvod.grids.analysis.hampel_filtering.aggr_hampel_cell_sid_parallelized</code> with spatial batching.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.create_processed_data_hampel_parallel_complete--parameters","level":5,"title":"Parameters","text":"<p>start_date, end_date : date or datetime     Temporal extent to process. force_recreate : bool     Overwrite existing filtered data. threshold : float     MAD-based outlier threshold. min_obs_per_sid : int     Minimum observations per cell-SID combination. spatial_batch_size : int     Cells per spatial batch. n_workers : int, optional     Parallel workers.  <code>None</code> → auto-detect. temporal_agg : str, optional     Post-filtering aggregation frequency (e.g. <code>'1H'</code>, <code>'1D'</code>). agg_method : str, optional     Aggregation method (e.g. <code>'mean'</code>).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.create_processed_data_hampel_parallel_complete--returns","level":5,"title":"Returns","text":"<p>str or None     Icechunk snapshot ID, or <code>None</code> if existing data was kept.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def create_processed_data_hampel_parallel_complete(\n    self,\n    start_date: datetime.date | datetime.datetime,\n    end_date: datetime.date | datetime.datetime,\n    force_recreate: bool = False,\n    threshold: float = 3.0,\n    min_obs_per_sid: int = 20,\n    spatial_batch_size: int = 500,\n    n_workers: int | None = None,\n    temporal_agg: str | None = None,\n    agg_method: str | None = None,\n) -&gt; str | None:\n    \"\"\"Run the parallelised cell-SID Hampel pipeline end-to-end.\n\n    Loads the complete requested time range (no temporal chunking) and\n    applies\n    :func:`canvod.grids.analysis.hampel_filtering.aggr_hampel_cell_sid_parallelized`\n    with spatial batching.\n\n    Parameters\n    ----------\n    start_date, end_date : date or datetime\n        Temporal extent to process.\n    force_recreate : bool\n        Overwrite existing filtered data.\n    threshold : float\n        MAD-based outlier threshold.\n    min_obs_per_sid : int\n        Minimum observations per cell-SID combination.\n    spatial_batch_size : int\n        Cells per spatial batch.\n    n_workers : int, optional\n        Parallel workers.  ``None`` → auto-detect.\n    temporal_agg : str, optional\n        Post-filtering aggregation frequency (e.g. ``'1H'``, ``'1D'``).\n    agg_method : str, optional\n        Aggregation method (e.g. ``'mean'``).\n\n    Returns\n    -------\n    str or None\n        Icechunk snapshot ID, or ``None`` if existing data was kept.\n\n    \"\"\"\n    from canvod.grids import create_hemigrid\n    from canvod.grids.analysis.hampel_filtering import (\n        aggr_hampel_cell_sid_parallelized,\n    )\n    from canvod.grids.operations import add_cell_ids_to_ds_fast\n\n    logger.info(\"=\" * 60)\n    logger.info(\"PARALLEL HAMPEL — complete temporal coverage\")\n    logger.info(\n        \"Range: %s → %s | threshold=%.1f | min_obs=%d | batch=%d | workers=%s\",\n        start_date,\n        end_date,\n        threshold,\n        min_obs_per_sid,\n        spatial_batch_size,\n        n_workers or \"auto\",\n    )\n\n    # --- guard: existing data ---\n    if not self._force_or_skip(\"processing\", force_recreate):\n        return None\n\n    # --- load complete time range ---\n    logger.info(\"Loading complete time range for parallel processing\")\n    with self.vod_store.readonly_session(branch=\"main\") as session:\n        vod_ds = xr.open_zarr(session.store, group=\"reference_01_canopy_01\")\n\n    vod_ds_complete = vod_ds.sel(epoch=slice(start_date, end_date))\n\n    if \"cell_id_equal_area_2deg\" not in vod_ds_complete:\n        grid = create_hemigrid(grid_type=\"equal_area\", angular_resolution=2)\n        vod_ds_complete = add_cell_ids_to_ds_fast(\n            vod_ds_complete, grid, \"equal_area_2deg\", data_var=\"VOD\"\n        )\n\n    logger.info(\"Dataset loaded: %s\", dict(vod_ds_complete.sizes))\n\n    # --- filter ---\n    t0 = time.time()\n    vod_ds_filtered = aggr_hampel_cell_sid_parallelized(\n        vod_ds_complete,\n        threshold=threshold,\n        min_obs_per_sid=min_obs_per_sid,\n        spatial_batch_size=spatial_batch_size,\n        n_workers=n_workers,\n        temporal_agg=temporal_agg,\n        agg_method=agg_method,\n    )\n    logger.info(\"Parallel filtering completed in %.1f s\", time.time() - t0)\n\n    # --- persist ---\n    snapshot_id = self._persist_filtered(\n        vod_ds_filtered,\n        \"parallel Cell-SID Hampel\",\n    )\n    logger.info(\"Parallel Hampel complete. Snapshot: %s\", snapshot_id)\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.run_complete_workflow","level":3,"title":"<code>run_complete_workflow(group_name='reference_01_canopy_01', branch='auto', time_range=None, **kwargs)</code>","text":"<p>Orchestrate a complete analysis run.</p> <p>Auto-detection logic (<code>branch='auto'</code>) looks for Hampel-filtered data on the <code>processing</code> branch first.  If found and temporally compatible it is used directly; otherwise raw data from <code>main</code> is returned.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.run_complete_workflow--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Zarr group for the raw VOD data. branch : str     <code>'auto'</code> for detection, or an explicit branch name. time_range : tuple of date, optional     <code>(start, end)</code> to select.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.AdaptedVODWorkflow.run_complete_workflow--returns","level":5,"title":"Returns","text":"<p>dict     Keys: <code>final_data</code> (Dataset), <code>source_branch</code>,     <code>pre_filtered</code> (bool), <code>filter_type</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def run_complete_workflow(\n    self,\n    group_name: str = \"reference_01_canopy_01\",\n    branch: str = \"auto\",\n    time_range: tuple[datetime.date, datetime.date] | None = None,\n    **kwargs: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Orchestrate a complete analysis run.\n\n    Auto-detection logic (``branch='auto'``) looks for Hampel-filtered\n    data on the ``processing`` branch first.  If found and temporally\n    compatible it is used directly; otherwise raw data from ``main`` is\n    returned.\n\n    Parameters\n    ----------\n    group_name : str\n        Zarr group for the raw VOD data.\n    branch : str\n        ``'auto'`` for detection, or an explicit branch name.\n    time_range : tuple of date, optional\n        ``(start, end)`` to select.\n\n    Returns\n    -------\n    dict\n        Keys: ``final_data`` (Dataset), ``source_branch``,\n        ``pre_filtered`` (bool), ``filter_type``.\n\n    \"\"\"\n    logger.info(\"=\" * 60)\n    logger.info(\"HAMPEL-FILTERED VOD ANALYSIS WORKFLOW\")\n    logger.info(\"branch=%s group=%s time_range=%s\", branch, group_name, time_range)\n\n    results: dict[str, Any] = {}\n\n    if branch == \"auto\":\n        hampel_ds = self._try_load_hampel()\n\n        if hampel_ds is not None:\n            # Validate temporal coverage when a range is requested\n            if time_range is not None:\n                dataset_start = pd.to_datetime(hampel_ds.epoch.min().values).date()\n                dataset_end = pd.to_datetime(hampel_ds.epoch.max().values).date()\n\n                start_ok = normalize_datetime_for_comparison(\n                    time_range[0]\n                ) &gt;= normalize_datetime_for_comparison(dataset_start)\n                end_ok = normalize_datetime_for_comparison(\n                    time_range[1]\n                ) &lt;= normalize_datetime_for_comparison(dataset_end)\n\n                if start_ok and end_ok:\n                    hampel_ds = hampel_ds.sel(\n                        epoch=slice(time_range[0], time_range[1])\n                    )\n                else:\n                    logger.warning(\n                        \"Hampel data (%s→%s) does not cover requested \"\n                        \"range (%s→%s); falling back to main branch\",\n                        dataset_start,\n                        dataset_end,\n                        time_range[0],\n                        time_range[1],\n                    )\n                    hampel_ds = None\n\n            if hampel_ds is not None:\n                logger.info(\"Using Hampel filtered data from processing branch\")\n                return {\n                    \"final_data\": hampel_ds,\n                    \"source_branch\": \"processing\",\n                    \"pre_filtered\": True,\n                    \"filter_type\": \"hampel\",\n                }\n\n        logger.info(\"No usable Hampel data found; using raw data from main branch\")\n        branch = \"main\"\n\n    # --- main branch (raw) ---\n    logger.info(\"Loading raw data from branch=%s\", branch)\n    vod_ds = self.load_vod_data(group_name, branch)\n\n    if time_range is not None:\n        vod_ds = vod_ds.sel(epoch=slice(time_range[0], time_range[1]))\n\n    results = {\n        \"final_data\": vod_ds,\n        \"source_branch\": branch,\n        \"pre_filtered\": False,\n        \"filter_type\": \"none\",\n    }\n    logger.info(\"Workflow complete — source=%s\", branch)\n    return results\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.create_hemigrid","level":2,"title":"<code>create_hemigrid(grid_type, angular_resolution=10.0, **kwargs)</code>","text":"<p>Create hemisphere grid of specified type.</p> <p>Factory function for creating various hemisphere grid types commonly used in GNSS analysis.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.create_hemigrid--parameters","level":4,"title":"Parameters","text":"<p>grid_type : str     Type of grid to create:     - 'equal_area': Regular lat/lon grid with equal solid angle cells     - 'equal_angle': Regular angular spacing (not recommended)     - 'rectangular' or 'equirectangular': Simple rectangular grid     - 'HTM': Hierarchical Triangular Mesh     - 'geodesic': Geodesic sphere subdivision (icosahedron-based)     - 'healpix': HEALPix grid (requires healpy)     - 'fibonacci': Fibonacci sphere (requires scipy) angular_resolution : float, default 10.0     Angular resolution in degrees **kwargs     Additional grid-specific parameters:     - cutoff_theta : float - Maximum theta angle cutoff (degrees)     - phi_rotation : float - Rotation angle (degrees)     - subdivision_level : int - For geodesic/HTM grids     - htm_level : int - For HTM grids specifically     - nside : int - For HEALPix grids     - n_points : int - For Fibonacci grids</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.create_hemigrid--returns","level":4,"title":"Returns","text":"<p>GridData     Complete hemisphere grid data structure</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.create_hemigrid--examples","level":4,"title":"Examples","text":"","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.create_hemigrid--equal-area-grid-with-10-resolution","level":3,"title":"Equal area grid with 10° resolution","text":"<p>grid = create_hemigrid('equal_area', angular_resolution=10.0)</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.create_hemigrid--htm-grid-with-subdivision-level-3","level":3,"title":"HTM grid with subdivision level 3","text":"<p>grid = create_hemigrid('HTM', angular_resolution=5.0, htm_level=3)</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.create_hemigrid--geodesic-grid","level":3,"title":"Geodesic grid","text":"<p>grid = create_hemigrid('geodesic', angular_resolution=5.0, subdivision_level=2)</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.create_hemigrid--notes","level":4,"title":"Notes","text":"<p>Grid coordinates use navigation convention: - phi: azimuth angle, 0 to 2π (0 = North, π/2 = East, clockwise) - theta: polar angle from zenith, 0 to π/2 (0 = zenith, π/2 = horizon)</p> Source code in <code>packages/canvod-grids/src/canvod/grids/__init__.py</code> <pre><code>def create_hemigrid(\n    grid_type: Literal[\n        \"equal_area\",\n        \"equal_angle\",\n        \"rectangular\",\n        \"equirectangular\",\n        \"HTM\",\n        \"geodesic\",\n        \"healpix\",\n        \"fibonacci\",\n    ],\n    angular_resolution: float = 10.0,\n    **kwargs: Any,\n) -&gt; GridData:\n    \"\"\"Create hemisphere grid of specified type.\n\n    Factory function for creating various hemisphere grid types commonly\n    used in GNSS analysis.\n\n    Parameters\n    ----------\n    grid_type : str\n        Type of grid to create:\n        - 'equal_area': Regular lat/lon grid with equal solid angle cells\n        - 'equal_angle': Regular angular spacing (not recommended)\n        - 'rectangular' or 'equirectangular': Simple rectangular grid\n        - 'HTM': Hierarchical Triangular Mesh\n        - 'geodesic': Geodesic sphere subdivision (icosahedron-based)\n        - 'healpix': HEALPix grid (requires healpy)\n        - 'fibonacci': Fibonacci sphere (requires scipy)\n    angular_resolution : float, default 10.0\n        Angular resolution in degrees\n    **kwargs\n        Additional grid-specific parameters:\n        - cutoff_theta : float - Maximum theta angle cutoff (degrees)\n        - phi_rotation : float - Rotation angle (degrees)\n        - subdivision_level : int - For geodesic/HTM grids\n        - htm_level : int - For HTM grids specifically\n        - nside : int - For HEALPix grids\n        - n_points : int - For Fibonacci grids\n\n    Returns\n    -------\n    GridData\n        Complete hemisphere grid data structure\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Equal area grid with 10° resolution\n    &gt;&gt;&gt; grid = create_hemigrid('equal_area', angular_resolution=10.0)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # HTM grid with subdivision level 3\n    &gt;&gt;&gt; grid = create_hemigrid('HTM', angular_resolution=5.0, htm_level=3)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Geodesic grid\n    &gt;&gt;&gt; grid = create_hemigrid('geodesic', angular_resolution=5.0, subdivision_level=2)\n\n    Notes\n    -----\n    Grid coordinates use navigation convention:\n    - phi: azimuth angle, 0 to 2π (0 = North, π/2 = East, clockwise)\n    - theta: polar angle from zenith, 0 to π/2 (0 = zenith, π/2 = horizon)\n\n    \"\"\"\n    grid_type_lower = grid_type.lower()\n\n    if grid_type_lower == \"equal_area\":\n        builder = EqualAreaBuilder(\n            angular_resolution=angular_resolution,\n            **kwargs,\n        )\n    elif grid_type_lower == \"equal_angle\":\n        builder = EqualAngleBuilder(\n            angular_resolution=angular_resolution,\n            **kwargs,\n        )\n    elif grid_type_lower in [\"rectangular\", \"equirectangular\"]:\n        builder = EquirectangularBuilder(\n            angular_resolution=angular_resolution,\n            **kwargs,\n        )\n    elif grid_type_lower == \"htm\":\n        builder = HTMBuilder(\n            angular_resolution=angular_resolution,\n            **kwargs,\n        )\n    elif grid_type_lower == \"geodesic\":\n        builder = GeodesicBuilder(\n            angular_resolution=angular_resolution,\n            **kwargs,\n        )\n    elif grid_type_lower == \"healpix\":\n        builder = HEALPixBuilder(\n            angular_resolution=angular_resolution,\n            **kwargs,\n        )\n    elif grid_type_lower == \"fibonacci\":\n        builder = FibonacciBuilder(\n            angular_resolution=angular_resolution,\n            **kwargs,\n        )\n    else:\n        raise ValueError(\n            f\"Unknown grid type: {grid_type}. \"\n            f\"Available types: equal_area, equal_angle, rectangular, \"\n            f\"equirectangular, HTM, geodesic, healpix, fibonacci\"\n        )\n\n    return builder.build()\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#grid-core","level":2,"title":"Grid Core","text":"<p>Base class for hemisphere grid builders.</p> <p>Grid data container for hemisphere grids.</p> <p>Grid type definitions for hemisphere tessellation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_builder.BaseGridBuilder","level":2,"title":"<code>BaseGridBuilder</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for hemispherical grid builders.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_builder.BaseGridBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Angular resolution in degrees cutoff_theta : float     Maximum polar angle cutoff in degrees phi_rotation : float     Rotation angle in degrees (applied to all phi values)</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_builder.py</code> <pre><code>class BaseGridBuilder(ABC):\n    \"\"\"Abstract base for hemispherical grid builders.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Angular resolution in degrees\n    cutoff_theta : float\n        Maximum polar angle cutoff in degrees\n    phi_rotation : float\n        Rotation angle in degrees (applied to all phi values)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        angular_resolution: float = 2,\n        cutoff_theta: float = 0,\n        phi_rotation: float = 0,\n    ) -&gt; None:\n        \"\"\"Initialize the grid builder.\n\n        Parameters\n        ----------\n        angular_resolution : float, default 2\n            Angular resolution in degrees.\n        cutoff_theta : float, default 0\n            Maximum polar angle cutoff in degrees.\n        phi_rotation : float, default 0\n            Rotation angle in degrees.\n\n        \"\"\"\n        self.angular_resolution = angular_resolution\n        self.angular_resolution_rad = np.deg2rad(angular_resolution)\n        self.cutoff_theta = cutoff_theta\n        self.cutoff_theta_rad = np.deg2rad(cutoff_theta)\n        self.phi_rotation = phi_rotation\n        self.phi_rotation_rad = np.deg2rad(phi_rotation)\n        self._logger = _get_logger()\n\n    @abstractmethod\n    def _build_grid(\n        self,\n    ) -&gt; (\n        tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]\n        | tuple[\n            pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray], dict[str, Any]\n        ]\n    ):\n        \"\"\"Build grid.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            Grid cells\n        theta_lims : np.ndarray\n            Theta band limits\n        phi_lims : list[np.ndarray]\n            Phi limits per band\n        cell_ids : list[np.ndarray]\n            Cell IDs per band\n        extra_kwargs : dict, optional\n            Additional metadata\n\n        \"\"\"\n\n    @abstractmethod\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Get grid type identifier.\"\"\"\n\n    def build(self) -&gt; GridData:\n        \"\"\"Build hemisphere grid.\n\n        Returns\n        -------\n        GridData\n            Complete grid data structure\n\n        \"\"\"\n        self._logger.info(\n            \"grid_build_started\",\n            grid_type=self.get_grid_type(),\n            angular_resolution=self.angular_resolution,\n        )\n\n        result = self._build_grid()\n\n        if len(result) == 4:\n            grid, theta_lims, phi_lims, cell_ids = result\n            extra_kwargs = {}\n        elif len(result) == 5:\n            grid, theta_lims, phi_lims, cell_ids, extra_kwargs = result\n        else:\n            raise ValueError(f\"Invalid grid builder result: {len(result)} elements\")\n\n        # Apply phi rotation if specified (vectorized operations)\n        if self.phi_rotation_rad != 0:\n            grid = grid.with_columns(\n                [(pl.col(\"phi\") + self.phi_rotation_rad) % (2 * np.pi)]\n            )\n\n            if \"phi_min\" in grid.columns:\n                grid = grid.with_columns(\n                    [\n                        (\n                            (pl.col(\"phi_min\") + self.phi_rotation_rad) % (2 * np.pi)\n                        ).alias(\"phi_min\"),\n                        (\n                            (pl.col(\"phi_max\") + self.phi_rotation_rad) % (2 * np.pi)\n                        ).alias(\"phi_max\"),\n                    ]\n                )\n\n        self._logger.info(\"grid_build_complete\", ncells=len(grid))\n\n        # Merge builder metadata into any extra_kwargs metadata\n        builder_meta = {\n            \"angular_resolution\": self.angular_resolution,\n            \"cutoff_theta\": self.cutoff_theta,\n        }\n        if \"metadata\" in extra_kwargs and extra_kwargs[\"metadata\"]:\n            extra_kwargs[\"metadata\"] = {**builder_meta, **extra_kwargs[\"metadata\"]}\n        else:\n            extra_kwargs[\"metadata\"] = builder_meta\n\n        return GridData(\n            grid=grid,\n            theta_lims=theta_lims,\n            phi_lims=phi_lims,\n            cell_ids=cell_ids,\n            grid_type=self.get_grid_type(),\n            **extra_kwargs,\n        )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_builder.BaseGridBuilder.__init__","level":3,"title":"<code>__init__(angular_resolution=2, cutoff_theta=0, phi_rotation=0)</code>","text":"<p>Initialize the grid builder.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_builder.BaseGridBuilder.__init__--parameters","level":5,"title":"Parameters","text":"<p>angular_resolution : float, default 2     Angular resolution in degrees. cutoff_theta : float, default 0     Maximum polar angle cutoff in degrees. phi_rotation : float, default 0     Rotation angle in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_builder.py</code> <pre><code>def __init__(\n    self,\n    angular_resolution: float = 2,\n    cutoff_theta: float = 0,\n    phi_rotation: float = 0,\n) -&gt; None:\n    \"\"\"Initialize the grid builder.\n\n    Parameters\n    ----------\n    angular_resolution : float, default 2\n        Angular resolution in degrees.\n    cutoff_theta : float, default 0\n        Maximum polar angle cutoff in degrees.\n    phi_rotation : float, default 0\n        Rotation angle in degrees.\n\n    \"\"\"\n    self.angular_resolution = angular_resolution\n    self.angular_resolution_rad = np.deg2rad(angular_resolution)\n    self.cutoff_theta = cutoff_theta\n    self.cutoff_theta_rad = np.deg2rad(cutoff_theta)\n    self.phi_rotation = phi_rotation\n    self.phi_rotation_rad = np.deg2rad(phi_rotation)\n    self._logger = _get_logger()\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_builder.BaseGridBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>  <code>abstractmethod</code>","text":"<p>Get grid type identifier.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_builder.py</code> <pre><code>@abstractmethod\ndef get_grid_type(self) -&gt; str:\n    \"\"\"Get grid type identifier.\"\"\"\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_builder.BaseGridBuilder.build","level":3,"title":"<code>build()</code>","text":"<p>Build hemisphere grid.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_builder.BaseGridBuilder.build--returns","level":5,"title":"Returns","text":"<p>GridData     Complete grid data structure</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_builder.py</code> <pre><code>def build(self) -&gt; GridData:\n    \"\"\"Build hemisphere grid.\n\n    Returns\n    -------\n    GridData\n        Complete grid data structure\n\n    \"\"\"\n    self._logger.info(\n        \"grid_build_started\",\n        grid_type=self.get_grid_type(),\n        angular_resolution=self.angular_resolution,\n    )\n\n    result = self._build_grid()\n\n    if len(result) == 4:\n        grid, theta_lims, phi_lims, cell_ids = result\n        extra_kwargs = {}\n    elif len(result) == 5:\n        grid, theta_lims, phi_lims, cell_ids, extra_kwargs = result\n    else:\n        raise ValueError(f\"Invalid grid builder result: {len(result)} elements\")\n\n    # Apply phi rotation if specified (vectorized operations)\n    if self.phi_rotation_rad != 0:\n        grid = grid.with_columns(\n            [(pl.col(\"phi\") + self.phi_rotation_rad) % (2 * np.pi)]\n        )\n\n        if \"phi_min\" in grid.columns:\n            grid = grid.with_columns(\n                [\n                    (\n                        (pl.col(\"phi_min\") + self.phi_rotation_rad) % (2 * np.pi)\n                    ).alias(\"phi_min\"),\n                    (\n                        (pl.col(\"phi_max\") + self.phi_rotation_rad) % (2 * np.pi)\n                    ).alias(\"phi_max\"),\n                ]\n            )\n\n    self._logger.info(\"grid_build_complete\", ncells=len(grid))\n\n    # Merge builder metadata into any extra_kwargs metadata\n    builder_meta = {\n        \"angular_resolution\": self.angular_resolution,\n        \"cutoff_theta\": self.cutoff_theta,\n    }\n    if \"metadata\" in extra_kwargs and extra_kwargs[\"metadata\"]:\n        extra_kwargs[\"metadata\"] = {**builder_meta, **extra_kwargs[\"metadata\"]}\n    else:\n        extra_kwargs[\"metadata\"] = builder_meta\n\n    return GridData(\n        grid=grid,\n        theta_lims=theta_lims,\n        phi_lims=phi_lims,\n        cell_ids=cell_ids,\n        grid_type=self.get_grid_type(),\n        **extra_kwargs,\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_data.GridData","level":2,"title":"<code>GridData</code>  <code>dataclass</code>","text":"<p>Immutable container for hemispherical grid structure.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_data.GridData--parameters","level":4,"title":"Parameters","text":"<p>grid : pl.DataFrame     Grid cells with phi, theta, and bounds theta_lims : np.ndarray     Theta band limits phi_lims : list[np.ndarray]     Phi limits per theta band cell_ids : list[np.ndarray]     Cell IDs per theta band grid_type : str     Grid type identifier solid_angles : np.ndarray, optional     Solid angles per cell [steradians] metadata : dict, optional     Additional grid metadata voronoi : Any, optional     Voronoi tessellation object (for Fibonacci grids) vertices : np.ndarray, optional     3D vertices (for triangular grids) points_xyz : np.ndarray, optional     3D point cloud (for Fibonacci grids) vertex_phi : np.ndarray, optional     Vertex phi coordinates vertex_theta : np.ndarray, optional     Vertex theta coordinates</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_data.py</code> <pre><code>@dataclass(frozen=True)\nclass GridData:\n    \"\"\"Immutable container for hemispherical grid structure.\n\n    Parameters\n    ----------\n    grid : pl.DataFrame\n        Grid cells with phi, theta, and bounds\n    theta_lims : np.ndarray\n        Theta band limits\n    phi_lims : list[np.ndarray]\n        Phi limits per theta band\n    cell_ids : list[np.ndarray]\n        Cell IDs per theta band\n    grid_type : str\n        Grid type identifier\n    solid_angles : np.ndarray, optional\n        Solid angles per cell [steradians]\n    metadata : dict, optional\n        Additional grid metadata\n    voronoi : Any, optional\n        Voronoi tessellation object (for Fibonacci grids)\n    vertices : np.ndarray, optional\n        3D vertices (for triangular grids)\n    points_xyz : np.ndarray, optional\n        3D point cloud (for Fibonacci grids)\n    vertex_phi : np.ndarray, optional\n        Vertex phi coordinates\n    vertex_theta : np.ndarray, optional\n        Vertex theta coordinates\n\n    \"\"\"\n\n    grid: pl.DataFrame\n    theta_lims: np.ndarray\n    phi_lims: list[np.ndarray]\n    cell_ids: list[np.ndarray]\n    grid_type: str\n    solid_angles: np.ndarray | None = None\n    metadata: dict | None = None\n    voronoi: Any | None = None\n    vertices: np.ndarray | None = None\n    points_xyz: np.ndarray | None = None\n    vertex_phi: np.ndarray | None = None\n    vertex_theta: np.ndarray | None = None\n\n    @property\n    def coords(self) -&gt; pl.DataFrame:\n        \"\"\"Get cell coordinates.\"\"\"\n        return self.grid.select([\"phi\", \"theta\"])\n\n    @property\n    def ncells(self) -&gt; int:\n        \"\"\"Number of cells in grid.\"\"\"\n        return len(self.grid)\n\n    def get_patches(self) -&gt; pl.Series:\n        \"\"\"Create matplotlib patches for polar visualization.\"\"\"\n        patches = [\n            Rectangle(\n                (row[\"phi_min\"], row[\"theta_min\"]),\n                row[\"phi_max\"] - row[\"phi_min\"],\n                row[\"theta_max\"] - row[\"theta_min\"],\n                fill=True,\n            )\n            for row in self.grid.iter_rows(named=True)\n        ]\n        return pl.Series(\"Patches\", patches)\n\n    def get_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Calculate solid angle for each cell [steradians].\"\"\"\n        if self.solid_angles is not None:\n            return self.solid_angles\n\n        # HEALPix\n        if self.grid_type == \"healpix\" and \"healpix_nside\" in self.grid.columns:\n            try:\n                import healpy as hp\n\n                nside = int(self.grid[\"healpix_nside\"][0])\n                return np.full(\n                    len(self.grid), hp.nside2pixarea(nside), dtype=np.float64\n                )\n            except ImportError:\n                pass\n\n        # Geodesic\n        if self.grid_type == \"geodesic\" and \"geodesic_vertices\" in self.grid.columns:\n            return self._compute_geodesic_solid_angles()\n\n        # HTM\n        if self.grid_type == \"htm\" and \"htm_vertex_0\" in self.grid.columns:\n            return self._compute_htm_solid_angles()\n\n        # Fibonacci\n        if self.grid_type == \"fibonacci\" and \"voronoi_region\" in self.grid.columns:\n            return self._compute_voronoi_solid_angles()\n\n        # Default\n        return self._geometric_solid_angles()\n\n    def _compute_htm_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Compute solid angles for HTM triangular cells.\"\"\"\n        solid_angles = []\n\n        for row in self.grid.iter_rows(named=True):\n            v0 = np.array(row[\"htm_vertex_0\"])\n            v1 = np.array(row[\"htm_vertex_1\"])\n            v2 = np.array(row[\"htm_vertex_2\"])\n\n            # Spherical excess formula\n            a = np.arccos(np.clip(np.dot(v1, v2), -1, 1))\n            b = np.arccos(np.clip(np.dot(v0, v2), -1, 1))\n            c = np.arccos(np.clip(np.dot(v0, v1), -1, 1))\n\n            s = (a + b + c) / 2\n            tan_E_4 = np.sqrt(\n                np.tan(s / 2)\n                * np.tan((s - a) / 2)\n                * np.tan((s - b) / 2)\n                * np.tan((s - c) / 2)\n            )\n            E = 4 * np.arctan(tan_E_4)\n\n            solid_angles.append(E)\n\n        return np.array(solid_angles)\n\n    def _compute_geodesic_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Compute solid angles for geodesic triangular cells.\"\"\"\n        vertices = self.vertices\n        if vertices is None:\n            return self._geometric_solid_angles()\n\n        solid_angles = []\n        for row in self.grid.iter_rows(named=True):\n            v_indices = row[\"geodesic_vertices\"]\n            v0, v1, v2 = vertices[v_indices]\n\n            a = np.arccos(np.clip(np.dot(v1, v2), -1, 1))\n            b = np.arccos(np.clip(np.dot(v0, v2), -1, 1))\n            c = np.arccos(np.clip(np.dot(v0, v1), -1, 1))\n\n            s = (a + b + c) / 2\n            tan_E_4 = np.sqrt(\n                np.tan(s / 2)\n                * np.tan((s - a) / 2)\n                * np.tan((s - b) / 2)\n                * np.tan((s - c) / 2)\n            )\n            E = 4 * np.arctan(tan_E_4)\n\n            solid_angles.append(E)\n\n        return np.array(solid_angles)\n\n    def _compute_voronoi_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Compute solid angles for Voronoi cells.\"\"\"\n        if self.voronoi is None:\n            return self._geometric_solid_angles()\n\n        sv = self.voronoi\n        solid_angles = []\n        for row in self.grid.iter_rows(named=True):\n            region = row[\"voronoi_region\"]\n            if len(region) &lt; 3:\n                solid_angles.append(np.nan)\n                continue\n\n            vertices = sv.vertices[region]\n            center = np.array(\n                [\n                    np.sin(row[\"theta\"]) * np.cos(row[\"phi\"]),\n                    np.sin(row[\"theta\"]) * np.sin(row[\"phi\"]),\n                    np.cos(row[\"theta\"]),\n                ]\n            )\n\n            total_angle = 0\n            n = len(vertices)\n            for i in range(n):\n                v1 = vertices[i]\n                v2 = vertices[(i + 1) % n]\n                a = np.arccos(np.clip(np.dot(center, v1), -1, 1))\n                b = np.arccos(np.clip(np.dot(center, v2), -1, 1))\n                c = np.arccos(np.clip(np.dot(v1, v2), -1, 1))\n                s = (a + b + c) / 2\n                tan_E_4 = np.sqrt(\n                    np.tan(s / 2)\n                    * np.tan((s - a) / 2)\n                    * np.tan((s - b) / 2)\n                    * np.tan((s - c) / 2)\n                )\n                E = 4 * np.arctan(tan_E_4)\n                total_angle += E\n            solid_angles.append(total_angle)\n\n        return np.array(solid_angles)\n\n    def _geometric_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Fallback geometric calculation.\"\"\"\n        solid_angles = []\n        for row in self.grid.iter_rows(named=True):\n            delta_phi = row[\"phi_max\"] - row[\"phi_min\"]\n            cos_diff = np.cos(row[\"theta_min\"]) - np.cos(row[\"theta_max\"])\n            omega = delta_phi * cos_diff\n            solid_angles.append(omega)\n        return np.array(solid_angles)\n\n    def get_grid_stats(self) -&gt; dict:\n        \"\"\"Get grid statistics including solid angle uniformity.\"\"\"\n        solid_angles = self.get_solid_angles()\n\n        stats = {\n            \"total_cells\": self.ncells,\n            \"grid_type\": self.grid_type,\n            \"theta_bands\": len(self.theta_lims),\n            \"cells_per_band\": [len(ids) for ids in self.cell_ids],\n            \"solid_angle_mean_sr\": float(np.mean(solid_angles)),\n            \"solid_angle_std_sr\": float(np.std(solid_angles)),\n            \"solid_angle_cv_percent\": float(\n                np.std(solid_angles) / np.mean(solid_angles) * 100\n            ),\n            \"total_solid_angle_sr\": float(np.sum(solid_angles)),\n            \"hemisphere_solid_angle_sr\": 2 * np.pi,\n        }\n\n        # Add HEALPix-specific info\n        if self.grid_type == \"healpix\" and \"healpix_nside\" in self.grid.columns:\n            try:\n                import healpy as hp\n\n                nside = int(self.grid[\"healpix_nside\"][0])\n                stats[\"healpix_nside\"] = nside\n                stats[\"healpix_npix_total\"] = hp.nside2npix(nside)\n                stats[\"healpix_pixel_area_sr\"] = hp.nside2pixarea(nside)\n                stats[\"healpix_resolution_arcmin\"] = hp.nside2resol(nside, arcmin=True)\n            except ImportError:\n                pass\n\n        return stats\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_data.GridData.coords","level":3,"title":"<code>coords</code>  <code>property</code>","text":"<p>Get cell coordinates.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_data.GridData.ncells","level":3,"title":"<code>ncells</code>  <code>property</code>","text":"<p>Number of cells in grid.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_data.GridData.get_patches","level":3,"title":"<code>get_patches()</code>","text":"<p>Create matplotlib patches for polar visualization.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_data.py</code> <pre><code>def get_patches(self) -&gt; pl.Series:\n    \"\"\"Create matplotlib patches for polar visualization.\"\"\"\n    patches = [\n        Rectangle(\n            (row[\"phi_min\"], row[\"theta_min\"]),\n            row[\"phi_max\"] - row[\"phi_min\"],\n            row[\"theta_max\"] - row[\"theta_min\"],\n            fill=True,\n        )\n        for row in self.grid.iter_rows(named=True)\n    ]\n    return pl.Series(\"Patches\", patches)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_data.GridData.get_solid_angles","level":3,"title":"<code>get_solid_angles()</code>","text":"<p>Calculate solid angle for each cell [steradians].</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_data.py</code> <pre><code>def get_solid_angles(self) -&gt; np.ndarray:\n    \"\"\"Calculate solid angle for each cell [steradians].\"\"\"\n    if self.solid_angles is not None:\n        return self.solid_angles\n\n    # HEALPix\n    if self.grid_type == \"healpix\" and \"healpix_nside\" in self.grid.columns:\n        try:\n            import healpy as hp\n\n            nside = int(self.grid[\"healpix_nside\"][0])\n            return np.full(\n                len(self.grid), hp.nside2pixarea(nside), dtype=np.float64\n            )\n        except ImportError:\n            pass\n\n    # Geodesic\n    if self.grid_type == \"geodesic\" and \"geodesic_vertices\" in self.grid.columns:\n        return self._compute_geodesic_solid_angles()\n\n    # HTM\n    if self.grid_type == \"htm\" and \"htm_vertex_0\" in self.grid.columns:\n        return self._compute_htm_solid_angles()\n\n    # Fibonacci\n    if self.grid_type == \"fibonacci\" and \"voronoi_region\" in self.grid.columns:\n        return self._compute_voronoi_solid_angles()\n\n    # Default\n    return self._geometric_solid_angles()\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_data.GridData.get_grid_stats","level":3,"title":"<code>get_grid_stats()</code>","text":"<p>Get grid statistics including solid angle uniformity.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_data.py</code> <pre><code>def get_grid_stats(self) -&gt; dict:\n    \"\"\"Get grid statistics including solid angle uniformity.\"\"\"\n    solid_angles = self.get_solid_angles()\n\n    stats = {\n        \"total_cells\": self.ncells,\n        \"grid_type\": self.grid_type,\n        \"theta_bands\": len(self.theta_lims),\n        \"cells_per_band\": [len(ids) for ids in self.cell_ids],\n        \"solid_angle_mean_sr\": float(np.mean(solid_angles)),\n        \"solid_angle_std_sr\": float(np.std(solid_angles)),\n        \"solid_angle_cv_percent\": float(\n            np.std(solid_angles) / np.mean(solid_angles) * 100\n        ),\n        \"total_solid_angle_sr\": float(np.sum(solid_angles)),\n        \"hemisphere_solid_angle_sr\": 2 * np.pi,\n    }\n\n    # Add HEALPix-specific info\n    if self.grid_type == \"healpix\" and \"healpix_nside\" in self.grid.columns:\n        try:\n            import healpy as hp\n\n            nside = int(self.grid[\"healpix_nside\"][0])\n            stats[\"healpix_nside\"] = nside\n            stats[\"healpix_npix_total\"] = hp.nside2npix(nside)\n            stats[\"healpix_pixel_area_sr\"] = hp.nside2pixarea(nside)\n            stats[\"healpix_resolution_arcmin\"] = hp.nside2resol(nside, arcmin=True)\n        except ImportError:\n            pass\n\n    return stats\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.core.grid_types.GridType","level":2,"title":"<code>GridType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Available grid projection types for hemispherical tessellation.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/core/grid_types.py</code> <pre><code>class GridType(Enum):\n    \"\"\"Available grid projection types for hemispherical tessellation.\"\"\"\n\n    EQUAL_AREA = \"equal_area\"  # Equal solid angle (ring-based)\n    EQUAL_ANGLE = \"equal_angle\"  # Equal angular spacing\n    EQUIRECTANGULAR = \"equirectangular\"  # Simple rectangular\n    HEALPIX = \"healpix\"  # Hierarchical equal area\n    GEODESIC = \"geodesic\"  # Icosahedral triangular\n    FIBONACCI = \"fibonacci\"  # Golden spiral + Voronoi\n    HTM = \"htm\"  # Hierarchical Triangular Mesh\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#grid-builders","level":2,"title":"Grid Builders","text":"","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#equal-area-grid","level":3,"title":"Equal-Area Grid","text":"<p>Equal-area grid implementation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_area_grid.EqualAreaBuilder","level":2,"title":"<code>EqualAreaBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Equal solid angle tessellation using concentric theta bands.</p> <p>The hemisphere is divided into annular bands of constant width in theta. Within each band the number of azimuthal (phi) sectors is chosen so that every cell subtends approximately the same solid angle.  This is the only grid type that has been validated for scientific use in this codebase.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_area_grid.EqualAreaBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle measured from zenith (0 = straight up,   π/2 = horizon)</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_area_grid.EqualAreaBuilder--what-angular_resolution-means","level":4,"title":"What <code>angular_resolution</code> means","text":"<p><code>angular_resolution</code> (degrees) sets the width of each theta band. All bands have this same width Δθ.  The azimuthal width of cells varies by band: near the zenith cells are wide in phi; near the horizon they are narrow, so that the solid angle stays constant.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_area_grid.EqualAreaBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li> <p>Target solid angle per cell is chosen equal to the solid angle of a    cap of half-angle Δθ/2::</p> <p>Ω_target = 2π (1 − cos(Δθ/2))</p> </li> <li> <p>Zenith cap – a single cell covers [0, Δθ/2] in theta and the full    azimuth [0, 2π).</p> </li> <li> <p>Theta bands – edges are placed at Δθ/2, 3Δθ/2, 5Δθ/2, … up to    π/2 − cutoff_theta.  For each band [θ_inner, θ_outer] the band's    total solid angle is::</p> <p>Ω_band = 2π (cos θ_inner − cos θ_outer)</p> </li> <li> <p>Phi divisions – the number of sectors in the band is::</p> <p>n_phi = round(Ω_band / Ω_target)</p> </li> </ol> <p>Each sector spans Δφ = 2π / n_phi.  The cell centre is placed at the    geometric midpoint of its (phi, theta) rectangle.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_area_grid.EqualAreaBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Theta-band width in degrees.  Controls both the radial resolution and     (indirectly, via the equal-area constraint) the azimuthal resolution. cutoff_theta : float     Minimum elevation above the horizon in degrees.  Bands whose outer     edge is at or below this cutoff are omitted.  In GNSS terms this is     the satellite elevation mask angle. phi_rotation : float     Rigid rotation applied to all phi coordinates after grid construction,     in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equal_area_grid.py</code> <pre><code>class EqualAreaBuilder(BaseGridBuilder):\n    \"\"\"Equal solid angle tessellation using concentric theta bands.\n\n    The hemisphere is divided into annular bands of constant width in theta.\n    Within each band the number of azimuthal (phi) sectors is chosen so that\n    every cell subtends approximately the same solid angle.  This is the only\n    grid type that has been validated for scientific use in this codebase.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle measured from zenith (0 = straight up,\n      π/2 = horizon)\n\n    What ``angular_resolution`` means\n    ----------------------------------\n    ``angular_resolution`` (degrees) sets the **width of each theta band**.\n    All bands have this same width Δθ.  The *azimuthal* width of cells varies\n    by band: near the zenith cells are wide in phi; near the horizon they are\n    narrow, so that the solid angle stays constant.\n\n    Mathematical construction\n    -------------------------\n    1. **Target solid angle** per cell is chosen equal to the solid angle of a\n       cap of half-angle Δθ/2::\n\n           Ω_target = 2π (1 − cos(Δθ/2))\n\n    2. **Zenith cap** – a single cell covers [0, Δθ/2] in theta and the full\n       azimuth [0, 2π).\n\n    3. **Theta bands** – edges are placed at Δθ/2, 3Δθ/2, 5Δθ/2, … up to\n       π/2 − cutoff_theta.  For each band [θ_inner, θ_outer] the band's\n       total solid angle is::\n\n           Ω_band = 2π (cos θ_inner − cos θ_outer)\n\n    4. **Phi divisions** – the number of sectors in the band is::\n\n           n_phi = round(Ω_band / Ω_target)\n\n       Each sector spans Δφ = 2π / n_phi.  The cell centre is placed at the\n       geometric midpoint of its (phi, theta) rectangle.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Theta-band width in degrees.  Controls both the radial resolution and\n        (indirectly, via the equal-area constraint) the azimuthal resolution.\n    cutoff_theta : float\n        Minimum elevation above the horizon in degrees.  Bands whose outer\n        edge is at or below this cutoff are omitted.  In GNSS terms this is\n        the satellite elevation mask angle.\n    phi_rotation : float\n        Rigid rotation applied to all phi coordinates after grid construction,\n        in degrees.\n\n    \"\"\"\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"equal_area\"``\n\n        \"\"\"\n        return GridType.EQUAL_AREA.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]:\n        \"\"\"Construct the equal-area hemisphere grid.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per cell with columns: phi, theta, phi_min, phi_max,\n            theta_min, theta_max, cell_id.\n        theta_lims : np.ndarray\n            Outer theta edge of each band (radians).\n        phi_lims : list[np.ndarray]\n            Array of phi_min values for each band.\n        cell_ids : list[np.ndarray]\n            Cell-id arrays, one per band.\n\n        \"\"\"\n        # Theta band edges (from zenith to horizon)\n        max_theta = np.pi / 2  # horizon\n        theta_edges = np.arange(\n            self.angular_resolution_rad / 2,\n            max_theta - self.cutoff_theta_rad,\n            self.angular_resolution_rad,\n        )\n\n        # Target solid angle per cell\n        target_omega = 2 * np.pi * (1 - np.cos(self.angular_resolution_rad / 2))\n\n        cells = []\n        theta_lims = []\n        phi_lims = []\n        cell_ids = []\n\n        # Zenith cell (special case) - only if cutoff allows\n        next_cell_id = 0\n        zenith_theta_max = self.angular_resolution_rad / 2\n\n        if self.cutoff_theta_rad &lt; zenith_theta_max:\n            cells.append(\n                pl.DataFrame(\n                    {\n                        \"phi\": [0.0],\n                        \"theta\": [0.0],\n                        \"phi_min\": [0.0],\n                        \"phi_max\": [2 * np.pi],\n                        \"theta_min\": [max(0.0, self.cutoff_theta_rad)],\n                        \"theta_max\": [zenith_theta_max],\n                    }\n                )\n            )\n            theta_lims.append(zenith_theta_max)\n            phi_lims.append(np.array([0.0]))\n            cell_ids.append(np.array([0]))\n            next_cell_id = 1\n\n        # Build theta bands\n        for iband, theta_outer in enumerate(theta_edges[1:]):\n            theta_inner = theta_edges[iband]\n\n            # Skip bands below cutoff\n            if theta_outer &lt;= self.cutoff_theta_rad:\n                continue\n\n            # Solid angle of this band\n            band_omega = 2 * np.pi * (np.cos(theta_inner) - np.cos(theta_outer))\n\n            # Number of phi divisions\n            n_phi = max(1, round(band_omega / target_omega))\n            phi_span = 2 * np.pi / n_phi\n\n            cell_id_list = list(range(next_cell_id, next_cell_id + n_phi))\n            next_cell_id = cell_id_list[-1] + 1\n\n            # Use arange for better precision than linspace\n            phi_min_arr = np.arange(n_phi) * phi_span\n            phi_max_arr = (np.arange(n_phi) + 1) * phi_span\n            phi_max_arr[-1] = 2 * np.pi  # Force exact closure\n\n            cells.append(\n                pl.DataFrame(\n                    {\n                        \"phi\": (phi_min_arr + phi_max_arr) / 2,\n                        \"theta\": np.full(n_phi, (theta_inner + theta_outer) / 2),\n                        \"phi_min\": phi_min_arr,\n                        \"phi_max\": phi_max_arr,\n                        \"theta_min\": np.full(n_phi, theta_inner),\n                        \"theta_max\": np.full(n_phi, theta_outer),\n                    }\n                )\n            )\n\n            theta_lims.append(theta_outer)\n            phi_lims.append(phi_min_arr)\n            cell_ids.append(np.array(cell_id_list))\n\n        if len(cells) == 0:\n            raise ValueError(\n                \"No cells generated - check cutoff_theta and angular_resolution\"\n            )\n\n        grid = pl.concat(cells).with_columns(pl.int_range(0, pl.len()).alias(\"cell_id\"))\n\n        return grid, np.array(theta_lims), phi_lims, cell_ids\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_area_grid.EqualAreaBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_area_grid.EqualAreaBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"equal_area\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equal_area_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"equal_area\"``\n\n    \"\"\"\n    return GridType.EQUAL_AREA.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#equal-angle-grid","level":3,"title":"Equal-Angle Grid","text":"<p>Equal-angle grid implementation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_angle_grid.EqualAngleBuilder","level":2,"title":"<code>EqualAngleBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Equal angular spacing in both theta and phi (NOT equal area).</p> <p>Every cell is a rectangle of the same angular size Δθ × Δφ in the (theta, phi) parameter space.  Because solid angle depends on cos(theta), cells near the zenith subtend more solid angle than cells near the horizon.  This makes the grid biased toward the zenith for any solid-angle-weighted statistic.  Not recommended for scientific analysis – use <code>EqualAreaBuilder</code> instead.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_angle_grid.EqualAngleBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle from zenith</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_angle_grid.EqualAngleBuilder--what-angular_resolution-means","level":4,"title":"What <code>angular_resolution</code> means","text":"<p><code>angular_resolution</code> (degrees) is used as both the theta-band width and the phi-sector width.  The number of phi divisions is constant across all bands::</p> <pre><code>n_phi = round(2π / Δθ)\n</code></pre> <p>and does not change with latitude.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_angle_grid.EqualAngleBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li>A zenith cap cell covers [0, Δθ/2] × [0, 2π).</li> <li>Theta band edges are placed at Δθ/2, 3Δθ/2, … up to π/2.</li> <li>Within every band, the full azimuth is split into <code>n_phi</code> sectors of    equal width Δφ = 2π / n_phi.</li> <li>Cell centres are at the midpoint of each (phi, theta) rectangle.</li> </ol>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_angle_grid.EqualAngleBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Angular spacing in degrees, applied identically in both theta and phi. cutoff_theta : float     Elevation mask angle in degrees (bands below this are omitted). phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equal_angle_grid.py</code> <pre><code>class EqualAngleBuilder(BaseGridBuilder):\n    \"\"\"Equal angular spacing in both theta and phi (NOT equal area).\n\n    Every cell is a rectangle of the same angular size Δθ × Δφ in the\n    (theta, phi) parameter space.  Because solid angle depends on cos(theta),\n    cells near the zenith subtend *more* solid angle than cells near the\n    horizon.  This makes the grid biased toward the zenith for any\n    solid-angle-weighted statistic.  **Not recommended for scientific\n    analysis** – use ``EqualAreaBuilder`` instead.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle from zenith\n\n    What ``angular_resolution`` means\n    ----------------------------------\n    ``angular_resolution`` (degrees) is used as **both** the theta-band width\n    and the phi-sector width.  The number of phi divisions is constant across\n    all bands::\n\n        n_phi = round(2π / Δθ)\n\n    and does not change with latitude.\n\n    Mathematical construction\n    -------------------------\n    1. A zenith cap cell covers [0, Δθ/2] × [0, 2π).\n    2. Theta band edges are placed at Δθ/2, 3Δθ/2, … up to π/2.\n    3. Within every band, the full azimuth is split into ``n_phi`` sectors of\n       equal width Δφ = 2π / n_phi.\n    4. Cell centres are at the midpoint of each (phi, theta) rectangle.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Angular spacing in degrees, applied identically in both theta and phi.\n    cutoff_theta : float\n        Elevation mask angle in degrees (bands below this are omitted).\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    \"\"\"\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"equal_angle\"``\n\n        \"\"\"\n        return GridType.EQUAL_ANGLE.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]:\n        \"\"\"Construct the equal-angle hemisphere grid.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per cell.\n        theta_lims : np.ndarray\n            Outer theta edge of each band (radians).\n        phi_lims : list[np.ndarray]\n            Array of phi_min values for each band (identical across bands).\n        cell_ids : list[np.ndarray]\n            Cell-id arrays, one per band.\n\n        \"\"\"\n        max_theta = np.pi / 2\n        theta_edges = np.arange(\n            self.angular_resolution_rad / 2,\n            max_theta - self.cutoff_theta_rad,\n            self.angular_resolution_rad,\n        )\n\n        n_phi_divisions = int(2 * np.pi / self.angular_resolution_rad)\n\n        cells = []\n        theta_lims = []\n        phi_lims = []\n        cell_ids = []\n\n        # Zenith\n        cells.append(\n            pl.DataFrame(\n                {\n                    \"phi\": [0.0],\n                    \"theta\": [0.0],\n                    \"phi_min\": [0.0],\n                    \"phi_max\": [2 * np.pi],\n                    \"theta_min\": [0.0],\n                    \"theta_max\": [self.angular_resolution_rad / 2],\n                }\n            )\n        )\n        theta_lims.append(self.angular_resolution_rad / 2)\n        phi_lims.append(np.array([0.0]))\n        cell_ids.append(np.array([0]))\n        next_cell_id = 1\n\n        for iband, theta_outer in enumerate(theta_edges[1:]):\n            theta_inner = theta_edges[iband]\n            phi_span = 2 * np.pi / n_phi_divisions\n\n            cell_id_list = list(range(next_cell_id, next_cell_id + n_phi_divisions))\n            next_cell_id = cell_id_list[-1] + 1\n\n            phi_min_arr = np.linspace(0, 2 * np.pi - phi_span, n_phi_divisions)\n            phi_max_arr = np.concatenate((phi_min_arr[1:], [2 * np.pi]))\n\n            cells.append(\n                pl.DataFrame(\n                    {\n                        \"phi\": (phi_min_arr + phi_max_arr) / 2,\n                        \"theta\": np.full(\n                            n_phi_divisions,\n                            (theta_inner + theta_outer) / 2,\n                        ),\n                        \"phi_min\": phi_min_arr,\n                        \"phi_max\": phi_max_arr,\n                        \"theta_min\": np.full(n_phi_divisions, theta_inner),\n                        \"theta_max\": np.full(n_phi_divisions, theta_outer),\n                    }\n                )\n            )\n\n            theta_lims.append(theta_outer)\n            phi_lims.append(phi_min_arr)\n            cell_ids.append(np.array(cell_id_list))\n\n        grid = pl.concat(cells).with_row_index(\"cell_id\")\n        return grid, np.array(theta_lims), phi_lims, cell_ids\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_angle_grid.EqualAngleBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equal_angle_grid.EqualAngleBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"equal_angle\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equal_angle_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"equal_angle\"``\n\n    \"\"\"\n    return GridType.EQUAL_ANGLE.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#equirectangular-grid","level":3,"title":"Equirectangular Grid","text":"<p>Equirectangular grid implementation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equirectangular_grid.EquirectangularBuilder","level":2,"title":"<code>EquirectangularBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Simple rectangular grid in (theta, phi) space.</p> <p>The hemisphere is divided into a regular rectangular array: a constant number of theta bands, each containing the same constant number of phi sectors.  Every cell is an identical rectangle in angular coordinates. This is structurally identical to <code>EqualAngleBuilder</code> except for one difference in the zenith treatment: <code>EqualAngleBuilder</code> collapses the first band into a single zenith cap, while this builder does not — every band has the same number of sectors.</p> <p>Because solid angle depends on cos(theta), cells near the zenith subtend more solid angle than cells near the horizon.  This makes the grid biased toward the zenith for any solid-angle-weighted statistic. Not recommended for scientific analysis – use <code>EqualAreaBuilder</code> instead.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equirectangular_grid.EquirectangularBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,   π/2 = horizon)</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equirectangular_grid.EquirectangularBuilder--what-angular_resolution-means","level":4,"title":"What <code>angular_resolution</code> means","text":"<p><code>angular_resolution</code> (degrees) is used as both the theta-band width and the phi-sector width.  The grid is therefore square in angular coordinates::</p> <pre><code>n_theta = round((π/2 − cutoff) / Δθ)\nn_phi   = round(2π / Δθ)\ntotal cells = n_theta × n_phi\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equirectangular_grid.EquirectangularBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li>Theta edges are placed at <code>cutoff_theta</code>, <code>cutoff_theta + Δθ</code>,    <code>cutoff_theta + 2Δθ</code>, … up to π/2.</li> <li>Phi edges are placed at 0, Δθ, 2Δθ, … up to 2π.</li> <li>Every (theta_band, phi_sector) combination produces one cell.  The    cell centre is the midpoint of the rectangle.</li> <li>No special zenith cap is created; the band nearest the zenith has    the same number of phi sectors as all other bands.</li> </ol>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equirectangular_grid.EquirectangularBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Angular spacing in degrees, applied identically in both theta and phi. cutoff_theta : float     Elevation mask angle in degrees.  Bands whose inner edge is at or     below <code>π/2 − cutoff_theta</code> are omitted. phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equirectangular_grid.py</code> <pre><code>class EquirectangularBuilder(BaseGridBuilder):\n    \"\"\"Simple rectangular grid in (theta, phi) space.\n\n    The hemisphere is divided into a regular rectangular array: a constant\n    number of theta bands, each containing the same constant number of phi\n    sectors.  Every cell is an identical rectangle in angular coordinates.\n    This is *structurally* identical to ``EqualAngleBuilder`` except for one\n    difference in the zenith treatment: ``EqualAngleBuilder`` collapses the\n    first band into a single zenith cap, while this builder does not — every\n    band has the same number of sectors.\n\n    Because solid angle depends on cos(theta), cells near the zenith subtend\n    *more* solid angle than cells near the horizon.  This makes the grid\n    biased toward the zenith for any solid-angle-weighted statistic.\n    **Not recommended for scientific analysis** – use ``EqualAreaBuilder``\n    instead.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,\n      π/2 = horizon)\n\n    What ``angular_resolution`` means\n    ----------------------------------\n    ``angular_resolution`` (degrees) is used as **both** the theta-band width\n    *and* the phi-sector width.  The grid is therefore square in angular\n    coordinates::\n\n        n_theta = round((π/2 − cutoff) / Δθ)\n        n_phi   = round(2π / Δθ)\n        total cells = n_theta × n_phi\n\n    Mathematical construction\n    -------------------------\n    1. Theta edges are placed at ``cutoff_theta``, ``cutoff_theta + Δθ``,\n       ``cutoff_theta + 2Δθ``, … up to π/2.\n    2. Phi edges are placed at 0, Δθ, 2Δθ, … up to 2π.\n    3. Every (theta_band, phi_sector) combination produces one cell.  The\n       cell centre is the midpoint of the rectangle.\n    4. No special zenith cap is created; the band nearest the zenith has\n       the same number of phi sectors as all other bands.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Angular spacing in degrees, applied identically in both theta and phi.\n    cutoff_theta : float\n        Elevation mask angle in degrees.  Bands whose *inner* edge is at or\n        below ``π/2 − cutoff_theta`` are omitted.\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    \"\"\"\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"equirectangular\"``\n\n        \"\"\"\n        return GridType.EQUIRECTANGULAR.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]:\n        \"\"\"Construct the equirectangular hemisphere grid.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per cell with columns: phi, theta, phi_min, phi_max,\n            theta_min, theta_max, cell_id.\n        theta_lims : np.ndarray\n            Inner theta edge of each band (radians).\n        phi_lims : list[np.ndarray]\n            Array of phi_min values for each band (identical across bands).\n        cell_ids : list[np.ndarray]\n            Cell-id arrays, one per band.\n\n        \"\"\"\n        max_theta = np.pi / 2\n\n        theta_edges = np.arange(\n            self.cutoff_theta_rad,\n            max_theta + self.angular_resolution_rad,\n            self.angular_resolution_rad,\n        )\n        phi_edges = np.arange(\n            0, 2 * np.pi + self.angular_resolution_rad, self.angular_resolution_rad\n        )\n\n        cells = []\n\n        for i in range(len(theta_edges) - 1):\n            theta_min, theta_max = theta_edges[i], theta_edges[i + 1]\n\n            for j in range(len(phi_edges) - 1):\n                phi_min, phi_max = phi_edges[j], phi_edges[j + 1]\n\n                cells.append(\n                    {\n                        \"phi\": (phi_min + phi_max) / 2,\n                        \"theta\": (theta_min + theta_max) / 2,\n                        \"phi_min\": phi_min,\n                        \"phi_max\": min(2 * np.pi, phi_max),\n                        \"theta_min\": theta_min,\n                        \"theta_max\": theta_max,\n                    }\n                )\n\n        grid = pl.DataFrame(cells).with_columns(\n            pl.int_range(0, pl.len()).alias(\"cell_id\")\n        )\n\n        theta_lims = theta_edges[:-1]\n        phi_lims = [phi_edges[:-1] for _ in range(len(theta_edges) - 1)]\n        cell_ids_list = [\n            np.arange(i * (len(phi_edges) - 1), (i + 1) * (len(phi_edges) - 1))\n            for i in range(len(theta_edges) - 1)\n        ]\n\n        return grid, theta_lims, phi_lims, cell_ids_list\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equirectangular_grid.EquirectangularBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.equirectangular_grid.EquirectangularBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"equirectangular\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/equirectangular_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"equirectangular\"``\n\n    \"\"\"\n    return GridType.EQUIRECTANGULAR.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#healpix-grid","level":3,"title":"HEALPix Grid","text":"<p>HEALPix grid implementation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder","level":2,"title":"<code>HEALPixBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>HEALPix tessellation (Hierarchical Equal Area isoLatitude Pixelization).</p> <p>HEALPix partitions the sphere into 12 base pixels arranged at equal latitudes.  Each base pixel is recursively subdivided into 4 children, producing <code>12 × nside²</code> pixels on the full sphere, all with exactly the same solid angle.  This strict equal-area property makes HEALPix the gold standard for pixelisations that must be unbiased under solid-angle weighting.</p> <p>This builder delegates the pixel geometry entirely to the <code>healpy</code> library.  It filters the full-sphere pixelisation down to the northern hemisphere and stores approximate bounding boxes (<code>phi_min/max</code>, <code>theta_min/max</code>) derived from the pixel resolution.  The bounding boxes are not the true pixel boundaries (which are curvilinear); they are only approximations suitable for quick spatial queries.  For exact pixel membership use <code>healpy.ang2pix</code> directly.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder--coordinate-convention","level":4,"title":"Coordinate convention","text":"<p>HEALPix natively uses colatitude <code>theta ∈ [0, π]</code> (0 = North Pole) and longitude <code>phi ∈ [0, 2π)</code>.  This matches the GNSS convention used elsewhere in canvodpy: theta = 0 is the zenith, theta = π/2 is the horizon.  No coordinate transform is applied.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder--what-nside-resolution-means","level":4,"title":"What <code>nside</code> (resolution) means","text":"<p><code>nside</code> is the single resolution parameter of HEALPix.  It must be a power of 2.  The key derived quantities are::</p> <pre><code>n_pixels   = 12 × nside²           (full sphere)\npixel_area = 4π / n_pixels          (steradians, exact)\nresolution ≈ √(pixel_area)          (approximate angular diameter)\n           ≈ 58.6° / nside         (degrees)\n</code></pre> nside Pixels (full) Approx resolution Pixel area (sr) 1 12 58.6° 1.049 2 48 29.3° 0.262 4 192 14.7° 0.065 8 768 7.3° 0.016 16 3 072 3.7° 0.004 32 12 288 1.8° 0.001 <p>When <code>nside</code> is not provided, it is estimated from <code>angular_resolution</code> and rounded to the nearest power of 2::</p> <pre><code>nside_estimate = round_to_pow2( √(3/π) × 60 / angular_resolution )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<p>HEALPix construction is performed entirely by <code>healpy</code>.  At a high level:</p> <ol> <li>The sphere is divided into 12 congruent base pixels (a curvilinear    quadrilateral arrangement at three latitude zones: polar caps and    equatorial belt).</li> <li>Each base pixel is subdivided into <code>nside²</code> equal-area children    using a hierarchical quadtree.</li> <li>Pixel centres are returned by <code>healpy.pix2ang(nside, ipix)</code> in    RING ordering (pixels ordered by increasing colatitude).</li> <li>This builder keeps only pixels with <code>theta ≤ π/2 − cutoff_theta</code>    (northern hemisphere above the elevation mask).</li> </ol>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Approximate angular resolution in degrees.  Used only to derive     <code>nside</code> when that parameter is not given explicitly. cutoff_theta : float     Elevation mask angle in degrees.  Pixels with colatitude     <code>theta &gt; π/2 − cutoff_theta</code> (i.e. below the mask) are excluded. nside : int or None     HEALPix resolution parameter.  Must be a power of 2.  If <code>None</code>,     estimated from <code>angular_resolution</code>. phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder--raises","level":4,"title":"Raises","text":"<p>ImportError     If <code>healpy</code> is not installed. ValueError     If <code>nside</code> is not a power of 2.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/healpix_grid.py</code> <pre><code>class HEALPixBuilder(BaseGridBuilder):\n    \"\"\"HEALPix tessellation (Hierarchical Equal Area isoLatitude Pixelization).\n\n    HEALPix partitions the sphere into 12 base pixels arranged at equal\n    latitudes.  Each base pixel is recursively subdivided into 4 children,\n    producing ``12 × nside²`` pixels on the full sphere, all with *exactly*\n    the same solid angle.  This strict equal-area property makes HEALPix\n    the gold standard for pixelisations that must be unbiased under\n    solid-angle weighting.\n\n    This builder delegates the pixel geometry entirely to the ``healpy``\n    library.  It filters the full-sphere pixelisation down to the northern\n    hemisphere and stores approximate bounding boxes (``phi_min/max``,\n    ``theta_min/max``) derived from the pixel resolution.  The bounding\n    boxes are **not** the true pixel boundaries (which are curvilinear);\n    they are only approximations suitable for quick spatial queries.  For\n    exact pixel membership use ``healpy.ang2pix`` directly.\n\n    Coordinate convention\n    ---------------------\n    HEALPix natively uses colatitude ``theta ∈ [0, π]`` (0 = North Pole)\n    and longitude ``phi ∈ [0, 2π)``.  This matches the GNSS convention used\n    elsewhere in canvodpy: theta = 0 is the zenith, theta = π/2 is the\n    horizon.  **No coordinate transform is applied.**\n\n    What ``nside`` (resolution) means\n    ----------------------------------\n    ``nside`` is the single resolution parameter of HEALPix.  It must be a\n    power of 2.  The key derived quantities are::\n\n        n_pixels   = 12 × nside²           (full sphere)\n        pixel_area = 4π / n_pixels          (steradians, exact)\n        resolution ≈ √(pixel_area)          (approximate angular diameter)\n                   ≈ 58.6° / nside         (degrees)\n\n    | nside | Pixels (full) | Approx resolution | Pixel area (sr) |\n    |-------|---------------|-------------------|-----------------|\n    | 1     | 12            | 58.6°             | 1.049           |\n    | 2     | 48            | 29.3°             | 0.262           |\n    | 4     | 192           | 14.7°             | 0.065           |\n    | 8     | 768           | 7.3°              | 0.016           |\n    | 16    | 3 072         | 3.7°              | 0.004           |\n    | 32    | 12 288        | 1.8°              | 0.001           |\n\n    When ``nside`` is not provided, it is estimated from ``angular_resolution``\n    and rounded to the nearest power of 2::\n\n        nside_estimate = round_to_pow2( √(3/π) × 60 / angular_resolution )\n\n    Mathematical construction\n    -------------------------\n    HEALPix construction is performed entirely by ``healpy``.  At a high\n    level:\n\n    1. The sphere is divided into 12 congruent base pixels (a curvilinear\n       quadrilateral arrangement at three latitude zones: polar caps and\n       equatorial belt).\n    2. Each base pixel is subdivided into ``nside²`` equal-area children\n       using a hierarchical quadtree.\n    3. Pixel centres are returned by ``healpy.pix2ang(nside, ipix)`` in\n       RING ordering (pixels ordered by increasing colatitude).\n    4. This builder keeps only pixels with ``theta ≤ π/2 − cutoff_theta``\n       (northern hemisphere above the elevation mask).\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Approximate angular resolution in degrees.  Used only to derive\n        ``nside`` when that parameter is not given explicitly.\n    cutoff_theta : float\n        Elevation mask angle in degrees.  Pixels with colatitude\n        ``theta &gt; π/2 − cutoff_theta`` (i.e. below the mask) are excluded.\n    nside : int or None\n        HEALPix resolution parameter.  Must be a power of 2.  If ``None``,\n        estimated from ``angular_resolution``.\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    Raises\n    ------\n    ImportError\n        If ``healpy`` is not installed.\n    ValueError\n        If ``nside`` is not a power of 2.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        angular_resolution: float = 2,\n        cutoff_theta: float = 0,\n        nside: int | None = None,\n        phi_rotation: float = 0,\n    ) -&gt; None:\n        \"\"\"Initialize the HEALPix grid builder.\n\n        Parameters\n        ----------\n        angular_resolution : float, default 2\n            Angular resolution in degrees.\n        cutoff_theta : float, default 0\n            Maximum polar angle cutoff in degrees.\n        nside : int | None, optional\n            HEALPix nside parameter.\n        phi_rotation : float, default 0\n            Rotation angle in degrees.\n\n        \"\"\"\n        super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n        # Determine nside\n        if nside is None:\n            nside_estimate = int(np.sqrt(3 / np.pi) * 60 / angular_resolution)\n            self.nside = 2 ** max(0, int(np.round(np.log2(nside_estimate))))\n        else:\n            if nside &lt; 1 or (nside &amp; (nside - 1)) != 0:\n                raise ValueError(f\"nside must be a power of 2, got {nside}\")\n            self.nside = nside\n\n        # Import healpy\n        try:\n            import healpy as hp\n\n            self.hp = hp\n        except ImportError:\n            raise ImportError(\n                \"healpy is required for HEALPix grid. Install with: pip install healpy\"\n            )\n\n        pixel_size_arcmin = self.hp.nside2resol(self.nside, arcmin=True)\n        self.actual_angular_resolution = pixel_size_arcmin / 60.0\n\n        self._logger.info(\n            f\"HEALPix: nside={self.nside}, \"\n            f\"requested_res={angular_resolution:.2f}°, \"\n            f\"actual_res={self.actual_angular_resolution:.2f}°\"\n        )\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"healpix\"``\n\n        \"\"\"\n        return GridType.HEALPIX.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]:\n        \"\"\"Build HEALPix grid for the northern hemisphere.\n\n        Iterates over all ``12 × nside²`` pixels, retains those with\n        ``theta ≤ π/2 − cutoff_theta``, and constructs approximate\n        bounding boxes from the pixel resolution.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per pixel.  Contains phi, theta (centre), approximate\n            bounding-box limits, ``healpix_ipix`` (RING-ordered pixel index),\n            and ``healpix_nside``.\n        theta_lims : np.ndarray\n            Synthetic evenly-spaced theta limits (interface compatibility only).\n        phi_lims : list[np.ndarray]\n            Synthetic evenly-spaced phi limits (interface compatibility only).\n        cell_ids : list[np.ndarray]\n            Single-element list containing the valid pixel indices.\n\n        \"\"\"\n        npix = self.hp.nside2npix(self.nside)\n\n        cells = []\n        valid_pixels = []\n\n        for ipix in range(npix):\n            theta, phi = self.hp.pix2ang(self.nside, ipix)\n\n            # Keep only northern hemisphere above the elevation mask\n            if theta &gt; (np.pi / 2 - self.cutoff_theta_rad):\n                continue\n\n            pixel_radius = self.hp.nside2resol(self.nside)\n\n            cells.append(\n                {\n                    \"phi\": float(phi),\n                    \"theta\": float(theta),\n                    \"phi_min\": float(max(0, phi - pixel_radius / 2)),\n                    \"phi_max\": float(min(2 * np.pi, phi + pixel_radius / 2)),\n                    \"theta_min\": float(max(0, theta - pixel_radius / 2)),\n                    \"theta_max\": float(min(np.pi / 2, theta + pixel_radius / 2)),\n                    \"healpix_ipix\": int(ipix),\n                    \"healpix_nside\": int(self.nside),\n                }\n            )\n            valid_pixels.append(int(ipix))\n\n        if len(cells) == 0:\n            raise ValueError(\"No valid HEALPix pixels found in hemisphere\")\n\n        grid = pl.DataFrame(cells)\n\n        grid = grid.with_columns(\n            [\n                pl.col(\"healpix_ipix\").cast(pl.Int64),\n                pl.col(\"healpix_nside\").cast(pl.Int64),\n            ]\n        )\n\n        theta_unique = sorted(grid[\"theta\"].unique())\n        n_theta_bands = len(theta_unique)\n\n        # NOTE: These limits are SYNTHETIC and do NOT correspond to actual\n        # HEALPix pixel boundaries. They exist only for interface\n        # compatibility with ring-based grids. For spatial queries, use the\n        # per-pixel theta_min/max and phi_min/max columns instead.\n        theta_lims = np.linspace(0, np.pi / 2, min(n_theta_bands, 20))\n        phi_lims = [np.linspace(0, 2 * np.pi, 20) for _ in range(len(theta_lims))]\n\n        cell_ids_list = [np.array(valid_pixels, dtype=np.int64)]\n\n        return grid, theta_lims, phi_lims, cell_ids_list\n\n    def get_healpix_info(self) -&gt; dict:\n        \"\"\"Get HEALPix-specific information.\n\n        Returns\n        -------\n        info : dict\n            Keys: ``nside``, ``npix_total``, ``pixel_area_sr``,\n            ``pixel_area_arcmin2``, ``resolution_arcmin``,\n            ``resolution_deg``, ``max_pixel_radius_deg``.\n\n        \"\"\"\n        return {\n            \"nside\": self.nside,\n            \"npix_total\": self.hp.nside2npix(self.nside),\n            \"pixel_area_sr\": self.hp.nside2pixarea(self.nside),\n            \"pixel_area_arcmin2\": (\n                self.hp.nside2pixarea(self.nside, degrees=True) * 3600\n            ),\n            \"resolution_arcmin\": self.hp.nside2resol(self.nside, arcmin=True),\n            \"resolution_deg\": self.actual_angular_resolution,\n            \"max_pixel_radius_deg\": np.rad2deg(self.hp.max_pixrad(self.nside)),\n        }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder.__init__","level":3,"title":"<code>__init__(angular_resolution=2, cutoff_theta=0, nside=None, phi_rotation=0)</code>","text":"<p>Initialize the HEALPix grid builder.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder.__init__--parameters","level":5,"title":"Parameters","text":"<p>angular_resolution : float, default 2     Angular resolution in degrees. cutoff_theta : float, default 0     Maximum polar angle cutoff in degrees. nside : int | None, optional     HEALPix nside parameter. phi_rotation : float, default 0     Rotation angle in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/healpix_grid.py</code> <pre><code>def __init__(\n    self,\n    angular_resolution: float = 2,\n    cutoff_theta: float = 0,\n    nside: int | None = None,\n    phi_rotation: float = 0,\n) -&gt; None:\n    \"\"\"Initialize the HEALPix grid builder.\n\n    Parameters\n    ----------\n    angular_resolution : float, default 2\n        Angular resolution in degrees.\n    cutoff_theta : float, default 0\n        Maximum polar angle cutoff in degrees.\n    nside : int | None, optional\n        HEALPix nside parameter.\n    phi_rotation : float, default 0\n        Rotation angle in degrees.\n\n    \"\"\"\n    super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n    # Determine nside\n    if nside is None:\n        nside_estimate = int(np.sqrt(3 / np.pi) * 60 / angular_resolution)\n        self.nside = 2 ** max(0, int(np.round(np.log2(nside_estimate))))\n    else:\n        if nside &lt; 1 or (nside &amp; (nside - 1)) != 0:\n            raise ValueError(f\"nside must be a power of 2, got {nside}\")\n        self.nside = nside\n\n    # Import healpy\n    try:\n        import healpy as hp\n\n        self.hp = hp\n    except ImportError:\n        raise ImportError(\n            \"healpy is required for HEALPix grid. Install with: pip install healpy\"\n        )\n\n    pixel_size_arcmin = self.hp.nside2resol(self.nside, arcmin=True)\n    self.actual_angular_resolution = pixel_size_arcmin / 60.0\n\n    self._logger.info(\n        f\"HEALPix: nside={self.nside}, \"\n        f\"requested_res={angular_resolution:.2f}°, \"\n        f\"actual_res={self.actual_angular_resolution:.2f}°\"\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"healpix\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/healpix_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"healpix\"``\n\n    \"\"\"\n    return GridType.HEALPIX.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder.get_healpix_info","level":3,"title":"<code>get_healpix_info()</code>","text":"<p>Get HEALPix-specific information.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.healpix_grid.HEALPixBuilder.get_healpix_info--returns","level":5,"title":"Returns","text":"<p>info : dict     Keys: <code>nside</code>, <code>npix_total</code>, <code>pixel_area_sr</code>,     <code>pixel_area_arcmin2</code>, <code>resolution_arcmin</code>,     <code>resolution_deg</code>, <code>max_pixel_radius_deg</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/healpix_grid.py</code> <pre><code>def get_healpix_info(self) -&gt; dict:\n    \"\"\"Get HEALPix-specific information.\n\n    Returns\n    -------\n    info : dict\n        Keys: ``nside``, ``npix_total``, ``pixel_area_sr``,\n        ``pixel_area_arcmin2``, ``resolution_arcmin``,\n        ``resolution_deg``, ``max_pixel_radius_deg``.\n\n    \"\"\"\n    return {\n        \"nside\": self.nside,\n        \"npix_total\": self.hp.nside2npix(self.nside),\n        \"pixel_area_sr\": self.hp.nside2pixarea(self.nside),\n        \"pixel_area_arcmin2\": (\n            self.hp.nside2pixarea(self.nside, degrees=True) * 3600\n        ),\n        \"resolution_arcmin\": self.hp.nside2resol(self.nside, arcmin=True),\n        \"resolution_deg\": self.actual_angular_resolution,\n        \"max_pixel_radius_deg\": np.rad2deg(self.hp.max_pixrad(self.nside)),\n    }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#geodesic-grid","level":3,"title":"Geodesic Grid","text":"<p>Geodesic grid implementation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder","level":2,"title":"<code>GeodesicBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Geodesic grid based on a subdivided icosahedron.</p> <p>The sphere is tessellated into triangular cells by starting with an icosahedron (20 equilateral triangles) and recursively subdividing each triangle into four smaller triangles.  All vertices are projected back onto the unit sphere after each subdivision step, so the final cells are spherical triangles.  The grid has no polar singularity and provides near-uniform cell areas, though strict equal-area is not guaranteed — cell areas vary by a few percent depending on how they inherit the icosahedral symmetry axes.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,   π/2 = horizon)</li> </ul> <p>Cell centres are computed as the 3D Cartesian mean of the three vertices, re-normalised onto the unit sphere.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder--what-angular_resolution-means","level":4,"title":"What <code>angular_resolution</code> means","text":"<p><code>angular_resolution</code> is not used directly as a cell size.  Instead it is used only when <code>subdivision_level</code> is not explicitly supplied, to estimate an appropriate subdivision level.  The heuristic targets an approximate triangle edge length of <code>2 × angular_resolution</code>::</p> <pre><code>target_edge ≈ 2 × angular_resolution   (degrees)\nsubdivision_level = ceil(log₂(63.4 / target_edge))\n</code></pre> <p>The number 63.4° is the edge length of a regular icosahedron inscribed in a unit sphere.  Each subdivision halves the edge length, so the actual edge length at level n is approximately::</p> <pre><code>edge ≈ 63.4° / 2ⁿ   (degrees)\n</code></pre> <p>The total number of triangles on the full sphere is <code>20 × 4ⁿ</code>. Roughly half fall in the northern hemisphere (exact count depends on the hemisphere boundary).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li>Icosahedron – 12 vertices placed at the intersections of three    mutually perpendicular golden-ratio rectangles, normalised to the    unit sphere.  20 triangular faces connect them.</li> <li>Subdivision – each triangle is split into 4 by inserting edge    midpoints.  Each midpoint is projected onto the unit sphere    (re-normalised) before the next subdivision.  This is repeated    <code>subdivision_level</code> times.</li> <li>Hemisphere filter – faces are kept if any of their three    vertices satisfies <code>theta ≤ π/2 − cutoff_theta</code>.  Consequently,    boundary triangles that straddle the horizon are included and    extend slightly below it.</li> <li>Phi wrapping – for triangles that straddle the 0/2π azimuthal    boundary, vertex phis below π are shifted by +2π before computing    bounding-box limits, then wrapped back.</li> </ol>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Approximate angular resolution in degrees.  Used only to derive     <code>subdivision_level</code> when that parameter is not given explicitly. cutoff_theta : float     Elevation mask angle in degrees.  Triangles are excluded only if     all their vertices are below this elevation. subdivision_level : int or None     Number of icosahedral subdivisions.  If <code>None</code>, estimated from     <code>angular_resolution</code>.  Typical range 0–5. phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder--notes","level":4,"title":"Notes","text":"<p>The <code>theta_lims</code>, <code>phi_lims</code>, and <code>cell_ids</code> fields of the returned <code>GridData</code> are synthetic evenly-spaced arrays kept only for interface compatibility with ring-based grids.  They do not describe the actual triangular cell layout.  Use the <code>geodesic_vertices</code> column and the <code>vertices</code> array in <code>GridData.vertices</code> for the true geometry.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/geodesic_grid.py</code> <pre><code>class GeodesicBuilder(BaseGridBuilder):\n    \"\"\"Geodesic grid based on a subdivided icosahedron.\n\n    The sphere is tessellated into triangular cells by starting with an\n    icosahedron (20 equilateral triangles) and recursively subdividing each\n    triangle into four smaller triangles.  All vertices are projected back\n    onto the unit sphere after each subdivision step, so the final cells are\n    *spherical* triangles.  The grid has no polar singularity and provides\n    near-uniform cell areas, though strict equal-area is *not* guaranteed —\n    cell areas vary by a few percent depending on how they inherit the\n    icosahedral symmetry axes.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,\n      π/2 = horizon)\n\n    Cell centres are computed as the 3D Cartesian mean of the three vertices,\n    re-normalised onto the unit sphere.\n\n    What ``angular_resolution`` means\n    ----------------------------------\n    ``angular_resolution`` is **not** used directly as a cell size.  Instead it\n    is used only when ``subdivision_level`` is *not* explicitly supplied, to\n    *estimate* an appropriate subdivision level.  The heuristic targets an\n    approximate triangle edge length of ``2 × angular_resolution``::\n\n        target_edge ≈ 2 × angular_resolution   (degrees)\n        subdivision_level = ceil(log₂(63.4 / target_edge))\n\n    The number 63.4° is the edge length of a regular icosahedron inscribed in\n    a unit sphere.  Each subdivision halves the edge length, so the actual\n    edge length at level *n* is approximately::\n\n        edge ≈ 63.4° / 2ⁿ   (degrees)\n\n    The total number of triangles on the **full sphere** is ``20 × 4ⁿ``.\n    Roughly half fall in the northern hemisphere (exact count depends on\n    the hemisphere boundary).\n\n    Mathematical construction\n    -------------------------\n    1. **Icosahedron** – 12 vertices placed at the intersections of three\n       mutually perpendicular golden-ratio rectangles, normalised to the\n       unit sphere.  20 triangular faces connect them.\n    2. **Subdivision** – each triangle is split into 4 by inserting edge\n       midpoints.  Each midpoint is projected onto the unit sphere\n       (re-normalised) before the next subdivision.  This is repeated\n       ``subdivision_level`` times.\n    3. **Hemisphere filter** – faces are kept if *any* of their three\n       vertices satisfies ``theta ≤ π/2 − cutoff_theta``.  Consequently,\n       boundary triangles that straddle the horizon *are* included and\n       extend slightly below it.\n    4. **Phi wrapping** – for triangles that straddle the 0/2π azimuthal\n       boundary, vertex phis below π are shifted by +2π before computing\n       bounding-box limits, then wrapped back.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Approximate angular resolution in degrees.  Used only to derive\n        ``subdivision_level`` when that parameter is not given explicitly.\n    cutoff_theta : float\n        Elevation mask angle in degrees.  Triangles are excluded only if\n        *all* their vertices are below this elevation.\n    subdivision_level : int or None\n        Number of icosahedral subdivisions.  If ``None``, estimated from\n        ``angular_resolution``.  Typical range 0–5.\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    Notes\n    -----\n    The ``theta_lims``, ``phi_lims``, and ``cell_ids`` fields of the returned\n    ``GridData`` are *synthetic* evenly-spaced arrays kept only for interface\n    compatibility with ring-based grids.  They do **not** describe the actual\n    triangular cell layout.  Use the ``geodesic_vertices`` column and the\n    ``vertices`` array in ``GridData.vertices`` for the true geometry.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        angular_resolution: float = 2,\n        cutoff_theta: float = 0,\n        subdivision_level: int | None = None,\n        phi_rotation: float = 0,\n    ) -&gt; None:\n        \"\"\"Initialize the geodesic grid builder.\n\n        Parameters\n        ----------\n        angular_resolution : float, default 2\n            Angular resolution in degrees.\n        cutoff_theta : float, default 0\n            Maximum polar angle cutoff in degrees.\n        subdivision_level : int | None, optional\n            Subdivision level override.\n        phi_rotation : float, default 0\n            Rotation angle in degrees.\n\n        \"\"\"\n        super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n        self._triangles: np.ndarray | None = None\n\n        if subdivision_level is None:\n            target_edge_deg = angular_resolution * 2\n            self.subdivision_level = max(\n                0,\n                int(np.ceil(np.log2(63.4 / target_edge_deg))),\n            )\n        else:\n            self.subdivision_level = subdivision_level\n\n        self._logger.info(\n            f\"Geodesic: subdivision_level={self.subdivision_level}, \"\n            f\"~{20 * 4**self.subdivision_level} triangles\"\n        )\n\n    def get_triangles(self) -&gt; np.ndarray | None:\n        \"\"\"Return triangle vertex coordinates for visualization.\n\n        Returns\n        -------\n        triangles : np.ndarray or None\n            Array of shape ``(n_faces, 3, 3)`` where ``triangles[i]`` contains\n            the three 3D unit-sphere vertices of triangle *i*.  ``None`` if\n            the grid has not been built yet.\n\n        \"\"\"\n        return self._triangles\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"geodesic\"``\n\n        \"\"\"\n        return GridType.GEODESIC.value\n\n    def _extract_triangle_vertices(\n        self, vertices: np.ndarray, faces: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Extract 3D vertex coordinates for each face.\n\n        Parameters\n        ----------\n        vertices : np.ndarray\n            All sphere vertices, shape ``(n_vertices, 3)``.\n        faces : np.ndarray\n            Face index array, shape ``(n_faces, 3)``.\n\n        Returns\n        -------\n        triangles : np.ndarray\n            Shape ``(n_faces, 3, 3)`` – three 3D vertices per face.\n\n        \"\"\"\n        # Vectorized: use NumPy advanced indexing instead of loop\n        return vertices[faces]\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[\n        pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray], dict[str, Any]\n    ]:\n        \"\"\"Build geodesic grid from subdivided icosahedron.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per triangular cell.  Columns include phi, theta (centre),\n            bounding-box limits, ``geodesic_vertices`` (3 vertex indices into\n            the ``vertices`` array), and ``geodesic_subdivision``.\n        theta_lims : np.ndarray\n            Synthetic evenly-spaced theta limits (interface compatibility only).\n        phi_lims : list[np.ndarray]\n            Synthetic evenly-spaced phi limits (interface compatibility only).\n        cell_ids : list[np.ndarray]\n            Single-element list containing all cell ids.\n        extra_kwargs : dict\n            Contains ``vertices`` (shape ``(n_vertices, 3)``),\n            ``vertex_phi``, and ``vertex_theta`` arrays for the full\n            subdivided icosahedron.\n\n        \"\"\"\n        vertices, faces = self._create_icosahedron()\n\n        # Subdivide\n        for _ in range(self.subdivision_level):\n            vertices, faces = self._subdivide_mesh(vertices, faces)\n\n        # Project to unit sphere\n        vertices = vertices / np.linalg.norm(vertices, axis=1, keepdims=True)\n\n        # Convert to spherical\n        x, y, z = vertices[:, 0], vertices[:, 1], vertices[:, 2]\n        theta = np.arccos(np.clip(z, -1, 1))\n        phi = np.arctan2(y, x)\n        phi = np.mod(phi, 2 * np.pi)\n\n        # Filter to northern hemisphere\n        hemisphere_mask = theta &lt;= (np.pi / 2 - self.cutoff_theta_rad)\n\n        # Filter faces\n        valid_faces = []\n        for face in faces:\n            if any(hemisphere_mask[v] for v in face):\n                valid_faces.append(face)\n\n        if len(valid_faces) == 0:\n            raise ValueError(\"No valid faces in hemisphere\")\n\n        valid_faces = np.array(valid_faces)\n\n        # Create cells\n        cells = []\n        for face in valid_faces:\n            v_indices = face\n            face_phi = phi[v_indices]\n            face_theta = theta[v_indices]\n\n            # Handle phi wrapping for triangles crossing 0/2π boundary\n            phi_range = np.ptp(face_phi)\n            if phi_range &gt; np.pi:\n                # Triangle crosses the wraparound - unwrap relative to median\n                ref_phi = np.median(face_phi)\n                face_phi_unwrapped = face_phi.copy()\n                # Unwrap angles that are &gt; π away from reference\n                mask_low = (ref_phi - face_phi_unwrapped) &gt; np.pi\n                mask_high = (face_phi_unwrapped - ref_phi) &gt; np.pi\n                face_phi_unwrapped[mask_low] += 2 * np.pi\n                face_phi_unwrapped[mask_high] -= 2 * np.pi\n                phi_min = float(np.min(face_phi_unwrapped) % (2 * np.pi))\n                phi_max = float(np.max(face_phi_unwrapped) % (2 * np.pi))\n            else:\n                phi_min = float(np.min(face_phi))\n                phi_max = float(np.max(face_phi))\n\n            # Cell center - 3D Cartesian mean\n            face_vertices_3d = vertices[v_indices]\n            center_3d = np.mean(face_vertices_3d, axis=0)\n            center_3d = center_3d / np.linalg.norm(center_3d)\n\n            center_theta = np.arccos(np.clip(center_3d[2], -1, 1))\n            center_phi = np.arctan2(center_3d[1], center_3d[0])\n            center_phi = np.mod(center_phi, 2 * np.pi)\n\n            # Cell bounds (theta from vertices, phi already computed above)\n            theta_min = float(np.min(face_theta))\n            theta_max = float(np.max(face_theta))\n\n            cells.append(\n                {\n                    \"phi\": center_phi,\n                    \"theta\": center_theta,\n                    \"phi_min\": phi_min,\n                    \"phi_max\": phi_max,\n                    \"theta_min\": theta_min,\n                    \"theta_max\": theta_max,\n                    \"geodesic_vertices\": v_indices.tolist(),\n                    \"geodesic_subdivision\": self.subdivision_level,\n                }\n            )\n\n        grid = pl.DataFrame(cells).with_columns(\n            pl.int_range(0, pl.len()).alias(\"cell_id\")\n        )\n\n        extra_kwargs: dict[str, Any] = {\n            \"vertices\": vertices,\n            \"vertex_phi\": phi,\n            \"vertex_theta\": theta,\n        }\n\n        theta_lims = np.linspace(0, np.pi / 2, 10)\n        phi_lims = [np.linspace(0, 2 * np.pi, 20) for _ in range(len(theta_lims))]\n        cell_ids_list = [np.arange(grid.height)]\n\n        self._triangles = self._extract_triangle_vertices(vertices, faces)\n\n        return grid, theta_lims, phi_lims, cell_ids_list, extra_kwargs\n\n    def _create_icosahedron(self) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Create a unit-sphere icosahedron.\n\n        Returns\n        -------\n        vertices : np.ndarray\n            Shape ``(12, 3)`` – vertices on the unit sphere.\n        faces : np.ndarray\n            Shape ``(20, 3)`` – integer vertex indices per triangular face.\n\n        \"\"\"\n        phi_golden = (1 + np.sqrt(5)) / 2\n\n        vertices = np.array(\n            [\n                [-1, phi_golden, 0],\n                [1, phi_golden, 0],\n                [-1, -phi_golden, 0],\n                [1, -phi_golden, 0],\n                [0, -1, phi_golden],\n                [0, 1, phi_golden],\n                [0, -1, -phi_golden],\n                [0, 1, -phi_golden],\n                [phi_golden, 0, -1],\n                [phi_golden, 0, 1],\n                [-phi_golden, 0, -1],\n                [-phi_golden, 0, 1],\n            ],\n            dtype=np.float64,\n        )\n\n        vertices = vertices / np.linalg.norm(vertices, axis=1, keepdims=True)\n\n        faces = np.array(\n            [\n                [0, 11, 5],\n                [0, 5, 1],\n                [0, 1, 7],\n                [0, 7, 10],\n                [0, 10, 11],\n                [1, 5, 9],\n                [5, 11, 4],\n                [11, 10, 2],\n                [10, 7, 6],\n                [7, 1, 8],\n                [3, 9, 4],\n                [3, 4, 2],\n                [3, 2, 6],\n                [3, 6, 8],\n                [3, 8, 9],\n                [4, 9, 5],\n                [2, 4, 11],\n                [6, 2, 10],\n                [8, 6, 7],\n                [9, 8, 1],\n            ],\n            dtype=np.int64,\n        )\n\n        return vertices, faces\n\n    def _subdivide_mesh(\n        self, vertices: np.ndarray, faces: np.ndarray\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Subdivide each triangle into 4 smaller triangles.\n\n        Each edge midpoint is computed, normalised onto the unit sphere, and\n        cached so that shared edges produce only one new vertex.\n\n        Parameters\n        ----------\n        vertices : np.ndarray\n            Current vertex array, shape ``(n_vertices, 3)``.\n        faces : np.ndarray\n            Current face array, shape ``(n_faces, 3)``.\n\n        Returns\n        -------\n        new_vertices : np.ndarray\n            Expanded vertex array, shape ``(n_vertices + n_new_midpoints, 3)``.\n        new_faces : np.ndarray\n            New face array, shape ``(4 × n_faces, 3)``.\n\n        \"\"\"\n        new_faces = []\n        edge_midpoints: dict[tuple[int, int], int] = {}\n\n        def get_midpoint(v1: int, v2: int) -&gt; int:\n            \"\"\"Return midpoint vertex index for an edge.\n\n            Parameters\n            ----------\n            v1 : int\n                First vertex index.\n            v2 : int\n                Second vertex index.\n\n            Returns\n            -------\n            int\n                Index of the midpoint vertex.\n\n            \"\"\"\n            edge = tuple(sorted([v1, v2]))\n            if edge not in edge_midpoints:\n                edge_midpoints[edge] = len(vertices) + len(edge_midpoints)\n            return edge_midpoints[edge]\n\n        for face in faces:\n            v0, v1, v2 = face\n\n            m01 = get_midpoint(v0, v1)\n            m12 = get_midpoint(v1, v2)\n            m20 = get_midpoint(v2, v0)\n\n            new_faces.extend(\n                [\n                    [v0, m01, m20],\n                    [v1, m12, m01],\n                    [v2, m20, m12],\n                    [m01, m12, m20],\n                ]\n            )\n\n        n_original = len(vertices)\n        n_new = len(edge_midpoints)\n        final_vertices = np.zeros((n_original + n_new, 3))\n        final_vertices[:n_original] = vertices\n\n        for edge, idx in edge_midpoints.items():\n            v1, v2 = edge\n            midpoint = (vertices[v1] + vertices[v2]) / 2\n            midpoint = midpoint / np.linalg.norm(midpoint)\n            final_vertices[idx] = midpoint\n\n        return final_vertices, np.array(new_faces)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder.__init__","level":3,"title":"<code>__init__(angular_resolution=2, cutoff_theta=0, subdivision_level=None, phi_rotation=0)</code>","text":"<p>Initialize the geodesic grid builder.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder.__init__--parameters","level":5,"title":"Parameters","text":"<p>angular_resolution : float, default 2     Angular resolution in degrees. cutoff_theta : float, default 0     Maximum polar angle cutoff in degrees. subdivision_level : int | None, optional     Subdivision level override. phi_rotation : float, default 0     Rotation angle in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/geodesic_grid.py</code> <pre><code>def __init__(\n    self,\n    angular_resolution: float = 2,\n    cutoff_theta: float = 0,\n    subdivision_level: int | None = None,\n    phi_rotation: float = 0,\n) -&gt; None:\n    \"\"\"Initialize the geodesic grid builder.\n\n    Parameters\n    ----------\n    angular_resolution : float, default 2\n        Angular resolution in degrees.\n    cutoff_theta : float, default 0\n        Maximum polar angle cutoff in degrees.\n    subdivision_level : int | None, optional\n        Subdivision level override.\n    phi_rotation : float, default 0\n        Rotation angle in degrees.\n\n    \"\"\"\n    super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n    self._triangles: np.ndarray | None = None\n\n    if subdivision_level is None:\n        target_edge_deg = angular_resolution * 2\n        self.subdivision_level = max(\n            0,\n            int(np.ceil(np.log2(63.4 / target_edge_deg))),\n        )\n    else:\n        self.subdivision_level = subdivision_level\n\n    self._logger.info(\n        f\"Geodesic: subdivision_level={self.subdivision_level}, \"\n        f\"~{20 * 4**self.subdivision_level} triangles\"\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder.get_triangles","level":3,"title":"<code>get_triangles()</code>","text":"<p>Return triangle vertex coordinates for visualization.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder.get_triangles--returns","level":5,"title":"Returns","text":"<p>triangles : np.ndarray or None     Array of shape <code>(n_faces, 3, 3)</code> where <code>triangles[i]</code> contains     the three 3D unit-sphere vertices of triangle i.  <code>None</code> if     the grid has not been built yet.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/geodesic_grid.py</code> <pre><code>def get_triangles(self) -&gt; np.ndarray | None:\n    \"\"\"Return triangle vertex coordinates for visualization.\n\n    Returns\n    -------\n    triangles : np.ndarray or None\n        Array of shape ``(n_faces, 3, 3)`` where ``triangles[i]`` contains\n        the three 3D unit-sphere vertices of triangle *i*.  ``None`` if\n        the grid has not been built yet.\n\n    \"\"\"\n    return self._triangles\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.geodesic_grid.GeodesicBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"geodesic\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/geodesic_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"geodesic\"``\n\n    \"\"\"\n    return GridType.GEODESIC.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#fibonacci-grid","level":3,"title":"Fibonacci Grid","text":"<p>Fibonacci sphere grid implementation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.fibonacci_grid.FibonacciBuilder","level":2,"title":"<code>FibonacciBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Fibonacci sphere grid with spherical Voronoi tessellation.</p> <p>Points are distributed on the sphere using the Fibonacci lattice (golden-spiral method), which provides one of the most uniform point distributions achievable on a sphere without iterative optimisation.  Each point then becomes the centre of a spherical Voronoi cell — the region of the sphere closer to that point than to any other.  The resulting tessellation has no polar singularities and near-uniform cell areas.</p> <p>The tessellation is computed by <code>scipy.spatial.SphericalVoronoi</code>. Because Voronoi cells have curvilinear boundaries, the <code>phi_min/max</code> and <code>theta_min/max</code> columns in the grid are axis-aligned bounding boxes, not the true cell boundaries.  They are unreliable for spatial queries — use the <code>voronoi_region</code> column (vertex indices into the <code>SphericalVoronoi.vertices</code> array) for exact geometry.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.fibonacci_grid.FibonacciBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,   π/2 = horizon)</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.fibonacci_grid.FibonacciBuilder--what-n_points-resolution-means","level":4,"title":"What <code>n_points</code> (resolution) means","text":"<p>Resolution is controlled by <code>n_points</code>, the number of Voronoi cells in the hemisphere.  When <code>n_points</code> is not supplied it is estimated from <code>angular_resolution</code> via::</p> <pre><code>cell_area  ≈ angular_resolution²   (radians²)\nn_points   = max(10, round(2π / cell_area))\n</code></pre> <p>The approximate cell \"diameter\" (assuming a circular cell of equal area) is::</p> <pre><code>d ≈ 2 √(2π / n_points)   (radians)\n  ≈ 2 × angular_resolution\n</code></pre> <p><code>angular_resolution</code> therefore has no direct geometric meaning for this grid type — it is only a convenience for the <code>n_points</code> estimator.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.fibonacci_grid.FibonacciBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li> <p>Full-sphere Fibonacci lattice – <code>2 × n_points</code> points are    generated on the unit sphere.  Point i has::</p> <p>θᵢ = arccos(1 − 2(i + 0.5) / N)    φᵢ = 2π (i + 0.5) / φ_golden   (mod 2π)</p> </li> </ol> <p>where <code>N = 2 × n_points</code> and <code>φ_golden = (1+√5)/2</code>.  The    <code>+0.5</code> offset avoids placing points exactly at the poles. 2. Hemisphere filter – points with <code>θ &gt; π/2 − cutoff_theta</code>    are discarded. 3. Spherical Voronoi tessellation –    <code>scipy.spatial.SphericalVoronoi</code> computes the Voronoi diagram    on the unit sphere.  Regions are sorted so that vertices appear    in counter-clockwise order around each cell. 4. Bounding boxes – axis-aligned bounding boxes in (phi, theta)    are computed from the Voronoi vertex coordinates.  These are    approximations only (see caveat above).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.fibonacci_grid.FibonacciBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Approximate angular resolution in degrees.  Used only to estimate     <code>n_points</code> when that parameter is not given explicitly. cutoff_theta : float     Elevation mask angle in degrees.  Points below this elevation are     excluded before tessellation. n_points : int or None     Target number of Voronoi cells in the hemisphere.  If <code>None</code>,     estimated from <code>angular_resolution</code>. phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.fibonacci_grid.FibonacciBuilder--raises","level":4,"title":"Raises","text":"<p>ImportError     If <code>scipy</code> is not installed. ValueError     If fewer than 4 points survive the hemisphere filter.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.fibonacci_grid.FibonacciBuilder--notes","level":4,"title":"Notes","text":"<p>The <code>theta_lims</code>, <code>phi_lims</code>, and <code>cell_ids</code> fields of the returned <code>GridData</code> are synthetic evenly-spaced arrays kept only for interface compatibility with ring-based grids.  They do not describe the actual Voronoi cell layout.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/fibonacci_grid.py</code> <pre><code>class FibonacciBuilder(BaseGridBuilder):\n    \"\"\"Fibonacci sphere grid with spherical Voronoi tessellation.\n\n    Points are distributed on the sphere using the *Fibonacci lattice*\n    (golden-spiral method), which provides one of the most uniform\n    point distributions achievable on a sphere without iterative\n    optimisation.  Each point then becomes the centre of a *spherical\n    Voronoi cell* — the region of the sphere closer to that point than\n    to any other.  The resulting tessellation has no polar singularities\n    and near-uniform cell areas.\n\n    The tessellation is computed by ``scipy.spatial.SphericalVoronoi``.\n    Because Voronoi cells have curvilinear boundaries, the ``phi_min/max``\n    and ``theta_min/max`` columns in the grid are axis-aligned *bounding\n    boxes*, **not** the true cell boundaries.  They are unreliable for\n    spatial queries — use the ``voronoi_region`` column (vertex indices\n    into the ``SphericalVoronoi.vertices`` array) for exact geometry.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,\n      π/2 = horizon)\n\n    What ``n_points`` (resolution) means\n    -------------------------------------\n    Resolution is controlled by ``n_points``, the number of Voronoi cells\n    in the hemisphere.  When ``n_points`` is not supplied it is estimated\n    from ``angular_resolution`` via::\n\n        cell_area  ≈ angular_resolution²   (radians²)\n        n_points   = max(10, round(2π / cell_area))\n\n    The approximate cell \"diameter\" (assuming a circular cell of equal area)\n    is::\n\n        d ≈ 2 √(2π / n_points)   (radians)\n          ≈ 2 × angular_resolution\n\n    ``angular_resolution`` therefore has **no direct geometric meaning** for\n    this grid type — it is only a convenience for the ``n_points`` estimator.\n\n    Mathematical construction\n    -------------------------\n    1. **Full-sphere Fibonacci lattice** – ``2 × n_points`` points are\n       generated on the unit sphere.  Point *i* has::\n\n           θᵢ = arccos(1 − 2(i + 0.5) / N)\n           φᵢ = 2π (i + 0.5) / φ_golden   (mod 2π)\n\n       where ``N = 2 × n_points`` and ``φ_golden = (1+√5)/2``.  The\n       ``+0.5`` offset avoids placing points exactly at the poles.\n    2. **Hemisphere filter** – points with ``θ &gt; π/2 − cutoff_theta``\n       are discarded.\n    3. **Spherical Voronoi tessellation** –\n       ``scipy.spatial.SphericalVoronoi`` computes the Voronoi diagram\n       on the unit sphere.  Regions are sorted so that vertices appear\n       in counter-clockwise order around each cell.\n    4. **Bounding boxes** – axis-aligned bounding boxes in (phi, theta)\n       are computed from the Voronoi vertex coordinates.  These are\n       approximations only (see caveat above).\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Approximate angular resolution in degrees.  Used only to estimate\n        ``n_points`` when that parameter is not given explicitly.\n    cutoff_theta : float\n        Elevation mask angle in degrees.  Points below this elevation are\n        excluded before tessellation.\n    n_points : int or None\n        Target number of Voronoi cells in the hemisphere.  If ``None``,\n        estimated from ``angular_resolution``.\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    Raises\n    ------\n    ImportError\n        If ``scipy`` is not installed.\n    ValueError\n        If fewer than 4 points survive the hemisphere filter.\n\n    Notes\n    -----\n    The ``theta_lims``, ``phi_lims``, and ``cell_ids`` fields of the returned\n    ``GridData`` are *synthetic* evenly-spaced arrays kept only for interface\n    compatibility with ring-based grids.  They do **not** describe the actual\n    Voronoi cell layout.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        angular_resolution: float = 2,\n        cutoff_theta: float = 0,\n        n_points: int | None = None,\n        phi_rotation: float = 0,\n    ) -&gt; None:\n        \"\"\"Initialize the Fibonacci grid builder.\n\n        Parameters\n        ----------\n        angular_resolution : float, default 2\n            Angular resolution in degrees.\n        cutoff_theta : float, default 0\n            Maximum polar angle cutoff in degrees.\n        n_points : int | None, optional\n            Number of points to generate.\n        phi_rotation : float, default 0\n            Rotation angle in degrees.\n\n        \"\"\"\n        super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n        if n_points is None:\n            cell_area = self.angular_resolution_rad**2\n            hemisphere_area = 2 * np.pi\n            self.n_points = max(10, int(hemisphere_area / cell_area))\n        else:\n            self.n_points = n_points\n\n        self._logger.info(f\"Fibonacci: generating {self.n_points} points\")\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"fibonacci\"``\n\n        \"\"\"\n        return GridType.FIBONACCI.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[\n        pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray], dict[str, Any]\n    ]:\n        \"\"\"Build Fibonacci sphere grid with Voronoi tessellation.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per Voronoi cell.  Contains phi, theta (centre),\n            bounding-box limits, ``voronoi_region`` (list of vertex indices\n            into the Voronoi vertex array), and ``n_vertices``.\n        theta_lims : np.ndarray\n            Synthetic evenly-spaced theta limits (interface compatibility only).\n        phi_lims : list[np.ndarray]\n            Synthetic evenly-spaced phi limits (interface compatibility only).\n        cell_ids : list[np.ndarray]\n            Single-element list containing all cell ids.\n        extra_kwargs : dict\n            Contains ``voronoi`` (the ``SphericalVoronoi`` object) and\n            ``points_xyz`` (the hemisphere point cloud, shape\n            ``(n_points, 3)``).\n\n        \"\"\"\n        points_xyz = self._generate_fibonacci_sphere(self.n_points * 2)\n\n        # Convert to spherical\n        x, y, z = points_xyz[:, 0], points_xyz[:, 1], points_xyz[:, 2]\n        theta = np.arccos(np.clip(z, -1, 1))\n        phi = np.arctan2(y, x)\n        phi = np.mod(phi, 2 * np.pi)\n\n        # Filter to northern hemisphere\n        mask = (theta &lt;= (np.pi / 2 - self.cutoff_theta_rad)) &amp; (theta &gt;= 0)\n\n        phi = phi[mask]\n        theta = theta[mask]\n        points_xyz = points_xyz[mask]\n\n        if len(points_xyz) &lt; 4:\n            raise ValueError(\"Not enough points in hemisphere for Voronoi tessellation\")\n\n        # Compute spherical Voronoi tessellation\n        try:\n            from scipy.spatial import SphericalVoronoi\n\n            sv = SphericalVoronoi(points_xyz, radius=1, threshold=1e-10)\n            sv.sort_vertices_of_regions()\n\n        except ImportError:\n            raise ImportError(\n                \"scipy required for Fibonacci grid. Install: pip install scipy\"\n            )\n\n        # Create cells\n        cells = []\n        for point_idx, (p_phi, p_theta) in enumerate(zip(phi, theta)):\n            region_vertices = sv.regions[point_idx]\n\n            if -1 in region_vertices:\n                continue\n\n            region_coords = sv.vertices[region_vertices]\n\n            # Convert region vertices to spherical\n            rv_x, rv_y, rv_z = (\n                region_coords[:, 0],\n                region_coords[:, 1],\n                region_coords[:, 2],\n            )\n            rv_theta = np.arccos(np.clip(rv_z, -1, 1))\n            rv_phi = np.arctan2(rv_y, rv_x)\n            rv_phi = np.mod(rv_phi, 2 * np.pi)\n\n            cells.append(\n                {\n                    \"phi\": p_phi,\n                    \"theta\": p_theta,\n                    \"phi_min\": np.min(rv_phi),\n                    \"phi_max\": np.max(rv_phi),\n                    \"theta_min\": np.min(rv_theta),\n                    \"theta_max\": np.max(rv_theta),\n                    \"voronoi_region\": (\n                        region_vertices\n                        if isinstance(region_vertices, list)\n                        else region_vertices.tolist()\n                    ),\n                    \"n_vertices\": len(region_vertices),\n                }\n            )\n\n        grid = pl.DataFrame(cells).with_columns(\n            pl.int_range(0, pl.len()).alias(\"cell_id\")\n        )\n\n        extra_kwargs: dict[str, Any] = {\n            \"voronoi\": sv,\n            \"points_xyz\": points_xyz,\n        }\n\n        theta_lims = np.linspace(0, np.pi / 2, 10)\n        phi_lims = [np.linspace(0, 2 * np.pi, 20) for _ in range(len(theta_lims))]\n        cell_ids_list = [np.arange(grid.height)]\n\n        return grid, theta_lims, phi_lims, cell_ids_list, extra_kwargs\n\n    def _generate_fibonacci_sphere(self, n: int) -&gt; np.ndarray:\n        \"\"\"Generate points on the unit sphere using the golden-spiral lattice.\n\n        Parameters\n        ----------\n        n : int\n            Total number of points on the full sphere.\n\n        Returns\n        -------\n        points : np.ndarray\n            Shape ``(n, 3)`` – Cartesian (x, y, z) coordinates on the unit\n            sphere.\n\n        \"\"\"\n        golden_ratio = (1 + np.sqrt(5)) / 2\n\n        indices = np.arange(0, n, dtype=np.float64) + 0.5\n\n        # Polar angle\n        theta = np.arccos(1 - 2 * indices / n)\n\n        # Azimuthal angle\n        phi = 2 * np.pi * indices / golden_ratio\n        phi = np.mod(phi, 2 * np.pi)\n\n        # Convert to Cartesian\n        x = np.sin(theta) * np.cos(phi)\n        y = np.sin(theta) * np.sin(phi)\n        z = np.cos(theta)\n\n        return np.column_stack([x, y, z])\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.fibonacci_grid.FibonacciBuilder.__init__","level":3,"title":"<code>__init__(angular_resolution=2, cutoff_theta=0, n_points=None, phi_rotation=0)</code>","text":"<p>Initialize the Fibonacci grid builder.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.fibonacci_grid.FibonacciBuilder.__init__--parameters","level":5,"title":"Parameters","text":"<p>angular_resolution : float, default 2     Angular resolution in degrees. cutoff_theta : float, default 0     Maximum polar angle cutoff in degrees. n_points : int | None, optional     Number of points to generate. phi_rotation : float, default 0     Rotation angle in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/fibonacci_grid.py</code> <pre><code>def __init__(\n    self,\n    angular_resolution: float = 2,\n    cutoff_theta: float = 0,\n    n_points: int | None = None,\n    phi_rotation: float = 0,\n) -&gt; None:\n    \"\"\"Initialize the Fibonacci grid builder.\n\n    Parameters\n    ----------\n    angular_resolution : float, default 2\n        Angular resolution in degrees.\n    cutoff_theta : float, default 0\n        Maximum polar angle cutoff in degrees.\n    n_points : int | None, optional\n        Number of points to generate.\n    phi_rotation : float, default 0\n        Rotation angle in degrees.\n\n    \"\"\"\n    super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n    if n_points is None:\n        cell_area = self.angular_resolution_rad**2\n        hemisphere_area = 2 * np.pi\n        self.n_points = max(10, int(hemisphere_area / cell_area))\n    else:\n        self.n_points = n_points\n\n    self._logger.info(f\"Fibonacci: generating {self.n_points} points\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.fibonacci_grid.FibonacciBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.fibonacci_grid.FibonacciBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"fibonacci\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/fibonacci_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"fibonacci\"``\n\n    \"\"\"\n    return GridType.FIBONACCI.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#htm-grid","level":3,"title":"HTM Grid","text":"<p>HTM (Hierarchical Triangular Mesh) grid implementation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder","level":2,"title":"<code>HTMBuilder</code>","text":"<p>               Bases: <code>BaseGridBuilder</code></p> <p>Hierarchical Triangular Mesh (HTM) grid.</p> <p>HTM divides the sphere into an octahedron (8 triangular faces), then recursively subdivides each face into 4 smaller triangles by inserting edge-midpoint vertices projected onto the unit sphere.  The recursion depth is controlled by <code>htm_level</code>.  This produces a strictly hierarchical triangulation: every triangle at level n is the union of exactly 4 triangles at level n + 1.</p> <p>Cell areas are approximately equal but not strictly so — area uniformity improves with level because the icosahedral edge-length asymmetry averages out over many subdivisions.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder--coordinate-convention-physics-gnss","level":4,"title":"Coordinate convention (physics / GNSS)","text":"<ul> <li>phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)</li> <li>theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,   π/2 = horizon)</li> </ul> <p>Cell centres are the 3D Cartesian mean of the three triangle vertices, re-normalised onto the unit sphere.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder--what-htm_level-resolution-means","level":4,"title":"What <code>htm_level</code> (resolution) means","text":"<p>The resolution is set by <code>htm_level</code>, not by <code>angular_resolution</code>. <code>angular_resolution</code> is used only to estimate an appropriate level when <code>htm_level</code> is not supplied explicitly.  The heuristic is::</p> <pre><code>target_edge ≈ 2 × angular_resolution   (degrees)\nhtm_level   = min(15, ceil(log₂(90 / target_edge)))\n</code></pre> <p>The approximate triangle edge length at level n is::</p> <pre><code>edge ≈ 90° / 2ⁿ\n</code></pre> Level Triangles (full sphere) Approx edge 0 8 90° 1 32 45° 2 128 22.5° 3 512 11.25° 4 2 048 5.6° n 8 × 4ⁿ 90° / 2ⁿ","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder--mathematical-construction","level":4,"title":"Mathematical construction","text":"<ol> <li>Octahedron – 6 vertices at ±x, ±y, ±z on the unit sphere, forming    8 triangular faces (4 northern, 4 southern).</li> <li> <p>Subdivision – for each triangle [v₀, v₁, v₂], three edge    midpoints are computed and projected onto the unit sphere::</p> <p>m₀ = normalise((v₀ + v₁) / 2)    m₁ = normalise((v₁ + v₂) / 2)    m₂ = normalise((v₂ + v₀) / 2)</p> </li> </ol> <p>The four children are [v₀, m₀, m₂], [v₁, m₁, m₀], [v₂, m₂, m₁],    and [m₀, m₁, m₂].  This is repeated <code>htm_level</code> times. 3. Hemisphere filter – a triangle is kept if any of its three    vertices satisfies <code>theta ≤ π/2 − cutoff_theta</code>.  Boundary    triangles that straddle the horizon are therefore included and may    extend slightly below it. 4. Each leaf triangle becomes one cell; its centre, bounding box, and    three vertex coordinates are stored.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder--parameters","level":4,"title":"Parameters","text":"<p>angular_resolution : float     Approximate angular resolution in degrees.  Used only to derive     <code>htm_level</code> when that parameter is not given explicitly. cutoff_theta : float     Elevation mask angle in degrees.  Triangles are excluded only when     all their vertices are below this elevation. htm_level : int or None     HTM subdivision depth.  If <code>None</code>, estimated from     <code>angular_resolution</code>.  Practical range 0–15. phi_rotation : float     Rigid azimuthal rotation applied after construction, in degrees.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder--notes","level":4,"title":"Notes","text":"<p>The <code>theta_lims</code>, <code>phi_lims</code>, and <code>cell_ids</code> fields of the returned <code>GridData</code> are synthetic evenly-spaced arrays kept only for interface compatibility with ring-based grids.  They do not describe the actual triangular cell layout.</p> <p>HTM IDs in this implementation use a decimal-digit scheme (<code>parent_id × 10 + child_index</code>) which diverges from the original SDSS HTM binary-coded ID scheme.  This is adequate for indexing but should not be compared with external HTM catalogues.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder--references","level":4,"title":"References","text":"<p>Kunszt et al. (2001): \"The Hierarchical Triangular Mesh\" https://www.sdss.org/dr12/algorithms/htm/</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/htm_grid.py</code> <pre><code>class HTMBuilder(BaseGridBuilder):\n    \"\"\"Hierarchical Triangular Mesh (HTM) grid.\n\n    HTM divides the sphere into an octahedron (8 triangular faces), then\n    recursively subdivides each face into 4 smaller triangles by inserting\n    edge-midpoint vertices projected onto the unit sphere.  The recursion\n    depth is controlled by ``htm_level``.  This produces a strictly\n    hierarchical triangulation: every triangle at level *n* is the union of\n    exactly 4 triangles at level *n* + 1.\n\n    Cell areas are *approximately* equal but not strictly so — area\n    uniformity improves with level because the icosahedral edge-length\n    asymmetry averages out over many subdivisions.\n\n    Coordinate convention (physics / GNSS)\n    ---------------------------------------\n    * phi  ∈ [0, 2π)  – azimuthal angle from North, clockwise (navigation convention)\n    * theta ∈ [0, π/2] – polar angle from zenith (0 = straight up,\n      π/2 = horizon)\n\n    Cell centres are the 3D Cartesian mean of the three triangle vertices,\n    re-normalised onto the unit sphere.\n\n    What ``htm_level`` (resolution) means\n    --------------------------------------\n    The resolution is set by ``htm_level``, **not** by ``angular_resolution``.\n    ``angular_resolution`` is used only to *estimate* an appropriate level\n    when ``htm_level`` is not supplied explicitly.  The heuristic is::\n\n        target_edge ≈ 2 × angular_resolution   (degrees)\n        htm_level   = min(15, ceil(log₂(90 / target_edge)))\n\n    The approximate triangle edge length at level *n* is::\n\n        edge ≈ 90° / 2ⁿ\n\n    | Level | Triangles (full sphere) | Approx edge |\n    |-------|-------------------------|-------------|\n    | 0     | 8                       | 90°         |\n    | 1     | 32                      | 45°         |\n    | 2     | 128                     | 22.5°       |\n    | 3     | 512                     | 11.25°      |\n    | 4     | 2 048                   | 5.6°        |\n    | n     | 8 × 4ⁿ                  | 90° / 2ⁿ   |\n\n    Mathematical construction\n    -------------------------\n    1. **Octahedron** – 6 vertices at ±x, ±y, ±z on the unit sphere, forming\n       8 triangular faces (4 northern, 4 southern).\n    2. **Subdivision** – for each triangle [v₀, v₁, v₂], three edge\n       midpoints are computed and projected onto the unit sphere::\n\n           m₀ = normalise((v₀ + v₁) / 2)\n           m₁ = normalise((v₁ + v₂) / 2)\n           m₂ = normalise((v₂ + v₀) / 2)\n\n       The four children are [v₀, m₀, m₂], [v₁, m₁, m₀], [v₂, m₂, m₁],\n       and [m₀, m₁, m₂].  This is repeated ``htm_level`` times.\n    3. **Hemisphere filter** – a triangle is kept if *any* of its three\n       vertices satisfies ``theta ≤ π/2 − cutoff_theta``.  Boundary\n       triangles that straddle the horizon are therefore included and may\n       extend slightly below it.\n    4. Each leaf triangle becomes one cell; its centre, bounding box, and\n       three vertex coordinates are stored.\n\n    Parameters\n    ----------\n    angular_resolution : float\n        Approximate angular resolution in degrees.  Used only to derive\n        ``htm_level`` when that parameter is not given explicitly.\n    cutoff_theta : float\n        Elevation mask angle in degrees.  Triangles are excluded only when\n        *all* their vertices are below this elevation.\n    htm_level : int or None\n        HTM subdivision depth.  If ``None``, estimated from\n        ``angular_resolution``.  Practical range 0–15.\n    phi_rotation : float\n        Rigid azimuthal rotation applied after construction, in degrees.\n\n    Notes\n    -----\n    The ``theta_lims``, ``phi_lims``, and ``cell_ids`` fields of the returned\n    ``GridData`` are *synthetic* evenly-spaced arrays kept only for interface\n    compatibility with ring-based grids.  They do **not** describe the actual\n    triangular cell layout.\n\n    HTM IDs in this implementation use a decimal-digit scheme\n    (``parent_id × 10 + child_index``) which diverges from the original\n    SDSS HTM binary-coded ID scheme.  This is adequate for indexing but\n    should not be compared with external HTM catalogues.\n\n    References\n    ----------\n    Kunszt et al. (2001): \"The Hierarchical Triangular Mesh\"\n    https://www.sdss.org/dr12/algorithms/htm/\n\n    \"\"\"\n\n    def __init__(\n        self,\n        angular_resolution: float = 2,\n        cutoff_theta: float = 0,\n        htm_level: int | None = None,\n        phi_rotation: float = 0,\n    ) -&gt; None:\n        \"\"\"Initialize the HTM grid builder.\n\n        Parameters\n        ----------\n        angular_resolution : float, default 2\n            Angular resolution in degrees.\n        cutoff_theta : float, default 0\n            Maximum polar angle cutoff in degrees.\n        htm_level : int | None, optional\n            HTM subdivision level.\n        phi_rotation : float, default 0\n            Rotation angle in degrees.\n\n        \"\"\"\n        super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n        if htm_level is None:\n            target_edge_deg = angular_resolution * 2\n            self.htm_level = max(\n                0,\n                int(np.ceil(np.log2(90 / target_edge_deg))),\n            )\n            self.htm_level = min(self.htm_level, 15)\n        else:\n            self.htm_level = htm_level\n\n        self._logger.info(\n            f\"HTM: level={self.htm_level}, ~{8 * 4**self.htm_level} triangles\"\n        )\n\n    def get_grid_type(self) -&gt; str:\n        \"\"\"Return the grid-type identifier string.\n\n        Returns\n        -------\n        str\n            ``\"htm\"``\n\n        \"\"\"\n        return GridType.HTM.value\n\n    def _build_grid(\n        self,\n    ) -&gt; tuple[pl.DataFrame, np.ndarray, list[np.ndarray], list[np.ndarray]]:\n        \"\"\"Build HTM grid by recursive octahedron subdivision.\n\n        Returns\n        -------\n        grid : pl.DataFrame\n            One row per triangular cell.  Contains phi, theta (centre),\n            bounding-box limits, ``htm_id``, ``htm_level``, and the three\n            vertex coordinate columns ``htm_vertex_0/1/2`` (each a list of\n            3 floats in Cartesian xyz).\n        theta_lims : np.ndarray\n            Synthetic evenly-spaced theta limits (interface compatibility only).\n        phi_lims : list[np.ndarray]\n            Synthetic evenly-spaced phi limits (interface compatibility only).\n        cell_ids : list[np.ndarray]\n            Single-element list containing all cell ids.\n\n        \"\"\"\n        base_vertices = np.array(\n            [\n                [0, 0, 1],  # 0: North pole\n                [1, 0, 0],  # 1: +X\n                [0, 1, 0],  # 2: +Y\n                [-1, 0, 0],  # 3: -X\n                [0, -1, 0],  # 4: -Y\n                [0, 0, -1],  # 5: South pole\n            ],\n            dtype=np.float64,\n        )\n\n        base_faces = [\n            [0, 1, 2],\n            [0, 2, 3],\n            [0, 3, 4],\n            [0, 4, 1],  # Northern\n            [5, 2, 1],\n            [5, 3, 2],\n            [5, 4, 3],\n            [5, 1, 4],  # Southern\n        ]\n\n        all_triangles = []\n        all_htm_ids = []\n\n        for base_idx, base_face in enumerate(base_faces):\n            v0 = base_vertices[base_face[0]]\n            v1 = base_vertices[base_face[1]]\n            v2 = base_vertices[base_face[2]]\n\n            triangles, ids = self._subdivide_htm([v0, v1, v2], base_idx, self.htm_level)\n            all_triangles.extend(triangles)\n            all_htm_ids.extend(ids)\n\n        # Convert to cells\n        cells = []\n        for tri, htm_id in zip(all_triangles, all_htm_ids):\n            v0, v1, v2 = tri\n\n            # Center\n            center = (v0 + v1 + v2) / 3\n            center = center / np.linalg.norm(center)\n\n            theta_center = np.arccos(np.clip(center[2], -1, 1))\n            phi_center = np.arctan2(center[1], center[0])\n            phi_center = np.mod(phi_center, 2 * np.pi)\n\n            # Filter hemisphere\n            vertex_thetas = [np.arccos(np.clip(v[2], -1, 1)) for v in [v0, v1, v2]]\n            if all(t &gt; (np.pi / 2 - self.cutoff_theta_rad) for t in vertex_thetas):\n                continue\n\n            # Vertex coords\n            thetas, phis = [], []\n            for v in [v0, v1, v2]:\n                t = np.arccos(np.clip(v[2], -1, 1))\n                p = np.arctan2(v[1], v[0])\n                p = np.mod(p, 2 * np.pi)\n                thetas.append(t)\n                phis.append(p)\n\n            cells.append(\n                {\n                    \"phi\": phi_center,\n                    \"theta\": theta_center,\n                    \"phi_min\": min(phis),\n                    \"phi_max\": max(phis),\n                    \"theta_min\": min(thetas),\n                    \"theta_max\": max(thetas),\n                    \"htm_id\": htm_id,\n                    \"htm_level\": self.htm_level,\n                    \"htm_vertex_0\": v0.tolist(),\n                    \"htm_vertex_1\": v1.tolist(),\n                    \"htm_vertex_2\": v2.tolist(),\n                }\n            )\n\n        grid = pl.DataFrame(cells).with_columns(\n            pl.int_range(0, pl.len()).alias(\"cell_id\")\n        )\n\n        theta_lims = np.linspace(0, np.pi / 2, 10)\n        phi_lims = [np.linspace(0, 2 * np.pi, 20) for _ in range(len(theta_lims))]\n        cell_ids_list = [grid[\"cell_id\"].to_numpy()]\n\n        return grid, theta_lims, phi_lims, cell_ids_list\n\n    def _subdivide_htm(\n        self,\n        tri: list,\n        htm_id: int,\n        target_level: int,\n        current_level: int = 0,\n    ) -&gt; tuple[list, list]:\n        \"\"\"Recursively subdivide a single triangle.\n\n        Parameters\n        ----------\n        tri : list of np.ndarray\n            Three vertex arrays [v₀, v₁, v₂], each shape ``(3,)``.\n        htm_id : int\n            Current HTM identifier for this triangle.\n        target_level : int\n            Recursion depth to reach.\n        current_level : int\n            Current recursion depth.\n\n        Returns\n        -------\n        triangles : list of list\n            Leaf triangles at ``target_level``.\n        ids : list of int\n            Corresponding HTM identifiers.\n\n        \"\"\"\n        if current_level == target_level:\n            return [tri], [htm_id]\n\n        v0, v1, v2 = tri\n\n        # Midpoints on sphere\n        m0 = (v0 + v1) / 2\n        m0 = m0 / np.linalg.norm(m0)\n        m1 = (v1 + v2) / 2\n        m1 = m1 / np.linalg.norm(m1)\n        m2 = (v2 + v0) / 2\n        m2 = m2 / np.linalg.norm(m2)\n\n        # 4 children\n        children = [[v0, m0, m2], [v1, m1, m0], [v2, m2, m1], [m0, m1, m2]]\n\n        all_tris = []\n        all_ids = []\n\n        for child_idx, child in enumerate(children):\n            child_id = htm_id * 10 + child_idx\n            tris, ids = self._subdivide_htm(\n                child,\n                child_id,\n                target_level,\n                current_level + 1,\n            )\n            all_tris.extend(tris)\n            all_ids.extend(ids)\n\n        return all_tris, all_ids\n\n    def get_htm_info(self) -&gt; dict:\n        \"\"\"Get HTM-specific information.\n\n        Returns\n        -------\n        info : dict\n            Keys: ``htm_level``, ``n_triangles_full_sphere``,\n            ``approx_edge_length_deg``, ``approx_edge_length_arcmin``.\n\n        \"\"\"\n        n_triangles = 8 * 4**self.htm_level\n        approx_edge_deg = 90 / (2**self.htm_level)\n\n        return {\n            \"htm_level\": self.htm_level,\n            \"n_triangles_full_sphere\": n_triangles,\n            \"approx_edge_length_deg\": approx_edge_deg,\n            \"approx_edge_length_arcmin\": approx_edge_deg * 60,\n        }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder.__init__","level":3,"title":"<code>__init__(angular_resolution=2, cutoff_theta=0, htm_level=None, phi_rotation=0)</code>","text":"<p>Initialize the HTM grid builder.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder.__init__--parameters","level":5,"title":"Parameters","text":"<p>angular_resolution : float, default 2     Angular resolution in degrees. cutoff_theta : float, default 0     Maximum polar angle cutoff in degrees. htm_level : int | None, optional     HTM subdivision level. phi_rotation : float, default 0     Rotation angle in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/htm_grid.py</code> <pre><code>def __init__(\n    self,\n    angular_resolution: float = 2,\n    cutoff_theta: float = 0,\n    htm_level: int | None = None,\n    phi_rotation: float = 0,\n) -&gt; None:\n    \"\"\"Initialize the HTM grid builder.\n\n    Parameters\n    ----------\n    angular_resolution : float, default 2\n        Angular resolution in degrees.\n    cutoff_theta : float, default 0\n        Maximum polar angle cutoff in degrees.\n    htm_level : int | None, optional\n        HTM subdivision level.\n    phi_rotation : float, default 0\n        Rotation angle in degrees.\n\n    \"\"\"\n    super().__init__(angular_resolution, cutoff_theta, phi_rotation)\n\n    if htm_level is None:\n        target_edge_deg = angular_resolution * 2\n        self.htm_level = max(\n            0,\n            int(np.ceil(np.log2(90 / target_edge_deg))),\n        )\n        self.htm_level = min(self.htm_level, 15)\n    else:\n        self.htm_level = htm_level\n\n    self._logger.info(\n        f\"HTM: level={self.htm_level}, ~{8 * 4**self.htm_level} triangles\"\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder.get_grid_type","level":3,"title":"<code>get_grid_type()</code>","text":"<p>Return the grid-type identifier string.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder.get_grid_type--returns","level":5,"title":"Returns","text":"<p>str     <code>\"htm\"</code></p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/htm_grid.py</code> <pre><code>def get_grid_type(self) -&gt; str:\n    \"\"\"Return the grid-type identifier string.\n\n    Returns\n    -------\n    str\n        ``\"htm\"``\n\n    \"\"\"\n    return GridType.HTM.value\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder.get_htm_info","level":3,"title":"<code>get_htm_info()</code>","text":"<p>Get HTM-specific information.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.grids_impl.htm_grid.HTMBuilder.get_htm_info--returns","level":5,"title":"Returns","text":"<p>info : dict     Keys: <code>htm_level</code>, <code>n_triangles_full_sphere</code>,     <code>approx_edge_length_deg</code>, <code>approx_edge_length_arcmin</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/grids_impl/htm_grid.py</code> <pre><code>def get_htm_info(self) -&gt; dict:\n    \"\"\"Get HTM-specific information.\n\n    Returns\n    -------\n    info : dict\n        Keys: ``htm_level``, ``n_triangles_full_sphere``,\n        ``approx_edge_length_deg``, ``approx_edge_length_arcmin``.\n\n    \"\"\"\n    n_triangles = 8 * 4**self.htm_level\n    approx_edge_deg = 90 / (2**self.htm_level)\n\n    return {\n        \"htm_level\": self.htm_level,\n        \"n_triangles_full_sphere\": n_triangles,\n        \"approx_edge_length_deg\": approx_edge_deg,\n        \"approx_edge_length_arcmin\": approx_edge_deg * 60,\n    }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#grid-operations","level":2,"title":"Grid Operations","text":"<p>Cell assignment and vertex extraction for hemisphere grids.</p> <p>Functions in this module operate on :class:<code>~canvod.grids.core.GridData</code> instances and VOD xarray Datasets.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations--cell-assignment","level":3,"title":"Cell assignment","text":"<p><code>add_cell_ids_to_vod_fast</code>   – vectorised KDTree lookup (preferred) <code>add_cell_ids_to_vod</code>        – element-wise fallback <code>add_cell_ids_to_ds_fast</code>    – dask-lazy variant for out-of-core data</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations--vertex-grid-conversion","level":3,"title":"Vertex / grid conversion","text":"<p><code>extract_grid_vertices</code>      – flat (x, y, z) arrays for 3-D visualisation <code>grid_to_dataset</code>            – xarray Dataset with vertices and solid angles</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.add_cell_ids_to_vod_fast","level":2,"title":"<code>add_cell_ids_to_vod_fast(vod_ds, grid, grid_name)</code>","text":"<p>Assign grid cells to every observation in a VOD dataset (vectorised).</p> <p>Uses a KDTree built from the grid cell centres for O(n log m) lookup.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.add_cell_ids_to_vod_fast--parameters","level":4,"title":"Parameters","text":"<p>vod_ds : xr.Dataset     VOD dataset with <code>phi(epoch, sid)</code> and <code>theta(epoch, sid)</code>     coordinate variables and a <code>VOD</code> data variable. grid : GridData     Hemisphere grid instance. grid_name : str     Grid identifier used to name the output coordinate     (<code>cell_id_&lt;grid_name&gt;</code>).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.add_cell_ids_to_vod_fast--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     vod_ds with an additional <code>cell_id_&lt;grid_name&gt;(epoch, sid)</code>     variable.  Observations with non-finite φ or θ receive NaN.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/operations.py</code> <pre><code>def add_cell_ids_to_vod_fast(\n    vod_ds: xr.Dataset, grid: GridData, grid_name: str\n) -&gt; xr.Dataset:\n    \"\"\"Assign grid cells to every observation in a VOD dataset (vectorised).\n\n    Uses a KDTree built from the grid cell centres for O(n log m) lookup.\n\n    Parameters\n    ----------\n    vod_ds : xr.Dataset\n        VOD dataset with ``phi(epoch, sid)`` and ``theta(epoch, sid)``\n        coordinate variables and a ``VOD`` data variable.\n    grid : GridData\n        Hemisphere grid instance.\n    grid_name : str\n        Grid identifier used to name the output coordinate\n        (``cell_id_&lt;grid_name&gt;``).\n\n    Returns\n    -------\n    xr.Dataset\n        *vod_ds* with an additional ``cell_id_&lt;grid_name&gt;(epoch, sid)``\n        variable.  Observations with non-finite φ or θ receive NaN.\n\n    \"\"\"\n    start_time = time.time()\n    print(f\"\\nAssigning cells for '{grid_name}'...\")\n\n    log.info(\n        \"cell_assignment_started\",\n        grid_name=grid_name,\n        grid_cells=len(grid.grid),\n        observations=vod_ds[\"VOD\"].size,\n        method=\"kdtree_fast\",\n    )\n\n    tree = _build_kdtree(grid)\n    cell_id_col = grid.grid[\"cell_id\"].to_numpy()\n\n    phi = vod_ds[\"phi\"].values.ravel()\n    theta = vod_ds[\"theta\"].values.ravel()\n\n    valid = np.isfinite(phi) &amp; np.isfinite(theta)\n\n    cell_ids = np.full(len(phi), np.nan, dtype=np.float64)\n\n    if np.any(valid):\n        cell_ids[valid] = _query_points(tree, cell_id_col, phi[valid], theta[valid])\n\n    cell_ids_2d = cell_ids.reshape(vod_ds[\"VOD\"].shape)\n\n    coord_name = f\"cell_id_{grid_name}\"\n    vod_ds[coord_name] = ((\"epoch\", \"sid\"), cell_ids_2d)\n\n    n_assigned = np.sum(np.isfinite(cell_ids_2d))\n    n_unique = len(np.unique(cell_ids[np.isfinite(cell_ids)]))\n    duration = time.time() - start_time\n\n    print(f\"  ✓ Assigned: {n_assigned:,} / {cell_ids_2d.size:,} observations\")\n    print(f\"  ✓ Unique cells: {n_unique:,}\")\n\n    log.info(\n        \"cell_assignment_complete\",\n        grid_name=grid_name,\n        duration_seconds=round(duration, 2),\n        observations_assigned=int(n_assigned),\n        observations_total=cell_ids_2d.size,\n        unique_cells=int(n_unique),\n        coverage_percent=round(100 * n_assigned / cell_ids_2d.size, 2),\n    )\n\n    return vod_ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.add_cell_ids_to_vod","level":2,"title":"<code>add_cell_ids_to_vod(vod_ds, grid, grid_name)</code>","text":"<p>Assign grid cells to a VOD dataset (element-wise fallback).</p> <p>Slower than :func:<code>add_cell_ids_to_vod_fast</code>; kept for cases where the full dataset does not fit in memory as numpy arrays.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.add_cell_ids_to_vod--parameters","level":4,"title":"Parameters","text":"<p>vod_ds : xr.Dataset     VOD dataset with <code>phi</code>, <code>theta</code>, and <code>VOD</code> variables. grid : GridData     Hemisphere grid instance. grid_name : str     Grid identifier for the output coordinate name.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.add_cell_ids_to_vod--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     vod_ds with <code>cell_id_&lt;grid_name&gt;(epoch, sid)</code> added.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/operations.py</code> <pre><code>def add_cell_ids_to_vod(\n    vod_ds: xr.Dataset, grid: GridData, grid_name: str\n) -&gt; xr.Dataset:\n    \"\"\"Assign grid cells to a VOD dataset (element-wise fallback).\n\n    Slower than :func:`add_cell_ids_to_vod_fast`; kept for cases where the\n    full dataset does not fit in memory as numpy arrays.\n\n    Parameters\n    ----------\n    vod_ds : xr.Dataset\n        VOD dataset with ``phi``, ``theta``, and ``VOD`` variables.\n    grid : GridData\n        Hemisphere grid instance.\n    grid_name : str\n        Grid identifier for the output coordinate name.\n\n    Returns\n    -------\n    xr.Dataset\n        *vod_ds* with ``cell_id_&lt;grid_name&gt;(epoch, sid)`` added.\n\n    \"\"\"\n    print(f\"\\nAssigning cells for '{grid_name}'...\")\n\n    tree = _build_kdtree(grid)\n    cell_id_col = grid.grid[\"cell_id\"].to_numpy()\n\n    phi_flat = vod_ds[\"phi\"].to_numpy().ravel()\n    theta_flat = vod_ds[\"theta\"].to_numpy().ravel()\n\n    cell_ids_flat = np.full(vod_ds[\"VOD\"].size, np.nan)\n\n    for i in range(len(phi_flat)):\n        if np.isfinite(phi_flat[i]) and np.isfinite(theta_flat[i]):\n            cell_ids_flat[i] = _query_points(\n                tree, cell_id_col, np.array([phi_flat[i]]), np.array([theta_flat[i]])\n            )[0]\n\n    cell_ids_2d = cell_ids_flat.reshape(vod_ds[\"VOD\"].shape)\n\n    coord_name = f\"cell_id_{grid_name}\"\n    vod_ds[coord_name] = ((\"epoch\", \"sid\"), cell_ids_2d)\n\n    n_assigned = np.sum(~np.isnan(cell_ids_2d))\n    print(f\"  ✓ Added coordinate '{coord_name}'\")\n    print(f\"  ✓ Assigned: {n_assigned:,} / {cell_ids_2d.size:,} observations\")\n\n    # Track grid references in dataset attrs\n    if \"grid_references\" not in vod_ds.attrs:\n        vod_ds.attrs[\"grid_references\"] = []\n    vod_ds.attrs[\"grid_references\"].append(f\"grids/{grid_name}\")\n\n    return vod_ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.add_cell_ids_to_ds_fast","level":2,"title":"<code>add_cell_ids_to_ds_fast(ds, grid, grid_name, data_var='VOD')</code>","text":"<p>Assign grid cells lazily via dask (avoids loading full arrays).</p> <p>The output <code>cell_id_&lt;grid_name&gt;</code> variable is a dask array that computes on access or save.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.add_cell_ids_to_ds_fast--parameters","level":4,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with dask-backed <code>phi</code> and <code>theta</code> arrays. grid : GridData     Hemisphere grid instance. grid_name : str     Grid identifier for the output coordinate name. data_var : str     Name of the main data variable (used only for shape reference).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.add_cell_ids_to_ds_fast--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     ds with a lazy <code>cell_id_&lt;grid_name&gt;(epoch, sid)</code> variable.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/operations.py</code> <pre><code>def add_cell_ids_to_ds_fast(\n    ds: xr.Dataset, grid: GridData, grid_name: str, data_var: str = \"VOD\"\n) -&gt; xr.Dataset:\n    \"\"\"Assign grid cells lazily via dask (avoids loading full arrays).\n\n    The output ``cell_id_&lt;grid_name&gt;`` variable is a dask array that\n    computes on access or save.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with dask-backed ``phi`` and ``theta`` arrays.\n    grid : GridData\n        Hemisphere grid instance.\n    grid_name : str\n        Grid identifier for the output coordinate name.\n    data_var : str\n        Name of the main data variable (used only for shape reference).\n\n    Returns\n    -------\n    xr.Dataset\n        *ds* with a lazy ``cell_id_&lt;grid_name&gt;(epoch, sid)`` variable.\n\n    \"\"\"\n    import dask.array as da\n\n    print(f\"\\nAssigning cells for '{grid_name}'...\")\n\n    tree = _build_kdtree(grid)\n    cell_id_col = grid.grid[\"cell_id\"].to_numpy()\n\n    def _assign_chunk(\n        phi_chunk: np.ndarray,\n        theta_chunk: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"Assign cell IDs for a chunk of data.\n\n        Parameters\n        ----------\n        phi_chunk : np.ndarray\n            Chunk of azimuth values.\n        theta_chunk : np.ndarray\n            Chunk of elevation values.\n\n        Returns\n        -------\n        np.ndarray\n            Chunk of cell IDs.\n\n        \"\"\"\n        phi_flat = phi_chunk.ravel()\n        theta_flat = theta_chunk.ravel()\n\n        valid = np.isfinite(phi_flat) &amp; np.isfinite(theta_flat)\n        cell_ids = np.full(len(phi_flat), np.nan, dtype=np.float32)\n\n        if np.any(valid):\n            cell_ids[valid] = _query_points(\n                tree, cell_id_col, phi_flat[valid], theta_flat[valid]\n            )\n\n        return cell_ids.reshape(phi_chunk.shape)\n\n    cell_ids_dask = da.map_blocks(\n        _assign_chunk,\n        ds[\"phi\"].data,\n        ds[\"theta\"].data,\n        dtype=np.float32,\n        drop_axis=[],\n    )\n\n    coord_name = f\"cell_id_{grid_name}\"\n    ds[coord_name] = ((\"epoch\", \"sid\"), cell_ids_dask)\n\n    print(\"  ✓ Cell IDs assigned as lazy dask array\")\n    print(\"  ✓ Will compute on access/save\")\n\n    return ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.extract_grid_vertices","level":2,"title":"<code>extract_grid_vertices(grid)</code>","text":"<p>Extract 3D vertices from hemisphere grid cells.</p> <p>Dispatches to a grid-type–specific extractor.  The returned arrays are flat (not per-cell); use them directly for 3-D scatter plots.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.extract_grid_vertices--parameters","level":4,"title":"Parameters","text":"<p>grid : GridData     Hemisphere grid instance.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.extract_grid_vertices--returns","level":4,"title":"Returns","text":"<p>x_vertices, y_vertices, z_vertices : np.ndarray     Cartesian vertex coordinates on the unit sphere.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/operations.py</code> <pre><code>def extract_grid_vertices(grid: GridData) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Extract 3D vertices from hemisphere grid cells.\n\n    Dispatches to a grid-type–specific extractor.  The returned arrays are\n    flat (not per-cell); use them directly for 3-D scatter plots.\n\n    Parameters\n    ----------\n    grid : GridData\n        Hemisphere grid instance.\n\n    Returns\n    -------\n    x_vertices, y_vertices, z_vertices : np.ndarray\n        Cartesian vertex coordinates on the unit sphere.\n\n    \"\"\"\n    extractors = {\n        \"htm\": _extract_htm_vertices,\n        \"geodesic\": _extract_geodesic_vertices,\n        \"equal_area\": _extract_rectangular_vertices,\n        \"equal_angle\": _extract_rectangular_vertices,\n        \"equirectangular\": _extract_rectangular_vertices,\n        \"healpix\": _extract_healpix_vertices,\n        \"fibonacci\": _extract_fibonacci_vertices,\n    }\n    extractor = extractors.get(grid.grid_type)\n    if extractor is None:\n        raise ValueError(f\"Unknown grid type: {grid.grid_type}\")\n    return extractor(grid)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.grid_to_dataset","level":2,"title":"<code>grid_to_dataset(grid)</code>","text":"<p>Convert a HemiGrid to a unified xarray Dataset with vertices.</p> <p>The returned Dataset carries cell centres, NaN-padded vertex arrays, vertex counts, and solid angles — all indexed by <code>cell_id</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.grid_to_dataset--parameters","level":4,"title":"Parameters","text":"<p>grid : GridData     Hemisphere grid instance.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.grid_to_dataset--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Dataset with dimensions <code>(cell_id, vertex)</code> and variables     <code>cell_phi</code>, <code>cell_theta</code>, <code>vertices_phi</code>, <code>vertices_theta</code>,     <code>n_vertices</code>, <code>solid_angle</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.grid_to_dataset--notes","level":4,"title":"Notes","text":"<p>This function is distinct from :meth:<code>HemiGridStorageAdapter._prepare_vertices_dataframe</code> in <code>canvod-store</code>. That method produces a long-form DataFrame for zarr ragged-array storage; this one produces a rectangular xarray Dataset suitable for analysis and visualisation.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/operations.py</code> <pre><code>def grid_to_dataset(grid: GridData) -&gt; xr.Dataset:\n    \"\"\"Convert a HemiGrid to a unified xarray Dataset with vertices.\n\n    The returned Dataset carries cell centres, NaN-padded vertex arrays,\n    vertex counts, and solid angles — all indexed by ``cell_id``.\n\n    Parameters\n    ----------\n    grid : GridData\n        Hemisphere grid instance.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with dimensions ``(cell_id, vertex)`` and variables\n        ``cell_phi``, ``cell_theta``, ``vertices_phi``, ``vertices_theta``,\n        ``n_vertices``, ``solid_angle``.\n\n    Notes\n    -----\n    This function is distinct from\n    :meth:`HemiGridStorageAdapter._prepare_vertices_dataframe` in\n    ``canvod-store``. That method produces a long-form DataFrame for zarr\n    ragged-array storage; this one produces a rectangular xarray Dataset\n    suitable for analysis and visualisation.\n\n    \"\"\"\n    # Reuse the commented-out logic pattern from gnssvodpy vertices.py:\n    # extract per-cell vertices into (n_cells, max_vertices) arrays.\n    n_cells = grid.ncells\n    grid_type = grid.grid_type\n\n    if grid_type in (\"equal_area\", \"equal_angle\", \"equirectangular\"):\n        max_v = 4\n        vertices_phi = np.full((n_cells, max_v), np.nan)\n        vertices_theta = np.full((n_cells, max_v), np.nan)\n        n_vertices = np.full(n_cells, 4, dtype=np.int32)\n\n        for i, row in enumerate(grid.grid.iter_rows(named=True)):\n            phi_min, phi_max = row[\"phi_min\"], row[\"phi_max\"]\n            theta_min, theta_max = row[\"theta_min\"], row[\"theta_max\"]\n            vertices_phi[i, :] = [phi_min, phi_max, phi_max, phi_min]\n            vertices_theta[i, :] = [theta_min, theta_min, theta_max, theta_max]\n\n    elif grid_type == \"htm\":\n        max_v = 4  # padded to 4 for rectangular layout; only 3 used\n        vertices_phi = np.full((n_cells, max_v), np.nan)\n        vertices_theta = np.full((n_cells, max_v), np.nan)\n        n_vertices = np.full(n_cells, 3, dtype=np.int32)\n\n        for i, row in enumerate(grid.grid.iter_rows(named=True)):\n            for j, col in enumerate([\"htm_vertex_0\", \"htm_vertex_1\", \"htm_vertex_2\"]):\n                v = np.array(row[col], dtype=float)\n                r = np.linalg.norm(v)\n                if r == 0:\n                    continue\n                x, y, z = v / r\n                vertices_theta[i, j] = np.arccos(np.clip(z, -1, 1))\n                vertices_phi[i, j] = np.mod(np.arctan2(y, x), 2 * np.pi)\n\n    elif grid_type == \"geodesic\":\n        max_v = 3\n        vertices_phi = np.full((n_cells, max_v), np.nan)\n        vertices_theta = np.full((n_cells, max_v), np.nan)\n        n_vertices = np.full(n_cells, 3, dtype=np.int32)\n\n        shared = grid.vertices  # shared vertex array from GridData\n        if shared is not None and \"geodesic_vertices\" in grid.grid.columns:\n            shared = np.asarray(shared, dtype=float)\n            for i, row in enumerate(grid.grid.iter_rows(named=True)):\n                indices = row[\"geodesic_vertices\"]\n                for j, v_idx in enumerate(indices):\n                    v = shared[int(v_idx)]\n                    r = np.linalg.norm(v)\n                    if r == 0:\n                        continue\n                    x, y, z = v / r\n                    vertices_theta[i, j] = np.arccos(np.clip(z, -1, 1))\n                    vertices_phi[i, j] = np.mod(np.arctan2(y, x), 2 * np.pi)\n        else:\n            # Fallback: use cell centres as single-point \"vertices\"\n            n_vertices[:] = 1\n            vertices_phi[:, 0] = grid.grid[\"phi\"].to_numpy()\n            vertices_theta[:, 0] = grid.grid[\"theta\"].to_numpy()\n\n    elif grid_type == \"healpix\":\n        max_v = 4\n        vertices_phi = np.full((n_cells, max_v), np.nan)\n        vertices_theta = np.full((n_cells, max_v), np.nan)\n        n_vertices = np.full(n_cells, 4, dtype=np.int32)\n\n        for i, row in enumerate(grid.grid.iter_rows(named=True)):\n            phi_min, phi_max = row[\"phi_min\"], row[\"phi_max\"]\n            theta_min, theta_max = row[\"theta_min\"], row[\"theta_max\"]\n            vertices_phi[i, :] = [phi_min, phi_max, phi_max, phi_min]\n            vertices_theta[i, :] = [theta_min, theta_min, theta_max, theta_max]\n\n    elif grid_type == \"fibonacci\":\n        # Point-based: single vertex per cell (the centre)\n        max_v = 1\n        vertices_phi = grid.grid[\"phi\"].to_numpy().reshape(n_cells, 1)\n        vertices_theta = grid.grid[\"theta\"].to_numpy().reshape(n_cells, 1)\n        n_vertices = np.ones(n_cells, dtype=np.int32)\n\n    else:\n        raise ValueError(f\"Unknown grid type: {grid_type}\")\n\n    # Cell centres\n    cell_phi = grid.grid[\"phi\"].to_numpy()\n    cell_theta = grid.grid[\"theta\"].to_numpy()\n    solid_angles = grid.get_solid_angles()\n\n    ds = xr.Dataset(\n        {\n            \"cell_phi\": ([\"cell_id\"], cell_phi),\n            \"cell_theta\": ([\"cell_id\"], cell_theta),\n            \"vertices_phi\": ([\"cell_id\", \"vertex\"], vertices_phi),\n            \"vertices_theta\": ([\"cell_id\", \"vertex\"], vertices_theta),\n            \"n_vertices\": ([\"cell_id\"], n_vertices),\n            \"solid_angle\": ([\"cell_id\"], solid_angles),\n        },\n        coords={\n            \"cell_id\": np.arange(n_cells),\n            \"vertex\": np.arange(max_v),\n        },\n        attrs={\n            \"grid_type\": grid.grid_type,\n            \"angular_resolution_deg\": float(\n                grid.metadata.get(\"angular_resolution\", 0.0) if grid.metadata else 0.0\n            ),\n            \"angular_resolution_description\": _RESOLUTION_DESCRIPTIONS.get(\n                grid.grid_type, \"Angular resolution in degrees\"\n            ),\n            \"cutoff_theta_deg\": float(\n                grid.metadata.get(\"cutoff_theta\", 0.0) if grid.metadata else 0.0\n            ),\n            \"n_cells\": n_cells,\n        },\n    )\n\n    return ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.store_grid","level":2,"title":"<code>store_grid(grid, store, grid_name)</code>","text":"<p>Store grid in unified xarray format to Icechunk store.</p> <p>Converts the grid to an xarray Dataset with vertex information and writes it to the <code>grids/</code> group in the store.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.store_grid--parameters","level":4,"title":"Parameters","text":"<p>grid : GridData     Hemisphere grid instance to store. store     Icechunk store instance (e.g., MyIcechunkStore). grid_name : str     Grid identifier for storage path (e.g., 'equal_area_4deg').</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.store_grid--returns","level":4,"title":"Returns","text":"<p>str     Snapshot ID from the commit.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.store_grid--examples","level":4,"title":"Examples","text":"<p>from canvod.grids import create_hemigrid, store_grid grid = create_hemigrid(angular_resolution=4, grid_type='equal_area') snapshot_id = store_grid(grid, my_store, 'equal_area_4deg')</p> Source code in <code>packages/canvod-grids/src/canvod/grids/operations.py</code> <pre><code>def store_grid(\n    grid: GridData,\n    store: Any,\n    grid_name: str,\n) -&gt; str:\n    \"\"\"Store grid in unified xarray format to Icechunk store.\n\n    Converts the grid to an xarray Dataset with vertex information\n    and writes it to the ``grids/`` group in the store.\n\n    Parameters\n    ----------\n    grid : GridData\n        Hemisphere grid instance to store.\n    store\n        Icechunk store instance (e.g., MyIcechunkStore).\n    grid_name : str\n        Grid identifier for storage path (e.g., 'equal_area_4deg').\n\n    Returns\n    -------\n    str\n        Snapshot ID from the commit.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import create_hemigrid, store_grid\n    &gt;&gt;&gt; grid = create_hemigrid(angular_resolution=4, grid_type='equal_area')\n    &gt;&gt;&gt; snapshot_id = store_grid(grid, my_store, 'equal_area_4deg')\n\n    \"\"\"\n    print(f\"\\nStoring grid '{grid_name}'...\")\n\n    # Convert to unified xarray format\n    ds_grid = grid_to_dataset(grid)\n\n    # Store in grids/ directory\n    group_path = f\"grids/{grid_name}\"\n\n    with store.writable_session() as session:\n        from icechunk.xarray import to_icechunk\n\n        to_icechunk(ds_grid, session, group=group_path, mode=\"w\")\n        snapshot_id = session.commit(f\"Stored {grid_name} grid structure\")\n\n    print(f\"  ✓ Stored to '{group_path}'\")\n    print(f\"  ✓ Snapshot: {snapshot_id[:8]}...\")\n    print(f\"  ✓ Cells: {grid.ncells}, Type: {grid.grid_type}\")\n\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.store_dataset_with_cell_ids","level":2,"title":"<code>store_dataset_with_cell_ids(ds, store, group_name)</code>","text":"<p>Write a dataset (with cell-ID LUT columns) back to the store.</p> <p>After calling :func:<code>add_cell_ids_to_vod_fast</code> or :func:<code>add_cell_ids_to_ds_fast</code>, the dataset carries <code>cell_id_&lt;grid_name&gt;</code> variables and a <code>grid_references</code> attr. This function persists those additions to the Icechunk store.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.store_dataset_with_cell_ids--parameters","level":4,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with cell-ID variables already added. store     Icechunk store instance (e.g., MyIcechunkStore). group_name : str     Zarr group path, e.g. <code>'reference_01_canopy_01'</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.store_dataset_with_cell_ids--returns","level":4,"title":"Returns","text":"<p>str     Snapshot ID from the commit.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.store_dataset_with_cell_ids--examples","level":4,"title":"Examples","text":"<p>from canvod.grids import ( ...     add_cell_ids_to_ds_fast, store_dataset_with_cell_ids, ... ) ds = add_cell_ids_to_ds_fast(vod_ds, grid, \"equal_area_2deg\") snapshot_id = store_dataset_with_cell_ids(ds, store, \"receiver_01\")</p> Source code in <code>packages/canvod-grids/src/canvod/grids/operations.py</code> <pre><code>def store_dataset_with_cell_ids(\n    ds: xr.Dataset,\n    store: Any,\n    group_name: str,\n) -&gt; str:\n    \"\"\"Write a dataset (with cell-ID LUT columns) back to the store.\n\n    After calling :func:`add_cell_ids_to_vod_fast` or\n    :func:`add_cell_ids_to_ds_fast`, the dataset carries\n    ``cell_id_&lt;grid_name&gt;`` variables and a ``grid_references`` attr.\n    This function persists those additions to the Icechunk store.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with cell-ID variables already added.\n    store\n        Icechunk store instance (e.g., MyIcechunkStore).\n    group_name : str\n        Zarr group path, e.g. ``'reference_01_canopy_01'``.\n\n    Returns\n    -------\n    str\n        Snapshot ID from the commit.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import (\n    ...     add_cell_ids_to_ds_fast, store_dataset_with_cell_ids,\n    ... )\n    &gt;&gt;&gt; ds = add_cell_ids_to_ds_fast(vod_ds, grid, \"equal_area_2deg\")\n    &gt;&gt;&gt; snapshot_id = store_dataset_with_cell_ids(ds, store, \"receiver_01\")\n\n    \"\"\"\n    cell_id_vars = [v for v in ds.data_vars if str(v).startswith(\"cell_id_\")]\n    grid_refs = ds.attrs.get(\"grid_references\", [])\n\n    print(f\"\\nStoring dataset to '{group_name}'...\")\n    print(f\"  Cell-ID variables: {cell_id_vars}\")\n    print(f\"  Grid references: {grid_refs}\")\n\n    with store.writable_session() as session:\n        from icechunk.xarray import to_icechunk\n\n        to_icechunk(ds, session, group=group_name, mode=\"w\")\n        snapshot_id = session.commit(\n            f\"Stored {group_name} with cell-ID mappings ({', '.join(cell_id_vars)})\"\n        )\n\n    print(f\"  ✓ Snapshot: {snapshot_id[:8]}...\")\n\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.load_grid","level":2,"title":"<code>load_grid(store, grid_name)</code>","text":"<p>Load a grid from Icechunk store.</p> <p>Loads the grid structure from <code>grids/{grid_name}</code> and reconstructs a GridData object.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.load_grid--parameters","level":4,"title":"Parameters","text":"<p>store     Icechunk store instance (e.g., MyIcechunkStore). grid_name : str     Grid identifier (e.g., 'equal_area_4deg').</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.load_grid--returns","level":4,"title":"Returns","text":"<p>GridData     Reconstructed grid instance.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.operations.load_grid--examples","level":4,"title":"Examples","text":"<p>from canvod.grids import load_grid grid = load_grid(my_store, 'equal_area_4deg') print(f\"Loaded {grid.ncells} cells\")</p> Source code in <code>packages/canvod-grids/src/canvod/grids/operations.py</code> <pre><code>def load_grid(\n    store: Any,\n    grid_name: str,\n) -&gt; GridData:\n    \"\"\"Load a grid from Icechunk store.\n\n    Loads the grid structure from ``grids/{grid_name}`` and reconstructs\n    a GridData object.\n\n    Parameters\n    ----------\n    store\n        Icechunk store instance (e.g., MyIcechunkStore).\n    grid_name : str\n        Grid identifier (e.g., 'equal_area_4deg').\n\n    Returns\n    -------\n    GridData\n        Reconstructed grid instance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import load_grid\n    &gt;&gt;&gt; grid = load_grid(my_store, 'equal_area_4deg')\n    &gt;&gt;&gt; print(f\"Loaded {grid.ncells} cells\")\n\n    \"\"\"\n    import polars as pl\n\n    print(f\"\\nLoading grid '{grid_name}'...\")\n\n    group_path = f\"grids/{grid_name}\"\n\n    # Load from store\n    with store.readonly_session() as session:\n        ds_grid = xr.open_zarr(session.store, group=group_path, consolidated=False)\n\n    # Extract metadata (support both old and new attr names)\n    grid_type = ds_grid.attrs.get(\"grid_type\")\n    angular_resolution = ds_grid.attrs.get(\n        \"angular_resolution_deg\",\n        ds_grid.attrs.get(\"angular_resolution\", 0.0),\n    )\n    cutoff_theta = ds_grid.attrs.get(\n        \"cutoff_theta_deg\",\n        ds_grid.attrs.get(\"cutoff_theta\", 0.0),\n    )\n\n    if not grid_type:\n        raise ValueError(f\"Grid '{grid_name}' missing grid_type attribute\")\n\n    # Reconstruct grid DataFrame from cell centers\n    cell_phi = ds_grid[\"cell_phi\"].values\n    cell_theta = ds_grid[\"cell_theta\"].values\n    n_cells = len(cell_phi)\n\n    # Build basic grid DataFrame\n    grid_data = {\n        \"cell_id\": np.arange(n_cells),\n        \"phi\": cell_phi,\n        \"theta\": cell_theta,\n    }\n\n    # Add grid-type-specific columns\n    if grid_type in (\"equal_area\", \"equal_angle\", \"equirectangular\"):\n        # Reconstruct boundaries from vertices\n        vertices_phi = ds_grid[\"vertices_phi\"].values\n        vertices_theta = ds_grid[\"vertices_theta\"].values\n\n        phi_min = vertices_phi[:, 0]  # All 4 vertices are corners\n        phi_max = vertices_phi[:, 1]\n        theta_min = vertices_theta[:, 0]\n        theta_max = vertices_theta[:, 2]\n\n        grid_data.update(\n            {\n                \"phi_min\": phi_min,\n                \"phi_max\": phi_max,\n                \"theta_min\": theta_min,\n                \"theta_max\": theta_max,\n            }\n        )\n\n    elif grid_type == \"htm\":\n        # Reconstruct HTM vertices from spherical to Cartesian\n        vertices_phi = ds_grid[\"vertices_phi\"].values\n        vertices_theta = ds_grid[\"vertices_theta\"].values\n\n        htm_v0 = []\n        htm_v1 = []\n        htm_v2 = []\n\n        for i in range(n_cells):\n            vertices = []\n            for j in range(3):\n                phi_v = vertices_phi[i, j]\n                theta_v = vertices_theta[i, j]\n                x = np.sin(theta_v) * np.cos(phi_v)\n                y = np.sin(theta_v) * np.sin(phi_v)\n                z = np.cos(theta_v)\n                vertices.append([x, y, z])\n\n            htm_v0.append(vertices[0])\n            htm_v1.append(vertices[1])\n            htm_v2.append(vertices[2])\n\n        grid_data.update(\n            {\n                \"htm_vertex_0\": htm_v0,\n                \"htm_vertex_1\": htm_v1,\n                \"htm_vertex_2\": htm_v2,\n            }\n        )\n\n    # Create Polars DataFrame\n    df_grid = pl.DataFrame(grid_data)\n\n    # Reconstruct theta_lims, phi_lims, and cell_ids from grid\n    # These are required for GridData but not critical for basic usage\n    unique_theta = sorted(df_grid[\"theta\"].unique().to_list())\n    theta_lims = np.array(unique_theta)\n\n    # Group cells by theta bin\n    phi_lims_list = []\n    cell_ids_list = []\n    for theta_val in unique_theta:\n        theta_cells = df_grid.filter(pl.col(\"theta\") == theta_val)\n        phi_vals = sorted(theta_cells[\"phi\"].to_list())\n        cell_ids = theta_cells[\"cell_id\"].to_list()\n\n        phi_lims_list.append(np.array(phi_vals))\n        cell_ids_list.append(np.array(cell_ids))\n\n    # Create GridData instance\n    from canvod.grids.core import GridData\n\n    grid = GridData(\n        grid=df_grid,\n        theta_lims=theta_lims,\n        phi_lims=phi_lims_list,\n        cell_ids=cell_ids_list,\n        grid_type=grid_type,\n        metadata={\n            \"angular_resolution\": angular_resolution,\n            \"cutoff_theta\": cutoff_theta,\n        },\n    )\n\n    print(f\"  ✓ Loaded from '{group_path}'\")\n    print(f\"  ✓ Cells: {grid.ncells}, Type: {grid.grid_type}\")\n\n    return grid\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#aggregation","level":2,"title":"Aggregation","text":"<p>Per-cell aggregation of VOD observations onto hemisphere grids.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation--top-level-entry-points","level":3,"title":"Top-level entry points","text":"<p><code>aggregate_data_to_grid</code>       – single-statistic spatial aggregation <code>compute_percell_timeseries</code>   – chunked (cell × time) time-series</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation--analysis-helpers","level":3,"title":"Analysis helpers","text":"<p><code>compute_global_average</code>       – observation-count–weighted global mean <code>compute_regional_average</code>     – same, restricted to a cell subset <code>analyze_diurnal_patterns</code>     – hourly groupby <code>analyze_spatial_patterns</code>     – time-averaged spatial field</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation--convenience-wrappers","level":3,"title":"Convenience wrappers","text":"<p><code>compute_hemisphere_percell</code>   – daily, full hemisphere <code>compute_zenith_percell</code>       – daily, θ ≤ 30°</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.CellAggregator","level":2,"title":"<code>CellAggregator</code>","text":"<p>Polars-based per-cell aggregation helpers.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>class CellAggregator:\n    \"\"\"Polars-based per-cell aggregation helpers.\"\"\"\n\n    @staticmethod\n    def aggregate_by_cell(\n        df: pl.DataFrame,\n        value_var: str = \"VOD\",\n        method: str = \"mean\",\n    ) -&gt; pl.DataFrame:\n        \"\"\"Aggregate values by ``cell_id``.\n\n        Parameters\n        ----------\n        df : pl.DataFrame\n            Must contain ``cell_id`` and *value_var* columns.\n        value_var : str\n            Column to aggregate.\n        method : {'mean', 'median', 'std', 'count'}\n            Aggregation method.\n\n        Returns\n        -------\n        pl.DataFrame\n            Two-column DataFrame: ``cell_id``, *value_var*.\n\n        \"\"\"\n        if \"cell_id\" not in df.columns:\n            raise ValueError(\"pl.DataFrame must have 'cell_id' column\")\n        if value_var not in df.columns:\n            raise ValueError(f\"pl.DataFrame must have '{value_var}' column\")\n\n        agg_map = {\n            \"mean\": pl.col(value_var).mean(),\n            \"median\": pl.col(value_var).median(),\n            \"std\": pl.col(value_var).std(),\n            \"count\": pl.col(value_var).count(),\n        }\n        if method not in agg_map:\n            raise ValueError(f\"Unknown method: {method}\")\n\n        return (\n            df.group_by(\"cell_id\").agg(agg_map[method].alias(value_var)).sort(\"cell_id\")\n        )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.CellAggregator.aggregate_by_cell","level":3,"title":"<code>aggregate_by_cell(df, value_var='VOD', method='mean')</code>  <code>staticmethod</code>","text":"<p>Aggregate values by <code>cell_id</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.CellAggregator.aggregate_by_cell--parameters","level":5,"title":"Parameters","text":"<p>df : pl.DataFrame     Must contain <code>cell_id</code> and value_var columns. value_var : str     Column to aggregate. method : {'mean', 'median', 'std', 'count'}     Aggregation method.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.CellAggregator.aggregate_by_cell--returns","level":5,"title":"Returns","text":"<p>pl.DataFrame     Two-column DataFrame: <code>cell_id</code>, value_var.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>@staticmethod\ndef aggregate_by_cell(\n    df: pl.DataFrame,\n    value_var: str = \"VOD\",\n    method: str = \"mean\",\n) -&gt; pl.DataFrame:\n    \"\"\"Aggregate values by ``cell_id``.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Must contain ``cell_id`` and *value_var* columns.\n    value_var : str\n        Column to aggregate.\n    method : {'mean', 'median', 'std', 'count'}\n        Aggregation method.\n\n    Returns\n    -------\n    pl.DataFrame\n        Two-column DataFrame: ``cell_id``, *value_var*.\n\n    \"\"\"\n    if \"cell_id\" not in df.columns:\n        raise ValueError(\"pl.DataFrame must have 'cell_id' column\")\n    if value_var not in df.columns:\n        raise ValueError(f\"pl.DataFrame must have '{value_var}' column\")\n\n    agg_map = {\n        \"mean\": pl.col(value_var).mean(),\n        \"median\": pl.col(value_var).median(),\n        \"std\": pl.col(value_var).std(),\n        \"count\": pl.col(value_var).count(),\n    }\n    if method not in agg_map:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    return (\n        df.group_by(\"cell_id\").agg(agg_map[method].alias(value_var)).sort(\"cell_id\")\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.aggregate_data_to_grid","level":2,"title":"<code>aggregate_data_to_grid(data_ds, grid, value_var='VOD', cell_var='cell_id_equal_area_2deg', sid=None, time_range=None, stat='median')</code>","text":"<p>Aggregate VOD data across all timestamps and SIDs to per-cell statistics.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.aggregate_data_to_grid--parameters","level":4,"title":"Parameters","text":"<p>data_ds : xr.Dataset     Full VOD dataset from the Icechunk store. grid : GridData     Grid definition. value_var : str     Name of the VOD variable. cell_var : str     Name of the cell-ID variable in data_ds. sid : list[str], optional     Satellite IDs to include.  <code>None</code> → all. time_range : tuple, optional     <code>(start, end)</code> datetimes for epoch filtering. stat : {'mean', 'median', 'std'}     Statistic to compute per cell.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.aggregate_data_to_grid--returns","level":4,"title":"Returns","text":"<p>np.ndarray     Array of length <code>grid.ncells</code> with per-cell aggregated values     (NaN where no observations exist).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>def aggregate_data_to_grid(\n    data_ds: xr.Dataset,\n    grid: GridData,\n    value_var: str = \"VOD\",\n    cell_var: str = \"cell_id_equal_area_2deg\",\n    sid: list[str] | None = None,\n    time_range: tuple | None = None,\n    stat: str = \"median\",\n) -&gt; np.ndarray:\n    \"\"\"Aggregate VOD data across all timestamps and SIDs to per-cell statistics.\n\n    Parameters\n    ----------\n    data_ds : xr.Dataset\n        Full VOD dataset from the Icechunk store.\n    grid : GridData\n        Grid definition.\n    value_var : str\n        Name of the VOD variable.\n    cell_var : str\n        Name of the cell-ID variable in *data_ds*.\n    sid : list[str], optional\n        Satellite IDs to include.  ``None`` → all.\n    time_range : tuple, optional\n        ``(start, end)`` datetimes for epoch filtering.\n    stat : {'mean', 'median', 'std'}\n        Statistic to compute per cell.\n\n    Returns\n    -------\n    np.ndarray\n        Array of length ``grid.ncells`` with per-cell aggregated values\n        (NaN where no observations exist).\n\n    \"\"\"\n    vod = data_ds[value_var]\n    cell_ids = data_ds[cell_var]\n\n    if sid is not None and \"sid\" in vod.dims:\n        vod = vod.sel(sid=sid)\n        cell_ids = cell_ids.sel(sid=sid)\n        print(f\"Using SIDs: {sid}\")\n    elif not sid and \"sid\" in vod.dims:\n        available_sids = vod.sid.values.tolist()\n        print(f\"Using all available SIDs: {available_sids}\")\n    else:\n        raise ValueError(\"No SID dimension in data.\")\n\n    if time_range is not None:\n        vod = vod.sel(epoch=slice(time_range[0], time_range[1]))\n        cell_ids = cell_ids.sel(epoch=slice(time_range[0], time_range[1]))\n\n    vod_flat = np.asarray(vod.values).ravel()\n    cell_flat = np.asarray(cell_ids.values).ravel()\n    valid = np.isfinite(vod_flat) &amp; np.isfinite(cell_flat)\n\n    df = pl.DataFrame(\n        {\n            \"cell_id\": cell_flat[valid].astype(np.int64),\n            \"vod\": vod_flat[valid],\n        }\n    )\n\n    agg_expr = {\n        \"mean\": pl.col(\"vod\").mean(),\n        \"median\": pl.col(\"vod\").median(),\n        \"std\": pl.col(\"vod\").std(),\n    }\n    if stat not in agg_expr:\n        raise ValueError(f\"Unsupported stat: {stat}\")\n\n    agg = df.group_by(\"cell_id\").agg(agg_expr[stat].alias(\"vod_stat\"))\n\n    result = np.full(grid.ncells, np.nan, dtype=float)\n    result[agg[\"cell_id\"].to_numpy()] = agg[\"vod_stat\"].to_numpy()\n    return result\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.compute_percell_timeseries","level":2,"title":"<code>compute_percell_timeseries(data_ds, grid, value_var='VOD', cell_var='cell_id_equal_area_2deg', theta_range=None, phi_range=None, selected_sids=None, time_range=None, temporal_resolution='1D', chunk_days=21, min_obs_per_cell_time=1)</code>","text":"<p>Compute time series per cell with SID aggregation.</p> <p>Processing is chunked over time to bound memory usage.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.compute_percell_timeseries--parameters","level":4,"title":"Parameters","text":"<p>data_ds : xr.Dataset     Full VOD dataset. grid : GridData     Grid definition. value_var : str     VOD variable name. cell_var : str     Cell-ID variable name. theta_range : tuple, optional     <code>(min_deg, max_deg)</code> elevation filter. phi_range : tuple, optional     <code>(min_deg, max_deg)</code> azimuth filter (wraps at 360°). selected_sids : list[str], optional     Satellite IDs to include. time_range : tuple, optional     <code>(start, end)</code> epoch slice. temporal_resolution : str     Pandas/polars frequency string (e.g. <code>\"1D\"</code>, <code>\"30min\"</code>). chunk_days : int     Days per processing chunk. min_obs_per_cell_time : int     Minimum SID observations per (cell, time-bin) to retain.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.compute_percell_timeseries--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Dataset with dimensions <code>(cell, time)</code> and variables     <code>cell_timeseries</code>, <code>cell_weights</code>, <code>cell_counts</code>,     <code>cell_theta</code>, <code>cell_phi</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>def compute_percell_timeseries(\n    data_ds: xr.Dataset,\n    grid: GridData,\n    value_var: str = \"VOD\",\n    cell_var: str = \"cell_id_equal_area_2deg\",\n    theta_range: tuple[float, float] | None = None,\n    phi_range: tuple[float, float] | None = None,\n    selected_sids: list[str] | None = None,\n    time_range: tuple | None = None,\n    temporal_resolution: str = \"1D\",\n    chunk_days: int = 21,\n    min_obs_per_cell_time: int = 1,\n) -&gt; xr.Dataset:\n    \"\"\"Compute time series per cell with SID aggregation.\n\n    Processing is chunked over time to bound memory usage.\n\n    Parameters\n    ----------\n    data_ds : xr.Dataset\n        Full VOD dataset.\n    grid : GridData\n        Grid definition.\n    value_var : str\n        VOD variable name.\n    cell_var : str\n        Cell-ID variable name.\n    theta_range : tuple, optional\n        ``(min_deg, max_deg)`` elevation filter.\n    phi_range : tuple, optional\n        ``(min_deg, max_deg)`` azimuth filter (wraps at 360°).\n    selected_sids : list[str], optional\n        Satellite IDs to include.\n    time_range : tuple, optional\n        ``(start, end)`` epoch slice.\n    temporal_resolution : str\n        Pandas/polars frequency string (e.g. ``\"1D\"``, ``\"30min\"``).\n    chunk_days : int\n        Days per processing chunk.\n    min_obs_per_cell_time : int\n        Minimum SID observations per (cell, time-bin) to retain.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with dimensions ``(cell, time)`` and variables\n        ``cell_timeseries``, ``cell_weights``, ``cell_counts``,\n        ``cell_theta``, ``cell_phi``.\n\n    \"\"\"\n    print(\"📍 PER-CELL TIME SERIES AGGREGATION\")\n    print(\"=\" * 60)\n    print(f\"📦 Chunk size: {chunk_days} days\")\n    print(f\"🕒 Resolution: {temporal_resolution}\")\n\n    start_time = time.time()\n\n    selected_cells = _create_spatial_selection(grid, theta_range, phi_range)\n    print(f\"📍 Selected cells: {len(selected_cells)}\")\n\n    if selected_sids is not None and \"sid\" in data_ds.dims:\n        data_ds = data_ds.sel(sid=selected_sids)\n        print(f\"🛰️  Selected SIDs: {len(selected_sids)}\")\n\n    if time_range is not None:\n        data_ds = data_ds.sel(epoch=slice(time_range[0], time_range[1]))\n\n    print(f\"📊 Data shape: {data_ds[value_var].shape}\")\n\n    time_start, time_end = data_ds.epoch.values[0], data_ds.epoch.values[-1]\n\n    # Normalise frequency string for pandas\n    pandas_freq = _normalise_pandas_freq(temporal_resolution)\n\n    output_times = pd.date_range(\n        start=pd.to_datetime(time_start),\n        end=pd.to_datetime(time_end),\n        freq=pandas_freq,\n    )\n\n    n_times = len(output_times)\n    n_cells = len(selected_cells)\n\n    print(f\"📅 Output shape: {n_cells} cells × {n_times} time bins\")\n\n    # Result arrays (cell × time)\n    cell_timeseries = np.full((n_cells, n_times), np.nan)\n    cell_weights = np.zeros((n_cells, n_times))\n    cell_counts = np.zeros((n_cells, n_times), dtype=int)\n\n    cell_to_idx = {int(cell_id): i for i, cell_id in enumerate(selected_cells)}\n    time_to_idx = {pd.Timestamp(t): i for i, t in enumerate(output_times)}\n\n    chunk_starts = pd.date_range(\n        start=pd.to_datetime(time_start),\n        end=pd.to_datetime(time_end),\n        freq=f\"{chunk_days}D\",\n    )\n\n    print(f\"🔄 Processing {len(chunk_starts)} chunks...\")\n\n    for chunk_start in tqdm(chunk_starts, desc=\"Processing chunks\"):\n        chunk_end = min(\n            chunk_start + pd.Timedelta(days=chunk_days),\n            pd.to_datetime(time_end),\n        )\n        chunk_data = data_ds.sel(epoch=slice(chunk_start, chunk_end))\n\n        if len(chunk_data.epoch) == 0:\n            continue\n\n        chunk_results = _process_chunk_percell(\n            chunk_data,\n            selected_cells,\n            temporal_resolution,\n            value_var,\n            cell_var,\n            min_obs_per_cell_time,\n        )\n\n        if chunk_results:\n            _merge_percell_results(\n                chunk_results,\n                cell_timeseries,\n                cell_weights,\n                cell_counts,\n                cell_to_idx,\n                time_to_idx,\n            )\n\n        del chunk_data\n        gc.collect()\n\n    processing_time = time.time() - start_time\n\n    result_ds = _create_percell_dataset(\n        cell_timeseries,\n        cell_weights,\n        cell_counts,\n        selected_cells,\n        output_times,\n        grid,\n        processing_time,\n        theta_range,\n        phi_range,\n        temporal_resolution,\n        chunk_days,\n    )\n\n    valid_cell_times = np.sum(np.isfinite(cell_timeseries))\n    total_cell_times = n_cells * n_times\n    coverage = (valid_cell_times / total_cell_times) * 100\n\n    print(\"\\n✅ PER-CELL AGGREGATION COMPLETE!\")\n    print(f\"⚡ Time: {processing_time / 60:.2f} minutes\")\n    print(f\"📊 Output: {n_cells} cells × {n_times} times\")\n    print(f\"📈 Coverage: {valid_cell_times:,} / {total_cell_times:,} ({coverage:.1f}%)\")\n    print(\"🎯 Ready for diurnal analysis, spatial patterns, custom aggregations!\")\n\n    return result_ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.compute_global_average","level":2,"title":"<code>compute_global_average(percell_ds)</code>","text":"<p>Compute observation-count–weighted global average from per-cell data.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.compute_global_average--parameters","level":4,"title":"Parameters","text":"<p>percell_ds : xr.Dataset     Output of :func:<code>compute_percell_timeseries</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.compute_global_average--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Variables: <code>global_timeseries</code>, <code>spatial_std</code>,     <code>total_weights</code>, <code>active_cells</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>def compute_global_average(percell_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Compute observation-count–weighted global average from per-cell data.\n\n    Parameters\n    ----------\n    percell_ds : xr.Dataset\n        Output of :func:`compute_percell_timeseries`.\n\n    Returns\n    -------\n    xr.Dataset\n        Variables: ``global_timeseries``, ``spatial_std``,\n        ``total_weights``, ``active_cells``.\n\n    \"\"\"\n    weights = percell_ds.cell_weights\n    values = percell_ds.cell_timeseries\n\n    weighted_sum = (values * weights).sum(dim=\"cell\", skipna=True)\n    total_weights = weights.sum(dim=\"cell\", skipna=True)\n\n    global_mean = weighted_sum / total_weights\n\n    valid_mask = np.isfinite(values)\n    spatial_std = values.where(valid_mask).std(dim=\"cell\", skipna=True)\n\n    return xr.Dataset(\n        {\n            \"global_timeseries\": global_mean,\n            \"spatial_std\": spatial_std,\n            \"total_weights\": total_weights,\n            \"active_cells\": valid_mask.sum(dim=\"cell\"),\n        }\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.compute_regional_average","level":2,"title":"<code>compute_regional_average(percell_ds, region_cells)</code>","text":"<p>Compute observation-count–weighted average for a cell subset.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.compute_regional_average--parameters","level":4,"title":"Parameters","text":"<p>percell_ds : xr.Dataset     Output of :func:<code>compute_percell_timeseries</code>. region_cells : array-like     Cell IDs defining the region.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.compute_regional_average--returns","level":4,"title":"Returns","text":"<p>xr.DataArray     Weighted regional mean time series.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>def compute_regional_average(\n    percell_ds: xr.Dataset, region_cells: list[int] | np.ndarray\n) -&gt; xr.DataArray:\n    \"\"\"Compute observation-count–weighted average for a cell subset.\n\n    Parameters\n    ----------\n    percell_ds : xr.Dataset\n        Output of :func:`compute_percell_timeseries`.\n    region_cells : array-like\n        Cell IDs defining the region.\n\n    Returns\n    -------\n    xr.DataArray\n        Weighted regional mean time series.\n\n    \"\"\"\n    regional_data = percell_ds.sel(cell=region_cells)\n\n    weights = regional_data.cell_weights\n    values = regional_data.cell_timeseries\n\n    weighted_sum = (values * weights).sum(dim=\"cell\", skipna=True)\n    total_weights = weights.sum(dim=\"cell\", skipna=True)\n\n    return weighted_sum / total_weights\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.analyze_diurnal_patterns","level":2,"title":"<code>analyze_diurnal_patterns(percell_ds)</code>","text":"<p>Compute hourly means from per-cell time series.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.analyze_diurnal_patterns--parameters","level":4,"title":"Parameters","text":"<p>percell_ds : xr.Dataset     Output of :func:<code>compute_percell_timeseries</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.analyze_diurnal_patterns--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Grouped by <code>time.hour</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>def analyze_diurnal_patterns(percell_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Compute hourly means from per-cell time series.\n\n    Parameters\n    ----------\n    percell_ds : xr.Dataset\n        Output of :func:`compute_percell_timeseries`.\n\n    Returns\n    -------\n    xr.Dataset\n        Grouped by ``time.hour``.\n\n    \"\"\"\n    return percell_ds.groupby(\"time.hour\").mean(dim=\"time\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.analyze_spatial_patterns","level":2,"title":"<code>analyze_spatial_patterns(percell_ds)</code>","text":"<p>Compute time-averaged spatial field.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.analyze_spatial_patterns--parameters","level":4,"title":"Parameters","text":"<p>percell_ds : xr.Dataset     Output of :func:<code>compute_percell_timeseries</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.analyze_spatial_patterns--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Time-averaged dataset.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>def analyze_spatial_patterns(percell_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Compute time-averaged spatial field.\n\n    Parameters\n    ----------\n    percell_ds : xr.Dataset\n        Output of :func:`compute_percell_timeseries`.\n\n    Returns\n    -------\n    xr.Dataset\n        Time-averaged dataset.\n\n    \"\"\"\n    return percell_ds.mean(dim=\"time\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.compute_hemisphere_percell","level":2,"title":"<code>compute_hemisphere_percell(data_ds, grid, **kwargs)</code>","text":"<p>Daily per-cell time series for the full hemisphere.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>def compute_hemisphere_percell(\n    data_ds: xr.Dataset,\n    grid: GridData,\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"Daily per-cell time series for the full hemisphere.\"\"\"\n    return compute_percell_timeseries(\n        data_ds=data_ds, grid=grid, temporal_resolution=\"1D\", **kwargs\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.aggregation.compute_zenith_percell","level":2,"title":"<code>compute_zenith_percell(data_ds, grid, **kwargs)</code>","text":"<p>Daily per-cell time series restricted to θ ≤ 30°.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/aggregation.py</code> <pre><code>def compute_zenith_percell(\n    data_ds: xr.Dataset,\n    grid: GridData,\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"Daily per-cell time series restricted to θ ≤ 30°.\"\"\"\n    return compute_percell_timeseries(\n        data_ds=data_ds,\n        grid=grid,\n        theta_range=(0, 30),\n        temporal_resolution=\"1D\",\n        **kwargs,\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#analysis","level":2,"title":"Analysis","text":"","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#filtering","level":3,"title":"Filtering","text":"<p>Global (dataset-wide) outlier filters for gridded VOD data.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering--classes","level":3,"title":"Classes","text":"<p>Filter                  – abstract base; <code>compute_mask</code> / <code>apply</code> contract. ZScoreFilter            – mean ± k·σ rejection. IQRFilter               – Q1 – f·IQR / Q3 + f·IQR rejection. RangeFilter             – hard min/max bounds. PercentileFilter        – lower/upper percentile bounds. CustomFilter            – user-supplied callable mask. FilterPipeline          – sequential or combined multi-filter application.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering--convenience-functions","level":3,"title":"Convenience functions","text":"<p><code>create_zscore_filter</code>   – one-liner z-score filter. <code>create_range_filter</code>    – one-liner range filter.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering--notes","level":3,"title":"Notes","text":"<ul> <li>Filters never modify original data.  <code>apply</code> returns a new   <code>xr.Dataset</code> with <code>&lt;var&gt;_filtered_&lt;n&gt;</code> and <code>mask_&lt;n&gt;</code>   variables appended.</li> <li>Both numpy and dask-backed arrays are supported; dask paths compute   only the scalar statistics eagerly while the mask itself stays lazy.</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.Filter","level":2,"title":"<code>Filter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all filters.  Filters NEVER modify original data.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>class Filter(ABC):\n    \"\"\"Base class for all filters.  Filters NEVER modify original data.\"\"\"\n\n    def __init__(self, name: str) -&gt; None:\n        \"\"\"Initialize the filter.\n\n        Parameters\n        ----------\n        name : str\n            Filter name.\n\n        \"\"\"\n        self.name = name\n        self.metadata: dict = {\n            \"filter_type\": self.__class__.__name__,\n            \"timestamp\": datetime.now().isoformat(),\n        }\n\n    @abstractmethod\n    def compute_mask(\n        self,\n        data: xr.DataArray,\n        **kwargs: Any,\n    ) -&gt; xr.DataArray:\n        \"\"\"Compute boolean mask (True = keep, False = remove).\"\"\"\n        ...\n\n    def apply(\n        self,\n        ds: xr.Dataset,\n        var_name: str,\n        output_suffix: str | None = None,\n        **kwargs: Any,\n    ) -&gt; xr.Dataset:\n        \"\"\"Apply filter to *ds*, returning a copy with filtered variable added.\n\n        New variables\n        -------------\n        ``&lt;var_name&gt;_filtered_&lt;suffix&gt;`` : filtered data (NaN where masked).\n        ``mask_&lt;suffix&gt;``               : boolean keep-mask.\n        \"\"\"\n        suffix = output_suffix or self.name\n\n        data = ds[var_name]\n        mask = self.compute_mask(data, **kwargs)\n        filtered_data = data.where(mask)\n\n        n_total = int(mask.size)\n        n_removed = int((~mask).sum().values)\n\n        metadata = {\n            **self.metadata,\n            **kwargs,\n            \"applied_to\": var_name,\n            \"n_total\": n_total,\n            \"n_removed\": n_removed,\n            \"fraction_removed\": float(n_removed / n_total),\n            \"filter_chain\": [self.name],\n        }\n\n        ds_out = ds.copy()\n\n        filtered_var_name = f\"{var_name}_filtered_{suffix}\"\n        mask_var_name = f\"mask_{suffix}\"\n\n        ds_out[filtered_var_name] = filtered_data\n        ds_out[filtered_var_name].attrs = metadata\n\n        ds_out[mask_var_name] = mask\n        ds_out[mask_var_name].attrs = {\n            \"description\": f\"Boolean mask for {self.name} filter\",\n            \"True\": \"keep\",\n            \"False\": \"filtered out\",\n            **metadata,\n        }\n\n        return ds_out\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.Filter.__init__","level":3,"title":"<code>__init__(name)</code>","text":"<p>Initialize the filter.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.Filter.__init__--parameters","level":5,"title":"Parameters","text":"<p>name : str     Filter name.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"Initialize the filter.\n\n    Parameters\n    ----------\n    name : str\n        Filter name.\n\n    \"\"\"\n    self.name = name\n    self.metadata: dict = {\n        \"filter_type\": self.__class__.__name__,\n        \"timestamp\": datetime.now().isoformat(),\n    }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.Filter.compute_mask","level":3,"title":"<code>compute_mask(data, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Compute boolean mask (True = keep, False = remove).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>@abstractmethod\ndef compute_mask(\n    self,\n    data: xr.DataArray,\n    **kwargs: Any,\n) -&gt; xr.DataArray:\n    \"\"\"Compute boolean mask (True = keep, False = remove).\"\"\"\n    ...\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.Filter.apply","level":3,"title":"<code>apply(ds, var_name, output_suffix=None, **kwargs)</code>","text":"<p>Apply filter to ds, returning a copy with filtered variable added.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.Filter.apply--new-variables","level":5,"title":"New variables","text":"<p><code>&lt;var_name&gt;_filtered_&lt;suffix&gt;</code> : filtered data (NaN where masked). <code>mask_&lt;suffix&gt;</code>               : boolean keep-mask.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def apply(\n    self,\n    ds: xr.Dataset,\n    var_name: str,\n    output_suffix: str | None = None,\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"Apply filter to *ds*, returning a copy with filtered variable added.\n\n    New variables\n    -------------\n    ``&lt;var_name&gt;_filtered_&lt;suffix&gt;`` : filtered data (NaN where masked).\n    ``mask_&lt;suffix&gt;``               : boolean keep-mask.\n    \"\"\"\n    suffix = output_suffix or self.name\n\n    data = ds[var_name]\n    mask = self.compute_mask(data, **kwargs)\n    filtered_data = data.where(mask)\n\n    n_total = int(mask.size)\n    n_removed = int((~mask).sum().values)\n\n    metadata = {\n        **self.metadata,\n        **kwargs,\n        \"applied_to\": var_name,\n        \"n_total\": n_total,\n        \"n_removed\": n_removed,\n        \"fraction_removed\": float(n_removed / n_total),\n        \"filter_chain\": [self.name],\n    }\n\n    ds_out = ds.copy()\n\n    filtered_var_name = f\"{var_name}_filtered_{suffix}\"\n    mask_var_name = f\"mask_{suffix}\"\n\n    ds_out[filtered_var_name] = filtered_data\n    ds_out[filtered_var_name].attrs = metadata\n\n    ds_out[mask_var_name] = mask\n    ds_out[mask_var_name].attrs = {\n        \"description\": f\"Boolean mask for {self.name} filter\",\n        \"True\": \"keep\",\n        \"False\": \"filtered out\",\n        **metadata,\n    }\n\n    return ds_out\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.ZScoreFilter","level":2,"title":"<code>ZScoreFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Remove statistical outliers using z-score method.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>class ZScoreFilter(Filter):\n    \"\"\"Remove statistical outliers using z-score method.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the filter.\"\"\"\n        super().__init__(\"zscore\")\n\n    def compute_mask(self, data: xr.DataArray, threshold: float = 3.0) -&gt; xr.DataArray:\n        \"\"\"Compute z-score mask.\n\n        Parameters\n        ----------\n        data : xr.DataArray\n            Input data.\n        threshold : float\n            Z-score threshold (default: 3.0).\n\n        Returns\n        -------\n        xr.DataArray\n            Boolean mask (True = keep).\n\n        \"\"\"\n        if isinstance(data.data, da.Array):\n            mean = da.nanmean(data.data).compute()\n            std = da.nanstd(data.data).compute()\n            z_scores = da.fabs((data.data - mean) / std)\n            mask_data = z_scores &lt;= threshold\n            mask = xr.DataArray(mask_data, dims=data.dims, coords=data.coords)\n        else:\n            mean = data.mean(skipna=True)\n            std = data.std(skipna=True)\n            z_scores = np.abs((data - mean) / std)\n            mask = z_scores &lt;= threshold\n\n        return mask\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.ZScoreFilter.__init__","level":3,"title":"<code>__init__()</code>","text":"<p>Initialize the filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the filter.\"\"\"\n    super().__init__(\"zscore\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.ZScoreFilter.compute_mask","level":3,"title":"<code>compute_mask(data, threshold=3.0)</code>","text":"<p>Compute z-score mask.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.ZScoreFilter.compute_mask--parameters","level":5,"title":"Parameters","text":"<p>data : xr.DataArray     Input data. threshold : float     Z-score threshold (default: 3.0).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.ZScoreFilter.compute_mask--returns","level":5,"title":"Returns","text":"<p>xr.DataArray     Boolean mask (True = keep).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def compute_mask(self, data: xr.DataArray, threshold: float = 3.0) -&gt; xr.DataArray:\n    \"\"\"Compute z-score mask.\n\n    Parameters\n    ----------\n    data : xr.DataArray\n        Input data.\n    threshold : float\n        Z-score threshold (default: 3.0).\n\n    Returns\n    -------\n    xr.DataArray\n        Boolean mask (True = keep).\n\n    \"\"\"\n    if isinstance(data.data, da.Array):\n        mean = da.nanmean(data.data).compute()\n        std = da.nanstd(data.data).compute()\n        z_scores = da.fabs((data.data - mean) / std)\n        mask_data = z_scores &lt;= threshold\n        mask = xr.DataArray(mask_data, dims=data.dims, coords=data.coords)\n    else:\n        mean = data.mean(skipna=True)\n        std = data.std(skipna=True)\n        z_scores = np.abs((data - mean) / std)\n        mask = z_scores &lt;= threshold\n\n    return mask\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.IQRFilter","level":2,"title":"<code>IQRFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Remove outliers using Interquartile Range method.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>class IQRFilter(Filter):\n    \"\"\"Remove outliers using Interquartile Range method.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the filter.\"\"\"\n        super().__init__(\"iqr\")\n\n    def compute_mask(self, data: xr.DataArray, factor: float = 1.5) -&gt; xr.DataArray:\n        \"\"\"Compute IQR mask.\n\n        Parameters\n        ----------\n        data : xr.DataArray\n            Input data.\n        factor : float\n            IQR factor (default: 1.5).\n\n        Returns\n        -------\n        xr.DataArray\n            Boolean mask (True = keep).\n\n        \"\"\"\n        if isinstance(data.data, da.Array):\n            flat_data = data.data.ravel()\n            q1_val = da.percentile(flat_data, 25, method=\"linear\").compute()\n            q3_val = da.percentile(flat_data, 75, method=\"linear\").compute()\n\n            iqr = q3_val - q1_val\n            lower_bound = q1_val - factor * iqr\n            upper_bound = q3_val + factor * iqr\n\n            mask_data = (data.data &gt;= lower_bound) &amp; (data.data &lt;= upper_bound)\n            mask = xr.DataArray(mask_data, dims=data.dims, coords=data.coords)\n        else:\n            q1 = data.quantile(0.25, skipna=True)\n            q3 = data.quantile(0.75, skipna=True)\n            iqr = q3 - q1\n\n            lower_bound = q1 - factor * iqr\n            upper_bound = q3 + factor * iqr\n\n            mask = (data &gt;= lower_bound) &amp; (data &lt;= upper_bound)\n\n        return mask\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.IQRFilter.__init__","level":3,"title":"<code>__init__()</code>","text":"<p>Initialize the filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the filter.\"\"\"\n    super().__init__(\"iqr\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.IQRFilter.compute_mask","level":3,"title":"<code>compute_mask(data, factor=1.5)</code>","text":"<p>Compute IQR mask.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.IQRFilter.compute_mask--parameters","level":5,"title":"Parameters","text":"<p>data : xr.DataArray     Input data. factor : float     IQR factor (default: 1.5).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.IQRFilter.compute_mask--returns","level":5,"title":"Returns","text":"<p>xr.DataArray     Boolean mask (True = keep).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def compute_mask(self, data: xr.DataArray, factor: float = 1.5) -&gt; xr.DataArray:\n    \"\"\"Compute IQR mask.\n\n    Parameters\n    ----------\n    data : xr.DataArray\n        Input data.\n    factor : float\n        IQR factor (default: 1.5).\n\n    Returns\n    -------\n    xr.DataArray\n        Boolean mask (True = keep).\n\n    \"\"\"\n    if isinstance(data.data, da.Array):\n        flat_data = data.data.ravel()\n        q1_val = da.percentile(flat_data, 25, method=\"linear\").compute()\n        q3_val = da.percentile(flat_data, 75, method=\"linear\").compute()\n\n        iqr = q3_val - q1_val\n        lower_bound = q1_val - factor * iqr\n        upper_bound = q3_val + factor * iqr\n\n        mask_data = (data.data &gt;= lower_bound) &amp; (data.data &lt;= upper_bound)\n        mask = xr.DataArray(mask_data, dims=data.dims, coords=data.coords)\n    else:\n        q1 = data.quantile(0.25, skipna=True)\n        q3 = data.quantile(0.75, skipna=True)\n        iqr = q3 - q1\n\n        lower_bound = q1 - factor * iqr\n        upper_bound = q3 + factor * iqr\n\n        mask = (data &gt;= lower_bound) &amp; (data &lt;= upper_bound)\n\n    return mask\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.RangeFilter","level":2,"title":"<code>RangeFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter values outside specified range.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>class RangeFilter(Filter):\n    \"\"\"Filter values outside specified range.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the filter.\"\"\"\n        super().__init__(\"range\")\n\n    def compute_mask(\n        self,\n        data: xr.DataArray,\n        min_value: float | None = None,\n        max_value: float | None = None,\n    ) -&gt; xr.DataArray:\n        \"\"\"Compute range mask.\n\n        Parameters\n        ----------\n        data : xr.DataArray\n            Input data.\n        min_value : float, optional\n            Minimum allowed value.\n        max_value : float, optional\n            Maximum allowed value.\n\n        Returns\n        -------\n        xr.DataArray\n            Boolean mask (True = keep).\n\n        \"\"\"\n        mask = xr.ones_like(data, dtype=bool)\n\n        if min_value is not None:\n            mask = mask &amp; (data &gt;= min_value)\n        if max_value is not None:\n            mask = mask &amp; (data &lt;= max_value)\n\n        return mask\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.RangeFilter.__init__","level":3,"title":"<code>__init__()</code>","text":"<p>Initialize the filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the filter.\"\"\"\n    super().__init__(\"range\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.RangeFilter.compute_mask","level":3,"title":"<code>compute_mask(data, min_value=None, max_value=None)</code>","text":"<p>Compute range mask.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.RangeFilter.compute_mask--parameters","level":5,"title":"Parameters","text":"<p>data : xr.DataArray     Input data. min_value : float, optional     Minimum allowed value. max_value : float, optional     Maximum allowed value.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.RangeFilter.compute_mask--returns","level":5,"title":"Returns","text":"<p>xr.DataArray     Boolean mask (True = keep).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def compute_mask(\n    self,\n    data: xr.DataArray,\n    min_value: float | None = None,\n    max_value: float | None = None,\n) -&gt; xr.DataArray:\n    \"\"\"Compute range mask.\n\n    Parameters\n    ----------\n    data : xr.DataArray\n        Input data.\n    min_value : float, optional\n        Minimum allowed value.\n    max_value : float, optional\n        Maximum allowed value.\n\n    Returns\n    -------\n    xr.DataArray\n        Boolean mask (True = keep).\n\n    \"\"\"\n    mask = xr.ones_like(data, dtype=bool)\n\n    if min_value is not None:\n        mask = mask &amp; (data &gt;= min_value)\n    if max_value is not None:\n        mask = mask &amp; (data &lt;= max_value)\n\n    return mask\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.PercentileFilter","level":2,"title":"<code>PercentileFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter values outside percentile range.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>class PercentileFilter(Filter):\n    \"\"\"Filter values outside percentile range.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the filter.\"\"\"\n        super().__init__(\"percentile\")\n\n    def compute_mask(\n        self, data: xr.DataArray, lower: float = 5.0, upper: float = 95.0\n    ) -&gt; xr.DataArray:\n        \"\"\"Compute percentile mask.\n\n        Parameters\n        ----------\n        data : xr.DataArray\n            Input data.\n        lower : float\n            Lower percentile (0–100).\n        upper : float\n            Upper percentile (0–100).\n\n        Returns\n        -------\n        xr.DataArray\n            Boolean mask (True = keep).\n\n        \"\"\"\n        lower_val = data.quantile(lower / 100.0, skipna=True)\n        upper_val = data.quantile(upper / 100.0, skipna=True)\n\n        mask = (data &gt;= lower_val) &amp; (data &lt;= upper_val)\n\n        return mask\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.PercentileFilter.__init__","level":3,"title":"<code>__init__()</code>","text":"<p>Initialize the filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the filter.\"\"\"\n    super().__init__(\"percentile\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.PercentileFilter.compute_mask","level":3,"title":"<code>compute_mask(data, lower=5.0, upper=95.0)</code>","text":"<p>Compute percentile mask.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.PercentileFilter.compute_mask--parameters","level":5,"title":"Parameters","text":"<p>data : xr.DataArray     Input data. lower : float     Lower percentile (0–100). upper : float     Upper percentile (0–100).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.PercentileFilter.compute_mask--returns","level":5,"title":"Returns","text":"<p>xr.DataArray     Boolean mask (True = keep).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def compute_mask(\n    self, data: xr.DataArray, lower: float = 5.0, upper: float = 95.0\n) -&gt; xr.DataArray:\n    \"\"\"Compute percentile mask.\n\n    Parameters\n    ----------\n    data : xr.DataArray\n        Input data.\n    lower : float\n        Lower percentile (0–100).\n    upper : float\n        Upper percentile (0–100).\n\n    Returns\n    -------\n    xr.DataArray\n        Boolean mask (True = keep).\n\n    \"\"\"\n    lower_val = data.quantile(lower / 100.0, skipna=True)\n    upper_val = data.quantile(upper / 100.0, skipna=True)\n\n    mask = (data &gt;= lower_val) &amp; (data &lt;= upper_val)\n\n    return mask\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.CustomFilter","level":2,"title":"<code>CustomFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Apply a user-supplied callable as filter.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.CustomFilter--parameters","level":4,"title":"Parameters","text":"<p>name : str     Filter identifier. func : callable     <code>(xr.DataArray, **kwargs) -&gt; xr.DataArray</code> returning a boolean mask.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>class CustomFilter(Filter):\n    \"\"\"Apply a user-supplied callable as filter.\n\n    Parameters\n    ----------\n    name : str\n        Filter identifier.\n    func : callable\n        ``(xr.DataArray, **kwargs) -&gt; xr.DataArray`` returning a boolean mask.\n\n    \"\"\"\n\n    def __init__(self, name: str, func: Callable[..., xr.DataArray]) -&gt; None:\n        \"\"\"Initialize the custom filter.\n\n        Parameters\n        ----------\n        name : str\n            Filter identifier.\n        func : Callable[..., xr.DataArray]\n            Callable returning a boolean mask.\n\n        \"\"\"\n        super().__init__(name)\n        self.func = func\n\n    def compute_mask(\n        self,\n        data: xr.DataArray,\n        **kwargs: Any,\n    ) -&gt; xr.DataArray:\n        \"\"\"Apply custom function.\"\"\"\n        return self.func(data, **kwargs)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.CustomFilter.__init__","level":3,"title":"<code>__init__(name, func)</code>","text":"<p>Initialize the custom filter.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.CustomFilter.__init__--parameters","level":5,"title":"Parameters","text":"<p>name : str     Filter identifier. func : Callable[..., xr.DataArray]     Callable returning a boolean mask.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def __init__(self, name: str, func: Callable[..., xr.DataArray]) -&gt; None:\n    \"\"\"Initialize the custom filter.\n\n    Parameters\n    ----------\n    name : str\n        Filter identifier.\n    func : Callable[..., xr.DataArray]\n        Callable returning a boolean mask.\n\n    \"\"\"\n    super().__init__(name)\n    self.func = func\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.CustomFilter.compute_mask","level":3,"title":"<code>compute_mask(data, **kwargs)</code>","text":"<p>Apply custom function.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def compute_mask(\n    self,\n    data: xr.DataArray,\n    **kwargs: Any,\n) -&gt; xr.DataArray:\n    \"\"\"Apply custom function.\"\"\"\n    return self.func(data, **kwargs)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.FilterPipeline","level":2,"title":"<code>FilterPipeline</code>","text":"<p>Manage multiple filters applied sequentially or combined.</p> <p>Non-destructive: creates new DataArrays, never modifies originals.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.FilterPipeline--parameters","level":4,"title":"Parameters","text":"<p>ds : xr.Dataset     Input dataset. var_name : str     Variable to filter (default: <code>'VOD'</code>).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>class FilterPipeline:\n    \"\"\"Manage multiple filters applied sequentially or combined.\n\n    Non-destructive: creates new DataArrays, never modifies originals.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Input dataset.\n    var_name : str\n        Variable to filter (default: ``'VOD'``).\n\n    \"\"\"\n\n    def __init__(self, ds: xr.Dataset, var_name: str = \"VOD\") -&gt; None:\n        \"\"\"Initialize the filter pipeline.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Input dataset.\n        var_name : str, default \"VOD\"\n            Variable to filter.\n\n        \"\"\"\n        self.ds = ds\n        self.var_name = var_name\n        self.filters: list[tuple[Filter, dict]] = []\n\n    def add_filter(self, filter_obj: Filter | str, **kwargs: Any) -&gt; FilterPipeline:\n        \"\"\"Add filter to pipeline.\n\n        Parameters\n        ----------\n        filter_obj : Filter or str\n            Filter instance or short name\n            (``'zscore'``, ``'iqr'``, ``'range'``, ``'percentile'``).\n        **kwargs\n            Parameters forwarded to ``compute_mask``.\n\n        Returns\n        -------\n        FilterPipeline\n            Self (for chaining).\n\n        \"\"\"\n        if isinstance(filter_obj, str):\n            _filter_map = {\n                \"zscore\": ZScoreFilter,\n                \"iqr\": IQRFilter,\n                \"range\": RangeFilter,\n                \"percentile\": PercentileFilter,\n            }\n            if filter_obj not in _filter_map:\n                raise ValueError(f\"Unknown filter: {filter_obj}\")\n            filter_obj = _filter_map[filter_obj]()\n\n        self.filters.append((filter_obj, kwargs))\n        return self\n\n    def apply(\n        self, mode: str = \"sequential\", output_name: str | None = None\n    ) -&gt; xr.Dataset:\n        \"\"\"Apply all filters in the pipeline.\n\n        Parameters\n        ----------\n        mode : {'sequential', 'combined'}\n            ``'sequential'`` – masks accumulate (AND) after each filter;\n            intermediate filtered variables are written.\n            ``'combined'``   – all masks computed independently on the\n            original data, then AND-ed once.\n        output_name : str, optional\n            Alias for the final filtered variable.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with filtered variables appended.\n\n        \"\"\"\n        if not self.filters:\n            raise ValueError(\"No filters in pipeline\")\n\n        ds_out = self.ds.copy()\n\n        if mode == \"sequential\":\n            masks: list[xr.DataArray] = []\n            filter_names: list[str] = []\n            all_params: list[dict] = []\n\n            for filter_obj, kwargs in self.filters:\n                mask = filter_obj.compute_mask(ds_out[self.var_name], **kwargs)\n                masks.append(mask)\n                filter_names.append(filter_obj.name)\n                all_params.append(kwargs)\n\n                # Store individual mask\n                mask_name = f\"mask_{filter_obj.name}\"\n                if mask_name not in ds_out:\n                    ds_out[mask_name] = mask\n                    ds_out[mask_name].attrs = {\n                        \"filter_type\": filter_obj.name,\n                        **kwargs,\n                    }\n\n                # Accumulate masks (AND)\n                cumulative_mask = masks[0]\n                for m in masks[1:]:\n                    cumulative_mask = cumulative_mask &amp; m\n\n                cumulative_suffix = \"_\".join(filter_names)\n                filtered_data = ds_out[self.var_name].where(cumulative_mask)\n\n                filtered_var_name = f\"{self.var_name}_filtered_{cumulative_suffix}\"\n                cumulative_mask_name = f\"mask_{cumulative_suffix}\"\n\n                ds_out[filtered_var_name] = filtered_data\n                ds_out[cumulative_mask_name] = cumulative_mask\n\n                n_total = int(cumulative_mask.size)\n                n_removed = int((~cumulative_mask).sum().values)\n\n                metadata: dict = {\n                    \"filter_chain\": filter_names.copy(),\n                    \"mode\": \"sequential\",\n                    \"applied_to\": self.var_name,\n                    \"n_total\": n_total,\n                    \"n_removed\": n_removed,\n                    \"fraction_removed\": float(n_removed / n_total),\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"filters\": {\n                        fname: params for fname, params in zip(filter_names, all_params)\n                    },\n                }\n\n                ds_out[filtered_var_name].attrs = metadata\n                ds_out[cumulative_mask_name].attrs = metadata\n\n            if output_name:\n                final_var = f\"{self.var_name}_filtered_{'_'.join(filter_names)}\"\n                final_mask = f\"mask_{'_'.join(filter_names)}\"\n\n                ds_out[f\"{self.var_name}_filtered_{output_name}\"] = ds_out[final_var]\n                ds_out[f\"mask_{output_name}\"] = ds_out[final_mask]\n                ds_out[f\"{self.var_name}_filtered_{output_name}\"].attrs = ds_out[\n                    final_var\n                ].attrs\n                ds_out[f\"mask_{output_name}\"].attrs = ds_out[final_mask].attrs\n\n        elif mode == \"combined\":\n            masks = []\n            filter_names = []\n            all_params = []\n\n            for filter_obj, kwargs in self.filters:\n                mask = filter_obj.compute_mask(ds_out[self.var_name], **kwargs)\n                masks.append(mask)\n                filter_names.append(filter_obj.name)\n                all_params.append(kwargs)\n\n                mask_name = f\"mask_{filter_obj.name}\"\n                if mask_name not in ds_out:\n                    ds_out[mask_name] = mask\n                    ds_out[mask_name].attrs = {\n                        \"filter_type\": filter_obj.name,\n                        **kwargs,\n                    }\n\n            combined_mask = masks[0]\n            for mask in masks[1:]:\n                combined_mask = combined_mask &amp; mask\n\n            suffix = output_name or \"combined\"\n            filtered_data = ds_out[self.var_name].where(combined_mask)\n\n            filtered_var_name = f\"{self.var_name}_filtered_{suffix}\"\n            mask_var_name = f\"mask_{suffix}\"\n\n            ds_out[filtered_var_name] = filtered_data\n            ds_out[mask_var_name] = combined_mask\n\n            n_total = int(combined_mask.size)\n            n_removed = int((~combined_mask).sum().values)\n\n            metadata = {\n                \"filter_chain\": filter_names,\n                \"mode\": \"combined\",\n                \"applied_to\": self.var_name,\n                \"n_total\": n_total,\n                \"n_removed\": n_removed,\n                \"fraction_removed\": float(n_removed / n_total),\n                \"timestamp\": datetime.now().isoformat(),\n                \"filters\": {\n                    fname: params for fname, params in zip(filter_names, all_params)\n                },\n            }\n\n            ds_out[filtered_var_name].attrs = metadata\n            ds_out[mask_var_name].attrs = metadata\n\n        else:\n            raise ValueError(f\"Unknown mode: {mode}\")\n\n        return ds_out\n\n    def summary(self) -&gt; str:\n        \"\"\"Return a human-readable summary of the pipeline.\"\"\"\n        lines = [f\"Filter Pipeline for '{self.var_name}':\", \"\"]\n        for i, (filter_obj, kwargs) in enumerate(self.filters):\n            lines.append(f\"{i + 1}. {filter_obj.name}\")\n            for key, val in kwargs.items():\n                lines.append(f\"   - {key}: {val}\")\n        return \"\\n\".join(lines)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.FilterPipeline.__init__","level":3,"title":"<code>__init__(ds, var_name='VOD')</code>","text":"<p>Initialize the filter pipeline.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.FilterPipeline.__init__--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Input dataset. var_name : str, default \"VOD\"     Variable to filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def __init__(self, ds: xr.Dataset, var_name: str = \"VOD\") -&gt; None:\n    \"\"\"Initialize the filter pipeline.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Input dataset.\n    var_name : str, default \"VOD\"\n        Variable to filter.\n\n    \"\"\"\n    self.ds = ds\n    self.var_name = var_name\n    self.filters: list[tuple[Filter, dict]] = []\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.FilterPipeline.add_filter","level":3,"title":"<code>add_filter(filter_obj, **kwargs)</code>","text":"<p>Add filter to pipeline.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.FilterPipeline.add_filter--parameters","level":5,"title":"Parameters","text":"<p>filter_obj : Filter or str     Filter instance or short name     (<code>'zscore'</code>, <code>'iqr'</code>, <code>'range'</code>, <code>'percentile'</code>). **kwargs     Parameters forwarded to <code>compute_mask</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.FilterPipeline.add_filter--returns","level":5,"title":"Returns","text":"<p>FilterPipeline     Self (for chaining).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def add_filter(self, filter_obj: Filter | str, **kwargs: Any) -&gt; FilterPipeline:\n    \"\"\"Add filter to pipeline.\n\n    Parameters\n    ----------\n    filter_obj : Filter or str\n        Filter instance or short name\n        (``'zscore'``, ``'iqr'``, ``'range'``, ``'percentile'``).\n    **kwargs\n        Parameters forwarded to ``compute_mask``.\n\n    Returns\n    -------\n    FilterPipeline\n        Self (for chaining).\n\n    \"\"\"\n    if isinstance(filter_obj, str):\n        _filter_map = {\n            \"zscore\": ZScoreFilter,\n            \"iqr\": IQRFilter,\n            \"range\": RangeFilter,\n            \"percentile\": PercentileFilter,\n        }\n        if filter_obj not in _filter_map:\n            raise ValueError(f\"Unknown filter: {filter_obj}\")\n        filter_obj = _filter_map[filter_obj]()\n\n    self.filters.append((filter_obj, kwargs))\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.FilterPipeline.apply","level":3,"title":"<code>apply(mode='sequential', output_name=None)</code>","text":"<p>Apply all filters in the pipeline.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.FilterPipeline.apply--parameters","level":5,"title":"Parameters","text":"<p>mode : {'sequential', 'combined'}     <code>'sequential'</code> – masks accumulate (AND) after each filter;     intermediate filtered variables are written.     <code>'combined'</code>   – all masks computed independently on the     original data, then AND-ed once. output_name : str, optional     Alias for the final filtered variable.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.FilterPipeline.apply--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with filtered variables appended.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def apply(\n    self, mode: str = \"sequential\", output_name: str | None = None\n) -&gt; xr.Dataset:\n    \"\"\"Apply all filters in the pipeline.\n\n    Parameters\n    ----------\n    mode : {'sequential', 'combined'}\n        ``'sequential'`` – masks accumulate (AND) after each filter;\n        intermediate filtered variables are written.\n        ``'combined'``   – all masks computed independently on the\n        original data, then AND-ed once.\n    output_name : str, optional\n        Alias for the final filtered variable.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with filtered variables appended.\n\n    \"\"\"\n    if not self.filters:\n        raise ValueError(\"No filters in pipeline\")\n\n    ds_out = self.ds.copy()\n\n    if mode == \"sequential\":\n        masks: list[xr.DataArray] = []\n        filter_names: list[str] = []\n        all_params: list[dict] = []\n\n        for filter_obj, kwargs in self.filters:\n            mask = filter_obj.compute_mask(ds_out[self.var_name], **kwargs)\n            masks.append(mask)\n            filter_names.append(filter_obj.name)\n            all_params.append(kwargs)\n\n            # Store individual mask\n            mask_name = f\"mask_{filter_obj.name}\"\n            if mask_name not in ds_out:\n                ds_out[mask_name] = mask\n                ds_out[mask_name].attrs = {\n                    \"filter_type\": filter_obj.name,\n                    **kwargs,\n                }\n\n            # Accumulate masks (AND)\n            cumulative_mask = masks[0]\n            for m in masks[1:]:\n                cumulative_mask = cumulative_mask &amp; m\n\n            cumulative_suffix = \"_\".join(filter_names)\n            filtered_data = ds_out[self.var_name].where(cumulative_mask)\n\n            filtered_var_name = f\"{self.var_name}_filtered_{cumulative_suffix}\"\n            cumulative_mask_name = f\"mask_{cumulative_suffix}\"\n\n            ds_out[filtered_var_name] = filtered_data\n            ds_out[cumulative_mask_name] = cumulative_mask\n\n            n_total = int(cumulative_mask.size)\n            n_removed = int((~cumulative_mask).sum().values)\n\n            metadata: dict = {\n                \"filter_chain\": filter_names.copy(),\n                \"mode\": \"sequential\",\n                \"applied_to\": self.var_name,\n                \"n_total\": n_total,\n                \"n_removed\": n_removed,\n                \"fraction_removed\": float(n_removed / n_total),\n                \"timestamp\": datetime.now().isoformat(),\n                \"filters\": {\n                    fname: params for fname, params in zip(filter_names, all_params)\n                },\n            }\n\n            ds_out[filtered_var_name].attrs = metadata\n            ds_out[cumulative_mask_name].attrs = metadata\n\n        if output_name:\n            final_var = f\"{self.var_name}_filtered_{'_'.join(filter_names)}\"\n            final_mask = f\"mask_{'_'.join(filter_names)}\"\n\n            ds_out[f\"{self.var_name}_filtered_{output_name}\"] = ds_out[final_var]\n            ds_out[f\"mask_{output_name}\"] = ds_out[final_mask]\n            ds_out[f\"{self.var_name}_filtered_{output_name}\"].attrs = ds_out[\n                final_var\n            ].attrs\n            ds_out[f\"mask_{output_name}\"].attrs = ds_out[final_mask].attrs\n\n    elif mode == \"combined\":\n        masks = []\n        filter_names = []\n        all_params = []\n\n        for filter_obj, kwargs in self.filters:\n            mask = filter_obj.compute_mask(ds_out[self.var_name], **kwargs)\n            masks.append(mask)\n            filter_names.append(filter_obj.name)\n            all_params.append(kwargs)\n\n            mask_name = f\"mask_{filter_obj.name}\"\n            if mask_name not in ds_out:\n                ds_out[mask_name] = mask\n                ds_out[mask_name].attrs = {\n                    \"filter_type\": filter_obj.name,\n                    **kwargs,\n                }\n\n        combined_mask = masks[0]\n        for mask in masks[1:]:\n            combined_mask = combined_mask &amp; mask\n\n        suffix = output_name or \"combined\"\n        filtered_data = ds_out[self.var_name].where(combined_mask)\n\n        filtered_var_name = f\"{self.var_name}_filtered_{suffix}\"\n        mask_var_name = f\"mask_{suffix}\"\n\n        ds_out[filtered_var_name] = filtered_data\n        ds_out[mask_var_name] = combined_mask\n\n        n_total = int(combined_mask.size)\n        n_removed = int((~combined_mask).sum().values)\n\n        metadata = {\n            \"filter_chain\": filter_names,\n            \"mode\": \"combined\",\n            \"applied_to\": self.var_name,\n            \"n_total\": n_total,\n            \"n_removed\": n_removed,\n            \"fraction_removed\": float(n_removed / n_total),\n            \"timestamp\": datetime.now().isoformat(),\n            \"filters\": {\n                fname: params for fname, params in zip(filter_names, all_params)\n            },\n        }\n\n        ds_out[filtered_var_name].attrs = metadata\n        ds_out[mask_var_name].attrs = metadata\n\n    else:\n        raise ValueError(f\"Unknown mode: {mode}\")\n\n    return ds_out\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.FilterPipeline.summary","level":3,"title":"<code>summary()</code>","text":"<p>Return a human-readable summary of the pipeline.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Return a human-readable summary of the pipeline.\"\"\"\n    lines = [f\"Filter Pipeline for '{self.var_name}':\", \"\"]\n    for i, (filter_obj, kwargs) in enumerate(self.filters):\n        lines.append(f\"{i + 1}. {filter_obj.name}\")\n        for key, val in kwargs.items():\n            lines.append(f\"   - {key}: {val}\")\n    return \"\\n\".join(lines)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.create_zscore_filter","level":2,"title":"<code>create_zscore_filter(ds, var_name='VOD', threshold=3.0, suffix='zscore')</code>","text":"<p>One-liner z-score filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def create_zscore_filter(\n    ds: xr.Dataset,\n    var_name: str = \"VOD\",\n    threshold: float = 3.0,\n    suffix: str = \"zscore\",\n) -&gt; xr.Dataset:\n    \"\"\"One-liner z-score filter.\"\"\"\n    return ZScoreFilter().apply(ds, var_name, suffix, threshold=threshold)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.filtering.create_range_filter","level":2,"title":"<code>create_range_filter(ds, var_name='VOD', min_value=None, max_value=None, suffix='range')</code>","text":"<p>One-liner range filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/filtering.py</code> <pre><code>def create_range_filter(\n    ds: xr.Dataset,\n    var_name: str = \"VOD\",\n    min_value: float | None = None,\n    max_value: float | None = None,\n    suffix: str = \"range\",\n) -&gt; xr.Dataset:\n    \"\"\"One-liner range filter.\"\"\"\n    return RangeFilter().apply(\n        ds, var_name, suffix, min_value=min_value, max_value=max_value\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#per-cell-filtering","level":3,"title":"Per-Cell Filtering","text":"<p>Per-cell outlier filters for gridded VOD data.</p> <p>Unlike the global filters in :mod:<code>~canvod.grids.analysis.filtering</code>, these operate independently on each grid cell, preserving spatial structure while removing temporal outliers within cells.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering--classes","level":3,"title":"Classes","text":"<p>PerCellFilter               – abstract base with auto cell-id detection. PerCellIQRFilter            – per-cell IQR rejection. PerCellZScoreFilter         – per-cell z-score rejection. PerCellRangeFilter          – per-cell hard bounds. PerCellPercentileFilter     – per-cell percentile bounds. PerCellFilterPipeline       – sequential or combined multi-filter application.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering--convenience-functions","level":3,"title":"Convenience functions","text":"<p><code>create_per_cell_iqr_filter</code>     – one-liner per-cell IQR. <code>create_per_cell_zscore_filter</code>  – one-liner per-cell z-score.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilter","level":2,"title":"<code>PerCellFilter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for per-cell filtering operations.</p> <p>Sub-classes implement :meth:<code>compute_cell_mask</code> for a single cell's 1-D data array; the base class handles iteration over cells, auto-detection of the <code>cell_id_*</code> variable, and output assembly.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>class PerCellFilter(ABC):\n    \"\"\"Base class for per-cell filtering operations.\n\n    Sub-classes implement :meth:`compute_cell_mask` for a single cell's\n    1-D data array; the base class handles iteration over cells,\n    auto-detection of the ``cell_id_*`` variable, and output assembly.\n    \"\"\"\n\n    def __init__(self, filter_name: str) -&gt; None:\n        \"\"\"Initialize the per-cell filter.\n\n        Parameters\n        ----------\n        filter_name : str\n            Filter name.\n\n        \"\"\"\n        self.filter_name = filter_name\n\n    @abstractmethod\n    def compute_cell_mask(\n        self,\n        cell_data: np.ndarray,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        \"\"\"Return a boolean keep-mask for one cell's data.\n\n        Parameters\n        ----------\n        cell_data : np.ndarray\n            1-D array of values for one cell across time.\n\n        Returns\n        -------\n        np.ndarray\n            Boolean mask (True = keep).\n\n        \"\"\"\n        ...\n\n    def apply(\n        self,\n        ds: xr.Dataset,\n        var_name: str = \"VOD\",\n        cell_id_var: str | None = None,\n        output_suffix: str | None = None,\n        min_observations: int = 5,\n        **kwargs: Any,\n    ) -&gt; xr.Dataset:\n        \"\"\"Apply per-cell filtering to *ds*.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Input dataset (must contain a ``cell_id_*`` variable).\n        var_name : str\n            Variable to filter.\n        cell_id_var : str, optional\n            Cell-ID variable name.  Auto-detected from ``cell_id_*`` if *None*.\n        output_suffix : str, optional\n            Suffix for output variables (default: ``filter_name``).\n        min_observations : int\n            Minimum observations per cell required for filtering.\n        **kwargs\n            Forwarded to :meth:`compute_cell_mask`.\n\n        Returns\n        -------\n        xr.Dataset\n            Copy of *ds* with ``&lt;var&gt;_filtered_&lt;suffix&gt;`` and\n            ``mask_&lt;suffix&gt;`` appended.\n\n        \"\"\"\n        if output_suffix is None:\n            output_suffix = self.filter_name\n\n        # Auto-detect cell_id variable\n        if cell_id_var is None:\n            cell_id_vars = [v for v in ds.data_vars if v.startswith(\"cell_id_\")]\n            if not cell_id_vars:\n                raise ValueError(\"No cell_id variable found in dataset\")\n            cell_id_var = cell_id_vars[0]\n            logger.info(\"Auto-detected cell_id variable: %s\", cell_id_var)\n\n        if var_name not in ds:\n            raise ValueError(f\"Variable '{var_name}' not found in dataset\")\n        if cell_id_var not in ds:\n            raise ValueError(f\"Cell ID variable '{cell_id_var}' not found in dataset\")\n\n        logger.info(\"Applying %s filter per-cell to '%s'\", self.filter_name, var_name)\n\n        var_data = ds[var_name]\n        cell_ids = ds[cell_id_var]\n\n        filtered_data, mask = self._apply_per_cell_filtering(\n            var_data, cell_ids, min_observations, **kwargs\n        )\n\n        result = ds.copy()\n        result[f\"{var_name}_filtered_{output_suffix}\"] = filtered_data\n        result[f\"mask_{output_suffix}\"] = mask\n\n        result[f\"{var_name}_filtered_{output_suffix}\"].attrs.update(\n            {\n                \"filter_type\": self.filter_name,\n                \"filter_params\": str(kwargs),\n                \"min_observations\": min_observations,\n                \"source_variable\": var_name,\n            }\n        )\n\n        return result\n\n    def _apply_per_cell_filtering(\n        self,\n        var_data: xr.DataArray,\n        cell_ids: xr.DataArray,\n        min_observations: int,\n        **kwargs: Any,\n    ) -&gt; tuple[xr.DataArray, xr.DataArray]:\n        \"\"\"Iterate over unique cells and apply the mask function.\n\n        Parameters\n        ----------\n        var_data : xr.DataArray\n            Data to filter.\n        cell_ids : xr.DataArray\n            Cell ID assignments.\n        min_observations : int\n            Minimum observations per cell.\n        **kwargs : Any\n            Forwarded to ``compute_cell_mask``.\n\n        Returns\n        -------\n        Tuple[xr.DataArray, xr.DataArray]\n            Filtered values and mask arrays.\n\n        \"\"\"\n        values = var_data.values\n        cells = cell_ids.values\n\n        filtered_values = values.copy()\n        mask_values = np.ones_like(values, dtype=bool)\n\n        unique_cells = np.unique(cells[np.isfinite(cells)])\n        logger.info(\"Processing %d unique cells\", len(unique_cells))\n\n        cells_processed = 0\n        cells_filtered = 0\n        total_filtered = 0\n\n        for cell_id in unique_cells:\n            cell_mask = (cells == cell_id) &amp; np.isfinite(values) &amp; np.isfinite(cells)\n            cell_indices = np.where(cell_mask)\n\n            if len(cell_indices[0]) &lt; min_observations:\n                continue\n\n            cell_data = values[cell_mask]\n            cell_filter_mask = self.compute_cell_mask(cell_data, **kwargs)\n\n            mask_values[cell_mask] = cell_filter_mask\n            filtered_values[cell_mask] = np.where(cell_filter_mask, cell_data, np.nan)\n\n            cells_processed += 1\n            if not np.all(cell_filter_mask):\n                cells_filtered += 1\n                total_filtered += int(np.sum(~cell_filter_mask))\n\n        logger.info(\n            \"Processed %d cells, filtered %d cells, removed %d observations\",\n            cells_processed,\n            cells_filtered,\n            total_filtered,\n        )\n\n        filtered_da = xr.DataArray(\n            filtered_values,\n            dims=var_data.dims,\n            coords=var_data.coords,\n            attrs=var_data.attrs,\n        )\n        mask_da = xr.DataArray(mask_values, dims=var_data.dims, coords=var_data.coords)\n\n        return filtered_da, mask_da\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilter.__init__","level":3,"title":"<code>__init__(filter_name)</code>","text":"<p>Initialize the per-cell filter.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilter.__init__--parameters","level":5,"title":"Parameters","text":"<p>filter_name : str     Filter name.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def __init__(self, filter_name: str) -&gt; None:\n    \"\"\"Initialize the per-cell filter.\n\n    Parameters\n    ----------\n    filter_name : str\n        Filter name.\n\n    \"\"\"\n    self.filter_name = filter_name\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilter.compute_cell_mask","level":3,"title":"<code>compute_cell_mask(cell_data, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Return a boolean keep-mask for one cell's data.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilter.compute_cell_mask--parameters","level":5,"title":"Parameters","text":"<p>cell_data : np.ndarray     1-D array of values for one cell across time.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilter.compute_cell_mask--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Boolean mask (True = keep).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>@abstractmethod\ndef compute_cell_mask(\n    self,\n    cell_data: np.ndarray,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    \"\"\"Return a boolean keep-mask for one cell's data.\n\n    Parameters\n    ----------\n    cell_data : np.ndarray\n        1-D array of values for one cell across time.\n\n    Returns\n    -------\n    np.ndarray\n        Boolean mask (True = keep).\n\n    \"\"\"\n    ...\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilter.apply","level":3,"title":"<code>apply(ds, var_name='VOD', cell_id_var=None, output_suffix=None, min_observations=5, **kwargs)</code>","text":"<p>Apply per-cell filtering to ds.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilter.apply--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Input dataset (must contain a <code>cell_id_*</code> variable). var_name : str     Variable to filter. cell_id_var : str, optional     Cell-ID variable name.  Auto-detected from <code>cell_id_*</code> if None. output_suffix : str, optional     Suffix for output variables (default: <code>filter_name</code>). min_observations : int     Minimum observations per cell required for filtering. **kwargs     Forwarded to :meth:<code>compute_cell_mask</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilter.apply--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Copy of ds with <code>&lt;var&gt;_filtered_&lt;suffix&gt;</code> and     <code>mask_&lt;suffix&gt;</code> appended.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def apply(\n    self,\n    ds: xr.Dataset,\n    var_name: str = \"VOD\",\n    cell_id_var: str | None = None,\n    output_suffix: str | None = None,\n    min_observations: int = 5,\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"Apply per-cell filtering to *ds*.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Input dataset (must contain a ``cell_id_*`` variable).\n    var_name : str\n        Variable to filter.\n    cell_id_var : str, optional\n        Cell-ID variable name.  Auto-detected from ``cell_id_*`` if *None*.\n    output_suffix : str, optional\n        Suffix for output variables (default: ``filter_name``).\n    min_observations : int\n        Minimum observations per cell required for filtering.\n    **kwargs\n        Forwarded to :meth:`compute_cell_mask`.\n\n    Returns\n    -------\n    xr.Dataset\n        Copy of *ds* with ``&lt;var&gt;_filtered_&lt;suffix&gt;`` and\n        ``mask_&lt;suffix&gt;`` appended.\n\n    \"\"\"\n    if output_suffix is None:\n        output_suffix = self.filter_name\n\n    # Auto-detect cell_id variable\n    if cell_id_var is None:\n        cell_id_vars = [v for v in ds.data_vars if v.startswith(\"cell_id_\")]\n        if not cell_id_vars:\n            raise ValueError(\"No cell_id variable found in dataset\")\n        cell_id_var = cell_id_vars[0]\n        logger.info(\"Auto-detected cell_id variable: %s\", cell_id_var)\n\n    if var_name not in ds:\n        raise ValueError(f\"Variable '{var_name}' not found in dataset\")\n    if cell_id_var not in ds:\n        raise ValueError(f\"Cell ID variable '{cell_id_var}' not found in dataset\")\n\n    logger.info(\"Applying %s filter per-cell to '%s'\", self.filter_name, var_name)\n\n    var_data = ds[var_name]\n    cell_ids = ds[cell_id_var]\n\n    filtered_data, mask = self._apply_per_cell_filtering(\n        var_data, cell_ids, min_observations, **kwargs\n    )\n\n    result = ds.copy()\n    result[f\"{var_name}_filtered_{output_suffix}\"] = filtered_data\n    result[f\"mask_{output_suffix}\"] = mask\n\n    result[f\"{var_name}_filtered_{output_suffix}\"].attrs.update(\n        {\n            \"filter_type\": self.filter_name,\n            \"filter_params\": str(kwargs),\n            \"min_observations\": min_observations,\n            \"source_variable\": var_name,\n        }\n    )\n\n    return result\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellIQRFilter","level":2,"title":"<code>PerCellIQRFilter</code>","text":"<p>               Bases: <code>PerCellFilter</code></p> <p>Per-cell IQR outlier rejection.</p> <p>Values outside <code>[Q1 − factor·IQR, Q3 + factor·IQR]</code> are removed independently within each cell.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>class PerCellIQRFilter(PerCellFilter):\n    \"\"\"Per-cell IQR outlier rejection.\n\n    Values outside ``[Q1 − factor·IQR, Q3 + factor·IQR]`` are removed\n    independently within each cell.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the filter.\"\"\"\n        super().__init__(\"iqr\")\n\n    def compute_cell_mask(\n        self, cell_data: np.ndarray, factor: float = 1.5\n    ) -&gt; np.ndarray:\n        \"\"\"IQR mask for a single cell.\n\n        Parameters\n        ----------\n        cell_data : np.ndarray\n            1-D cell data.\n        factor : float\n            IQR multiplier (default 1.5).\n\n        Returns\n        -------\n        np.ndarray\n            Boolean mask.\n\n        \"\"\"\n        valid_data = cell_data[np.isfinite(cell_data)]\n        if len(valid_data) &lt; 4:\n            return np.ones_like(cell_data, dtype=bool)\n\n        q1 = np.percentile(valid_data, 25)\n        q3 = np.percentile(valid_data, 75)\n        iqr = q3 - q1\n\n        if iqr == 0:\n            return np.ones_like(cell_data, dtype=bool)\n\n        lower_bound = q1 - factor * iqr\n        upper_bound = q3 + factor * iqr\n\n        return (cell_data &gt;= lower_bound) &amp; (cell_data &lt;= upper_bound)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellIQRFilter.__init__","level":3,"title":"<code>__init__()</code>","text":"<p>Initialize the filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the filter.\"\"\"\n    super().__init__(\"iqr\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellIQRFilter.compute_cell_mask","level":3,"title":"<code>compute_cell_mask(cell_data, factor=1.5)</code>","text":"<p>IQR mask for a single cell.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellIQRFilter.compute_cell_mask--parameters","level":5,"title":"Parameters","text":"<p>cell_data : np.ndarray     1-D cell data. factor : float     IQR multiplier (default 1.5).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellIQRFilter.compute_cell_mask--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Boolean mask.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def compute_cell_mask(\n    self, cell_data: np.ndarray, factor: float = 1.5\n) -&gt; np.ndarray:\n    \"\"\"IQR mask for a single cell.\n\n    Parameters\n    ----------\n    cell_data : np.ndarray\n        1-D cell data.\n    factor : float\n        IQR multiplier (default 1.5).\n\n    Returns\n    -------\n    np.ndarray\n        Boolean mask.\n\n    \"\"\"\n    valid_data = cell_data[np.isfinite(cell_data)]\n    if len(valid_data) &lt; 4:\n        return np.ones_like(cell_data, dtype=bool)\n\n    q1 = np.percentile(valid_data, 25)\n    q3 = np.percentile(valid_data, 75)\n    iqr = q3 - q1\n\n    if iqr == 0:\n        return np.ones_like(cell_data, dtype=bool)\n\n    lower_bound = q1 - factor * iqr\n    upper_bound = q3 + factor * iqr\n\n    return (cell_data &gt;= lower_bound) &amp; (cell_data &lt;= upper_bound)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellZScoreFilter","level":2,"title":"<code>PerCellZScoreFilter</code>","text":"<p>               Bases: <code>PerCellFilter</code></p> <p>Per-cell z-score outlier rejection.</p> <p>Values with <code>|z| &gt; threshold</code> are removed independently within each cell.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>class PerCellZScoreFilter(PerCellFilter):\n    \"\"\"Per-cell z-score outlier rejection.\n\n    Values with ``|z| &gt; threshold`` are removed independently within each cell.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the filter.\"\"\"\n        super().__init__(\"zscore\")\n\n    def compute_cell_mask(\n        self, cell_data: np.ndarray, threshold: float = 3.0\n    ) -&gt; np.ndarray:\n        \"\"\"Z-score mask for a single cell.\n\n        Parameters\n        ----------\n        cell_data : np.ndarray\n            1-D cell data.\n        threshold : float\n            Z-score threshold (default 3.0).\n\n        Returns\n        -------\n        np.ndarray\n            Boolean mask.\n\n        \"\"\"\n        if len(cell_data) &lt; 3:\n            return np.ones_like(cell_data, dtype=bool)\n\n        mean = np.nanmean(cell_data)\n        std = np.nanstd(cell_data)\n\n        if std == 0:\n            return np.ones_like(cell_data, dtype=bool)\n\n        z_scores = np.abs((cell_data - mean) / std)\n        return z_scores &lt;= threshold\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellZScoreFilter.__init__","level":3,"title":"<code>__init__()</code>","text":"<p>Initialize the filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the filter.\"\"\"\n    super().__init__(\"zscore\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellZScoreFilter.compute_cell_mask","level":3,"title":"<code>compute_cell_mask(cell_data, threshold=3.0)</code>","text":"<p>Z-score mask for a single cell.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellZScoreFilter.compute_cell_mask--parameters","level":5,"title":"Parameters","text":"<p>cell_data : np.ndarray     1-D cell data. threshold : float     Z-score threshold (default 3.0).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellZScoreFilter.compute_cell_mask--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Boolean mask.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def compute_cell_mask(\n    self, cell_data: np.ndarray, threshold: float = 3.0\n) -&gt; np.ndarray:\n    \"\"\"Z-score mask for a single cell.\n\n    Parameters\n    ----------\n    cell_data : np.ndarray\n        1-D cell data.\n    threshold : float\n        Z-score threshold (default 3.0).\n\n    Returns\n    -------\n    np.ndarray\n        Boolean mask.\n\n    \"\"\"\n    if len(cell_data) &lt; 3:\n        return np.ones_like(cell_data, dtype=bool)\n\n    mean = np.nanmean(cell_data)\n    std = np.nanstd(cell_data)\n\n    if std == 0:\n        return np.ones_like(cell_data, dtype=bool)\n\n    z_scores = np.abs((cell_data - mean) / std)\n    return z_scores &lt;= threshold\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellRangeFilter","level":2,"title":"<code>PerCellRangeFilter</code>","text":"<p>               Bases: <code>PerCellFilter</code></p> <p>Per-cell hard-bound range filter.</p> <p>Identical bounds applied to every cell.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>class PerCellRangeFilter(PerCellFilter):\n    \"\"\"Per-cell hard-bound range filter.\n\n    Identical bounds applied to every cell.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the filter.\"\"\"\n        super().__init__(\"range\")\n\n    def compute_cell_mask(\n        self,\n        cell_data: np.ndarray,\n        min_value: float | None = None,\n        max_value: float | None = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Range mask for a single cell.\n\n        Parameters\n        ----------\n        cell_data : np.ndarray\n            1-D cell data.\n        min_value : float, optional\n            Minimum allowed value.\n        max_value : float, optional\n            Maximum allowed value.\n\n        Returns\n        -------\n        np.ndarray\n            Boolean mask.\n\n        \"\"\"\n        mask = np.ones_like(cell_data, dtype=bool)\n        if min_value is not None:\n            mask = mask &amp; (cell_data &gt;= min_value)\n        if max_value is not None:\n            mask = mask &amp; (cell_data &lt;= max_value)\n        return mask\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellRangeFilter.__init__","level":3,"title":"<code>__init__()</code>","text":"<p>Initialize the filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the filter.\"\"\"\n    super().__init__(\"range\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellRangeFilter.compute_cell_mask","level":3,"title":"<code>compute_cell_mask(cell_data, min_value=None, max_value=None)</code>","text":"<p>Range mask for a single cell.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellRangeFilter.compute_cell_mask--parameters","level":5,"title":"Parameters","text":"<p>cell_data : np.ndarray     1-D cell data. min_value : float, optional     Minimum allowed value. max_value : float, optional     Maximum allowed value.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellRangeFilter.compute_cell_mask--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Boolean mask.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def compute_cell_mask(\n    self,\n    cell_data: np.ndarray,\n    min_value: float | None = None,\n    max_value: float | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Range mask for a single cell.\n\n    Parameters\n    ----------\n    cell_data : np.ndarray\n        1-D cell data.\n    min_value : float, optional\n        Minimum allowed value.\n    max_value : float, optional\n        Maximum allowed value.\n\n    Returns\n    -------\n    np.ndarray\n        Boolean mask.\n\n    \"\"\"\n    mask = np.ones_like(cell_data, dtype=bool)\n    if min_value is not None:\n        mask = mask &amp; (cell_data &gt;= min_value)\n    if max_value is not None:\n        mask = mask &amp; (cell_data &lt;= max_value)\n    return mask\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellPercentileFilter","level":2,"title":"<code>PerCellPercentileFilter</code>","text":"<p>               Bases: <code>PerCellFilter</code></p> <p>Per-cell percentile-bound filter.</p> <p>Bounds are computed independently within each cell.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>class PerCellPercentileFilter(PerCellFilter):\n    \"\"\"Per-cell percentile-bound filter.\n\n    Bounds are computed independently within each cell.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the filter.\"\"\"\n        super().__init__(\"percentile\")\n\n    def compute_cell_mask(\n        self, cell_data: np.ndarray, lower: float = 5.0, upper: float = 95.0\n    ) -&gt; np.ndarray:\n        \"\"\"Percentile mask for a single cell.\n\n        Parameters\n        ----------\n        cell_data : np.ndarray\n            1-D cell data.\n        lower : float\n            Lower percentile (0–100).\n        upper : float\n            Upper percentile (0–100).\n\n        Returns\n        -------\n        np.ndarray\n            Boolean mask.\n\n        \"\"\"\n        valid_data = cell_data[np.isfinite(cell_data)]\n        if len(valid_data) &lt; 5:\n            return np.ones_like(cell_data, dtype=bool)\n\n        lower_val = np.percentile(valid_data, lower)\n        upper_val = np.percentile(valid_data, upper)\n\n        return (cell_data &gt;= lower_val) &amp; (cell_data &lt;= upper_val)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellPercentileFilter.__init__","level":3,"title":"<code>__init__()</code>","text":"<p>Initialize the filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the filter.\"\"\"\n    super().__init__(\"percentile\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellPercentileFilter.compute_cell_mask","level":3,"title":"<code>compute_cell_mask(cell_data, lower=5.0, upper=95.0)</code>","text":"<p>Percentile mask for a single cell.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellPercentileFilter.compute_cell_mask--parameters","level":5,"title":"Parameters","text":"<p>cell_data : np.ndarray     1-D cell data. lower : float     Lower percentile (0–100). upper : float     Upper percentile (0–100).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellPercentileFilter.compute_cell_mask--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Boolean mask.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def compute_cell_mask(\n    self, cell_data: np.ndarray, lower: float = 5.0, upper: float = 95.0\n) -&gt; np.ndarray:\n    \"\"\"Percentile mask for a single cell.\n\n    Parameters\n    ----------\n    cell_data : np.ndarray\n        1-D cell data.\n    lower : float\n        Lower percentile (0–100).\n    upper : float\n        Upper percentile (0–100).\n\n    Returns\n    -------\n    np.ndarray\n        Boolean mask.\n\n    \"\"\"\n    valid_data = cell_data[np.isfinite(cell_data)]\n    if len(valid_data) &lt; 5:\n        return np.ones_like(cell_data, dtype=bool)\n\n    lower_val = np.percentile(valid_data, lower)\n    upper_val = np.percentile(valid_data, upper)\n\n    return (cell_data &gt;= lower_val) &amp; (cell_data &lt;= upper_val)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilterPipeline","level":2,"title":"<code>PerCellFilterPipeline</code>","text":"<p>Sequential or combined multi-filter pipeline for per-cell filtering.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilterPipeline--parameters","level":4,"title":"Parameters","text":"<p>ds : xr.Dataset     Input dataset. var_name : str     Variable to filter (default: <code>'VOD'</code>).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>class PerCellFilterPipeline:\n    \"\"\"Sequential or combined multi-filter pipeline for per-cell filtering.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Input dataset.\n    var_name : str\n        Variable to filter (default: ``'VOD'``).\n\n    \"\"\"\n\n    def __init__(self, ds: xr.Dataset, var_name: str = \"VOD\") -&gt; None:\n        \"\"\"Initialize the filter pipeline.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Input dataset.\n        var_name : str, default \"VOD\"\n            Variable to filter.\n\n        \"\"\"\n        self.ds = ds\n        self.var_name = var_name\n        self.filters: list[tuple[PerCellFilter, dict]] = []\n\n    def add_filter(\n        self,\n        filter_obj: PerCellFilter | str,\n        **kwargs: Any,\n    ) -&gt; PerCellFilterPipeline:\n        \"\"\"Add a filter.\n\n        Parameters\n        ----------\n        filter_obj : PerCellFilter or str\n            Filter instance or short name\n            (``'iqr'``, ``'zscore'``, ``'range'``, ``'percentile'``).\n        **kwargs\n            Parameters forwarded to the filter.\n\n        Returns\n        -------\n        PerCellFilterPipeline\n            Self (for chaining).\n\n        \"\"\"\n        if isinstance(filter_obj, str):\n            _filter_map = {\n                \"iqr\": PerCellIQRFilter,\n                \"zscore\": PerCellZScoreFilter,\n                \"range\": PerCellRangeFilter,\n                \"percentile\": PerCellPercentileFilter,\n            }\n            if filter_obj not in _filter_map:\n                raise ValueError(f\"Unknown filter: {filter_obj}\")\n            filter_obj = _filter_map[filter_obj]()\n\n        self.filters.append((filter_obj, kwargs))\n        return self\n\n    def apply(\n        self, mode: str = \"sequential\", output_name: str | None = None\n    ) -&gt; xr.Dataset:\n        \"\"\"Apply all filters.\n\n        Parameters\n        ----------\n        mode : {'sequential', 'combined'}\n            ``'sequential'`` – each filter operates on the previous output.\n            ``'combined'``   – all masks computed on the original, then AND-ed.\n        output_name : str, optional\n            Alias for the final filtered variable.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with filtered results.\n\n        \"\"\"\n        if not self.filters:\n            raise ValueError(\"No filters added to pipeline\")\n\n        result = self.ds.copy()\n        current_var = self.var_name\n\n        if mode == \"sequential\":\n            for i, (filter_obj, kwargs) in enumerate(self.filters):\n                suffix = (\n                    f\"{filter_obj.filter_name}_{i}\" if i &gt; 0 else filter_obj.filter_name\n                )\n                result = filter_obj.apply(\n                    result, current_var, output_suffix=suffix, **kwargs\n                )\n                current_var = f\"{self.var_name}_filtered_{suffix}\"\n\n        elif mode == \"combined\":\n            combined_mask = None\n            filter_names: list[str] = []\n\n            for filter_obj, kwargs in self.filters:\n                filtered = filter_obj.apply(result, self.var_name, **kwargs)\n                mask = filtered[f\"mask_{filter_obj.filter_name}\"]\n\n                if combined_mask is None:\n                    combined_mask = mask\n                else:\n                    combined_mask = combined_mask &amp; mask\n\n                filter_names.append(filter_obj.filter_name)\n\n            final_name = output_name or \"_\".join(filter_names)\n            result[f\"{self.var_name}_filtered_{final_name}\"] = result[\n                self.var_name\n            ].where(combined_mask)\n            result[f\"mask_{final_name}\"] = combined_mask\n\n        else:\n            raise ValueError(f\"Unknown mode: {mode}\")\n\n        return result\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilterPipeline.__init__","level":3,"title":"<code>__init__(ds, var_name='VOD')</code>","text":"<p>Initialize the filter pipeline.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilterPipeline.__init__--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Input dataset. var_name : str, default \"VOD\"     Variable to filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def __init__(self, ds: xr.Dataset, var_name: str = \"VOD\") -&gt; None:\n    \"\"\"Initialize the filter pipeline.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Input dataset.\n    var_name : str, default \"VOD\"\n        Variable to filter.\n\n    \"\"\"\n    self.ds = ds\n    self.var_name = var_name\n    self.filters: list[tuple[PerCellFilter, dict]] = []\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilterPipeline.add_filter","level":3,"title":"<code>add_filter(filter_obj, **kwargs)</code>","text":"<p>Add a filter.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilterPipeline.add_filter--parameters","level":5,"title":"Parameters","text":"<p>filter_obj : PerCellFilter or str     Filter instance or short name     (<code>'iqr'</code>, <code>'zscore'</code>, <code>'range'</code>, <code>'percentile'</code>). **kwargs     Parameters forwarded to the filter.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilterPipeline.add_filter--returns","level":5,"title":"Returns","text":"<p>PerCellFilterPipeline     Self (for chaining).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def add_filter(\n    self,\n    filter_obj: PerCellFilter | str,\n    **kwargs: Any,\n) -&gt; PerCellFilterPipeline:\n    \"\"\"Add a filter.\n\n    Parameters\n    ----------\n    filter_obj : PerCellFilter or str\n        Filter instance or short name\n        (``'iqr'``, ``'zscore'``, ``'range'``, ``'percentile'``).\n    **kwargs\n        Parameters forwarded to the filter.\n\n    Returns\n    -------\n    PerCellFilterPipeline\n        Self (for chaining).\n\n    \"\"\"\n    if isinstance(filter_obj, str):\n        _filter_map = {\n            \"iqr\": PerCellIQRFilter,\n            \"zscore\": PerCellZScoreFilter,\n            \"range\": PerCellRangeFilter,\n            \"percentile\": PerCellPercentileFilter,\n        }\n        if filter_obj not in _filter_map:\n            raise ValueError(f\"Unknown filter: {filter_obj}\")\n        filter_obj = _filter_map[filter_obj]()\n\n    self.filters.append((filter_obj, kwargs))\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilterPipeline.apply","level":3,"title":"<code>apply(mode='sequential', output_name=None)</code>","text":"<p>Apply all filters.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilterPipeline.apply--parameters","level":5,"title":"Parameters","text":"<p>mode : {'sequential', 'combined'}     <code>'sequential'</code> – each filter operates on the previous output.     <code>'combined'</code>   – all masks computed on the original, then AND-ed. output_name : str, optional     Alias for the final filtered variable.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.PerCellFilterPipeline.apply--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with filtered results.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def apply(\n    self, mode: str = \"sequential\", output_name: str | None = None\n) -&gt; xr.Dataset:\n    \"\"\"Apply all filters.\n\n    Parameters\n    ----------\n    mode : {'sequential', 'combined'}\n        ``'sequential'`` – each filter operates on the previous output.\n        ``'combined'``   – all masks computed on the original, then AND-ed.\n    output_name : str, optional\n        Alias for the final filtered variable.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with filtered results.\n\n    \"\"\"\n    if not self.filters:\n        raise ValueError(\"No filters added to pipeline\")\n\n    result = self.ds.copy()\n    current_var = self.var_name\n\n    if mode == \"sequential\":\n        for i, (filter_obj, kwargs) in enumerate(self.filters):\n            suffix = (\n                f\"{filter_obj.filter_name}_{i}\" if i &gt; 0 else filter_obj.filter_name\n            )\n            result = filter_obj.apply(\n                result, current_var, output_suffix=suffix, **kwargs\n            )\n            current_var = f\"{self.var_name}_filtered_{suffix}\"\n\n    elif mode == \"combined\":\n        combined_mask = None\n        filter_names: list[str] = []\n\n        for filter_obj, kwargs in self.filters:\n            filtered = filter_obj.apply(result, self.var_name, **kwargs)\n            mask = filtered[f\"mask_{filter_obj.filter_name}\"]\n\n            if combined_mask is None:\n                combined_mask = mask\n            else:\n                combined_mask = combined_mask &amp; mask\n\n            filter_names.append(filter_obj.filter_name)\n\n        final_name = output_name or \"_\".join(filter_names)\n        result[f\"{self.var_name}_filtered_{final_name}\"] = result[\n            self.var_name\n        ].where(combined_mask)\n        result[f\"mask_{final_name}\"] = combined_mask\n\n    else:\n        raise ValueError(f\"Unknown mode: {mode}\")\n\n    return result\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.create_per_cell_iqr_filter","level":2,"title":"<code>create_per_cell_iqr_filter(ds, var_name='VOD', factor=1.5, cell_id_var=None, min_observations=5)</code>","text":"<p>One-liner per-cell IQR filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def create_per_cell_iqr_filter(\n    ds: xr.Dataset,\n    var_name: str = \"VOD\",\n    factor: float = 1.5,\n    cell_id_var: str | None = None,\n    min_observations: int = 5,\n) -&gt; xr.Dataset:\n    \"\"\"One-liner per-cell IQR filter.\"\"\"\n    return PerCellIQRFilter().apply(\n        ds,\n        var_name,\n        cell_id_var=cell_id_var,\n        factor=factor,\n        min_observations=min_observations,\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_filtering.create_per_cell_zscore_filter","level":2,"title":"<code>create_per_cell_zscore_filter(ds, var_name='VOD', threshold=3.0, cell_id_var=None, min_observations=5)</code>","text":"<p>One-liner per-cell z-score filter.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_filtering.py</code> <pre><code>def create_per_cell_zscore_filter(\n    ds: xr.Dataset,\n    var_name: str = \"VOD\",\n    threshold: float = 3.0,\n    cell_id_var: str | None = None,\n    min_observations: int = 5,\n) -&gt; xr.Dataset:\n    \"\"\"One-liner per-cell z-score filter.\"\"\"\n    return PerCellZScoreFilter().apply(\n        ds,\n        var_name,\n        cell_id_var=cell_id_var,\n        threshold=threshold,\n        min_observations=min_observations,\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#hampel-filtering","level":3,"title":"Hampel Filtering","text":"<p>Parallelized Hampel filtering for gridded VOD data.</p> <p>Spatial-batch multiprocessing Hampel filter with complete temporal coverage (no temporal chunking).  Each (cell_id, SID) time series is filtered independently using median absolute deviation (MAD).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.hampel_filtering--functions","level":3,"title":"Functions","text":"<p><code>process_spatial_batch_worker</code>      – picklable worker for one spatial batch. <code>hampel_cell_sid_parallelized</code>      – main entry point (no temporal aggregation). <code>aggr_hampel_cell_sid_parallelized</code> – with optional temporal aggregation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.hampel_filtering--notes","level":3,"title":"Notes","text":"<ul> <li>Worker function is module-level so it can be pickled by   <code>multiprocessing.Pool</code>.</li> <li>Default spatial batch size is 500 cells; tune based on available   memory.</li> <li>Expected throughput: 300–700 K cell-SID combinations / s on a   typical multi-core machine.</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.hampel_filtering.process_spatial_batch_worker","level":2,"title":"<code>process_spatial_batch_worker(args)</code>","text":"<p>Process a single spatial batch for one SID.</p> <p>Designed to be pickled and dispatched to worker processes. Returns compact index lists to minimise IPC memory transfer.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.hampel_filtering.process_spatial_batch_worker--parameters","level":4,"title":"Parameters","text":"<p>args : tuple     <code>(batch_cells, vod_values_sid, cell_ids_sid, valid_indices,     threshold, min_obs_per_sid, batch_idx, sid_idx)</code></p> <pre><code>batch_cells : np.ndarray\n    Cell IDs in this batch.\nvod_values_sid : np.ndarray\n    VOD values for the current SID (valid entries only).\ncell_ids_sid : np.ndarray\n    Corresponding cell IDs (same length as *vod_values_sid*).\nvalid_indices : np.ndarray\n    Original epoch indices of the valid entries.\nthreshold : float\n    MAD threshold for outlier detection.\nmin_obs_per_sid : int\n    Minimum observations required per cell to run filter.\nbatch_idx : int\n    Batch index (for bookkeeping).\nsid_idx : int\n    SID index (for bookkeeping).\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.hampel_filtering.process_spatial_batch_worker--returns","level":4,"title":"Returns","text":"<p>dict     Keys: <code>batch_idx</code>, <code>sid_idx</code>, <code>outlier_indices</code>,     <code>processing_indices</code>, <code>combinations</code>, <code>filtered</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/hampel_filtering.py</code> <pre><code>def process_spatial_batch_worker(args: tuple) -&gt; dict:\n    \"\"\"Process a single spatial batch for one SID.\n\n    Designed to be pickled and dispatched to worker processes.\n    Returns compact index lists to minimise IPC memory transfer.\n\n    Parameters\n    ----------\n    args : tuple\n        ``(batch_cells, vod_values_sid, cell_ids_sid, valid_indices,\n        threshold, min_obs_per_sid, batch_idx, sid_idx)``\n\n        batch_cells : np.ndarray\n            Cell IDs in this batch.\n        vod_values_sid : np.ndarray\n            VOD values for the current SID (valid entries only).\n        cell_ids_sid : np.ndarray\n            Corresponding cell IDs (same length as *vod_values_sid*).\n        valid_indices : np.ndarray\n            Original epoch indices of the valid entries.\n        threshold : float\n            MAD threshold for outlier detection.\n        min_obs_per_sid : int\n            Minimum observations required per cell to run filter.\n        batch_idx : int\n            Batch index (for bookkeeping).\n        sid_idx : int\n            SID index (for bookkeeping).\n\n    Returns\n    -------\n    dict\n        Keys: ``batch_idx``, ``sid_idx``, ``outlier_indices``,\n        ``processing_indices``, ``combinations``, ``filtered``.\n\n    \"\"\"\n    (\n        batch_cells,\n        vod_values_sid,\n        cell_ids_sid,\n        valid_indices,\n        threshold,\n        min_obs_per_sid,\n        batch_idx,\n        sid_idx,\n    ) = args\n\n    batch_outlier_indices: list[int] = []\n    batch_processing_indices: list[int] = []\n    batch_combinations = 0\n    batch_filtered = 0\n\n    for cell_id in batch_cells:\n        batch_combinations += 1\n\n        cell_mask = cell_ids_sid == cell_id\n        if not np.any(cell_mask):\n            continue\n\n        cell_vod = vod_values_sid[cell_mask]\n        cell_indices = valid_indices[cell_mask]\n\n        if len(cell_vod) &lt; min_obs_per_sid:\n            continue\n\n        batch_processing_indices.extend(cell_indices)\n        batch_filtered += 1\n\n        median_val = np.median(cell_vod)\n        mad_val = np.median(np.abs(cell_vod - median_val))\n\n        if mad_val &gt; 0:\n            outliers = np.abs(cell_vod - median_val) &gt; threshold * mad_val\n            if np.any(outliers):\n                batch_outlier_indices.extend(cell_indices[outliers])\n\n    return {\n        \"batch_idx\": batch_idx,\n        \"sid_idx\": sid_idx,\n        \"outlier_indices\": batch_outlier_indices,\n        \"processing_indices\": batch_processing_indices,\n        \"combinations\": batch_combinations,\n        \"filtered\": batch_filtered,\n    }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.hampel_filtering.hampel_cell_sid_parallelized","level":2,"title":"<code>hampel_cell_sid_parallelized(vod_ds, grid_name='equal_area_2deg', threshold=3.0, min_obs_per_sid=20, spatial_batch_size=500, n_workers=None)</code>","text":"<p>Parallelized cell–SID Hampel filter with complete temporal coverage.</p> <p>Each (cell_id, SID) time series is filtered independently using global (non-chunked) statistics.  Spatial work is distributed across n_workers processes.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.hampel_filtering.hampel_cell_sid_parallelized--parameters","level":4,"title":"Parameters","text":"<p>vod_ds : xr.Dataset     Input dataset containing <code>'VOD'</code> and a <code>cell_id_&lt;grid_name&gt;</code>     variable, with dimensions <code>(epoch, sid)</code>. grid_name : str     Grid identifier used to locate the cell-ID variable, e.g.     <code>'equal_area_2deg'</code>. threshold : float     MAD multiplier for outlier detection. min_obs_per_sid : int     Minimum valid observations per cell-SID to run the filter. spatial_batch_size : int     Number of cells per parallel batch. n_workers : int or None     Number of worker processes.  Defaults to <code>min(cpu_count(), 8)</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.hampel_filtering.hampel_cell_sid_parallelized--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Copy of vod_ds with additional variables:</p> <pre><code>* ``VOD_filtered_hampel`` – filtered VOD (outliers set to NaN).\n* ``hampel_processing_mask`` – boolean mask of processed\n  observations.\n\nDataset-level attrs include full processing metadata.\n</code></pre> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/hampel_filtering.py</code> <pre><code>def hampel_cell_sid_parallelized(\n    vod_ds: xr.Dataset,\n    grid_name: str = \"equal_area_2deg\",\n    threshold: float = 3.0,\n    min_obs_per_sid: int = 20,\n    spatial_batch_size: int = 500,\n    n_workers: int | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Parallelized cell–SID Hampel filter with complete temporal coverage.\n\n    Each (cell_id, SID) time series is filtered independently using\n    global (non-chunked) statistics.  Spatial work is distributed\n    across *n_workers* processes.\n\n    Parameters\n    ----------\n    vod_ds : xr.Dataset\n        Input dataset containing ``'VOD'`` and a ``cell_id_&lt;grid_name&gt;``\n        variable, with dimensions ``(epoch, sid)``.\n    grid_name : str\n        Grid identifier used to locate the cell-ID variable, e.g.\n        ``'equal_area_2deg'``.\n    threshold : float\n        MAD multiplier for outlier detection.\n    min_obs_per_sid : int\n        Minimum valid observations per cell-SID to run the filter.\n    spatial_batch_size : int\n        Number of cells per parallel batch.\n    n_workers : int or None\n        Number of worker processes.  Defaults to ``min(cpu_count(), 8)``.\n\n    Returns\n    -------\n    xr.Dataset\n        Copy of *vod_ds* with additional variables:\n\n        * ``VOD_filtered_hampel`` – filtered VOD (outliers set to NaN).\n        * ``hampel_processing_mask`` – boolean mask of processed\n          observations.\n\n        Dataset-level attrs include full processing metadata.\n\n    \"\"\"\n    if n_workers is None:\n        n_workers = min(cpu_count(), 8)\n\n    cell_id_var = f\"cell_id_{grid_name}\"\n    n_epochs, n_sids = vod_ds.VOD.shape\n    cell_ids = vod_ds[cell_id_var].values\n    vod_values = vod_ds.VOD.values\n\n    logger.info(\n        \"Parallelized Hampel filter: shape=%s, threshold=%.1f, \"\n        \"min_obs=%d, workers=%d, batch_size=%d\",\n        vod_ds.VOD.shape,\n        threshold,\n        min_obs_per_sid,\n        n_workers,\n        spatial_batch_size,\n    )\n\n    # Unique cells and spatial batches\n    unique_cells = np.unique(cell_ids[np.isfinite(cell_ids)])\n    n_spatial_batches = int(np.ceil(len(unique_cells) / spatial_batch_size))\n\n    cell_batches = [\n        (unique_cells[i : i + spatial_batch_size], i // spatial_batch_size)\n        for i in range(0, len(unique_cells), spatial_batch_size)\n    ]\n\n    # Result arrays\n    outlier_mask = np.zeros((n_epochs, n_sids), dtype=bool)\n    processing_mask = np.zeros((n_epochs, n_sids), dtype=bool)\n\n    total_combinations_processed = 0\n    total_combinations_filtered = 0\n    overall_start = time.time()\n\n    for sid_idx in range(n_sids):\n        if sid_idx % 25 == 0:\n            elapsed = time.time() - overall_start\n            if sid_idx &gt; 0 and elapsed &gt; 0:\n                rate = total_combinations_processed / elapsed\n                logger.debug(\n                    \"SID %d/%d | rate=%.0f combinations/s\",\n                    sid_idx + 1,\n                    n_sids,\n                    rate,\n                )\n\n        valid_mask = np.isfinite(vod_values[:, sid_idx]) &amp; np.isfinite(\n            cell_ids[:, sid_idx]\n        )\n        if not np.any(valid_mask):\n            total_combinations_processed += len(unique_cells)\n            continue\n\n        valid_indices = np.where(valid_mask)[0]\n        sid_cells = cell_ids[valid_mask, sid_idx]\n        sid_vod = vod_values[valid_mask, sid_idx]\n\n        batch_args = [\n            (\n                batch_cells,\n                sid_vod,\n                sid_cells,\n                valid_indices,\n                threshold,\n                min_obs_per_sid,\n                batch_idx,\n                sid_idx,\n            )\n            for batch_cells, batch_idx in cell_batches\n        ]\n\n        with Pool(n_workers) as pool:\n            batch_results = pool.map(process_spatial_batch_worker, batch_args)\n\n        for result in batch_results:\n            if result[\"outlier_indices\"]:\n                outlier_mask[result[\"outlier_indices\"], sid_idx] = True\n            if result[\"processing_indices\"]:\n                processing_mask[result[\"processing_indices\"], sid_idx] = True\n            total_combinations_processed += result[\"combinations\"]\n            total_combinations_filtered += result[\"filtered\"]\n\n    total_time = time.time() - overall_start\n\n    # Build result dataset\n    vod_filtered = vod_values.copy()\n    vod_filtered[outlier_mask] = np.nan\n\n    result_ds = vod_ds.copy()\n    result_ds[\"VOD_filtered_hampel\"] = ([\"epoch\", \"sid\"], vod_filtered)\n    result_ds[\"hampel_processing_mask\"] = ([\"epoch\", \"sid\"], processing_mask)\n\n    # Statistics for logging\n    original_valid = int(np.sum(np.isfinite(vod_values)))\n    outliers_removed = int(np.sum(outlier_mask))\n    outlier_pct = outliers_removed / original_valid * 100 if original_valid &gt; 0 else 0.0\n\n    logger.info(\n        \"Hampel complete: time=%.1fs, rate=%.0f comb/s, \"\n        \"outliers=%d (%.2f%%), processed=%d combinations\",\n        total_time,\n        total_combinations_processed / total_time if total_time &gt; 0 else 0,\n        outliers_removed,\n        outlier_pct,\n        total_combinations_processed,\n    )\n\n    # Metadata\n    result_ds.attrs.update(\n        {\n            \"hampel_filtering\": \"parallelized_complete_temporal\",\n            \"hampel_threshold\": threshold,\n            \"min_obs_per_sid\": min_obs_per_sid,\n            \"spatial_batch_size\": spatial_batch_size,\n            \"n_workers\": n_workers,\n            \"spatial_batches\": n_spatial_batches,\n            \"temporal_chunking\": \"none\",\n            \"temporal_coverage\": \"complete\",\n            \"processing_time_seconds\": total_time,\n            \"processing_rate_combinations_per_second\": (\n                total_combinations_processed / total_time if total_time &gt; 0 else 0\n            ),\n            \"combinations_processed\": total_combinations_processed,\n            \"combinations_filtered\": total_combinations_filtered,\n            \"parallel_efficiency\": (\n                total_combinations_processed / (n_workers * total_time)\n                if total_time &gt; 0\n                else 0\n            ),\n            \"outliers_removed\": outliers_removed,\n            \"outlier_percentage\": outlier_pct,\n            \"scientific_validity\": \"complete_temporal_continuity\",\n            \"parallelization_method\": \"multiprocessing_spatial_batches\",\n        }\n    )\n\n    result_ds[\"VOD_filtered_hampel\"].attrs.update(\n        {\n            \"long_name\": (\n                \"VOD filtered with parallelized complete temporal Hampel method\"\n            ),\n            \"method\": \"global_statistics_per_cell_sid\",\n            \"temporal_coverage\": \"complete_no_chunking\",\n            \"parallelization\": f\"{n_workers}_workers_spatial_batching\",\n        }\n    )\n\n    result_ds[\"hampel_processing_mask\"].attrs.update(\n        {\n            \"long_name\": \"Mask of observations processed by parallel Hampel filter\",\n            \"temporal_coverage\": \"complete\",\n            \"processing_method\": \"parallel_spatial_batches\",\n        }\n    )\n\n    return result_ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.hampel_filtering.aggr_hampel_cell_sid_parallelized","level":2,"title":"<code>aggr_hampel_cell_sid_parallelized(vod_ds, grid_name='equal_area_2deg', threshold=3.0, min_obs_per_sid=20, spatial_batch_size=500, n_workers=None, temporal_agg=None, agg_method='mean')</code>","text":"<p>Parallelized cell–SID Hampel filter with optional temporal aggregation.</p> <p>Each (cell_id, SID) series is filtered independently.  When temporal_agg is set, data is first binned into temporal windows before filtering; φ, θ and cell IDs are reassigned consistently on the output.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.hampel_filtering.aggr_hampel_cell_sid_parallelized--parameters","level":4,"title":"Parameters","text":"<p>vod_ds : xr.Dataset     Input dataset with <code>'VOD'</code>, <code>'phi'</code>, <code>'theta'</code> and a     <code>cell_id_&lt;grid_name&gt;</code> variable. grid_name : str     Grid identifier (e.g. <code>'equal_area_2deg'</code>).  Used to locate     or create the cell-ID variable. threshold : float     MAD multiplier. min_obs_per_sid : int     Minimum valid observations per cell-SID. spatial_batch_size : int     Cells per parallel batch. n_workers : int or None     Worker count (defaults to <code>min(cpu_count(), 8)</code>). temporal_agg : str or None     Temporal aggregation frequency (e.g. <code>'1H'</code>, <code>'1D'</code>).     <code>None</code> → no aggregation. agg_method : str     <code>'mean'</code> or <code>'median'</code> for temporal aggregation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.hampel_filtering.aggr_hampel_cell_sid_parallelized--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     Dataset with <code>'VOD'</code> (aggregated raw), <code>'VOD_filtered_hampel'</code>,     <code>'hampel_outlier_mask'</code>, <code>'hampel_processing_mask'</code>,     reassigned <code>phi</code>, <code>theta</code> and <code>cell_id_&lt;grid_name&gt;</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/hampel_filtering.py</code> <pre><code>def aggr_hampel_cell_sid_parallelized(\n    vod_ds: xr.Dataset,\n    grid_name: str = \"equal_area_2deg\",\n    threshold: float = 3.0,\n    min_obs_per_sid: int = 20,\n    spatial_batch_size: int = 500,\n    n_workers: int | None = None,\n    temporal_agg: str | None = None,\n    agg_method: str = \"mean\",\n) -&gt; xr.Dataset:\n    \"\"\"Parallelized cell–SID Hampel filter with optional temporal aggregation.\n\n    Each (cell_id, SID) series is filtered independently.  When\n    *temporal_agg* is set, data is first binned into temporal windows\n    before filtering; φ, θ and cell IDs are reassigned consistently on\n    the output.\n\n    Parameters\n    ----------\n    vod_ds : xr.Dataset\n        Input dataset with ``'VOD'``, ``'phi'``, ``'theta'`` and a\n        ``cell_id_&lt;grid_name&gt;`` variable.\n    grid_name : str\n        Grid identifier (e.g. ``'equal_area_2deg'``).  Used to locate\n        or create the cell-ID variable.\n    threshold : float\n        MAD multiplier.\n    min_obs_per_sid : int\n        Minimum valid observations per cell-SID.\n    spatial_batch_size : int\n        Cells per parallel batch.\n    n_workers : int or None\n        Worker count (defaults to ``min(cpu_count(), 8)``).\n    temporal_agg : str or None\n        Temporal aggregation frequency (e.g. ``'1H'``, ``'1D'``).\n        ``None`` → no aggregation.\n    agg_method : str\n        ``'mean'`` or ``'median'`` for temporal aggregation.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with ``'VOD'`` (aggregated raw), ``'VOD_filtered_hampel'``,\n        ``'hampel_outlier_mask'``, ``'hampel_processing_mask'``,\n        reassigned ``phi``, ``theta`` and ``cell_id_&lt;grid_name&gt;``.\n\n    \"\"\"\n    if n_workers is None:\n        n_workers = min(cpu_count(), 8)\n\n    # --- Parse grid_name → (grid_type, resolution) ---\n    parts = grid_name.split(\"_\")\n    if not parts[-1].endswith(\"deg\"):\n        raise ValueError(f\"Grid name '{grid_name}' must end with '&lt;N&gt;deg'\")\n    try:\n        resolution = float(parts[-1].replace(\"deg\", \"\"))\n    except ValueError:\n        raise ValueError(f\"Could not parse resolution from grid name '{grid_name}'\")\n    grid_type = \"_\".join(parts[:-1])\n\n    grid = create_hemigrid(angular_resolution=resolution, grid_type=grid_type)\n\n    cell_id_var = f\"cell_id_{grid_name}\"\n    if cell_id_var not in vod_ds:\n        logger.info(\"No '%s' in input dataset — assigning now.\", cell_id_var)\n        vod_ds = add_cell_ids_to_vod_fast(vod_ds, grid=grid, grid_name=grid_name)\n\n    vod_values = vod_ds[\"VOD\"].values\n    cell_ids = vod_ds[cell_id_var].values\n    n_epochs, n_sids = vod_values.shape\n\n    unique_cells = np.unique(cell_ids[np.isfinite(cell_ids)])\n    cell_batches = [\n        (unique_cells[i : i + spatial_batch_size], i // spatial_batch_size)\n        for i in range(0, len(unique_cells), spatial_batch_size)\n    ]\n\n    logger.info(\n        \"aggr_hampel: shape=%s, threshold=%.1f, workers=%d, temporal_agg=%s\",\n        vod_ds.VOD.shape,\n        threshold,\n        n_workers,\n        temporal_agg,\n    )\n\n    # --- Epoch grid ---\n    if temporal_agg:\n        new_epochs = _compute_time_bins(vod_ds[\"epoch\"].values, temporal_agg)\n        logger.info(\"Temporal aggregation → %d epoch bins\", len(new_epochs))\n    else:\n        new_epochs = vod_ds[\"epoch\"].values\n\n    # --- Preallocate ---\n    vod_filtered = np.full((len(new_epochs), n_sids), np.nan)\n    vod_agg = np.full((len(new_epochs), n_sids), np.nan)\n    outlier_mask = np.zeros((len(new_epochs), n_sids), dtype=bool)\n    processing_mask = np.zeros((len(new_epochs), n_sids), dtype=bool)\n\n    total_combinations_processed = 0\n    total_combinations_filtered = 0\n    start_time = time.time()\n\n    for sid_idx in range(n_sids):\n        valid_mask = np.isfinite(vod_values[:, sid_idx]) &amp; np.isfinite(\n            cell_ids[:, sid_idx]\n        )\n        if not np.any(valid_mask):\n            continue\n\n        sid_cells = cell_ids[valid_mask, sid_idx]\n        sid_vod = vod_values[valid_mask, sid_idx]\n        valid_times = vod_ds[\"epoch\"].values[valid_mask]\n\n        if temporal_agg:\n            valid_times, sid_cells, sid_vod = _aggregate_temporally(\n                valid_times, sid_cells, sid_vod, temporal_agg, agg_method\n            )\n\n        # Parallel filtering\n        batch_args = [\n            (\n                batch_cells,\n                sid_vod,\n                sid_cells,\n                np.arange(len(valid_times)),\n                threshold,\n                min_obs_per_sid,\n                batch_idx,\n                sid_idx,\n            )\n            for batch_cells, batch_idx in cell_batches\n        ]\n\n        with Pool(n_workers) as pool:\n            batch_results = pool.map(process_spatial_batch_worker, batch_args)\n\n        for res in batch_results:\n            if res[\"outlier_indices\"]:\n                t_idx = np.searchsorted(\n                    new_epochs,\n                    valid_times[res[\"outlier_indices\"]],\n                )\n                outlier_mask[t_idx, sid_idx] = True\n            if res[\"processing_indices\"]:\n                t_idx = np.searchsorted(\n                    new_epochs,\n                    valid_times[res[\"processing_indices\"]],\n                )\n                processing_mask[t_idx, sid_idx] = True\n            total_combinations_processed += res[\"combinations\"]\n            total_combinations_filtered += res[\"filtered\"]\n\n        # Assign aggregated values\n        bin_index = np.searchsorted(new_epochs, valid_times)\n        for i, t_idx in enumerate(bin_index):\n            if 0 &lt;= t_idx &lt; len(new_epochs):\n                vod_agg[t_idx, sid_idx] = np.nanmean(\n                    [vod_agg[t_idx, sid_idx], sid_vod[i]]\n                )\n                vod_filtered[t_idx, sid_idx] = np.nanmean(\n                    [vod_filtered[t_idx, sid_idx], sid_vod[i]]\n                )\n\n    total_time = time.time() - start_time\n\n    # --- Build output dataset ---\n    result_ds = xr.Dataset(\n        data_vars={\n            \"VOD\": ([\"epoch\", \"sid\"], vod_agg),\n            \"VOD_filtered_hampel\": ([\"epoch\", \"sid\"], vod_filtered),\n            \"hampel_outlier_mask\": ([\"epoch\", \"sid\"], outlier_mask),\n            \"hampel_processing_mask\": ([\"epoch\", \"sid\"], processing_mask),\n        },\n        coords={\"epoch\": new_epochs, \"sid\": vod_ds[\"sid\"].values},\n        attrs=vod_ds.attrs.copy(),\n    )\n\n    # Copy geometry if present\n    if \"phi\" in vod_ds and \"theta\" in vod_ds:\n        phi_tmpl = vod_ds[\"phi\"].isel(epoch=0).values\n        theta_tmpl = vod_ds[\"theta\"].isel(epoch=0).values\n        result_ds[\"phi\"] = (\n            [\"epoch\", \"sid\"],\n            np.repeat(phi_tmpl[None, :], len(new_epochs), axis=0),\n        )\n        result_ds[\"theta\"] = (\n            [\"epoch\", \"sid\"],\n            np.repeat(theta_tmpl[None, :], len(new_epochs), axis=0),\n        )\n\n    # Reassign cell IDs consistently\n    logger.info(\"Reassigning cell IDs for output dataset.\")\n    result_ds = add_cell_ids_to_vod_fast(result_ds, grid=grid, grid_name=grid_name)\n\n    # Metadata\n    result_ds.attrs.update(\n        {\n            \"processing\": \"parallelized_hampel_with_optional_aggregation\",\n            \"temporal_aggregation\": temporal_agg or \"none\",\n            \"aggregation_method\": agg_method,\n            \"threshold\": threshold,\n            \"min_obs_per_sid\": min_obs_per_sid,\n            \"spatial_batch_size\": spatial_batch_size,\n            \"n_workers\": n_workers,\n            \"execution_time_s\": total_time,\n            \"combinations_processed\": int(total_combinations_processed),\n            \"combinations_filtered\": int(total_combinations_filtered),\n        }\n    )\n\n    logger.info(\n        \"aggr_hampel complete: epochs=%d, sids=%d, time=%.1fs\",\n        len(new_epochs),\n        n_sids,\n        total_time,\n    )\n\n    return result_ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#sigma-clip-filtering","level":3,"title":"Sigma-Clip Filtering","text":"<p>Vectorised numba-JIT Hampel filter with astropy sigma-clipping fallback.</p> <p>Two complementary high-performance filtering strategies for gridded VOD data:</p> <ul> <li>Vectorised + numba (<code>astropy_hampel_vectorized_fast</code>) –   sliding-window Hampel filter compiled with <code>numba.jit</code>.  Processes   temporal chunks in cell batches; targets sub-5-minute runtimes for   ~1.5 years of data.</li> <li>Ultra-fast (<code>astropy_hampel_ultra_fast</code>) – pure-numpy   vectorisation backed by <code>astropy.stats.sigma_clip</code>.  Drops the   per-window granularity in exchange for extreme throughput.</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter--functions","level":3,"title":"Functions","text":"<p><code>vectorized_sliding_window_hampel</code>        – numba-compiled core loop. <code>process_cell_batch_vectorized</code>           – batch dispatcher for one temporal chunk. <code>astropy_hampel_vectorized_fast</code>          – full pipeline (numba path). <code>astropy_hampel_ultra_fast</code>               – full pipeline (astropy path).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter--notes","level":3,"title":"Notes","text":"<ul> <li><code>vectorized_sliding_window_hampel</code> uses <code>numba.prange</code> so the   outer loop is distributed across all available cores automatically.</li> <li>The 1.4826 MAD scaling factor matches the convention used by   <code>astropy.stats.mad_std</code>.</li> <li>Both top-level functions expect a <code>cell_id_&lt;grid_name&gt;</code> variable   already present in the input dataset (see   :func:<code>canvod.grids.add_cell_ids_to_vod_fast</code>).</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.vectorized_sliding_window_hampel","level":2,"title":"<code>vectorized_sliding_window_hampel(data, times, window_ns, sigma_threshold=3.0, min_points=5)</code>","text":"<p>Sliding-window Hampel filter compiled with numba.</p> <p>Each point is compared against the robust statistics (median, scaled MAD) of its temporal neighbourhood.  Points whose deviation exceeds sigma_threshold × 1.4826 × MAD are flagged as outliers and set to <code>NaN</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.vectorized_sliding_window_hampel--parameters","level":4,"title":"Parameters","text":"<p>data : np.ndarray     1-D array of values to filter. times : np.ndarray     1-D array of timestamps as <code>int64</code> nanoseconds. window_ns : int     Half-window size in nanoseconds. sigma_threshold : float, optional     Number of scaled-MAD units for the outlier boundary. min_points : int, optional     Minimum number of finite points required in the window to     attempt filtering; points in smaller windows are left unchanged.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.vectorized_sliding_window_hampel--returns","level":4,"title":"Returns","text":"<p>filtered_data : np.ndarray     Copy of data with outliers replaced by <code>NaN</code>. outlier_mask : np.ndarray     Boolean mask; <code>True</code> where an outlier was detected.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/sigma_clip_filter.py</code> <pre><code>@jit(nopython=True, parallel=True)\ndef vectorized_sliding_window_hampel(\n    data: np.ndarray,\n    times: np.ndarray,\n    window_ns: int,\n    sigma_threshold: float = 3.0,\n    min_points: int = 5,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Sliding-window Hampel filter compiled with numba.\n\n    Each point is compared against the robust statistics (median,\n    scaled MAD) of its temporal neighbourhood.  Points whose deviation\n    exceeds *sigma_threshold* × 1.4826 × MAD are flagged as outliers\n    and set to ``NaN``.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        1-D array of values to filter.\n    times : np.ndarray\n        1-D array of timestamps as ``int64`` nanoseconds.\n    window_ns : int\n        Half-window size in nanoseconds.\n    sigma_threshold : float, optional\n        Number of scaled-MAD units for the outlier boundary.\n    min_points : int, optional\n        Minimum number of finite points required in the window to\n        attempt filtering; points in smaller windows are left unchanged.\n\n    Returns\n    -------\n    filtered_data : np.ndarray\n        Copy of *data* with outliers replaced by ``NaN``.\n    outlier_mask : np.ndarray\n        Boolean mask; ``True`` where an outlier was detected.\n\n    \"\"\"\n    n_points = len(data)\n    filtered_data = data.copy()\n    outlier_mask = np.zeros(n_points, dtype=np.bool_)\n\n    for i in prange(n_points):\n        if not np.isfinite(data[i]):\n            continue\n\n        current_time = times[i]\n        window_start = current_time - window_ns\n        window_end = current_time + window_ns\n\n        # Points inside the temporal window with finite values\n        window_indices = np.where(\n            (times &gt;= window_start) &amp; (times &lt;= window_end) &amp; np.isfinite(data)\n        )[0]\n\n        if len(window_indices) &lt; min_points:\n            continue\n\n        window_data = data[window_indices]\n\n        median_val = np.median(window_data)\n        mad_val = np.median(np.abs(window_data - median_val))\n\n        if mad_val &gt; 0:\n            threshold_value = sigma_threshold * 1.4826 * mad_val\n            if np.abs(data[i] - median_val) &gt; threshold_value:\n                outlier_mask[i] = True\n                filtered_data[i] = np.nan\n\n    return filtered_data, outlier_mask\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.process_cell_batch_vectorized","level":2,"title":"<code>process_cell_batch_vectorized(cell_batch, vod_chunk, times_chunk, cell_ids_chunk, window_hours, sigma_threshold, min_points)</code>","text":"<p>Apply the numba Hampel filter to a subset of cells in one temporal chunk.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.process_cell_batch_vectorized--parameters","level":4,"title":"Parameters","text":"<p>cell_batch : np.ndarray     1-D array of cell IDs to process in this batch. vod_chunk : np.ndarray     2-D <code>(epoch, sid)</code> VOD values for the current temporal chunk. times_chunk : np.ndarray     1-D <code>datetime64</code> timestamps for the chunk. cell_ids_chunk : np.ndarray     2-D <code>(epoch, sid)</code> cell-ID array matching vod_chunk. window_hours : float     Half-window size in hours. sigma_threshold : float     Outlier threshold in scaled-MAD units. min_points : int     Minimum finite points required per window.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.process_cell_batch_vectorized--returns","level":4,"title":"Returns","text":"<p>filtered_chunk : np.ndarray     Filtered copy of vod_chunk. outlier_chunk : np.ndarray     Boolean outlier mask with the same shape as vod_chunk.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/sigma_clip_filter.py</code> <pre><code>def process_cell_batch_vectorized(\n    cell_batch: np.ndarray,\n    vod_chunk: np.ndarray,\n    times_chunk: np.ndarray,\n    cell_ids_chunk: np.ndarray,\n    window_hours: float,\n    sigma_threshold: float,\n    min_points: int,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Apply the numba Hampel filter to a subset of cells in one temporal chunk.\n\n    Parameters\n    ----------\n    cell_batch : np.ndarray\n        1-D array of cell IDs to process in this batch.\n    vod_chunk : np.ndarray\n        2-D ``(epoch, sid)`` VOD values for the current temporal chunk.\n    times_chunk : np.ndarray\n        1-D ``datetime64`` timestamps for the chunk.\n    cell_ids_chunk : np.ndarray\n        2-D ``(epoch, sid)`` cell-ID array matching *vod_chunk*.\n    window_hours : float\n        Half-window size in hours.\n    sigma_threshold : float\n        Outlier threshold in scaled-MAD units.\n    min_points : int\n        Minimum finite points required per window.\n\n    Returns\n    -------\n    filtered_chunk : np.ndarray\n        Filtered copy of *vod_chunk*.\n    outlier_chunk : np.ndarray\n        Boolean outlier mask with the same shape as *vod_chunk*.\n\n    \"\"\"\n    epochs, sids = vod_chunk.shape\n    filtered_chunk = vod_chunk.copy()\n    outlier_chunk = np.zeros((epochs, sids), dtype=bool)\n\n    window_ns = int(window_hours * 3600 * 1e9)\n    times_ns = times_chunk.astype(\"datetime64[ns]\").astype(np.int64)\n\n    for sid_idx in range(sids):\n        sid_vod = vod_chunk[:, sid_idx]\n        sid_cells = cell_ids_chunk[:, sid_idx]\n\n        valid_mask = np.isfinite(sid_vod) &amp; np.isfinite(sid_cells)\n        if not np.any(valid_mask):\n            continue\n\n        for cell_id in cell_batch:\n            cell_mask = valid_mask &amp; (sid_cells == cell_id)\n            if not np.any(cell_mask):\n                continue\n\n            cell_indices = np.where(cell_mask)[0]\n            cell_vod = sid_vod[cell_indices]\n            cell_times = times_ns[cell_indices]\n\n            if len(cell_vod) &lt; min_points:\n                continue\n\n            try:\n                filtered_cell, outliers_cell = vectorized_sliding_window_hampel(\n                    cell_vod, cell_times, window_ns, sigma_threshold, min_points\n                )\n                filtered_chunk[cell_indices, sid_idx] = filtered_cell\n                outlier_chunk[cell_indices, sid_idx] = outliers_cell\n            except Exception:\n                # Keep original data if filtering fails for this cell-SID\n                continue\n\n    return filtered_chunk, outlier_chunk\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.astropy_hampel_vectorized_fast","level":2,"title":"<code>astropy_hampel_vectorized_fast(vod_ds, grid_name='equal_area_2deg', window_hours=1.0, sigma_threshold=3.0, min_points=5, cell_batch_size=200, n_workers=None)</code>","text":"<p>Numba-accelerated sliding-window Hampel filter over a VOD dataset.</p> <p>Temporal chunks (as stored in the dask-backed dataset) are iterated sequentially; within each chunk the unique cells are split into batches of cell_batch_size and dispatched to :func:<code>process_cell_batch_vectorized</code>, which in turn calls the numba-compiled :func:<code>vectorized_sliding_window_hampel</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.astropy_hampel_vectorized_fast--parameters","level":4,"title":"Parameters","text":"<p>vod_ds : xr.Dataset     VOD dataset containing a <code>cell_id_&lt;grid_name&gt;</code> variable. grid_name : str, optional     Suffix used to locate the cell-ID variable     (<code>cell_id_&lt;grid_name&gt;</code>). window_hours : float, optional     Half-window size in hours for the sliding window. sigma_threshold : float, optional     Outlier boundary in scaled-MAD units. min_points : int, optional     Minimum finite points required in a window. cell_batch_size : int, optional     Number of cells per batch (trades memory for cache locality). n_workers : int or None, optional     Unused in this implementation; reserved for future parallel     chunk processing.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.astropy_hampel_vectorized_fast--returns","level":4,"title":"Returns","text":"<p>result_ds : xr.Dataset     Copy of vod_ds with two additional variables:</p> <pre><code>``VOD_filtered``\n    Filtered VOD (outliers → NaN).\n``hampel_outlier_mask``\n    Boolean mask; ``True`` at outlier positions.\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.astropy_hampel_vectorized_fast--raises","level":4,"title":"Raises","text":"<p>ValueError     If the expected cell-ID variable is missing from vod_ds.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/sigma_clip_filter.py</code> <pre><code>def astropy_hampel_vectorized_fast(\n    vod_ds: xr.Dataset,\n    grid_name: str = \"equal_area_2deg\",\n    window_hours: float = 1.0,\n    sigma_threshold: float = 3.0,\n    min_points: int = 5,\n    cell_batch_size: int = 200,\n    n_workers: int | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Numba-accelerated sliding-window Hampel filter over a VOD dataset.\n\n    Temporal chunks (as stored in the dask-backed dataset) are iterated\n    sequentially; within each chunk the unique cells are split into\n    batches of *cell_batch_size* and dispatched to\n    :func:`process_cell_batch_vectorized`, which in turn calls the\n    numba-compiled :func:`vectorized_sliding_window_hampel`.\n\n    Parameters\n    ----------\n    vod_ds : xr.Dataset\n        VOD dataset containing a ``cell_id_&lt;grid_name&gt;`` variable.\n    grid_name : str, optional\n        Suffix used to locate the cell-ID variable\n        (``cell_id_&lt;grid_name&gt;``).\n    window_hours : float, optional\n        Half-window size in hours for the sliding window.\n    sigma_threshold : float, optional\n        Outlier boundary in scaled-MAD units.\n    min_points : int, optional\n        Minimum finite points required in a window.\n    cell_batch_size : int, optional\n        Number of cells per batch (trades memory for cache locality).\n    n_workers : int or None, optional\n        Unused in this implementation; reserved for future parallel\n        chunk processing.\n\n    Returns\n    -------\n    result_ds : xr.Dataset\n        Copy of *vod_ds* with two additional variables:\n\n        ``VOD_filtered``\n            Filtered VOD (outliers → NaN).\n        ``hampel_outlier_mask``\n            Boolean mask; ``True`` at outlier positions.\n\n    Raises\n    ------\n    ValueError\n        If the expected cell-ID variable is missing from *vod_ds*.\n\n    \"\"\"\n    cell_id_var = f\"cell_id_{grid_name}\"\n    if cell_id_var not in vod_ds:\n        raise ValueError(\n            f\"Cell ID variable '{cell_id_var}' not found in dataset. \"\n            \"Run add_cell_ids_to_vod_fast() first.\"\n        )\n\n    logger.info(\n        \"vectorized hampel: window=%.1fh, sigma=%.1f, batch_size=%d, shape=%s\",\n        window_hours,\n        sigma_threshold,\n        cell_batch_size,\n        vod_ds.VOD.shape,\n    )\n\n    start_time = time.time()\n\n    vod_data = vod_ds.VOD.data\n    cell_ids = vod_ds[cell_id_var].data\n    times = vod_ds.epoch.values\n\n    logger.info(\"processing %d temporal chunks ...\", vod_data.numblocks[0])\n\n    def _process_temporal_chunk(chunk_idx: int) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Process a single temporal chunk.\n\n        Parameters\n        ----------\n        chunk_idx : int\n            Chunk index along time axis.\n\n        Returns\n        -------\n        tuple[np.ndarray, np.ndarray]\n            Filtered values and outlier mask.\n\n        \"\"\"\n        chunk_start = chunk_idx * vod_data.chunksize[0]\n        chunk_end = min(chunk_start + vod_data.chunksize[0], vod_data.shape[0])\n\n        vod_chunk = vod_data[chunk_start:chunk_end].compute()\n        cell_chunk = cell_ids[chunk_start:chunk_end].compute()\n        times_chunk = times[chunk_start:chunk_end]\n\n        unique_cells = np.unique(cell_chunk[np.isfinite(cell_chunk)])\n        if len(unique_cells) == 0:\n            return vod_chunk, np.zeros_like(vod_chunk, dtype=bool)\n\n        n_cell_batches = int(np.ceil(len(unique_cells) / cell_batch_size))\n\n        filtered_chunk = vod_chunk.copy()\n        outlier_chunk = np.zeros_like(vod_chunk, dtype=bool)\n\n        for batch_idx in range(n_cell_batches):\n            batch_start = batch_idx * cell_batch_size\n            batch_end = min(batch_start + cell_batch_size, len(unique_cells))\n            cell_batch = unique_cells[batch_start:batch_end]\n\n            batch_filtered, batch_outliers = process_cell_batch_vectorized(\n                cell_batch=cell_batch,\n                vod_chunk=vod_chunk,\n                times_chunk=times_chunk,\n                cell_ids_chunk=cell_chunk,\n                window_hours=window_hours,\n                sigma_threshold=sigma_threshold,\n                min_points=min_points,\n            )\n\n            for cell_id in cell_batch:\n                cell_mask = cell_chunk == cell_id\n                filtered_chunk[cell_mask] = batch_filtered[cell_mask]\n                outlier_chunk[cell_mask] = batch_outliers[cell_mask]\n\n        return filtered_chunk, outlier_chunk\n\n    filtered_chunks: list[np.ndarray] = []\n    outlier_chunks: list[np.ndarray] = []\n\n    for chunk_idx in tqdm(range(vod_data.numblocks[0]), desc=\"Temporal chunks\"):\n        f_chunk, o_chunk = _process_temporal_chunk(chunk_idx)\n        filtered_chunks.append(f_chunk)\n        outlier_chunks.append(o_chunk)\n\n    logger.info(\"assembling results ...\")\n    filtered_data = np.concatenate(filtered_chunks, axis=0)\n    outlier_data = np.concatenate(outlier_chunks, axis=0)\n\n    total_time = time.time() - start_time\n\n    # Build result dataset\n    result_ds = vod_ds.copy()\n    result_ds[\"VOD_filtered\"] = ((\"epoch\", \"sid\"), filtered_data)\n    result_ds[\"hampel_outlier_mask\"] = ((\"epoch\", \"sid\"), outlier_data)\n\n    total_obs = int(np.isfinite(vod_ds.VOD.values).sum())\n    outliers_detected = int(outlier_data.sum())\n    outlier_pct = (outliers_detected / total_obs * 100) if total_obs &gt; 0 else 0.0\n\n    result_ds.attrs.update(\n        {\n            \"hampel_method\": \"vectorized_astropy_equivalent\",\n            \"hampel_window_hours\": window_hours,\n            \"hampel_sigma_threshold\": sigma_threshold,\n            \"hampel_min_points\": min_points,\n            \"hampel_processing_time_s\": total_time,\n            \"hampel_vectorized_optimized\": True,\n            \"performance_target_achieved\": total_time &lt; 300,\n        }\n    )\n    result_ds[\"VOD_filtered\"].attrs.update(\n        {\n            \"long_name\": \"VOD filtered with vectorized Hampel method\",\n            \"method\": \"vectorized_sliding_window_with_numba_jit\",\n            \"temporal_window_hours\": window_hours,\n            \"outlier_threshold_sigma\": sigma_threshold,\n        }\n    )\n\n    logger.info(\n        \"done in %.1fs | obs=%d outliers=%d (%.2f%%) | target_met=%s\",\n        total_time,\n        total_obs,\n        outliers_detected,\n        outlier_pct,\n        total_time &lt; 300,\n    )\n\n    return result_ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.astropy_hampel_ultra_fast","level":2,"title":"<code>astropy_hampel_ultra_fast(vod_ds, grid_name='equal_area_2deg', window_hours=1.0, sigma_threshold=3.0, min_points=5)</code>","text":"<p>Pure-numpy sigma-clipping filter via <code>astropy.stats</code>.</p> <p>Each (cell, SID) time series is clipped in one shot using :func:<code>astropy.stats.sigma_clip</code> with median centering and MAD scale estimation.  No per-window temporal granularity is applied; this trades precision for throughput.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.astropy_hampel_ultra_fast--parameters","level":4,"title":"Parameters","text":"<p>vod_ds : xr.Dataset     VOD dataset containing a <code>cell_id_&lt;grid_name&gt;</code> variable. grid_name : str, optional     Suffix for the cell-ID variable. window_hours : float, optional     Unused in ultra-fast mode; kept for API compatibility. sigma_threshold : float, optional     Sigma-clipping threshold passed to <code>astropy.stats.sigma_clip</code>. min_points : int, optional     Minimum finite observations required to attempt clipping.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.astropy_hampel_ultra_fast--returns","level":4,"title":"Returns","text":"<p>result_ds : xr.Dataset     Copy of vod_ds with <code>VOD_filtered</code> and     <code>hampel_outlier_mask</code> variables added.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.sigma_clip_filter.astropy_hampel_ultra_fast--raises","level":4,"title":"Raises","text":"<p>ValueError     If the expected cell-ID variable is missing from vod_ds.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/sigma_clip_filter.py</code> <pre><code>def astropy_hampel_ultra_fast(\n    vod_ds: xr.Dataset,\n    grid_name: str = \"equal_area_2deg\",\n    window_hours: float = 1.0,\n    sigma_threshold: float = 3.0,\n    min_points: int = 5,\n) -&gt; xr.Dataset:\n    \"\"\"Pure-numpy sigma-clipping filter via ``astropy.stats``.\n\n    Each (cell, SID) time series is clipped in one shot using\n    :func:`astropy.stats.sigma_clip` with median centering and MAD\n    scale estimation.  No per-window temporal granularity is applied;\n    this trades precision for throughput.\n\n    Parameters\n    ----------\n    vod_ds : xr.Dataset\n        VOD dataset containing a ``cell_id_&lt;grid_name&gt;`` variable.\n    grid_name : str, optional\n        Suffix for the cell-ID variable.\n    window_hours : float, optional\n        Unused in ultra-fast mode; kept for API compatibility.\n    sigma_threshold : float, optional\n        Sigma-clipping threshold passed to ``astropy.stats.sigma_clip``.\n    min_points : int, optional\n        Minimum finite observations required to attempt clipping.\n\n    Returns\n    -------\n    result_ds : xr.Dataset\n        Copy of *vod_ds* with ``VOD_filtered`` and\n        ``hampel_outlier_mask`` variables added.\n\n    Raises\n    ------\n    ValueError\n        If the expected cell-ID variable is missing from *vod_ds*.\n\n    \"\"\"\n    cell_id_var = f\"cell_id_{grid_name}\"\n    if cell_id_var not in vod_ds:\n        raise ValueError(\n            f\"Cell ID variable '{cell_id_var}' not found in dataset. \"\n            \"Run add_cell_ids_to_vod_fast() first.\"\n        )\n\n    logger.info(\n        \"ultra-fast hampel: sigma=%.1f, min_points=%d, shape=%s\",\n        sigma_threshold,\n        min_points,\n        vod_ds.VOD.shape,\n    )\n\n    start_time = time.time()\n\n    vod_values = vod_ds.VOD.values\n    cell_ids = vod_ds[cell_id_var].values\n\n    filtered_data = vod_values.copy()\n    outlier_mask = np.zeros_like(vod_values, dtype=bool)\n\n    unique_cells = np.unique(cell_ids[np.isfinite(cell_ids)])\n    logger.info(\n        \"processing %d cells × %d SIDs ...\", len(unique_cells), vod_values.shape[1]\n    )\n\n    for cell_id in tqdm(unique_cells, desc=\"Cells\"):\n        for sid_idx in range(vod_values.shape[1]):\n            cell_mask = (cell_ids[:, sid_idx] == cell_id) &amp; np.isfinite(\n                vod_values[:, sid_idx]\n            )\n            if not np.any(cell_mask):\n                continue\n\n            cell_data = vod_values[cell_mask, sid_idx]\n            if len(cell_data) &lt; min_points:\n                continue\n\n            try:\n                clipped = sigma_clip(\n                    cell_data,\n                    sigma=sigma_threshold,\n                    cenfunc=\"median\",\n                    stdfunc=mad_std,\n                    maxiters=1,\n                    masked=True,\n                )\n                cell_indices = np.where(cell_mask)[0]\n                filtered_data[cell_indices, sid_idx] = clipped.data\n                outlier_mask[cell_indices, sid_idx] = clipped.mask\n            except Exception:\n                continue\n\n    total_time = time.time() - start_time\n\n    result_ds = vod_ds.copy()\n    result_ds[\"VOD_filtered\"] = ((\"epoch\", \"sid\"), filtered_data)\n    result_ds[\"hampel_outlier_mask\"] = ((\"epoch\", \"sid\"), outlier_mask)\n\n    total_obs = int(np.isfinite(vod_values).sum())\n    outliers = int(outlier_mask.sum())\n\n    result_ds.attrs.update(\n        {\n            \"hampel_method\": \"ultra_fast_astropy_sigma_clip\",\n            \"hampel_sigma_threshold\": sigma_threshold,\n            \"hampel_min_points\": min_points,\n            \"hampel_processing_time_s\": total_time,\n        }\n    )\n    result_ds[\"VOD_filtered\"].attrs.update(\n        {\n            \"long_name\": \"VOD filtered with ultra-fast astropy sigma clipping\",\n            \"method\": \"astropy_sigma_clip_per_cell_sid\",\n            \"outlier_threshold_sigma\": sigma_threshold,\n        }\n    )\n\n    logger.info(\n        \"done in %.1fs | obs=%d outliers=%d (%.2f%%)\",\n        total_time,\n        total_obs,\n        outliers,\n        (outliers / total_obs * 100) if total_obs &gt; 0 else 0.0,\n    )\n\n    return result_ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#masking","level":3,"title":"Masking","text":"<p>Spatial masking for hemispherical grid cells.</p> <p>Provides tools to create boolean masks for selecting subsets of grid cells based on geometric constraints, data quality, or custom criteria.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking--classes","level":3,"title":"Classes","text":"<p>SpatialMask             – builder for boolean cell-selection masks.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking--convenience-functions","level":3,"title":"Convenience functions","text":"<p><code>create_hemisphere_mask</code>  – north / south / east / west mask. <code>create_elevation_mask</code>   – elevation-angle-based mask.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask","level":2,"title":"<code>SpatialMask</code>","text":"<p>Create spatial masks for grid cells.</p> <p>Masks are boolean arrays where <code>True</code> = include cell, <code>False</code> = exclude cell.  Multiple constraints can be combined with AND or OR logic.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask--parameters","level":4,"title":"Parameters","text":"<p>grid : GridData     Grid instance.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask--examples","level":4,"title":"Examples","text":"<p>mask = SpatialMask(grid) mask.add_phi_range(0, np.pi)          # Northern hemisphere mask.add_theta_range(0, np.pi / 3)    # Exclude low elevations mask.add_quality_threshold('mean_snr', min_value=40) spatial_mask = mask.compute()          # Returns boolean array</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>class SpatialMask:\n    \"\"\"Create spatial masks for grid cells.\n\n    Masks are boolean arrays where ``True`` = include cell,\n    ``False`` = exclude cell.  Multiple constraints can be combined\n    with AND or OR logic.\n\n    Parameters\n    ----------\n    grid : GridData\n        Grid instance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mask = SpatialMask(grid)\n    &gt;&gt;&gt; mask.add_phi_range(0, np.pi)          # Northern hemisphere\n    &gt;&gt;&gt; mask.add_theta_range(0, np.pi / 3)    # Exclude low elevations\n    &gt;&gt;&gt; mask.add_quality_threshold('mean_snr', min_value=40)\n    &gt;&gt;&gt; spatial_mask = mask.compute()          # Returns boolean array\n\n    \"\"\"\n\n    def __init__(self, grid: GridData) -&gt; None:\n        \"\"\"Initialize the spatial mask builder.\n\n        Parameters\n        ----------\n        grid : GridData\n            Grid instance.\n\n        \"\"\"\n        self.grid = grid\n        self.masks: list[tuple[str, np.ndarray]] = []\n        self._grid_df = grid.grid\n\n    # ------------------------------------------------------------------\n    # Constraint builders\n    # ------------------------------------------------------------------\n\n    def add_phi_range(\n        self, phi_min: float, phi_max: float, degrees: bool = False\n    ) -&gt; SpatialMask:\n        \"\"\"Add azimuth angle constraint.\n\n        Parameters\n        ----------\n        phi_min : float\n            Minimum azimuth angle.\n        phi_max : float\n            Maximum azimuth angle.\n        degrees : bool\n            If ``True``, angles are in degrees; otherwise radians.\n\n        Returns\n        -------\n        SpatialMask\n            Self for chaining.\n\n        Notes\n        -----\n        Handles wraparound at 0°/360° correctly.\n        Example: ``phi_range(350, 10)`` includes both 350°–360° and 0°–10°.\n\n        \"\"\"\n        if degrees:\n            phi_min = np.radians(phi_min)\n            phi_max = np.radians(phi_max)\n\n        phi = self._grid_df[\"phi\"].to_numpy()\n\n        # Normalise to [0, 2π)\n        phi = np.mod(phi, 2 * np.pi)\n        phi_min = np.mod(phi_min, 2 * np.pi)\n        phi_max = np.mod(phi_max, 2 * np.pi)\n\n        if phi_min &lt;= phi_max:\n            mask = (phi &gt;= phi_min) &amp; (phi &lt;= phi_max)\n        else:\n            # Wraparound: e.g. 350° to 10°\n            mask = (phi &gt;= phi_min) | (phi &lt;= phi_max)\n\n        self.masks.append((\"phi_range\", mask))\n        return self\n\n    def add_theta_range(\n        self, theta_min: float, theta_max: float, degrees: bool = False\n    ) -&gt; SpatialMask:\n        \"\"\"Add polar angle (zenith angle) constraint.\n\n        Parameters\n        ----------\n        theta_min : float\n            Minimum polar angle (0 = zenith).\n        theta_max : float\n            Maximum polar angle (π/2 = horizon).\n        degrees : bool\n            If ``True``, angles are in degrees; otherwise radians.\n\n        Returns\n        -------\n        SpatialMask\n            Self for chaining.\n\n        Notes\n        -----\n        ``theta = 0°`` is zenith (straight up), ``theta = 90°`` is horizon.\n        To exclude low elevations, use ``theta_max &lt; 90°``.\n\n        \"\"\"\n        if degrees:\n            theta_min = np.radians(theta_min)\n            theta_max = np.radians(theta_max)\n\n        theta = self._grid_df[\"theta\"].to_numpy()\n        mask = (theta &gt;= theta_min) &amp; (theta &lt;= theta_max)\n\n        self.masks.append((\"theta_range\", mask))\n        return self\n\n    def add_elevation_range(\n        self, elev_min: float, elev_max: float, degrees: bool = True\n    ) -&gt; SpatialMask:\n        \"\"\"Add elevation angle constraint (complementary to theta).\n\n        Parameters\n        ----------\n        elev_min : float\n            Minimum elevation angle (0 = horizon, 90 = zenith).\n        elev_max : float\n            Maximum elevation angle.\n        degrees : bool\n            If ``True``, angles are in degrees; otherwise radians.\n            Default: ``True``.\n\n        Returns\n        -------\n        SpatialMask\n            Self for chaining.\n\n        Notes\n        -----\n        ``elevation = 90° − theta``.  This is more intuitive for users\n        who think in elevation angles.\n\n        \"\"\"\n        if degrees:\n            elev_min_rad = np.radians(elev_min)\n            elev_max_rad = np.radians(elev_max)\n        else:\n            elev_min_rad = elev_min\n            elev_max_rad = elev_max\n\n        # Convert elevation → theta: theta = π/2 − elevation\n        theta_max = np.pi / 2 - elev_min_rad\n        theta_min = np.pi / 2 - elev_max_rad\n\n        return self.add_theta_range(theta_min, theta_max, degrees=False)\n\n    def add_cell_ids(self, cell_ids: list[int] | np.ndarray) -&gt; SpatialMask:\n        \"\"\"Include specific cell IDs.\n\n        Parameters\n        ----------\n        cell_ids : list[int] or np.ndarray\n            Cell IDs to include.\n\n        Returns\n        -------\n        SpatialMask\n            Self for chaining.\n\n        \"\"\"\n        mask = np.zeros(self.grid.ncells, dtype=bool)\n        mask[cell_ids] = True\n\n        self.masks.append((\"cell_ids\", mask))\n        return self\n\n    def add_exclude_cell_ids(self, cell_ids: list[int] | np.ndarray) -&gt; SpatialMask:\n        \"\"\"Exclude specific cell IDs.\n\n        Parameters\n        ----------\n        cell_ids : list[int] or np.ndarray\n            Cell IDs to exclude.\n\n        Returns\n        -------\n        SpatialMask\n            Self for chaining.\n\n        \"\"\"\n        mask = np.ones(self.grid.ncells, dtype=bool)\n        mask[cell_ids] = False\n\n        self.masks.append((\"exclude_cell_ids\", mask))\n        return self\n\n    def add_quality_threshold(\n        self,\n        var_name: str,\n        min_value: float | None = None,\n        max_value: float | None = None,\n    ) -&gt; SpatialMask:\n        \"\"\"Add data quality threshold based on grid cell properties.\n\n        Parameters\n        ----------\n        var_name : str\n            Variable name in grid (e.g. ``'mean_snr'``,\n            ``'n_observations'``).\n        min_value : float, optional\n            Minimum allowed value.\n        max_value : float, optional\n            Maximum allowed value.\n\n        Returns\n        -------\n        SpatialMask\n            Self for chaining.\n\n        Raises\n        ------\n        ValueError\n            If *var_name* doesn't exist in the grid DataFrame.\n\n        \"\"\"\n        if var_name not in self._grid_df.columns:\n            available = list(self._grid_df.columns)\n            raise ValueError(\n                f\"Variable '{var_name}' not found in grid. Available: {available}\"\n            )\n\n        values = self._grid_df[var_name].to_numpy()\n        mask = np.ones(self.grid.ncells, dtype=bool)\n\n        if min_value is not None:\n            mask = mask &amp; (values &gt;= min_value)\n        if max_value is not None:\n            mask = mask &amp; (values &lt;= max_value)\n\n        self.masks.append((f\"{var_name}_threshold\", mask))\n        return self\n\n    def add_boundary_cells(self, exclude: bool = True) -&gt; SpatialMask:\n        \"\"\"Include or exclude boundary cells.\n\n        Parameters\n        ----------\n        exclude : bool\n            If ``True``, exclude boundary cells; if ``False``, include\n            only boundary cells.\n\n        Returns\n        -------\n        SpatialMask\n            Self for chaining.\n\n        Raises\n        ------\n        ValueError\n            If the grid does not have an ``'is_boundary'`` column.\n\n        Notes\n        -----\n        Only works if the grid DataFrame contains an ``'is_boundary'``\n        column.\n\n        \"\"\"\n        if \"is_boundary\" not in self._grid_df.columns:\n            raise ValueError(\"Grid does not have 'is_boundary' information\")\n\n        is_boundary = self._grid_df[\"is_boundary\"].to_numpy()\n        mask = ~is_boundary if exclude else is_boundary\n\n        self.masks.append((\"boundary_cells\", mask))\n        return self\n\n    def add_custom_mask(\n        self,\n        mask: np.ndarray | Callable,\n        name: str = \"custom\",\n    ) -&gt; SpatialMask:\n        \"\"\"Add custom mask or mask-generating callable.\n\n        Parameters\n        ----------\n        mask : np.ndarray or callable\n            * If array: boolean mask of shape ``(ncells,)``.\n            * If callable: function ``(grid: GridData) -&gt; np.ndarray``.\n        name : str\n            Label for this mask (used in summary).\n\n        Returns\n        -------\n        SpatialMask\n            Self for chaining.\n\n        Raises\n        ------\n        ValueError\n            If the resulting array has wrong shape or dtype.\n\n        Examples\n        --------\n        &gt;&gt;&gt; custom = np.array([True, False, True, ...])\n        &gt;&gt;&gt; mask.add_custom_mask(custom)\n\n        &gt;&gt;&gt; def high_snr_north(grid):\n        ...     snr   = grid.grid['mean_snr'].to_numpy()\n        ...     phi   = grid.grid['phi'].to_numpy()\n        ...     return (snr &gt; 40) &amp; (phi &lt; np.pi)\n        &gt;&gt;&gt; mask.add_custom_mask(high_snr_north)\n\n        \"\"\"\n        if callable(mask):\n            mask_array = mask(self.grid)\n        else:\n            mask_array = mask\n\n        if not isinstance(mask_array, np.ndarray):\n            raise ValueError(\"Custom mask must be numpy array or return numpy array\")\n        if mask_array.shape != (self.grid.ncells,):\n            raise ValueError(\n                f\"Custom mask shape {mask_array.shape} doesn't match \"\n                f\"grid size ({self.grid.ncells},)\"\n            )\n        if mask_array.dtype != bool:\n            raise ValueError(\"Custom mask must be boolean dtype\")\n\n        self.masks.append((name, mask_array))\n        return self\n\n    def add_radial_sector(\n        self,\n        center_phi: float,\n        sector_width: float,\n        theta_min: float = 0.0,\n        theta_max: float = np.pi / 2,\n        degrees: bool = True,\n    ) -&gt; SpatialMask:\n        \"\"\"Add radial sector (wedge) mask.\n\n        Parameters\n        ----------\n        center_phi : float\n            Centre azimuth of sector.\n        sector_width : float\n            Full angular width of sector.\n        theta_min : float\n            Minimum polar angle (radial inner bound).\n        theta_max : float\n            Maximum polar angle (radial outer bound).\n        degrees : bool\n            If ``True``, all angles in degrees; otherwise radians.\n\n        Returns\n        -------\n        SpatialMask\n            Self for chaining.\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Northern sector, 30° wide, excluding low elevations\n        &gt;&gt;&gt; mask.add_radial_sector(center_phi=0, sector_width=30,\n        ...                        theta_max=60, degrees=True)\n\n        \"\"\"\n        if degrees:\n            center_phi = np.radians(center_phi)\n            sector_width = np.radians(sector_width)\n            theta_min = np.radians(theta_min)\n            theta_max = np.radians(theta_max)\n\n        half_width = sector_width / 2\n        self.add_phi_range(\n            center_phi - half_width,\n            center_phi + half_width,\n            degrees=False,\n        )\n        self.add_theta_range(theta_min, theta_max, degrees=False)\n\n        return self\n\n    # ------------------------------------------------------------------\n    # Computation &amp; introspection\n    # ------------------------------------------------------------------\n\n    def compute(self, mode: str = \"AND\") -&gt; np.ndarray:\n        \"\"\"Compute final combined boolean mask.\n\n        Parameters\n        ----------\n        mode : str\n            Combination mode:\n\n            * ``'AND'`` – all constraints must be satisfied (intersection).\n            * ``'OR'``  – at least one constraint must be satisfied (union).\n\n        Returns\n        -------\n        np.ndarray\n            Boolean array of shape ``(ncells,)`` where ``True`` = include.\n\n        Raises\n        ------\n        ValueError\n            If no masks have been added or *mode* is unknown.\n\n        \"\"\"\n        if not self.masks:\n            raise ValueError(\"No masks added. Use add_* methods before compute()\")\n\n        if mode.upper() == \"AND\":\n            combined = np.ones(self.grid.ncells, dtype=bool)\n            for _name, mask in self.masks:\n                combined = combined &amp; mask\n        elif mode.upper() == \"OR\":\n            combined = np.zeros(self.grid.ncells, dtype=bool)\n            for _name, mask in self.masks:\n                combined = combined | mask\n        else:\n            raise ValueError(f\"Unknown mode: {mode}. Use 'AND' or 'OR'\")\n\n        return combined\n\n    def get_mask_summary(self) -&gt; dict:\n        \"\"\"Get summary of all added masks.\n\n        Returns\n        -------\n        dict\n            Dictionary with mask names and per-mask / combined cell counts.\n\n        \"\"\"\n        summary: dict = {\"total_cells\": self.grid.ncells, \"masks\": []}\n\n        for name, mask in self.masks:\n            n_included = int(mask.sum())\n            summary[\"masks\"].append(\n                {\n                    \"name\": name,\n                    \"n_cells_included\": n_included,\n                    \"fraction_included\": n_included / self.grid.ncells,\n                }\n            )\n\n        if self.masks:\n            combined_and = self.compute(mode=\"AND\")\n            combined_or = self.compute(mode=\"OR\")\n            summary[\"combined\"] = {\n                \"AND\": {\n                    \"n_cells\": int(combined_and.sum()),\n                    \"fraction\": float(combined_and.sum() / self.grid.ncells),\n                },\n                \"OR\": {\n                    \"n_cells\": int(combined_or.sum()),\n                    \"fraction\": float(combined_or.sum() / self.grid.ncells),\n                },\n            }\n\n        return summary\n\n    def clear(self) -&gt; SpatialMask:\n        \"\"\"Clear all masks.\n\n        Returns\n        -------\n        SpatialMask\n            Self for chaining.\n\n        \"\"\"\n        self.masks = []\n        return self\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the developer-facing representation.\n\n        Returns\n        -------\n        str\n            Representation string.\n\n        \"\"\"\n        mask_names = [name for name, _ in self.masks]\n        return (\n            f\"SpatialMask(grid={self.grid.grid_type}, \"\n            f\"n_masks={len(self.masks)}, masks={mask_names})\"\n        )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.__init__","level":3,"title":"<code>__init__(grid)</code>","text":"<p>Initialize the spatial mask builder.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.__init__--parameters","level":5,"title":"Parameters","text":"<p>grid : GridData     Grid instance.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def __init__(self, grid: GridData) -&gt; None:\n    \"\"\"Initialize the spatial mask builder.\n\n    Parameters\n    ----------\n    grid : GridData\n        Grid instance.\n\n    \"\"\"\n    self.grid = grid\n    self.masks: list[tuple[str, np.ndarray]] = []\n    self._grid_df = grid.grid\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_phi_range","level":3,"title":"<code>add_phi_range(phi_min, phi_max, degrees=False)</code>","text":"<p>Add azimuth angle constraint.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_phi_range--parameters","level":5,"title":"Parameters","text":"<p>phi_min : float     Minimum azimuth angle. phi_max : float     Maximum azimuth angle. degrees : bool     If <code>True</code>, angles are in degrees; otherwise radians.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_phi_range--returns","level":5,"title":"Returns","text":"<p>SpatialMask     Self for chaining.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_phi_range--notes","level":5,"title":"Notes","text":"<p>Handles wraparound at 0°/360° correctly. Example: <code>phi_range(350, 10)</code> includes both 350°–360° and 0°–10°.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def add_phi_range(\n    self, phi_min: float, phi_max: float, degrees: bool = False\n) -&gt; SpatialMask:\n    \"\"\"Add azimuth angle constraint.\n\n    Parameters\n    ----------\n    phi_min : float\n        Minimum azimuth angle.\n    phi_max : float\n        Maximum azimuth angle.\n    degrees : bool\n        If ``True``, angles are in degrees; otherwise radians.\n\n    Returns\n    -------\n    SpatialMask\n        Self for chaining.\n\n    Notes\n    -----\n    Handles wraparound at 0°/360° correctly.\n    Example: ``phi_range(350, 10)`` includes both 350°–360° and 0°–10°.\n\n    \"\"\"\n    if degrees:\n        phi_min = np.radians(phi_min)\n        phi_max = np.radians(phi_max)\n\n    phi = self._grid_df[\"phi\"].to_numpy()\n\n    # Normalise to [0, 2π)\n    phi = np.mod(phi, 2 * np.pi)\n    phi_min = np.mod(phi_min, 2 * np.pi)\n    phi_max = np.mod(phi_max, 2 * np.pi)\n\n    if phi_min &lt;= phi_max:\n        mask = (phi &gt;= phi_min) &amp; (phi &lt;= phi_max)\n    else:\n        # Wraparound: e.g. 350° to 10°\n        mask = (phi &gt;= phi_min) | (phi &lt;= phi_max)\n\n    self.masks.append((\"phi_range\", mask))\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_theta_range","level":3,"title":"<code>add_theta_range(theta_min, theta_max, degrees=False)</code>","text":"<p>Add polar angle (zenith angle) constraint.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_theta_range--parameters","level":5,"title":"Parameters","text":"<p>theta_min : float     Minimum polar angle (0 = zenith). theta_max : float     Maximum polar angle (π/2 = horizon). degrees : bool     If <code>True</code>, angles are in degrees; otherwise radians.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_theta_range--returns","level":5,"title":"Returns","text":"<p>SpatialMask     Self for chaining.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_theta_range--notes","level":5,"title":"Notes","text":"<p><code>theta = 0°</code> is zenith (straight up), <code>theta = 90°</code> is horizon. To exclude low elevations, use <code>theta_max &lt; 90°</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def add_theta_range(\n    self, theta_min: float, theta_max: float, degrees: bool = False\n) -&gt; SpatialMask:\n    \"\"\"Add polar angle (zenith angle) constraint.\n\n    Parameters\n    ----------\n    theta_min : float\n        Minimum polar angle (0 = zenith).\n    theta_max : float\n        Maximum polar angle (π/2 = horizon).\n    degrees : bool\n        If ``True``, angles are in degrees; otherwise radians.\n\n    Returns\n    -------\n    SpatialMask\n        Self for chaining.\n\n    Notes\n    -----\n    ``theta = 0°`` is zenith (straight up), ``theta = 90°`` is horizon.\n    To exclude low elevations, use ``theta_max &lt; 90°``.\n\n    \"\"\"\n    if degrees:\n        theta_min = np.radians(theta_min)\n        theta_max = np.radians(theta_max)\n\n    theta = self._grid_df[\"theta\"].to_numpy()\n    mask = (theta &gt;= theta_min) &amp; (theta &lt;= theta_max)\n\n    self.masks.append((\"theta_range\", mask))\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_elevation_range","level":3,"title":"<code>add_elevation_range(elev_min, elev_max, degrees=True)</code>","text":"<p>Add elevation angle constraint (complementary to theta).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_elevation_range--parameters","level":5,"title":"Parameters","text":"<p>elev_min : float     Minimum elevation angle (0 = horizon, 90 = zenith). elev_max : float     Maximum elevation angle. degrees : bool     If <code>True</code>, angles are in degrees; otherwise radians.     Default: <code>True</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_elevation_range--returns","level":5,"title":"Returns","text":"<p>SpatialMask     Self for chaining.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_elevation_range--notes","level":5,"title":"Notes","text":"<p><code>elevation = 90° − theta</code>.  This is more intuitive for users who think in elevation angles.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def add_elevation_range(\n    self, elev_min: float, elev_max: float, degrees: bool = True\n) -&gt; SpatialMask:\n    \"\"\"Add elevation angle constraint (complementary to theta).\n\n    Parameters\n    ----------\n    elev_min : float\n        Minimum elevation angle (0 = horizon, 90 = zenith).\n    elev_max : float\n        Maximum elevation angle.\n    degrees : bool\n        If ``True``, angles are in degrees; otherwise radians.\n        Default: ``True``.\n\n    Returns\n    -------\n    SpatialMask\n        Self for chaining.\n\n    Notes\n    -----\n    ``elevation = 90° − theta``.  This is more intuitive for users\n    who think in elevation angles.\n\n    \"\"\"\n    if degrees:\n        elev_min_rad = np.radians(elev_min)\n        elev_max_rad = np.radians(elev_max)\n    else:\n        elev_min_rad = elev_min\n        elev_max_rad = elev_max\n\n    # Convert elevation → theta: theta = π/2 − elevation\n    theta_max = np.pi / 2 - elev_min_rad\n    theta_min = np.pi / 2 - elev_max_rad\n\n    return self.add_theta_range(theta_min, theta_max, degrees=False)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_cell_ids","level":3,"title":"<code>add_cell_ids(cell_ids)</code>","text":"<p>Include specific cell IDs.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_cell_ids--parameters","level":5,"title":"Parameters","text":"<p>cell_ids : list[int] or np.ndarray     Cell IDs to include.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_cell_ids--returns","level":5,"title":"Returns","text":"<p>SpatialMask     Self for chaining.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def add_cell_ids(self, cell_ids: list[int] | np.ndarray) -&gt; SpatialMask:\n    \"\"\"Include specific cell IDs.\n\n    Parameters\n    ----------\n    cell_ids : list[int] or np.ndarray\n        Cell IDs to include.\n\n    Returns\n    -------\n    SpatialMask\n        Self for chaining.\n\n    \"\"\"\n    mask = np.zeros(self.grid.ncells, dtype=bool)\n    mask[cell_ids] = True\n\n    self.masks.append((\"cell_ids\", mask))\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_exclude_cell_ids","level":3,"title":"<code>add_exclude_cell_ids(cell_ids)</code>","text":"<p>Exclude specific cell IDs.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_exclude_cell_ids--parameters","level":5,"title":"Parameters","text":"<p>cell_ids : list[int] or np.ndarray     Cell IDs to exclude.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_exclude_cell_ids--returns","level":5,"title":"Returns","text":"<p>SpatialMask     Self for chaining.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def add_exclude_cell_ids(self, cell_ids: list[int] | np.ndarray) -&gt; SpatialMask:\n    \"\"\"Exclude specific cell IDs.\n\n    Parameters\n    ----------\n    cell_ids : list[int] or np.ndarray\n        Cell IDs to exclude.\n\n    Returns\n    -------\n    SpatialMask\n        Self for chaining.\n\n    \"\"\"\n    mask = np.ones(self.grid.ncells, dtype=bool)\n    mask[cell_ids] = False\n\n    self.masks.append((\"exclude_cell_ids\", mask))\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_quality_threshold","level":3,"title":"<code>add_quality_threshold(var_name, min_value=None, max_value=None)</code>","text":"<p>Add data quality threshold based on grid cell properties.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_quality_threshold--parameters","level":5,"title":"Parameters","text":"<p>var_name : str     Variable name in grid (e.g. <code>'mean_snr'</code>,     <code>'n_observations'</code>). min_value : float, optional     Minimum allowed value. max_value : float, optional     Maximum allowed value.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_quality_threshold--returns","level":5,"title":"Returns","text":"<p>SpatialMask     Self for chaining.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_quality_threshold--raises","level":5,"title":"Raises","text":"<p>ValueError     If var_name doesn't exist in the grid DataFrame.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def add_quality_threshold(\n    self,\n    var_name: str,\n    min_value: float | None = None,\n    max_value: float | None = None,\n) -&gt; SpatialMask:\n    \"\"\"Add data quality threshold based on grid cell properties.\n\n    Parameters\n    ----------\n    var_name : str\n        Variable name in grid (e.g. ``'mean_snr'``,\n        ``'n_observations'``).\n    min_value : float, optional\n        Minimum allowed value.\n    max_value : float, optional\n        Maximum allowed value.\n\n    Returns\n    -------\n    SpatialMask\n        Self for chaining.\n\n    Raises\n    ------\n    ValueError\n        If *var_name* doesn't exist in the grid DataFrame.\n\n    \"\"\"\n    if var_name not in self._grid_df.columns:\n        available = list(self._grid_df.columns)\n        raise ValueError(\n            f\"Variable '{var_name}' not found in grid. Available: {available}\"\n        )\n\n    values = self._grid_df[var_name].to_numpy()\n    mask = np.ones(self.grid.ncells, dtype=bool)\n\n    if min_value is not None:\n        mask = mask &amp; (values &gt;= min_value)\n    if max_value is not None:\n        mask = mask &amp; (values &lt;= max_value)\n\n    self.masks.append((f\"{var_name}_threshold\", mask))\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_boundary_cells","level":3,"title":"<code>add_boundary_cells(exclude=True)</code>","text":"<p>Include or exclude boundary cells.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_boundary_cells--parameters","level":5,"title":"Parameters","text":"<p>exclude : bool     If <code>True</code>, exclude boundary cells; if <code>False</code>, include     only boundary cells.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_boundary_cells--returns","level":5,"title":"Returns","text":"<p>SpatialMask     Self for chaining.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_boundary_cells--raises","level":5,"title":"Raises","text":"<p>ValueError     If the grid does not have an <code>'is_boundary'</code> column.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_boundary_cells--notes","level":5,"title":"Notes","text":"<p>Only works if the grid DataFrame contains an <code>'is_boundary'</code> column.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def add_boundary_cells(self, exclude: bool = True) -&gt; SpatialMask:\n    \"\"\"Include or exclude boundary cells.\n\n    Parameters\n    ----------\n    exclude : bool\n        If ``True``, exclude boundary cells; if ``False``, include\n        only boundary cells.\n\n    Returns\n    -------\n    SpatialMask\n        Self for chaining.\n\n    Raises\n    ------\n    ValueError\n        If the grid does not have an ``'is_boundary'`` column.\n\n    Notes\n    -----\n    Only works if the grid DataFrame contains an ``'is_boundary'``\n    column.\n\n    \"\"\"\n    if \"is_boundary\" not in self._grid_df.columns:\n        raise ValueError(\"Grid does not have 'is_boundary' information\")\n\n    is_boundary = self._grid_df[\"is_boundary\"].to_numpy()\n    mask = ~is_boundary if exclude else is_boundary\n\n    self.masks.append((\"boundary_cells\", mask))\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_custom_mask","level":3,"title":"<code>add_custom_mask(mask, name='custom')</code>","text":"<p>Add custom mask or mask-generating callable.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_custom_mask--parameters","level":5,"title":"Parameters","text":"<p>mask : np.ndarray or callable     * If array: boolean mask of shape <code>(ncells,)</code>.     * If callable: function <code>(grid: GridData) -&gt; np.ndarray</code>. name : str     Label for this mask (used in summary).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_custom_mask--returns","level":5,"title":"Returns","text":"<p>SpatialMask     Self for chaining.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_custom_mask--raises","level":5,"title":"Raises","text":"<p>ValueError     If the resulting array has wrong shape or dtype.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_custom_mask--examples","level":5,"title":"Examples","text":"<p>custom = np.array([True, False, True, ...]) mask.add_custom_mask(custom)</p> <p>def high_snr_north(grid): ...     snr   = grid.grid['mean_snr'].to_numpy() ...     phi   = grid.grid['phi'].to_numpy() ...     return (snr &gt; 40) &amp; (phi &lt; np.pi) mask.add_custom_mask(high_snr_north)</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def add_custom_mask(\n    self,\n    mask: np.ndarray | Callable,\n    name: str = \"custom\",\n) -&gt; SpatialMask:\n    \"\"\"Add custom mask or mask-generating callable.\n\n    Parameters\n    ----------\n    mask : np.ndarray or callable\n        * If array: boolean mask of shape ``(ncells,)``.\n        * If callable: function ``(grid: GridData) -&gt; np.ndarray``.\n    name : str\n        Label for this mask (used in summary).\n\n    Returns\n    -------\n    SpatialMask\n        Self for chaining.\n\n    Raises\n    ------\n    ValueError\n        If the resulting array has wrong shape or dtype.\n\n    Examples\n    --------\n    &gt;&gt;&gt; custom = np.array([True, False, True, ...])\n    &gt;&gt;&gt; mask.add_custom_mask(custom)\n\n    &gt;&gt;&gt; def high_snr_north(grid):\n    ...     snr   = grid.grid['mean_snr'].to_numpy()\n    ...     phi   = grid.grid['phi'].to_numpy()\n    ...     return (snr &gt; 40) &amp; (phi &lt; np.pi)\n    &gt;&gt;&gt; mask.add_custom_mask(high_snr_north)\n\n    \"\"\"\n    if callable(mask):\n        mask_array = mask(self.grid)\n    else:\n        mask_array = mask\n\n    if not isinstance(mask_array, np.ndarray):\n        raise ValueError(\"Custom mask must be numpy array or return numpy array\")\n    if mask_array.shape != (self.grid.ncells,):\n        raise ValueError(\n            f\"Custom mask shape {mask_array.shape} doesn't match \"\n            f\"grid size ({self.grid.ncells},)\"\n        )\n    if mask_array.dtype != bool:\n        raise ValueError(\"Custom mask must be boolean dtype\")\n\n    self.masks.append((name, mask_array))\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_radial_sector","level":3,"title":"<code>add_radial_sector(center_phi, sector_width, theta_min=0.0, theta_max=np.pi / 2, degrees=True)</code>","text":"<p>Add radial sector (wedge) mask.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_radial_sector--parameters","level":5,"title":"Parameters","text":"<p>center_phi : float     Centre azimuth of sector. sector_width : float     Full angular width of sector. theta_min : float     Minimum polar angle (radial inner bound). theta_max : float     Maximum polar angle (radial outer bound). degrees : bool     If <code>True</code>, all angles in degrees; otherwise radians.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_radial_sector--returns","level":5,"title":"Returns","text":"<p>SpatialMask     Self for chaining.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_radial_sector--examples","level":5,"title":"Examples","text":"Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def add_radial_sector(\n    self,\n    center_phi: float,\n    sector_width: float,\n    theta_min: float = 0.0,\n    theta_max: float = np.pi / 2,\n    degrees: bool = True,\n) -&gt; SpatialMask:\n    \"\"\"Add radial sector (wedge) mask.\n\n    Parameters\n    ----------\n    center_phi : float\n        Centre azimuth of sector.\n    sector_width : float\n        Full angular width of sector.\n    theta_min : float\n        Minimum polar angle (radial inner bound).\n    theta_max : float\n        Maximum polar angle (radial outer bound).\n    degrees : bool\n        If ``True``, all angles in degrees; otherwise radians.\n\n    Returns\n    -------\n    SpatialMask\n        Self for chaining.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Northern sector, 30° wide, excluding low elevations\n    &gt;&gt;&gt; mask.add_radial_sector(center_phi=0, sector_width=30,\n    ...                        theta_max=60, degrees=True)\n\n    \"\"\"\n    if degrees:\n        center_phi = np.radians(center_phi)\n        sector_width = np.radians(sector_width)\n        theta_min = np.radians(theta_min)\n        theta_max = np.radians(theta_max)\n\n    half_width = sector_width / 2\n    self.add_phi_range(\n        center_phi - half_width,\n        center_phi + half_width,\n        degrees=False,\n    )\n    self.add_theta_range(theta_min, theta_max, degrees=False)\n\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.add_radial_sector--northern-sector-30-wide-excluding-low-elevations","level":4,"title":"Northern sector, 30° wide, excluding low elevations","text":"<p>mask.add_radial_sector(center_phi=0, sector_width=30, ...                        theta_max=60, degrees=True)</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.compute","level":3,"title":"<code>compute(mode='AND')</code>","text":"<p>Compute final combined boolean mask.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.compute--parameters","level":5,"title":"Parameters","text":"<p>mode : str     Combination mode:</p> <pre><code>* ``'AND'`` – all constraints must be satisfied (intersection).\n* ``'OR'``  – at least one constraint must be satisfied (union).\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.compute--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Boolean array of shape <code>(ncells,)</code> where <code>True</code> = include.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.compute--raises","level":5,"title":"Raises","text":"<p>ValueError     If no masks have been added or mode is unknown.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def compute(self, mode: str = \"AND\") -&gt; np.ndarray:\n    \"\"\"Compute final combined boolean mask.\n\n    Parameters\n    ----------\n    mode : str\n        Combination mode:\n\n        * ``'AND'`` – all constraints must be satisfied (intersection).\n        * ``'OR'``  – at least one constraint must be satisfied (union).\n\n    Returns\n    -------\n    np.ndarray\n        Boolean array of shape ``(ncells,)`` where ``True`` = include.\n\n    Raises\n    ------\n    ValueError\n        If no masks have been added or *mode* is unknown.\n\n    \"\"\"\n    if not self.masks:\n        raise ValueError(\"No masks added. Use add_* methods before compute()\")\n\n    if mode.upper() == \"AND\":\n        combined = np.ones(self.grid.ncells, dtype=bool)\n        for _name, mask in self.masks:\n            combined = combined &amp; mask\n    elif mode.upper() == \"OR\":\n        combined = np.zeros(self.grid.ncells, dtype=bool)\n        for _name, mask in self.masks:\n            combined = combined | mask\n    else:\n        raise ValueError(f\"Unknown mode: {mode}. Use 'AND' or 'OR'\")\n\n    return combined\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.get_mask_summary","level":3,"title":"<code>get_mask_summary()</code>","text":"<p>Get summary of all added masks.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.get_mask_summary--returns","level":5,"title":"Returns","text":"<p>dict     Dictionary with mask names and per-mask / combined cell counts.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def get_mask_summary(self) -&gt; dict:\n    \"\"\"Get summary of all added masks.\n\n    Returns\n    -------\n    dict\n        Dictionary with mask names and per-mask / combined cell counts.\n\n    \"\"\"\n    summary: dict = {\"total_cells\": self.grid.ncells, \"masks\": []}\n\n    for name, mask in self.masks:\n        n_included = int(mask.sum())\n        summary[\"masks\"].append(\n            {\n                \"name\": name,\n                \"n_cells_included\": n_included,\n                \"fraction_included\": n_included / self.grid.ncells,\n            }\n        )\n\n    if self.masks:\n        combined_and = self.compute(mode=\"AND\")\n        combined_or = self.compute(mode=\"OR\")\n        summary[\"combined\"] = {\n            \"AND\": {\n                \"n_cells\": int(combined_and.sum()),\n                \"fraction\": float(combined_and.sum() / self.grid.ncells),\n            },\n            \"OR\": {\n                \"n_cells\": int(combined_or.sum()),\n                \"fraction\": float(combined_or.sum() / self.grid.ncells),\n            },\n        }\n\n    return summary\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.clear","level":3,"title":"<code>clear()</code>","text":"<p>Clear all masks.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.clear--returns","level":5,"title":"Returns","text":"<p>SpatialMask     Self for chaining.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def clear(self) -&gt; SpatialMask:\n    \"\"\"Clear all masks.\n\n    Returns\n    -------\n    SpatialMask\n        Self for chaining.\n\n    \"\"\"\n    self.masks = []\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return the developer-facing representation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.SpatialMask.__repr__--returns","level":5,"title":"Returns","text":"<p>str     Representation string.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the developer-facing representation.\n\n    Returns\n    -------\n    str\n        Representation string.\n\n    \"\"\"\n    mask_names = [name for name, _ in self.masks]\n    return (\n        f\"SpatialMask(grid={self.grid.grid_type}, \"\n        f\"n_masks={len(self.masks)}, masks={mask_names})\"\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.create_hemisphere_mask","level":2,"title":"<code>create_hemisphere_mask(grid, hemisphere='north')</code>","text":"<p>Create mask for a cardinal hemisphere.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.create_hemisphere_mask--parameters","level":4,"title":"Parameters","text":"<p>grid : GridData     Grid instance. hemisphere : str     One of <code>'north'</code>, <code>'south'</code>, <code>'east'</code>, <code>'west'</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.create_hemisphere_mask--returns","level":4,"title":"Returns","text":"<p>np.ndarray     Boolean mask.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.create_hemisphere_mask--raises","level":4,"title":"Raises","text":"<p>ValueError     If hemisphere is not recognised.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def create_hemisphere_mask(grid: GridData, hemisphere: str = \"north\") -&gt; np.ndarray:\n    \"\"\"Create mask for a cardinal hemisphere.\n\n    Parameters\n    ----------\n    grid : GridData\n        Grid instance.\n    hemisphere : str\n        One of ``'north'``, ``'south'``, ``'east'``, ``'west'``.\n\n    Returns\n    -------\n    np.ndarray\n        Boolean mask.\n\n    Raises\n    ------\n    ValueError\n        If *hemisphere* is not recognised.\n\n    \"\"\"\n    _RANGES = {\n        \"north\": (315, 45),\n        \"south\": (135, 225),\n        \"east\": (45, 135),\n        \"west\": (225, 315),\n    }\n\n    key = hemisphere.lower()\n    if key not in _RANGES:\n        raise ValueError(\n            f\"Unknown hemisphere: {hemisphere}. Choose from {list(_RANGES.keys())}\"\n        )\n\n    mask = SpatialMask(grid)\n    mask.add_phi_range(*_RANGES[key], degrees=True)\n    return mask.compute()\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.create_elevation_mask","level":2,"title":"<code>create_elevation_mask(grid, min_elevation=30.0, max_elevation=90.0)</code>","text":"<p>Create mask based on elevation angle.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.create_elevation_mask--parameters","level":4,"title":"Parameters","text":"<p>grid : GridData     Grid instance. min_elevation : float     Minimum elevation angle in degrees (default: 30°). max_elevation : float     Maximum elevation angle in degrees (default: 90°).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.masking.create_elevation_mask--returns","level":4,"title":"Returns","text":"<p>np.ndarray     Boolean mask.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/masking.py</code> <pre><code>def create_elevation_mask(\n    grid: GridData, min_elevation: float = 30.0, max_elevation: float = 90.0\n) -&gt; np.ndarray:\n    \"\"\"Create mask based on elevation angle.\n\n    Parameters\n    ----------\n    grid : GridData\n        Grid instance.\n    min_elevation : float\n        Minimum elevation angle in degrees (default: 30°).\n    max_elevation : float\n        Maximum elevation angle in degrees (default: 90°).\n\n    Returns\n    -------\n    np.ndarray\n        Boolean mask.\n\n    \"\"\"\n    mask = SpatialMask(grid)\n    mask.add_elevation_range(min_elevation, max_elevation, degrees=True)\n    return mask.compute()\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#weighting","level":3,"title":"Weighting","text":"<p>Weighting strategies for spatial aggregation of hemispherical grid data.</p> <p>Provides tools to calculate and combine different weighting schemes for computing weighted means across grid cells.  Critical for unbiased spatial statistics when cells have different sizes or data quality.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting--classes","level":3,"title":"Classes","text":"<p>WeightCalculator        – builder for combined spatial weights.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting--convenience-functions","level":3,"title":"Convenience functions","text":"<p><code>compute_uniform_weights</code>  – equal weight per cell. <code>compute_area_weights</code>     – solid-angle-only weights.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting--notes","level":3,"title":"Notes","text":"<ul> <li>Supported weight types: <code>solid_angle</code>, <code>observation_count</code>,   <code>snr</code>, <code>sin_elevation</code>, <code>inverse_variance</code>, <code>custom</code>.</li> <li>Multiple weights are combined element-wise (multiply or add) and   optionally normalised to sum to 1.</li> <li>Dask-backed datasets are handled efficiently: only scalar statistics   are computed eagerly; masks stay lazy.</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator","level":2,"title":"<code>WeightCalculator</code>","text":"<p>Calculate and combine weights for spatial aggregation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator--parameters","level":4,"title":"Parameters","text":"<p>grid : GridData     Grid instance. ds : xr.Dataset or None     Dataset with data variables (required for data-dependent weights     such as <code>observation_count</code>, <code>snr</code>, <code>inverse_variance</code>).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator--examples","level":4,"title":"Examples","text":"<p>weights = WeightCalculator(grid, vod_ds) weights.add_weight('solid_angle') weights.add_weight('observation_count', normalize=True) total_weights = weights.compute()</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/weighting.py</code> <pre><code>class WeightCalculator:\n    \"\"\"Calculate and combine weights for spatial aggregation.\n\n    Parameters\n    ----------\n    grid : GridData\n        Grid instance.\n    ds : xr.Dataset or None\n        Dataset with data variables (required for data-dependent weights\n        such as ``observation_count``, ``snr``, ``inverse_variance``).\n\n    Examples\n    --------\n    &gt;&gt;&gt; weights = WeightCalculator(grid, vod_ds)\n    &gt;&gt;&gt; weights.add_weight('solid_angle')\n    &gt;&gt;&gt; weights.add_weight('observation_count', normalize=True)\n    &gt;&gt;&gt; total_weights = weights.compute()\n\n    \"\"\"\n\n    def __init__(self, grid: GridData, ds: xr.Dataset | None = None) -&gt; None:\n        \"\"\"Initialize the weight calculator.\n\n        Parameters\n        ----------\n        grid : GridData\n            Grid instance.\n        ds : xr.Dataset | None, optional\n            Dataset with data variables for data-dependent weights.\n\n        \"\"\"\n        self.grid = grid\n        self.ds = ds\n        self.weights: dict[str, np.ndarray] = {}\n        self.weight_params: dict[str, dict] = {}\n        self._grid_df = grid.grid\n\n    # ------------------------------------------------------------------\n    # Public API\n    # ------------------------------------------------------------------\n\n    def add_weight(\n        self,\n        weight_type: str,\n        normalize: bool = True,\n        **kwargs: Any,\n    ) -&gt; WeightCalculator:\n        \"\"\"Add a weight component.\n\n        Parameters\n        ----------\n        weight_type : str\n            One of ``'solid_angle'``, ``'observation_count'``,\n            ``'snr'``, ``'sin_elevation'``, ``'inverse_variance'``,\n            ``'custom'``.\n        normalize : bool\n            If ``True``, normalise this component to sum to 1.0 before\n            combination.\n        **kwargs\n            Weight-specific parameters (see individual ``_compute_*``\n            methods).\n\n        Returns\n        -------\n        WeightCalculator\n            Self for chaining.\n\n        Examples\n        --------\n        &gt;&gt;&gt; calc.add_weight('solid_angle')\n        &gt;&gt;&gt; calc.add_weight('observation_count', var_name='VOD', normalize=True)\n        &gt;&gt;&gt; calc.add_weight('custom', values=my_weights, normalize=False)\n\n        \"\"\"\n        if weight_type in self.weights:\n            logger.warning(f\"Weight '{weight_type}' already exists, overwriting\")\n\n        _DISPATCH = {\n            \"solid_angle\": self._compute_solid_angle_weight,\n            \"observation_count\": self._compute_observation_count_weight,\n            \"snr\": self._compute_snr_weight,\n            \"sin_elevation\": self._compute_sin_elevation_weight,\n            \"inverse_variance\": self._compute_inverse_variance_weight,\n            \"custom\": self._compute_custom_weight,\n        }\n\n        if weight_type not in _DISPATCH:\n            raise ValueError(\n                f\"Unknown weight_type: {weight_type}. Valid: {list(_DISPATCH.keys())}\"\n            )\n\n        weight = _DISPATCH[weight_type](**kwargs)\n\n        if normalize:\n            weight = self._normalize_weights(weight)\n\n        self.weights[weight_type] = weight\n        self.weight_params[weight_type] = {\"normalize\": normalize, **kwargs}\n        return self\n\n    def compute(\n        self,\n        combination: Literal[\"multiply\", \"add\"] = \"multiply\",\n        normalize_final: bool = True,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute final combined weights.\n\n        Parameters\n        ----------\n        combination : str\n            ``'multiply'`` – element-wise product (default).\n            ``'add'``      – element-wise sum.\n        normalize_final : bool\n            If ``True``, normalise the final array to sum to 1.0.\n\n        Returns\n        -------\n        np.ndarray\n            Weight array of shape ``(ncells,)``.\n\n        Raises\n        ------\n        ValueError\n            If no weights have been added or *combination* is unknown.\n\n        \"\"\"\n        if not self.weights:\n            raise ValueError(\"No weights added. Use add_weight() before compute()\")\n\n        if combination == \"multiply\":\n            combined = np.ones(self.grid.ncells)\n            for w in self.weights.values():\n                combined = combined * w\n        elif combination == \"add\":\n            combined = np.zeros(self.grid.ncells)\n            for w in self.weights.values():\n                combined = combined + w\n        else:\n            raise ValueError(f\"Unknown combination: {combination}\")\n\n        if normalize_final:\n            combined = self._normalize_weights(combined)\n\n        n_nonzero = int(np.sum(combined &gt; 0))\n        nonzero_vals = combined[combined &gt; 0]\n        logger.info(\n            f\"Computed weights: {n_nonzero}/{self.grid.ncells} cells with \"\n            f\"non-zero weight, min={nonzero_vals.min():.6f}, max={combined.max():.6f}\"\n        )\n        return combined\n\n    # ------------------------------------------------------------------\n    # Introspection\n    # ------------------------------------------------------------------\n\n    def get_weight_summary(self) -&gt; dict:\n        \"\"\"Summary statistics for each weight component.\n\n        Returns\n        -------\n        dict\n            Nested dict keyed by weight type.\n\n        \"\"\"\n        summary: dict = {\"components\": {}}\n        for wtype, weight in self.weights.items():\n            nonzero = weight &gt; 0\n            summary[\"components\"][wtype] = {\n                \"n_nonzero\": int(nonzero.sum()),\n                \"fraction_nonzero\": float(nonzero.sum() / self.grid.ncells),\n                \"min\": float(weight[nonzero].min()) if nonzero.any() else 0.0,\n                \"max\": float(weight.max()),\n                \"mean\": float(weight[nonzero].mean()) if nonzero.any() else 0.0,\n                \"params\": self.weight_params[wtype],\n            }\n        return summary\n\n    def remove_weight(self, weight_type: str) -&gt; WeightCalculator:\n        \"\"\"Remove a weight component.\n\n        Parameters\n        ----------\n        weight_type : str\n            Weight type to remove.\n\n        Returns\n        -------\n        WeightCalculator\n            Self for chaining.\n\n        \"\"\"\n        if weight_type in self.weights:\n            del self.weights[weight_type]\n            del self.weight_params[weight_type]\n            logger.debug(f\"Removed weight: {weight_type}\")\n        else:\n            logger.warning(f\"Weight '{weight_type}' not found\")\n        return self\n\n    def clear(self) -&gt; WeightCalculator:\n        \"\"\"Clear all weights.\n\n        Returns\n        -------\n        WeightCalculator\n            Self for chaining.\n\n        \"\"\"\n        self.weights = {}\n        self.weight_params = {}\n        return self\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the developer-facing representation.\n\n        Returns\n        -------\n        str\n            Representation string.\n\n        \"\"\"\n        return (\n            f\"WeightCalculator(grid={self.grid.grid_type}, \"\n            f\"weights={list(self.weights.keys())})\"\n        )\n\n    # ------------------------------------------------------------------\n    # Weight computation (private)\n    # ------------------------------------------------------------------\n\n    def _compute_solid_angle_weight(self, **kwargs: Any) -&gt; np.ndarray:\n        \"\"\"Weights based on cell solid angles (geometric fairness).\"\"\"\n        if \"solid_angle\" in self._grid_df.columns:\n            solid_angles = self._grid_df[\"solid_angle\"].to_numpy()\n        else:\n            logger.debug(\"Computing solid angles from grid geometry\")\n            solid_angles = self._compute_solid_angles_from_geometry()\n\n        if np.any(solid_angles &lt;= 0):\n            logger.warning(\"Found non-positive solid angles, setting to small value\")\n            solid_angles = np.maximum(solid_angles, 1e-10)\n\n        return solid_angles\n\n    def _compute_solid_angles_from_geometry(self) -&gt; np.ndarray:\n        \"\"\"Compute solid angles for each cell from grid geometry.\n\n        Returns\n        -------\n        np.ndarray\n            Solid angles in steradians.\n\n        Notes\n        -----\n        The sum of solid angles should equal the hemisphere area\n        (2π steradians) for a complete hemisphere.\n\n        \"\"\"\n        grid_type = self.grid.grid_type\n\n        if grid_type in (\"equal_area\", \"equal_angle\", \"equirectangular\"):\n            return self._compute_rectangular_solid_angles()\n        if grid_type == \"htm\":\n            return self._compute_htm_solid_angles()\n        if grid_type == \"geodesic\":\n            return self._compute_geodesic_solid_angles()\n        if grid_type in (\"healpix\", \"fibonacci\"):\n            # Both are (approximately) equal-area\n            theta = self._grid_df[\"theta\"].to_numpy()\n            hemisphere_cells = int(np.sum(theta &lt;= np.pi / 2))\n            if hemisphere_cells &gt; 0:\n                cell_area = (2 * np.pi) / hemisphere_cells\n                return np.full(self.grid.ncells, cell_area)\n            return np.zeros(self.grid.ncells)\n        logger.warning(f\"Unknown grid type: {grid_type}, using uniform weights\")\n        return np.full(self.grid.ncells, (2 * np.pi) / self.grid.ncells)\n\n    def _compute_rectangular_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Solid angle = Δφ × [cos(θ_min) − cos(θ_max)].\"\"\"\n        phi_min = self._grid_df[\"phi_min\"].to_numpy()\n        phi_max = self._grid_df[\"phi_max\"].to_numpy()\n        theta_min = self._grid_df[\"theta_min\"].to_numpy()\n        theta_max = self._grid_df[\"theta_max\"].to_numpy()\n\n        delta_phi = phi_max - phi_min\n        return delta_phi * (np.cos(theta_min) - np.cos(theta_max))\n\n    def _compute_htm_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Solid angles for HTM triangular cells via L'Huilier's theorem.\n\n        For each spherical triangle with vertices on the unit sphere the\n        solid angle is computed as:\n\n            Ω = 4 arctan √(tan(s/2) tan((s−a)/2) tan((s−b)/2) tan((s−c)/2))\n\n        where *a*, *b*, *c* are arc-lengths between vertex pairs and\n        *s* = (a+b+c)/2.\n        \"\"\"\n        solid_angles = np.zeros(self.grid.ncells)\n\n        for i, row in enumerate(self._grid_df.iter_rows(named=True)):\n            try:\n                v0 = np.array(row[\"htm_vertex_0\"], dtype=float)\n                v1 = np.array(row[\"htm_vertex_1\"], dtype=float)\n                v2 = np.array(row[\"htm_vertex_2\"], dtype=float)\n\n                # Skip cells significantly below the horizon\n                if v0[2] &lt; -0.01 or v1[2] &lt; -0.01 or v2[2] &lt; -0.01:\n                    continue\n\n                # Normalise to unit sphere\n                v0 = v0 / np.linalg.norm(v0)\n                v1 = v1 / np.linalg.norm(v1)\n                v2 = v2 / np.linalg.norm(v2)\n\n                # Arc lengths between vertex pairs\n                a = np.arccos(np.clip(np.dot(v1, v2), -1, 1))\n                b = np.arccos(np.clip(np.dot(v0, v2), -1, 1))\n                c = np.arccos(np.clip(np.dot(v0, v1), -1, 1))\n\n                s = (a + b + c) / 2  # semi-perimeter\n\n                product = (\n                    np.tan(s / 2)\n                    * np.tan((s - a) / 2)\n                    * np.tan((s - b) / 2)\n                    * np.tan((s - c) / 2)\n                )\n\n                solid_angles[i] = (\n                    4 * np.arctan(np.sqrt(product)) if product &gt; 0 else 0.0\n                )\n\n            except Exception as e:\n                logger.warning(f\"Error computing HTM solid angle {i}: {e}\")\n\n        return solid_angles\n\n    def _compute_geodesic_solid_angles(self) -&gt; np.ndarray:\n        \"\"\"Solid angles for geodesic cells from vertex data.\"\"\"\n        import polars as pl\n\n        solid_angles = np.zeros(self.grid.ncells)\n\n        if hasattr(self.grid, \"vertices\") and self.grid.vertices is not None:\n            vertices_df = self.grid.vertices\n\n            for cell_id in range(self.grid.ncells):\n                try:\n                    cell_verts = vertices_df.filter(pl.col(\"cell_id\") == cell_id).sort(\n                        \"vertex_idx\"\n                    )\n                    if len(cell_verts) &lt; 3:\n                        continue\n\n                    x = cell_verts[\"x\"].to_numpy()[:3]\n                    y = cell_verts[\"y\"].to_numpy()[:3]\n                    z = cell_verts[\"z\"].to_numpy()[:3]\n\n                    v0 = np.array([x[0], y[0], z[0]])\n                    v1 = np.array([x[1], y[1], z[1]])\n                    v2 = np.array([x[2], y[2], z[2]])\n\n                    v0 = v0 / np.linalg.norm(v0)\n                    v1 = v1 / np.linalg.norm(v1)\n                    v2 = v2 / np.linalg.norm(v2)\n\n                    numerator = np.abs(np.dot(v0, np.cross(v1, v2)))\n                    denominator = 1 + np.dot(v0, v1) + np.dot(v1, v2) + np.dot(v2, v0)\n\n                    solid_angles[cell_id] = 2 * np.arctan2(\n                        numerator,\n                        denominator,\n                    )\n\n                except Exception as e:\n                    logger.warning(\n                        f\"Error computing geodesic solid angle {cell_id}: {e}\"\n                    )\n        else:\n            # Approximate: equal area\n            solid_angles = np.full(\n                self.grid.ncells,\n                (2 * np.pi) / self.grid.ncells,\n            )\n\n        return solid_angles\n\n    def _compute_observation_count_weight(\n        self,\n        var_name: str = \"VOD\",\n        cell_id_var: str | None = None,\n        min_count: int = 1,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        \"\"\"Weights based on number of observations per cell.\n\n        Parameters\n        ----------\n        var_name : str\n            Variable to count observations for.\n        cell_id_var : str, optional\n            Cell-ID variable name (auto-detected if ``None``).\n        min_count : int\n            Cells with fewer observations get weight 0.\n\n        \"\"\"\n        if self.ds is None:\n            raise ValueError(\"Dataset required for observation_count weights.\")\n\n        cell_id_var = self._resolve_cell_id_var(cell_id_var)\n\n        if var_name not in self.ds:\n            raise ValueError(f\"Variable '{var_name}' not found in dataset\")\n\n        logger.info(\"Computing observation counts (this may take 30-60 seconds)...\")\n\n        cell_ids_da = self.ds[cell_id_var].data.ravel()\n        var_values_da = self.ds[var_name].data.ravel()\n\n        valid_mask = da.isfinite(cell_ids_da) &amp; da.isfinite(var_values_da)\n        valid_cell_ids = da.where(valid_mask, cell_ids_da, -1).astype(np.int32)\n\n        counts, _ = da.histogram(\n            valid_cell_ids[valid_cell_ids &gt;= 0],\n            bins=np.arange(-0.5, self.grid.ncells + 0.5),\n        )\n        counts = counts.compute()\n\n        weights = counts.astype(float)\n        weights[counts &lt; min_count] = 0.0\n\n        logger.info(\n            f\"Observation counts: min={counts.min()}, max={counts.max()}, \"\n            f\"mean={counts[counts &gt; 0].mean():.1f}, \"\n            f\"cells_with_data={np.sum(counts &gt; 0)}\"\n        )\n        return weights\n\n    def _compute_snr_weight(\n        self,\n        snr_var: str = \"SNR\",\n        cell_id_var: str | None = None,\n        aggregation: Literal[\"mean\", \"median\", \"max\"] = \"mean\",\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        \"\"\"Weights based on signal-to-noise ratio.\n\n        Parameters\n        ----------\n        snr_var : str\n            SNR variable name in the dataset.\n        cell_id_var : str, optional\n            Cell-ID variable name.\n        aggregation : str\n            Per-cell aggregation: ``'mean'``, ``'median'``, or ``'max'``.\n\n        \"\"\"\n        if self.ds is None:\n            raise ValueError(\"Dataset required for SNR weights\")\n\n        # Use pre-aggregated column if available\n        if \"mean_snr\" in self._grid_df.columns:\n            logger.info(\"Using pre-computed mean_snr from grid\")\n            return self._grid_df[\"mean_snr\"].to_numpy()\n\n        if snr_var not in self.ds:\n            raise ValueError(f\"SNR variable '{snr_var}' not found in dataset\")\n\n        cell_id_var = self._resolve_cell_id_var(cell_id_var)\n\n        cell_ids = self.ds[cell_id_var].values.ravel()\n        snr_values = self.ds[snr_var].values.ravel()\n\n        valid = np.isfinite(cell_ids) &amp; np.isfinite(snr_values)\n        cell_ids_valid = cell_ids[valid].astype(int)\n        snr_valid = snr_values[valid]\n\n        _AGG = {\"mean\": np.mean, \"median\": np.median, \"max\": np.max}\n        if aggregation not in _AGG:\n            raise ValueError(f\"Unknown aggregation: {aggregation}\")\n        agg_fn = _AGG[aggregation]\n\n        snr_per_cell = np.zeros(self.grid.ncells)\n        for cell_id in range(self.grid.ncells):\n            cell_mask = cell_ids_valid == cell_id\n            if np.any(cell_mask):\n                snr_per_cell[cell_id] = agg_fn(snr_valid[cell_mask])\n\n        return snr_per_cell\n\n    def _compute_sin_elevation_weight(self, **kwargs: Any) -&gt; np.ndarray:\n        \"\"\"Geometric correction: weight = sin(elevation) = cos(θ).\n\n        Higher elevation → shorter atmospheric path → higher weight.\n        \"\"\"\n        theta = self._grid_df[\"theta\"].to_numpy()\n        return np.maximum(np.cos(theta), 0.0)\n\n    def _compute_inverse_variance_weight(\n        self,\n        var_name: str = \"VOD\",\n        cell_id_var: str | None = None,\n        min_observations: int = 2,\n        regularization: float = 0.0,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        \"\"\"Precision weighting: weight = 1 / (variance + regularization).\n\n        Parameters\n        ----------\n        var_name : str\n            Variable to compute per-cell variance for.\n        cell_id_var : str, optional\n            Cell-ID variable name.\n        min_observations : int\n            Minimum observations to compute variance.\n        regularization : float\n            Added to variance to avoid division by zero.\n\n        \"\"\"\n        if self.ds is None:\n            raise ValueError(\"Dataset required for inverse_variance weights\")\n        if var_name not in self.ds:\n            raise ValueError(f\"Variable '{var_name}' not found in dataset\")\n\n        cell_id_var = self._resolve_cell_id_var(cell_id_var)\n\n        cell_ids = self.ds[cell_id_var].values.ravel()\n        var_values = self.ds[var_name].values.ravel()\n\n        valid = np.isfinite(cell_ids) &amp; np.isfinite(var_values)\n        cell_ids_valid = cell_ids[valid].astype(int)\n        var_valid = var_values[valid]\n\n        variances = np.full(self.grid.ncells, np.nan)\n        for cell_id in range(self.grid.ncells):\n            cell_mask = cell_ids_valid == cell_id\n            if np.sum(cell_mask) &gt;= min_observations:\n                variances[cell_id] = np.var(var_valid[cell_mask], ddof=1)\n\n        weights = np.zeros(self.grid.ncells)\n        valid_var = np.isfinite(variances) &amp; (variances &gt; 0)\n        if np.any(valid_var):\n            weights[valid_var] = 1.0 / (variances[valid_var] + regularization)\n\n        return weights\n\n    def _compute_custom_weight(\n        self,\n        values: np.ndarray,\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        \"\"\"User-provided weight array.\n\n        Parameters\n        ----------\n        values : np.ndarray\n            Must have shape ``(ncells,)``.\n\n        \"\"\"\n        if not isinstance(values, np.ndarray):\n            raise TypeError(\"Custom weights must be numpy array\")\n        if values.shape != (self.grid.ncells,):\n            raise ValueError(\n                f\"Custom weights shape {values.shape} doesn't match \"\n                f\"grid size ({self.grid.ncells},)\"\n            )\n        return values.copy()\n\n    # ------------------------------------------------------------------\n    # Helpers\n    # ------------------------------------------------------------------\n\n    def _resolve_cell_id_var(self, cell_id_var: str | None) -&gt; str:\n        \"\"\"Auto-detect cell-ID variable if not specified.\n\n        Parameters\n        ----------\n        cell_id_var : str or None\n            Explicit name, or ``None`` for auto-detection.\n\n        Returns\n        -------\n        str\n            Resolved variable name.\n\n        Raises\n        ------\n        ValueError\n            If no ``cell_id_*`` variable is found.\n\n        \"\"\"\n        if cell_id_var is not None:\n            return cell_id_var\n\n        candidate = f\"cell_id_{self.grid.grid_type}\"\n        if candidate in self.ds:\n            return candidate\n\n        cell_vars = [v for v in self.ds.data_vars if v.startswith(\"cell_id_\")]\n        if cell_vars:\n            logger.info(f\"Auto-detected cell_id variable: {cell_vars[0]}\")\n            return cell_vars[0]\n\n        raise ValueError(\"No cell_id variable found in dataset.\")\n\n    @staticmethod\n    def _normalize_weights(weights: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Normalise weights to sum to 1.0, handling NaN and zeros.\"\"\"\n        weights = np.nan_to_num(weights, nan=0.0)\n        weight_sum = weights.sum()\n        if weight_sum &gt; 0:\n            return weights / weight_sum\n        logger.warning(\"All weights are zero, returning uniform weights\")\n        return np.ones_like(weights) / len(weights)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.__init__","level":3,"title":"<code>__init__(grid, ds=None)</code>","text":"<p>Initialize the weight calculator.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.__init__--parameters","level":5,"title":"Parameters","text":"<p>grid : GridData     Grid instance. ds : xr.Dataset | None, optional     Dataset with data variables for data-dependent weights.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/weighting.py</code> <pre><code>def __init__(self, grid: GridData, ds: xr.Dataset | None = None) -&gt; None:\n    \"\"\"Initialize the weight calculator.\n\n    Parameters\n    ----------\n    grid : GridData\n        Grid instance.\n    ds : xr.Dataset | None, optional\n        Dataset with data variables for data-dependent weights.\n\n    \"\"\"\n    self.grid = grid\n    self.ds = ds\n    self.weights: dict[str, np.ndarray] = {}\n    self.weight_params: dict[str, dict] = {}\n    self._grid_df = grid.grid\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.add_weight","level":3,"title":"<code>add_weight(weight_type, normalize=True, **kwargs)</code>","text":"<p>Add a weight component.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.add_weight--parameters","level":5,"title":"Parameters","text":"<p>weight_type : str     One of <code>'solid_angle'</code>, <code>'observation_count'</code>,     <code>'snr'</code>, <code>'sin_elevation'</code>, <code>'inverse_variance'</code>,     <code>'custom'</code>. normalize : bool     If <code>True</code>, normalise this component to sum to 1.0 before     combination. **kwargs     Weight-specific parameters (see individual <code>_compute_*</code>     methods).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.add_weight--returns","level":5,"title":"Returns","text":"<p>WeightCalculator     Self for chaining.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.add_weight--examples","level":5,"title":"Examples","text":"<p>calc.add_weight('solid_angle') calc.add_weight('observation_count', var_name='VOD', normalize=True) calc.add_weight('custom', values=my_weights, normalize=False)</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/weighting.py</code> <pre><code>def add_weight(\n    self,\n    weight_type: str,\n    normalize: bool = True,\n    **kwargs: Any,\n) -&gt; WeightCalculator:\n    \"\"\"Add a weight component.\n\n    Parameters\n    ----------\n    weight_type : str\n        One of ``'solid_angle'``, ``'observation_count'``,\n        ``'snr'``, ``'sin_elevation'``, ``'inverse_variance'``,\n        ``'custom'``.\n    normalize : bool\n        If ``True``, normalise this component to sum to 1.0 before\n        combination.\n    **kwargs\n        Weight-specific parameters (see individual ``_compute_*``\n        methods).\n\n    Returns\n    -------\n    WeightCalculator\n        Self for chaining.\n\n    Examples\n    --------\n    &gt;&gt;&gt; calc.add_weight('solid_angle')\n    &gt;&gt;&gt; calc.add_weight('observation_count', var_name='VOD', normalize=True)\n    &gt;&gt;&gt; calc.add_weight('custom', values=my_weights, normalize=False)\n\n    \"\"\"\n    if weight_type in self.weights:\n        logger.warning(f\"Weight '{weight_type}' already exists, overwriting\")\n\n    _DISPATCH = {\n        \"solid_angle\": self._compute_solid_angle_weight,\n        \"observation_count\": self._compute_observation_count_weight,\n        \"snr\": self._compute_snr_weight,\n        \"sin_elevation\": self._compute_sin_elevation_weight,\n        \"inverse_variance\": self._compute_inverse_variance_weight,\n        \"custom\": self._compute_custom_weight,\n    }\n\n    if weight_type not in _DISPATCH:\n        raise ValueError(\n            f\"Unknown weight_type: {weight_type}. Valid: {list(_DISPATCH.keys())}\"\n        )\n\n    weight = _DISPATCH[weight_type](**kwargs)\n\n    if normalize:\n        weight = self._normalize_weights(weight)\n\n    self.weights[weight_type] = weight\n    self.weight_params[weight_type] = {\"normalize\": normalize, **kwargs}\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.compute","level":3,"title":"<code>compute(combination='multiply', normalize_final=True)</code>","text":"<p>Compute final combined weights.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.compute--parameters","level":5,"title":"Parameters","text":"<p>combination : str     <code>'multiply'</code> – element-wise product (default).     <code>'add'</code>      – element-wise sum. normalize_final : bool     If <code>True</code>, normalise the final array to sum to 1.0.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.compute--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Weight array of shape <code>(ncells,)</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.compute--raises","level":5,"title":"Raises","text":"<p>ValueError     If no weights have been added or combination is unknown.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/weighting.py</code> <pre><code>def compute(\n    self,\n    combination: Literal[\"multiply\", \"add\"] = \"multiply\",\n    normalize_final: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Compute final combined weights.\n\n    Parameters\n    ----------\n    combination : str\n        ``'multiply'`` – element-wise product (default).\n        ``'add'``      – element-wise sum.\n    normalize_final : bool\n        If ``True``, normalise the final array to sum to 1.0.\n\n    Returns\n    -------\n    np.ndarray\n        Weight array of shape ``(ncells,)``.\n\n    Raises\n    ------\n    ValueError\n        If no weights have been added or *combination* is unknown.\n\n    \"\"\"\n    if not self.weights:\n        raise ValueError(\"No weights added. Use add_weight() before compute()\")\n\n    if combination == \"multiply\":\n        combined = np.ones(self.grid.ncells)\n        for w in self.weights.values():\n            combined = combined * w\n    elif combination == \"add\":\n        combined = np.zeros(self.grid.ncells)\n        for w in self.weights.values():\n            combined = combined + w\n    else:\n        raise ValueError(f\"Unknown combination: {combination}\")\n\n    if normalize_final:\n        combined = self._normalize_weights(combined)\n\n    n_nonzero = int(np.sum(combined &gt; 0))\n    nonzero_vals = combined[combined &gt; 0]\n    logger.info(\n        f\"Computed weights: {n_nonzero}/{self.grid.ncells} cells with \"\n        f\"non-zero weight, min={nonzero_vals.min():.6f}, max={combined.max():.6f}\"\n    )\n    return combined\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.get_weight_summary","level":3,"title":"<code>get_weight_summary()</code>","text":"<p>Summary statistics for each weight component.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.get_weight_summary--returns","level":5,"title":"Returns","text":"<p>dict     Nested dict keyed by weight type.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/weighting.py</code> <pre><code>def get_weight_summary(self) -&gt; dict:\n    \"\"\"Summary statistics for each weight component.\n\n    Returns\n    -------\n    dict\n        Nested dict keyed by weight type.\n\n    \"\"\"\n    summary: dict = {\"components\": {}}\n    for wtype, weight in self.weights.items():\n        nonzero = weight &gt; 0\n        summary[\"components\"][wtype] = {\n            \"n_nonzero\": int(nonzero.sum()),\n            \"fraction_nonzero\": float(nonzero.sum() / self.grid.ncells),\n            \"min\": float(weight[nonzero].min()) if nonzero.any() else 0.0,\n            \"max\": float(weight.max()),\n            \"mean\": float(weight[nonzero].mean()) if nonzero.any() else 0.0,\n            \"params\": self.weight_params[wtype],\n        }\n    return summary\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.remove_weight","level":3,"title":"<code>remove_weight(weight_type)</code>","text":"<p>Remove a weight component.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.remove_weight--parameters","level":5,"title":"Parameters","text":"<p>weight_type : str     Weight type to remove.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.remove_weight--returns","level":5,"title":"Returns","text":"<p>WeightCalculator     Self for chaining.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/weighting.py</code> <pre><code>def remove_weight(self, weight_type: str) -&gt; WeightCalculator:\n    \"\"\"Remove a weight component.\n\n    Parameters\n    ----------\n    weight_type : str\n        Weight type to remove.\n\n    Returns\n    -------\n    WeightCalculator\n        Self for chaining.\n\n    \"\"\"\n    if weight_type in self.weights:\n        del self.weights[weight_type]\n        del self.weight_params[weight_type]\n        logger.debug(f\"Removed weight: {weight_type}\")\n    else:\n        logger.warning(f\"Weight '{weight_type}' not found\")\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.clear","level":3,"title":"<code>clear()</code>","text":"<p>Clear all weights.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.clear--returns","level":5,"title":"Returns","text":"<p>WeightCalculator     Self for chaining.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/weighting.py</code> <pre><code>def clear(self) -&gt; WeightCalculator:\n    \"\"\"Clear all weights.\n\n    Returns\n    -------\n    WeightCalculator\n        Self for chaining.\n\n    \"\"\"\n    self.weights = {}\n    self.weight_params = {}\n    return self\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return the developer-facing representation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.WeightCalculator.__repr__--returns","level":5,"title":"Returns","text":"<p>str     Representation string.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/weighting.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the developer-facing representation.\n\n    Returns\n    -------\n    str\n        Representation string.\n\n    \"\"\"\n    return (\n        f\"WeightCalculator(grid={self.grid.grid_type}, \"\n        f\"weights={list(self.weights.keys())})\"\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.compute_uniform_weights","level":2,"title":"<code>compute_uniform_weights(grid)</code>","text":"<p>Uniform weights (all cells equal).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.compute_uniform_weights--parameters","level":4,"title":"Parameters","text":"<p>grid : GridData     Grid instance.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.compute_uniform_weights--returns","level":4,"title":"Returns","text":"<p>np.ndarray     Array of <code>1 / ncells</code> for each cell.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/weighting.py</code> <pre><code>def compute_uniform_weights(grid: GridData) -&gt; np.ndarray:\n    \"\"\"Uniform weights (all cells equal).\n\n    Parameters\n    ----------\n    grid : GridData\n        Grid instance.\n\n    Returns\n    -------\n    np.ndarray\n        Array of ``1 / ncells`` for each cell.\n\n    \"\"\"\n    return np.ones(grid.ncells) / grid.ncells\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.compute_area_weights","level":2,"title":"<code>compute_area_weights(grid, normalize=True)</code>","text":"<p>Weights based on cell solid angles only.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.compute_area_weights--parameters","level":4,"title":"Parameters","text":"<p>grid : GridData     Grid instance. normalize : bool     If <code>True</code>, normalise to sum to 1.0.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.weighting.compute_area_weights--returns","level":4,"title":"Returns","text":"<p>np.ndarray     Area-based weights.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/weighting.py</code> <pre><code>def compute_area_weights(grid: GridData, normalize: bool = True) -&gt; np.ndarray:\n    \"\"\"Weights based on cell solid angles only.\n\n    Parameters\n    ----------\n    grid : GridData\n        Grid instance.\n    normalize : bool\n        If ``True``, normalise to sum to 1.0.\n\n    Returns\n    -------\n    np.ndarray\n        Area-based weights.\n\n    \"\"\"\n    calc = WeightCalculator(grid)\n    calc.add_weight(\"solid_angle\", normalize=normalize)\n    return calc.compute(normalize_final=normalize)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#solar-geometry","level":3,"title":"Solar Geometry","text":"<p>Solar position calculations and corrections for VOD data.</p> <p>Provides tools to compute solar positions and apply corrections to account for solar radiation effects on vegetation optical depth measurements.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar--classes","level":3,"title":"Classes","text":"<p>SolarPositionCalculator – solar zenith / azimuth and VOD correction.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar--convenience-functions","level":3,"title":"Convenience functions","text":"<p><code>compute_solar_zenith</code>   – quick zenith-only computation. <code>filter_daytime_data</code>    – mask nighttime observations.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar--notes","level":3,"title":"Notes","text":"<ul> <li>When pvlib is installed it is preferred for high-accuracy   calculations.  The built-in fallback uses NOAA algorithms   (accuracy ~0.01° for 1800–2100).</li> <li>All public methods accept either <code>np.ndarray</code> of <code>datetime64</code>   or <code>pd.DatetimeIndex</code>.</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator","level":2,"title":"<code>SolarPositionCalculator</code>","text":"<p>Calculate solar positions for VOD corrections.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator--parameters","level":4,"title":"Parameters","text":"<p>lat : float     Observer latitude in degrees (positive = North). lon : float     Observer longitude in degrees (positive = East). elevation : float     Elevation above sea level in metres (default: 0). use_pvlib : bool     If <code>True</code>, use pvlib when available (more accurate).     Falls back to built-in formulas automatically.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator--examples","level":4,"title":"Examples","text":"<p>calc = SolarPositionCalculator(lat=40.0, lon=-105.0, elevation=1655) times = pd.date_range('2025-01-01', periods=24, freq='1H') zenith, azimuth = calc.compute_solar_position(times) corrected = calc.apply_solar_correction(vod_data, times)</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/solar.py</code> <pre><code>class SolarPositionCalculator:\n    \"\"\"Calculate solar positions for VOD corrections.\n\n    Parameters\n    ----------\n    lat : float\n        Observer latitude in degrees (positive = North).\n    lon : float\n        Observer longitude in degrees (positive = East).\n    elevation : float\n        Elevation above sea level in metres (default: 0).\n    use_pvlib : bool\n        If ``True``, use *pvlib* when available (more accurate).\n        Falls back to built-in formulas automatically.\n\n    Examples\n    --------\n    &gt;&gt;&gt; calc = SolarPositionCalculator(lat=40.0, lon=-105.0, elevation=1655)\n    &gt;&gt;&gt; times = pd.date_range('2025-01-01', periods=24, freq='1H')\n    &gt;&gt;&gt; zenith, azimuth = calc.compute_solar_position(times)\n    &gt;&gt;&gt; corrected = calc.apply_solar_correction(vod_data, times)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        lat: float,\n        lon: float,\n        elevation: float = 0.0,\n        use_pvlib: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the solar position calculator.\n\n        Parameters\n        ----------\n        lat : float\n            Observer latitude in degrees.\n        lon : float\n            Observer longitude in degrees.\n        elevation : float, default 0.0\n            Elevation above sea level in metres.\n        use_pvlib : bool, default True\n            Whether to use pvlib if available.\n\n        \"\"\"\n        self.lat = lat\n        self.lon = lon\n        self.elevation = elevation\n        self.use_pvlib = use_pvlib\n        self.pvlib = None\n\n        if use_pvlib:\n            try:\n                import pvlib\n\n                self.pvlib = pvlib\n                logger.info(\"Using pvlib for solar position calculations\")\n            except ImportError:\n                logger.warning(\n                    \"pvlib not available, falling back to built-in formulas. \"\n                    \"Install pvlib for higher accuracy: pip install pvlib\"\n                )\n                self.use_pvlib = False\n\n    # ------------------------------------------------------------------\n    # Core position computation\n    # ------------------------------------------------------------------\n\n    def compute_solar_position(\n        self, times: np.ndarray | pd.DatetimeIndex\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Compute solar zenith and azimuth angles.\n\n        Parameters\n        ----------\n        times : np.ndarray or pd.DatetimeIndex\n            Array of ``datetime64`` or ``DatetimeIndex``.\n\n        Returns\n        -------\n        solar_zenith : np.ndarray\n            Solar zenith angles in degrees (0° = directly overhead).\n        solar_azimuth : np.ndarray\n            Solar azimuth angles in degrees (0° = North, 90° = East).\n\n        \"\"\"\n        if isinstance(times, np.ndarray):\n            times = pd.to_datetime(times)\n\n        if self.use_pvlib and self.pvlib is not None:\n            return self._compute_solar_position_pvlib(times)\n        return self._compute_solar_position_builtin(times)\n\n    def _compute_solar_position_pvlib(\n        self, times: pd.DatetimeIndex\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Compute solar position using pvlib (high accuracy).\"\"\"\n        location = self.pvlib.location.Location(\n            latitude=self.lat,\n            longitude=self.lon,\n            altitude=self.elevation,\n            tz=\"UTC\",\n        )\n\n        if times.tz is None:\n            times = times.tz_localize(\"UTC\")\n\n        solar_position = location.get_solarposition(times)\n\n        zenith = solar_position[\"apparent_zenith\"].values\n        azimuth = solar_position[\"azimuth\"].values\n        return zenith, azimuth\n\n    def _compute_solar_position_builtin(\n        self, times: pd.DatetimeIndex\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Compute solar position using built-in NOAA algorithms.\n\n        Accuracy ~0.01° for years 1800–2100.\n        \"\"\"\n        jd = self._datetime_to_julian_day(times)\n        jc = (jd - 2451545.0) / 36525.0\n\n        # Geometric mean longitude of sun (degrees)\n        geom_mean_long = (280.46646 + jc * (36000.76983 + jc * 0.0003032)) % 360\n\n        # Geometric mean anomaly of sun (degrees)\n        geom_mean_anom = 357.52911 + jc * (35999.05029 - 0.0001537 * jc)\n\n        # Eccentricity of Earth's orbit\n        eccent = 0.016708634 - jc * (0.000042037 + 0.0000001267 * jc)\n\n        # Sun equation of centre\n        sun_eq_ctr = (\n            np.sin(np.radians(geom_mean_anom))\n            * (1.914602 - jc * (0.004817 + 0.000014 * jc))\n            + np.sin(np.radians(2 * geom_mean_anom)) * (0.019993 - 0.000101 * jc)\n            + np.sin(np.radians(3 * geom_mean_anom)) * 0.000289\n        )\n\n        # Sun true longitude (degrees)\n        sun_true_long = geom_mean_long + sun_eq_ctr\n\n        # Sun apparent longitude (degrees)\n        sun_app_long = (\n            sun_true_long\n            - 0.00569\n            - 0.00478 * np.sin(np.radians(125.04 - 1934.136 * jc))\n        )\n\n        # Mean obliquity of ecliptic (degrees)\n        mean_obliq_ecliptic = (\n            23\n            + (26 + (21.448 - jc * (46.815 + jc * (0.00059 - jc * 0.001813))) / 60) / 60\n        )\n\n        # Obliquity correction (degrees)\n        obliq_corr = mean_obliq_ecliptic + 0.00256 * np.cos(\n            np.radians(125.04 - 1934.136 * jc)\n        )\n\n        # Sun declination (degrees)\n        sun_decl = np.degrees(\n            np.arcsin(np.sin(np.radians(obliq_corr)) * np.sin(np.radians(sun_app_long)))\n        )\n\n        # Equation of time (minutes)\n        var_y = np.tan(np.radians(obliq_corr / 2)) ** 2\n        eq_of_time = 4 * np.degrees(\n            var_y * np.sin(2 * np.radians(geom_mean_long))\n            - 2 * eccent * np.sin(np.radians(geom_mean_anom))\n            + 4\n            * eccent\n            * var_y\n            * np.sin(np.radians(geom_mean_anom))\n            * np.cos(2 * np.radians(geom_mean_long))\n            - 0.5 * var_y * var_y * np.sin(4 * np.radians(geom_mean_long))\n            - 1.25 * eccent * eccent * np.sin(2 * np.radians(geom_mean_anom))\n        )\n\n        # True solar time (minutes)\n        time_offset = eq_of_time + 4 * self.lon\n        hour = times.hour + times.minute / 60.0 + times.second / 3600.0\n        true_solar_time = (hour * 60 + time_offset) % 1440\n\n        # Hour angle (degrees)\n        hour_angle = true_solar_time / 4 - 180\n        hour_angle = np.where(hour_angle &lt; 0, hour_angle + 360, hour_angle)\n\n        # Solar zenith angle (degrees)\n        lat_rad = np.radians(self.lat)\n        decl_rad = np.radians(sun_decl)\n        ha_rad = np.radians(hour_angle)\n\n        zenith = np.degrees(\n            np.arccos(\n                np.sin(lat_rad) * np.sin(decl_rad)\n                + np.cos(lat_rad) * np.cos(decl_rad) * np.cos(ha_rad)\n            )\n        )\n\n        # Solar azimuth angle (degrees from North)\n        azimuth_rad = np.arccos(\n            (np.sin(lat_rad) * np.cos(np.radians(zenith)) - np.sin(decl_rad))\n            / (np.cos(lat_rad) * np.sin(np.radians(zenith)))\n        )\n        azimuth = np.degrees(azimuth_rad)\n\n        # Adjust for morning vs afternoon\n        azimuth = np.where(hour_angle &gt; 0, azimuth, 360 - azimuth)\n\n        return zenith, azimuth\n\n    @staticmethod\n    def _datetime_to_julian_day(times: pd.DatetimeIndex) -&gt; np.ndarray:\n        \"\"\"Convert datetime to Julian Day Number.\"\"\"\n        dt = pd.to_datetime(times if not isinstance(times, pd.DatetimeIndex) else times)\n        year = dt.year\n        month = dt.month\n        day = dt.day\n        hour = dt.hour\n        minute = dt.minute\n        second = dt.second\n\n        a = (14 - month) // 12\n        y = year + 4800 - a\n        m = month + 12 * a - 3\n\n        jdn = day + (153 * m + 2) // 5 + 365 * y + y // 4 - y // 100 + y // 400 - 32045\n        fraction = (hour - 12) / 24.0 + minute / 1440.0 + second / 86400.0\n\n        return jdn + fraction\n\n    # ------------------------------------------------------------------\n    # Derived quantities\n    # ------------------------------------------------------------------\n\n    def compute_solar_elevation(\n        self, times: np.ndarray | pd.DatetimeIndex\n    ) -&gt; np.ndarray:\n        \"\"\"Compute solar elevation angle (complementary to zenith).\n\n        Parameters\n        ----------\n        times : np.ndarray or pd.DatetimeIndex\n            Array of times.\n\n        Returns\n        -------\n        np.ndarray\n            Solar elevation angles in degrees (0° = horizon, 90° = overhead).\n\n        \"\"\"\n        zenith, _ = self.compute_solar_position(times)\n        return 90.0 - zenith\n\n    def is_daytime(\n        self,\n        times: np.ndarray | pd.DatetimeIndex,\n        twilight_angle: float = -6.0,\n    ) -&gt; np.ndarray:\n        \"\"\"Determine if times are during daytime.\n\n        Parameters\n        ----------\n        times : np.ndarray or pd.DatetimeIndex\n            Array of times.\n        twilight_angle : float\n            Solar elevation threshold in degrees.\n            Common values: ``0`` (geometric), ``-6`` (civil),\n            ``-12`` (nautical), ``-18`` (astronomical).\n\n        Returns\n        -------\n        np.ndarray\n            Boolean array (``True`` = daytime).\n\n        \"\"\"\n        elevation = self.compute_solar_elevation(times)\n        return elevation &gt; twilight_angle\n\n    # ------------------------------------------------------------------\n    # Solar correction\n    # ------------------------------------------------------------------\n\n    def apply_solar_correction(\n        self,\n        data: xr.DataArray,\n        method: Literal[\"normalize\", \"residual\", \"cos_correction\"] = \"normalize\",\n        reference_zenith: float = 45.0,\n    ) -&gt; xr.DataArray:\n        \"\"\"Apply solar correction to data.\n\n        Parameters\n        ----------\n        data : xr.DataArray\n            Input data with a time dimension (``'epoch'`` or ``'time'``).\n        method : str\n            Correction method:\n\n            * ``'normalize'``      – normalise by cos(zenith) relative to\n              *reference_zenith*.\n            * ``'residual'``       – subtract a 4th-order polynomial fitted\n              to the diurnal pattern (1-D data only; falls back to\n              ``'normalize'`` for multi-dimensional data).\n            * ``'cos_correction'`` – simple cosine correction.\n        reference_zenith : float\n            Reference zenith angle for normalisation (degrees).\n\n        Returns\n        -------\n        xr.DataArray\n            Solar-corrected data with correction metadata in attrs.\n\n        \"\"\"\n        time_dim = \"epoch\" if \"epoch\" in data.dims else \"time\"\n        times = pd.to_datetime(data[time_dim].values)\n\n        zenith, _ = self.compute_solar_position(times)\n        zenith_da = xr.DataArray(\n            zenith,\n            coords={time_dim: data[time_dim]},\n            dims=[time_dim],\n        )\n\n        if method == \"normalize\":\n            correction_factor = np.cos(np.radians(reference_zenith)) / np.cos(\n                np.radians(zenith_da)\n            )\n            correction_factor = correction_factor.clip(0.5, 2.0)\n\n            corrected = data * correction_factor\n            corrected.attrs[\"solar_correction\"] = \"normalized\"\n            corrected.attrs[\"reference_zenith\"] = reference_zenith\n\n        elif method == \"cos_correction\":\n            correction_factor = np.cos(np.radians(zenith_da))\n            correction_factor = correction_factor.clip(0.1, 1.0)\n\n            corrected = data / correction_factor\n            corrected.attrs[\"solar_correction\"] = \"cos_correction\"\n\n        elif method == \"residual\":\n            hour = times.hour + times.minute / 60.0\n\n            if len(data.dims) == 1:\n                valid = np.isfinite(data.values)\n                if np.sum(valid) &gt; 10:\n                    coeffs = np.polyfit(hour[valid], data.values[valid], deg=4)\n                    solar_model = np.polyval(coeffs, hour)\n                    solar_model_da = xr.DataArray(\n                        solar_model,\n                        coords={time_dim: data[time_dim]},\n                        dims=[time_dim],\n                    )\n                    corrected = data - solar_model_da\n                    corrected.attrs[\"solar_correction\"] = \"residual\"\n                    corrected.attrs[\"polynomial_degree\"] = 4\n                else:\n                    logger.warning(\"Insufficient data for residual correction\")\n                    corrected = data\n            else:\n                logger.warning(\n                    \"Residual correction for multi-dimensional data not yet \"\n                    \"implemented, using normalize instead\"\n                )\n                correction_factor = np.cos(np.radians(reference_zenith)) / np.cos(\n                    np.radians(zenith_da)\n                )\n                correction_factor = correction_factor.clip(0.5, 2.0)\n                corrected = data * correction_factor\n                corrected.attrs[\"solar_correction\"] = \"normalize_fallback\"\n        else:\n            raise ValueError(f\"Unknown correction method: {method}\")\n\n        return corrected\n\n    # ------------------------------------------------------------------\n    # Binning &amp; sunrise/sunset\n    # ------------------------------------------------------------------\n\n    def compute_solar_bins(\n        self,\n        times: np.ndarray | pd.DatetimeIndex,\n        n_bins: int = 12,\n    ) -&gt; np.ndarray:\n        \"\"\"Bin times by solar elevation angle.\n\n        Useful for solar-elevation-based composites instead of\n        hour-of-day composites.\n\n        Parameters\n        ----------\n        times : np.ndarray or pd.DatetimeIndex\n            Array of times.\n        n_bins : int\n            Number of solar elevation bins (range −20° to 90°).\n\n        Returns\n        -------\n        np.ndarray\n            Bin indices (0-based) for each time.\n\n        \"\"\"\n        elevation = self.compute_solar_elevation(times)\n        bin_edges = np.linspace(-20, 90, n_bins + 1)\n        bin_indices = np.digitize(elevation, bin_edges) - 1\n        return np.clip(bin_indices, 0, n_bins - 1)\n\n    def get_sunrise_sunset(\n        self, date: datetime\n    ) -&gt; tuple[pd.Timestamp | None, pd.Timestamp | None]:\n        \"\"\"Compute sunrise and sunset times for a given date.\n\n        Parameters\n        ----------\n        date : datetime\n            Date to compute sunrise/sunset for.\n\n        Returns\n        -------\n        sunrise : pd.Timestamp or None\n            Sunrise time in UTC, or ``None`` if the sun never rises.\n        sunset : pd.Timestamp or None\n            Sunset time in UTC, or ``None`` if the sun never sets.\n\n        \"\"\"\n        times = pd.date_range(\n            start=date.replace(hour=0, minute=0, second=0),\n            end=date.replace(hour=23, minute=59, second=59),\n            freq=\"1min\",\n        )\n\n        elevation = self.compute_solar_elevation(times)\n        above_horizon = np.where(elevation &gt; 0)[0]\n\n        sunrise = times[above_horizon[0]] if len(above_horizon) &gt; 0 else None\n        sunset = times[above_horizon[-1]] if len(above_horizon) &gt; 0 else None\n\n        return sunrise, sunset\n\n    # ------------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the developer-facing representation.\n\n        Returns\n        -------\n        str\n            Representation string.\n\n        \"\"\"\n        method = \"pvlib\" if self.use_pvlib else \"builtin\"\n        return (\n            f\"SolarPositionCalculator(lat={self.lat:.4f}°, lon={self.lon:.4f}°, \"\n            f\"elevation={self.elevation:.0f}m, method={method})\"\n        )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.__init__","level":3,"title":"<code>__init__(lat, lon, elevation=0.0, use_pvlib=True)</code>","text":"<p>Initialize the solar position calculator.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.__init__--parameters","level":5,"title":"Parameters","text":"<p>lat : float     Observer latitude in degrees. lon : float     Observer longitude in degrees. elevation : float, default 0.0     Elevation above sea level in metres. use_pvlib : bool, default True     Whether to use pvlib if available.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/solar.py</code> <pre><code>def __init__(\n    self,\n    lat: float,\n    lon: float,\n    elevation: float = 0.0,\n    use_pvlib: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the solar position calculator.\n\n    Parameters\n    ----------\n    lat : float\n        Observer latitude in degrees.\n    lon : float\n        Observer longitude in degrees.\n    elevation : float, default 0.0\n        Elevation above sea level in metres.\n    use_pvlib : bool, default True\n        Whether to use pvlib if available.\n\n    \"\"\"\n    self.lat = lat\n    self.lon = lon\n    self.elevation = elevation\n    self.use_pvlib = use_pvlib\n    self.pvlib = None\n\n    if use_pvlib:\n        try:\n            import pvlib\n\n            self.pvlib = pvlib\n            logger.info(\"Using pvlib for solar position calculations\")\n        except ImportError:\n            logger.warning(\n                \"pvlib not available, falling back to built-in formulas. \"\n                \"Install pvlib for higher accuracy: pip install pvlib\"\n            )\n            self.use_pvlib = False\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.compute_solar_position","level":3,"title":"<code>compute_solar_position(times)</code>","text":"<p>Compute solar zenith and azimuth angles.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.compute_solar_position--parameters","level":5,"title":"Parameters","text":"<p>times : np.ndarray or pd.DatetimeIndex     Array of <code>datetime64</code> or <code>DatetimeIndex</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.compute_solar_position--returns","level":5,"title":"Returns","text":"<p>solar_zenith : np.ndarray     Solar zenith angles in degrees (0° = directly overhead). solar_azimuth : np.ndarray     Solar azimuth angles in degrees (0° = North, 90° = East).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/solar.py</code> <pre><code>def compute_solar_position(\n    self, times: np.ndarray | pd.DatetimeIndex\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Compute solar zenith and azimuth angles.\n\n    Parameters\n    ----------\n    times : np.ndarray or pd.DatetimeIndex\n        Array of ``datetime64`` or ``DatetimeIndex``.\n\n    Returns\n    -------\n    solar_zenith : np.ndarray\n        Solar zenith angles in degrees (0° = directly overhead).\n    solar_azimuth : np.ndarray\n        Solar azimuth angles in degrees (0° = North, 90° = East).\n\n    \"\"\"\n    if isinstance(times, np.ndarray):\n        times = pd.to_datetime(times)\n\n    if self.use_pvlib and self.pvlib is not None:\n        return self._compute_solar_position_pvlib(times)\n    return self._compute_solar_position_builtin(times)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.compute_solar_elevation","level":3,"title":"<code>compute_solar_elevation(times)</code>","text":"<p>Compute solar elevation angle (complementary to zenith).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.compute_solar_elevation--parameters","level":5,"title":"Parameters","text":"<p>times : np.ndarray or pd.DatetimeIndex     Array of times.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.compute_solar_elevation--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Solar elevation angles in degrees (0° = horizon, 90° = overhead).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/solar.py</code> <pre><code>def compute_solar_elevation(\n    self, times: np.ndarray | pd.DatetimeIndex\n) -&gt; np.ndarray:\n    \"\"\"Compute solar elevation angle (complementary to zenith).\n\n    Parameters\n    ----------\n    times : np.ndarray or pd.DatetimeIndex\n        Array of times.\n\n    Returns\n    -------\n    np.ndarray\n        Solar elevation angles in degrees (0° = horizon, 90° = overhead).\n\n    \"\"\"\n    zenith, _ = self.compute_solar_position(times)\n    return 90.0 - zenith\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.is_daytime","level":3,"title":"<code>is_daytime(times, twilight_angle=-6.0)</code>","text":"<p>Determine if times are during daytime.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.is_daytime--parameters","level":5,"title":"Parameters","text":"<p>times : np.ndarray or pd.DatetimeIndex     Array of times. twilight_angle : float     Solar elevation threshold in degrees.     Common values: <code>0</code> (geometric), <code>-6</code> (civil),     <code>-12</code> (nautical), <code>-18</code> (astronomical).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.is_daytime--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Boolean array (<code>True</code> = daytime).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/solar.py</code> <pre><code>def is_daytime(\n    self,\n    times: np.ndarray | pd.DatetimeIndex,\n    twilight_angle: float = -6.0,\n) -&gt; np.ndarray:\n    \"\"\"Determine if times are during daytime.\n\n    Parameters\n    ----------\n    times : np.ndarray or pd.DatetimeIndex\n        Array of times.\n    twilight_angle : float\n        Solar elevation threshold in degrees.\n        Common values: ``0`` (geometric), ``-6`` (civil),\n        ``-12`` (nautical), ``-18`` (astronomical).\n\n    Returns\n    -------\n    np.ndarray\n        Boolean array (``True`` = daytime).\n\n    \"\"\"\n    elevation = self.compute_solar_elevation(times)\n    return elevation &gt; twilight_angle\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.apply_solar_correction","level":3,"title":"<code>apply_solar_correction(data, method='normalize', reference_zenith=45.0)</code>","text":"<p>Apply solar correction to data.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.apply_solar_correction--parameters","level":5,"title":"Parameters","text":"<p>data : xr.DataArray     Input data with a time dimension (<code>'epoch'</code> or <code>'time'</code>). method : str     Correction method:</p> <pre><code>* ``'normalize'``      – normalise by cos(zenith) relative to\n  *reference_zenith*.\n* ``'residual'``       – subtract a 4th-order polynomial fitted\n  to the diurnal pattern (1-D data only; falls back to\n  ``'normalize'`` for multi-dimensional data).\n* ``'cos_correction'`` – simple cosine correction.\n</code></pre> <p>reference_zenith : float     Reference zenith angle for normalisation (degrees).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.apply_solar_correction--returns","level":5,"title":"Returns","text":"<p>xr.DataArray     Solar-corrected data with correction metadata in attrs.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/solar.py</code> <pre><code>def apply_solar_correction(\n    self,\n    data: xr.DataArray,\n    method: Literal[\"normalize\", \"residual\", \"cos_correction\"] = \"normalize\",\n    reference_zenith: float = 45.0,\n) -&gt; xr.DataArray:\n    \"\"\"Apply solar correction to data.\n\n    Parameters\n    ----------\n    data : xr.DataArray\n        Input data with a time dimension (``'epoch'`` or ``'time'``).\n    method : str\n        Correction method:\n\n        * ``'normalize'``      – normalise by cos(zenith) relative to\n          *reference_zenith*.\n        * ``'residual'``       – subtract a 4th-order polynomial fitted\n          to the diurnal pattern (1-D data only; falls back to\n          ``'normalize'`` for multi-dimensional data).\n        * ``'cos_correction'`` – simple cosine correction.\n    reference_zenith : float\n        Reference zenith angle for normalisation (degrees).\n\n    Returns\n    -------\n    xr.DataArray\n        Solar-corrected data with correction metadata in attrs.\n\n    \"\"\"\n    time_dim = \"epoch\" if \"epoch\" in data.dims else \"time\"\n    times = pd.to_datetime(data[time_dim].values)\n\n    zenith, _ = self.compute_solar_position(times)\n    zenith_da = xr.DataArray(\n        zenith,\n        coords={time_dim: data[time_dim]},\n        dims=[time_dim],\n    )\n\n    if method == \"normalize\":\n        correction_factor = np.cos(np.radians(reference_zenith)) / np.cos(\n            np.radians(zenith_da)\n        )\n        correction_factor = correction_factor.clip(0.5, 2.0)\n\n        corrected = data * correction_factor\n        corrected.attrs[\"solar_correction\"] = \"normalized\"\n        corrected.attrs[\"reference_zenith\"] = reference_zenith\n\n    elif method == \"cos_correction\":\n        correction_factor = np.cos(np.radians(zenith_da))\n        correction_factor = correction_factor.clip(0.1, 1.0)\n\n        corrected = data / correction_factor\n        corrected.attrs[\"solar_correction\"] = \"cos_correction\"\n\n    elif method == \"residual\":\n        hour = times.hour + times.minute / 60.0\n\n        if len(data.dims) == 1:\n            valid = np.isfinite(data.values)\n            if np.sum(valid) &gt; 10:\n                coeffs = np.polyfit(hour[valid], data.values[valid], deg=4)\n                solar_model = np.polyval(coeffs, hour)\n                solar_model_da = xr.DataArray(\n                    solar_model,\n                    coords={time_dim: data[time_dim]},\n                    dims=[time_dim],\n                )\n                corrected = data - solar_model_da\n                corrected.attrs[\"solar_correction\"] = \"residual\"\n                corrected.attrs[\"polynomial_degree\"] = 4\n            else:\n                logger.warning(\"Insufficient data for residual correction\")\n                corrected = data\n        else:\n            logger.warning(\n                \"Residual correction for multi-dimensional data not yet \"\n                \"implemented, using normalize instead\"\n            )\n            correction_factor = np.cos(np.radians(reference_zenith)) / np.cos(\n                np.radians(zenith_da)\n            )\n            correction_factor = correction_factor.clip(0.5, 2.0)\n            corrected = data * correction_factor\n            corrected.attrs[\"solar_correction\"] = \"normalize_fallback\"\n    else:\n        raise ValueError(f\"Unknown correction method: {method}\")\n\n    return corrected\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.compute_solar_bins","level":3,"title":"<code>compute_solar_bins(times, n_bins=12)</code>","text":"<p>Bin times by solar elevation angle.</p> <p>Useful for solar-elevation-based composites instead of hour-of-day composites.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.compute_solar_bins--parameters","level":5,"title":"Parameters","text":"<p>times : np.ndarray or pd.DatetimeIndex     Array of times. n_bins : int     Number of solar elevation bins (range −20° to 90°).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.compute_solar_bins--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Bin indices (0-based) for each time.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/solar.py</code> <pre><code>def compute_solar_bins(\n    self,\n    times: np.ndarray | pd.DatetimeIndex,\n    n_bins: int = 12,\n) -&gt; np.ndarray:\n    \"\"\"Bin times by solar elevation angle.\n\n    Useful for solar-elevation-based composites instead of\n    hour-of-day composites.\n\n    Parameters\n    ----------\n    times : np.ndarray or pd.DatetimeIndex\n        Array of times.\n    n_bins : int\n        Number of solar elevation bins (range −20° to 90°).\n\n    Returns\n    -------\n    np.ndarray\n        Bin indices (0-based) for each time.\n\n    \"\"\"\n    elevation = self.compute_solar_elevation(times)\n    bin_edges = np.linspace(-20, 90, n_bins + 1)\n    bin_indices = np.digitize(elevation, bin_edges) - 1\n    return np.clip(bin_indices, 0, n_bins - 1)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.get_sunrise_sunset","level":3,"title":"<code>get_sunrise_sunset(date)</code>","text":"<p>Compute sunrise and sunset times for a given date.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.get_sunrise_sunset--parameters","level":5,"title":"Parameters","text":"<p>date : datetime     Date to compute sunrise/sunset for.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.get_sunrise_sunset--returns","level":5,"title":"Returns","text":"<p>sunrise : pd.Timestamp or None     Sunrise time in UTC, or <code>None</code> if the sun never rises. sunset : pd.Timestamp or None     Sunset time in UTC, or <code>None</code> if the sun never sets.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/solar.py</code> <pre><code>def get_sunrise_sunset(\n    self, date: datetime\n) -&gt; tuple[pd.Timestamp | None, pd.Timestamp | None]:\n    \"\"\"Compute sunrise and sunset times for a given date.\n\n    Parameters\n    ----------\n    date : datetime\n        Date to compute sunrise/sunset for.\n\n    Returns\n    -------\n    sunrise : pd.Timestamp or None\n        Sunrise time in UTC, or ``None`` if the sun never rises.\n    sunset : pd.Timestamp or None\n        Sunset time in UTC, or ``None`` if the sun never sets.\n\n    \"\"\"\n    times = pd.date_range(\n        start=date.replace(hour=0, minute=0, second=0),\n        end=date.replace(hour=23, minute=59, second=59),\n        freq=\"1min\",\n    )\n\n    elevation = self.compute_solar_elevation(times)\n    above_horizon = np.where(elevation &gt; 0)[0]\n\n    sunrise = times[above_horizon[0]] if len(above_horizon) &gt; 0 else None\n    sunset = times[above_horizon[-1]] if len(above_horizon) &gt; 0 else None\n\n    return sunrise, sunset\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return the developer-facing representation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.SolarPositionCalculator.__repr__--returns","level":5,"title":"Returns","text":"<p>str     Representation string.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/solar.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the developer-facing representation.\n\n    Returns\n    -------\n    str\n        Representation string.\n\n    \"\"\"\n    method = \"pvlib\" if self.use_pvlib else \"builtin\"\n    return (\n        f\"SolarPositionCalculator(lat={self.lat:.4f}°, lon={self.lon:.4f}°, \"\n        f\"elevation={self.elevation:.0f}m, method={method})\"\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.compute_solar_zenith","level":2,"title":"<code>compute_solar_zenith(lat, lon, times)</code>","text":"<p>Quick computation of solar zenith angles.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.compute_solar_zenith--parameters","level":4,"title":"Parameters","text":"<p>lat : float     Observer latitude in degrees. lon : float     Observer longitude in degrees. times : np.ndarray or pd.DatetimeIndex     Times to compute for.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.compute_solar_zenith--returns","level":4,"title":"Returns","text":"<p>np.ndarray     Solar zenith angles in degrees.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/solar.py</code> <pre><code>def compute_solar_zenith(\n    lat: float, lon: float, times: np.ndarray | pd.DatetimeIndex\n) -&gt; np.ndarray:\n    \"\"\"Quick computation of solar zenith angles.\n\n    Parameters\n    ----------\n    lat : float\n        Observer latitude in degrees.\n    lon : float\n        Observer longitude in degrees.\n    times : np.ndarray or pd.DatetimeIndex\n        Times to compute for.\n\n    Returns\n    -------\n    np.ndarray\n        Solar zenith angles in degrees.\n\n    \"\"\"\n    calc = SolarPositionCalculator(lat, lon)\n    zenith, _ = calc.compute_solar_position(times)\n    return zenith\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.filter_daytime_data","level":2,"title":"<code>filter_daytime_data(data, lat, lon, twilight_angle=-6.0)</code>","text":"<p>Filter data to include only daytime observations.</p> <p>Nighttime values are set to NaN.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.filter_daytime_data--parameters","level":4,"title":"Parameters","text":"<p>data : xr.DataArray     Data with a time dimension (<code>'epoch'</code> or <code>'time'</code>). lat : float     Observer latitude in degrees. lon : float     Observer longitude in degrees. twilight_angle : float     Elevation threshold for daytime (degrees below horizon).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.solar.filter_daytime_data--returns","level":4,"title":"Returns","text":"<p>xr.DataArray     Filtered data.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/solar.py</code> <pre><code>def filter_daytime_data(\n    data: xr.DataArray,\n    lat: float,\n    lon: float,\n    twilight_angle: float = -6.0,\n) -&gt; xr.DataArray:\n    \"\"\"Filter data to include only daytime observations.\n\n    Nighttime values are set to NaN.\n\n    Parameters\n    ----------\n    data : xr.DataArray\n        Data with a time dimension (``'epoch'`` or ``'time'``).\n    lat : float\n        Observer latitude in degrees.\n    lon : float\n        Observer longitude in degrees.\n    twilight_angle : float\n        Elevation threshold for daytime (degrees below horizon).\n\n    Returns\n    -------\n    xr.DataArray\n        Filtered data.\n\n    \"\"\"\n    calc = SolarPositionCalculator(lat, lon)\n    time_dim = \"epoch\" if \"epoch\" in data.dims else \"time\"\n    times = pd.to_datetime(data[time_dim].values)\n    is_day = calc.is_daytime(times, twilight_angle)\n    return data.where(is_day)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#temporal-analysis","level":3,"title":"Temporal Analysis","text":"<p>Temporal analysis of gridded VOD data.</p> <p>Weighted time-series computation, diurnal cycle analysis, and temporal statistics with optional solar-position correction.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal--classes","level":3,"title":"Classes","text":"<p><code>TemporalAnalysis</code>     Main analysis class; binds a VOD dataset to a grid and exposes     methods for aggregation, solar correction, diurnal binning, and     basic plotting.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal--notes","level":3,"title":"Notes","text":"<ul> <li>All spatial masks and weight arrays must be 1-D with length   <code>grid.ncells</code>.</li> <li>When a <code>SolarPositionCalculator</code> is attached (via site_lat /   site_lon), additional solar-corrected and solar-binned methods   become available.</li> <li>Plotting helpers are thin wrappers around <code>matplotlib</code>; they   return <code>(fig, ax)</code> so callers can continue customising the figure.</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis","level":2,"title":"<code>TemporalAnalysis</code>","text":"<p>Temporal analysis of gridded VOD data.</p> <p>Binds a VOD dataset (with pre-assigned cell IDs) to a grid and exposes weighted aggregation, diurnal analysis, and plotting.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis--parameters","level":4,"title":"Parameters","text":"<p>vod_ds : xr.Dataset     Dataset containing VOD data and a <code>cell_id_&lt;grid_name&gt;</code>     variable. grid : GridData     Grid instance (must expose <code>.ncells</code>). grid_name : str     Suffix for the cell-ID variable (e.g. <code>'htm_10deg'</code>). site_lat : float or None, optional     Site latitude in degrees.  Required for solar methods. site_lon : float or None, optional     Site longitude in degrees.  Required for solar methods. site_elevation : float, optional     Site elevation in metres (default 0).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis--raises","level":4,"title":"Raises","text":"<p>ValueError     If <code>cell_id_&lt;grid_name&gt;</code> is not present in vod_ds.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis--examples","level":4,"title":"Examples","text":"<p>analysis = TemporalAnalysis(vod_ds, grid, \"htm_10deg\") ts = analysis.compute_timeseries(aggregate=\"1D\")</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/temporal.py</code> <pre><code>class TemporalAnalysis:\n    \"\"\"Temporal analysis of gridded VOD data.\n\n    Binds a VOD dataset (with pre-assigned cell IDs) to a grid and\n    exposes weighted aggregation, diurnal analysis, and plotting.\n\n    Parameters\n    ----------\n    vod_ds : xr.Dataset\n        Dataset containing VOD data and a ``cell_id_&lt;grid_name&gt;``\n        variable.\n    grid : GridData\n        Grid instance (must expose ``.ncells``).\n    grid_name : str\n        Suffix for the cell-ID variable (e.g. ``'htm_10deg'``).\n    site_lat : float or None, optional\n        Site latitude in degrees.  Required for solar methods.\n    site_lon : float or None, optional\n        Site longitude in degrees.  Required for solar methods.\n    site_elevation : float, optional\n        Site elevation in metres (default 0).\n\n    Raises\n    ------\n    ValueError\n        If ``cell_id_&lt;grid_name&gt;`` is not present in *vod_ds*.\n\n    Examples\n    --------\n    &gt;&gt;&gt; analysis = TemporalAnalysis(vod_ds, grid, \"htm_10deg\")\n    &gt;&gt;&gt; ts = analysis.compute_timeseries(aggregate=\"1D\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        vod_ds: xr.Dataset,\n        grid: GridData,\n        grid_name: str,\n        site_lat: float | None = None,\n        site_lon: float | None = None,\n        site_elevation: float = 0.0,\n    ) -&gt; None:\n        \"\"\"Initialize the temporal analysis helper.\n\n        Parameters\n        ----------\n        vod_ds : xr.Dataset\n            VOD dataset containing cell IDs.\n        grid : GridData\n            Grid instance.\n        grid_name : str\n            Grid name suffix for cell IDs.\n        site_lat : float | None, optional\n            Site latitude in degrees.\n        site_lon : float | None, optional\n            Site longitude in degrees.\n        site_elevation : float, default 0.0\n            Site elevation in metres.\n\n        \"\"\"\n        self.vod_ds = vod_ds\n        self.grid = grid\n        self.grid_name = grid_name\n        self.cell_id_var = f\"cell_id_{grid_name}\"\n\n        # Solar calculator (optional)\n        if site_lat is not None and site_lon is not None:\n            self.solar_calc: SolarPositionCalculator | None = SolarPositionCalculator(\n                lat=site_lat, lon=site_lon, elevation=site_elevation\n            )\n            logger.info(\n                \"solar calculator enabled for (%.4f°, %.4f°)\",\n                site_lat,\n                site_lon,\n            )\n        else:\n            self.solar_calc = None\n\n        # Validate dataset\n        if self.cell_id_var not in vod_ds:\n            available = [v for v in vod_ds.data_vars if v.startswith(\"cell_id_\")]\n            raise ValueError(\n                f\"Cell ID variable '{self.cell_id_var}' not found in dataset. \"\n                f\"Available: {available}\"\n            )\n\n    # ------------------------------------------------------------------\n    # Core aggregation\n    # ------------------------------------------------------------------\n\n    def compute_timeseries(\n        self,\n        var_name: str = \"VOD\",\n        spatial_mask: np.ndarray | None = None,\n        weights: np.ndarray | None = None,\n        aggregate: str = \"1D\",\n        min_cells: int = 1,\n    ) -&gt; xr.Dataset:\n        \"\"\"Compute a weighted time-series aggregated over space.\n\n        Parameters\n        ----------\n        var_name : str, optional\n            Data variable to aggregate.\n        spatial_mask : np.ndarray or None, optional\n            Boolean mask of shape ``(grid.ncells,)``; ``True`` = include.\n        weights : np.ndarray or None, optional\n            Cell weights of shape ``(grid.ncells,)``; normalised\n            internally.  ``None`` → uniform weights.\n        aggregate : str, optional\n            Pandas-compatible frequency string for temporal resampling.\n        min_cells : int, optional\n            Minimum unique cells required per time bin.\n\n        Returns\n        -------\n        xr.Dataset\n            Variables: ``mean``, ``std``, ``n_cells``,\n            ``n_observations``, ``sum_weights``.\n\n        Raises\n        ------\n        ValueError\n            If *var_name* is missing or mask/weight shapes are wrong.\n\n        \"\"\"\n        if var_name not in self.vod_ds:\n            raise ValueError(f\"Variable '{var_name}' not found in dataset\")\n\n        logger.info(\n            \"compute_timeseries: var=%s aggregate=%s mask=%s weights=%s\",\n            var_name,\n            aggregate,\n            spatial_mask is not None,\n            weights is not None,\n        )\n\n        var_data = self.vod_ds[var_name]\n        cell_ids = self.vod_ds[self.cell_id_var]\n\n        # Apply spatial mask\n        if spatial_mask is not None:\n            if spatial_mask.shape != (self.grid.ncells,):\n                raise ValueError(\n                    f\"Spatial mask shape {spatial_mask.shape} doesn't match \"\n                    f\"grid size ({self.grid.ncells},)\"\n                )\n            data_mask = xr.zeros_like(cell_ids, dtype=bool)\n            for cell_id in np.where(spatial_mask)[0]:\n                data_mask = data_mask | (cell_ids == cell_id)\n            var_data = var_data.where(data_mask)\n            logger.debug(\n                \"spatial mask applied: %d/%d cells\",\n                spatial_mask.sum(),\n                self.grid.ncells,\n            )\n\n        # Prepare weights\n        weights = self._prepare_weights(weights)\n\n        return self._compute_weighted_timeseries(\n            var_data, cell_ids, weights, aggregate, min_cells\n        )\n\n    # ------------------------------------------------------------------\n    # Solar-corrected aggregation\n    # ------------------------------------------------------------------\n\n    def compute_timeseries_solar_corrected(\n        self,\n        var_name: str = \"VOD\",\n        spatial_mask: np.ndarray | None = None,\n        weights: np.ndarray | None = None,\n        aggregate: str = \"1D\",\n        min_cells: int = 1,\n        solar_correction: Literal[\n            \"normalize\", \"residual\", \"cos_correction\"\n        ] = \"normalize\",\n        reference_zenith: float = 45.0,\n        daytime_only: bool = False,\n        twilight_angle: float = -6.0,\n    ) -&gt; xr.Dataset:\n        \"\"\"Compute a time-series after applying a solar correction.\n\n        Parameters\n        ----------\n        var_name : str, optional\n            Data variable to correct and aggregate.\n        spatial_mask : np.ndarray or None, optional\n            Cell selection mask.\n        weights : np.ndarray or None, optional\n            Cell weights.\n        aggregate : str, optional\n            Temporal resampling frequency.\n        min_cells : int, optional\n            Minimum cells per time bin.\n        solar_correction : {'normalize', 'residual', 'cos_correction'}\n            Correction method passed to\n            :meth:`SolarPositionCalculator.apply_solar_correction`.\n        reference_zenith : float, optional\n            Reference zenith for normalisation (degrees).\n        daytime_only : bool, optional\n            If ``True``, mask out nighttime epochs.\n        twilight_angle : float, optional\n            Solar-elevation threshold for daytime (degrees).\n\n        Returns\n        -------\n        xr.Dataset\n            Solar-corrected time-series with additional metadata attrs.\n\n        Raises\n        ------\n        ValueError\n            If no solar calculator is configured.\n\n        \"\"\"\n        if self.solar_calc is None:\n            raise ValueError(\n                \"Solar calculator not initialized. \"\n                \"Provide site_lat and site_lon to TemporalAnalysis constructor.\"\n            )\n\n        logger.info(\n            \"solar-corrected timeseries: correction=%s daytime_only=%s\",\n            solar_correction,\n            daytime_only,\n        )\n\n        var_data = self.vod_ds[var_name]\n\n        # Apply solar correction\n        var_data_corrected = self.solar_calc.apply_solar_correction(\n            var_data, method=solar_correction, reference_zenith=reference_zenith\n        )\n\n        # Daytime filter\n        if daytime_only:\n            times = pd.to_datetime(var_data[\"epoch\"].values)\n            is_day = self.solar_calc.is_daytime(times, twilight_angle)\n            is_day_da = xr.DataArray(\n                is_day, coords={\"epoch\": var_data_corrected[\"epoch\"]}, dims=[\"epoch\"]\n            )\n            var_data_corrected = var_data_corrected.where(is_day_da)\n            logger.debug(\n                \"daytime filter: %d/%d timesteps kept\",\n                is_day.sum(),\n                len(is_day),\n            )\n\n        # Temporary dataset with corrected variable\n        corrected_name = f\"{var_name}_solar_corrected\"\n        ds_temp = self.vod_ds.copy()\n        ds_temp[corrected_name] = var_data_corrected\n\n        # Reuse compute_timeseries via a lightweight temporary instance\n        analysis_temp = TemporalAnalysis.__new__(TemporalAnalysis)\n        analysis_temp.vod_ds = ds_temp\n        analysis_temp.grid = self.grid\n        analysis_temp.grid_name = self.grid_name\n        analysis_temp.cell_id_var = self.cell_id_var\n        analysis_temp.solar_calc = self.solar_calc\n\n        ts = analysis_temp.compute_timeseries(\n            var_name=corrected_name,\n            spatial_mask=spatial_mask,\n            weights=weights,\n            aggregate=aggregate,\n            min_cells=min_cells,\n        )\n\n        # Solar metadata\n        ts.attrs[\"solar_correction\"] = solar_correction\n        ts.attrs[\"reference_zenith\"] = reference_zenith\n        ts.attrs[\"daytime_only\"] = daytime_only\n        if daytime_only:\n            ts.attrs[\"twilight_angle\"] = twilight_angle\n\n        return ts\n\n    # ------------------------------------------------------------------\n    # Internal helpers\n    # ------------------------------------------------------------------\n\n    def _prepare_weights(self, weights: np.ndarray | None) -&gt; np.ndarray:\n        \"\"\"Validate and normalise a weight array.\n\n        Returns uniform weights when *weights* is ``None``.\n        \"\"\"\n        if weights is None:\n            logger.debug(\"using uniform weights\")\n            return np.ones(self.grid.ncells) / self.grid.ncells\n\n        if weights.shape != (self.grid.ncells,):\n            raise ValueError(\n                f\"Weights shape {weights.shape} doesn't match \"\n                f\"grid size ({self.grid.ncells},)\"\n            )\n        w_sum = weights.sum()\n        if w_sum &gt; 0:\n            weights = weights / w_sum\n        logger.debug(\"using provided weights (sum before normalisation=%.4f)\", w_sum)\n        return weights\n\n    def _compute_weighted_timeseries(\n        self,\n        var_data: xr.DataArray,\n        cell_ids: xr.DataArray,\n        weights: np.ndarray,\n        aggregate: str,\n        min_cells: int,\n    ) -&gt; xr.Dataset:\n        \"\"\"Aggregate *var_data* into time bins with cell weights.\n\n        Returns\n        -------\n        xr.Dataset\n            ``mean``, ``std``, ``n_cells``, ``n_observations``,\n            ``sum_weights`` on the ``epoch`` dimension.\n\n        \"\"\"\n        times = var_data[\"epoch\"].values\n        n_sid = var_data.sizes.get(\"sid\", 1)\n\n        # Flatten (epoch × sid) → 1-D\n        values = var_data.values.ravel()\n        cells = cell_ids.values.ravel()\n\n        valid = np.isfinite(values) &amp; np.isfinite(cells)\n        times_valid = np.repeat(times, n_sid)[valid]\n        values_valid = values[valid]\n        cells_valid = cells[valid].astype(int)\n\n        df = pd.DataFrame(\n            {\"epoch\": times_valid, \"value\": values_valid, \"cell_id\": cells_valid}\n        )\n        df[\"weight\"] = df[\"cell_id\"].map(\n            lambda cid: weights[cid] if cid &lt; len(weights) else 0.0\n        )\n        df[\"epoch\"] = pd.to_datetime(df[\"epoch\"])\n        df = df.set_index(\"epoch\")\n\n        grouped = df.groupby(pd.Grouper(freq=aggregate))\n\n        result_rows: list[dict] = []\n        for time_bin, group in grouped:\n            if len(group) == 0:\n                continue\n            n_cells = group[\"cell_id\"].nunique()\n            if n_cells &lt; min_cells:\n                continue\n\n            w = group[\"weight\"].values\n            v = group[\"value\"].values\n            w_sum = w.sum()\n\n            if w_sum &gt; 0:\n                weighted_mean = np.average(v, weights=w)\n                weighted_std = np.sqrt(np.average((v - weighted_mean) ** 2, weights=w))\n                result_rows.append(\n                    {\n                        \"epoch\": time_bin,\n                        \"mean\": weighted_mean,\n                        \"std\": weighted_std,\n                        \"n_cells\": n_cells,\n                        \"n_observations\": len(group),\n                        \"sum_weights\": w_sum,\n                    }\n                )\n\n        if not result_rows:\n            logger.warning(\"no data after aggregation\")\n            return xr.Dataset()\n\n        result_df = pd.DataFrame(result_rows)\n        ds = xr.Dataset(\n            {\n                \"mean\": (\"epoch\", result_df[\"mean\"].values),\n                \"std\": (\"epoch\", result_df[\"std\"].values),\n                \"n_cells\": (\"epoch\", result_df[\"n_cells\"].values),\n                \"n_observations\": (\"epoch\", result_df[\"n_observations\"].values),\n                \"sum_weights\": (\"epoch\", result_df[\"sum_weights\"].values),\n            },\n            coords={\"epoch\": result_df[\"epoch\"].values},\n        )\n        ds.attrs[\"variable\"] = var_data.name\n        ds.attrs[\"grid\"] = self.grid_name\n        ds.attrs[\"aggregation\"] = aggregate\n        ds.attrs[\"min_cells\"] = min_cells\n\n        logger.info(\n            \"timeseries computed: %d steps, mean n_cells=%.1f\",\n            len(result_df),\n            result_df[\"n_cells\"].mean(),\n        )\n        return ds\n\n    # ------------------------------------------------------------------\n    # Diurnal cycle\n    # ------------------------------------------------------------------\n\n    def compute_diurnal_cycle(\n        self,\n        var_name: str = \"VOD\",\n        spatial_mask: np.ndarray | None = None,\n        weights: np.ndarray | None = None,\n        hour_bins: int = 24,\n        min_observations: int = 10,\n    ) -&gt; xr.Dataset:\n        \"\"\"Compute clock-time diurnal cycle (hour-of-day statistics).\n\n        Parameters\n        ----------\n        var_name : str, optional\n            Data variable to bin.\n        spatial_mask : np.ndarray or None, optional\n            Cell selection mask.\n        weights : np.ndarray or None, optional\n            Cell weights.\n        hour_bins : int, optional\n            Number of equal-width hour bins over [0, 24).\n        min_observations : int, optional\n            Minimum observations required per bin.\n\n        Returns\n        -------\n        xr.Dataset\n            ``mean``, ``std``, ``n_observations`` on the ``hour``\n            coordinate.\n\n        \"\"\"\n        if var_name not in self.vod_ds:\n            raise ValueError(f\"Variable '{var_name}' not found in dataset\")\n\n        logger.info(\"compute_diurnal_cycle: var=%s hour_bins=%d\", var_name, hour_bins)\n\n        var_data = self.vod_ds[var_name]\n        cell_ids = self.vod_ds[self.cell_id_var]\n\n        # Spatial mask\n        if spatial_mask is not None:\n            data_mask = xr.zeros_like(cell_ids, dtype=bool)\n            for cell_id in np.where(spatial_mask)[0]:\n                data_mask = data_mask | (cell_ids == cell_id)\n            var_data = var_data.where(data_mask)\n\n        weights = self._prepare_weights(weights)\n\n        # Hour of day (fractional)\n        times = pd.to_datetime(var_data[\"epoch\"].values)\n        hours = times.hour + times.minute / 60.0\n\n        n_sid = var_data.sizes.get(\"sid\", 1)\n        hour_edges = np.linspace(0, 24, hour_bins + 1)\n        hour_centers = (hour_edges[:-1] + hour_edges[1:]) / 2\n\n        # Flatten\n        values = var_data.values.ravel()\n        cells = cell_ids.values.ravel()\n        hours_flat = np.repeat(hours, n_sid)\n\n        valid = np.isfinite(values) &amp; np.isfinite(cells)\n        df = pd.DataFrame(\n            {\n                \"hour\": hours_flat[valid],\n                \"value\": values[valid],\n                \"cell_id\": cells[valid].astype(int),\n            }\n        )\n        df[\"weight\"] = df[\"cell_id\"].map(\n            lambda cid: weights[cid] if cid &lt; len(weights) else 0.0\n        )\n        df[\"hour_bin\"] = pd.cut(\n            df[\"hour\"], bins=hour_edges, labels=hour_centers, include_lowest=True\n        )\n\n        grouped = df.groupby(\"hour_bin\")\n\n        means, stds, n_obs = [], [], []\n        for hc in hour_centers:\n            if hc in grouped.groups:\n                group = grouped.get_group(hc)\n                if len(group) &gt;= min_observations:\n                    w = group[\"weight\"].values\n                    v = group[\"value\"].values\n                    if w.sum() &gt; 0:\n                        wm = np.average(v, weights=w)\n                        ws = np.sqrt(np.average((v - wm) ** 2, weights=w))\n                    else:\n                        wm, ws = np.nan, np.nan\n                    means.append(wm)\n                    stds.append(ws)\n                    n_obs.append(len(group))\n                else:\n                    means.append(np.nan)\n                    stds.append(np.nan)\n                    n_obs.append(0)\n            else:\n                means.append(np.nan)\n                stds.append(np.nan)\n                n_obs.append(0)\n\n        ds = xr.Dataset(\n            {\n                \"mean\": (\"hour\", np.array(means)),\n                \"std\": (\"hour\", np.array(stds)),\n                \"n_observations\": (\"hour\", np.array(n_obs)),\n            },\n            coords={\"hour\": hour_centers},\n        )\n        ds.attrs.update(\n            {\n                \"variable\": var_name,\n                \"grid\": self.grid_name,\n                \"hour_bins\": hour_bins,\n                \"min_observations\": min_observations,\n            }\n        )\n        logger.info(\n            \"diurnal cycle: %d bins, mean n_obs=%.1f\",\n            hour_bins,\n            np.nanmean(n_obs),\n        )\n        return ds\n\n    def compute_diurnal_cycle_solar(\n        self,\n        var_name: str = \"VOD\",\n        spatial_mask: np.ndarray | None = None,\n        weights: np.ndarray | None = None,\n        n_solar_bins: int = 12,\n        min_observations: int = 10,\n    ) -&gt; xr.Dataset:\n        \"\"\"Diurnal cycle binned by solar elevation instead of clock time.\n\n        Accounts for seasonal variation in solar position, producing a\n        more physically meaningful diurnal pattern.\n\n        Parameters\n        ----------\n        var_name : str, optional\n            Data variable to bin.\n        spatial_mask : np.ndarray or None, optional\n            Cell selection mask.\n        weights : np.ndarray or None, optional\n            Cell weights.\n        n_solar_bins : int, optional\n            Number of equal-width bins over [-20°, 90°].\n        min_observations : int, optional\n            Minimum observations per bin.\n\n        Returns\n        -------\n        xr.Dataset\n            ``mean``, ``std``, ``n_observations`` on the\n            ``solar_elevation`` coordinate.\n\n        Raises\n        ------\n        ValueError\n            If no solar calculator is configured.\n\n        \"\"\"\n        if self.solar_calc is None:\n            raise ValueError(\n                \"Solar calculator not initialized. \"\n                \"Provide site_lat and site_lon to TemporalAnalysis constructor.\"\n            )\n\n        logger.info(\"solar-binned diurnal cycle: n_bins=%d\", n_solar_bins)\n\n        var_data = self.vod_ds[var_name]\n        cell_ids = self.vod_ds[self.cell_id_var]\n\n        # Spatial mask\n        if spatial_mask is not None:\n            data_mask = xr.zeros_like(cell_ids, dtype=bool)\n            for cell_id in np.where(spatial_mask)[0]:\n                data_mask = data_mask | (cell_ids == cell_id)\n            var_data = var_data.where(data_mask)\n\n        weights = self._prepare_weights(weights)\n\n        # Solar bins per epoch\n        times = pd.to_datetime(var_data[\"epoch\"].values)\n        solar_bins = self.solar_calc.compute_solar_bins(times, n_bins=n_solar_bins)\n        bin_edges = np.linspace(-20, 90, n_solar_bins + 1)\n        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n        n_sid = var_data.sizes.get(\"sid\", 1)\n\n        # Flatten\n        values = var_data.values.ravel()\n        cells = cell_ids.values.ravel()\n        bins_flat = np.repeat(solar_bins, n_sid)\n\n        valid = np.isfinite(values) &amp; np.isfinite(cells)\n        df = pd.DataFrame(\n            {\n                \"solar_bin\": bins_flat[valid],\n                \"value\": values[valid],\n                \"cell_id\": cells[valid].astype(int),\n            }\n        )\n        df[\"weight\"] = df[\"cell_id\"].map(\n            lambda cid: weights[cid] if cid &lt; len(weights) else 0.0\n        )\n\n        grouped = df.groupby(\"solar_bin\")\n\n        means, stds, n_obs = [], [], []\n        for bin_idx in range(n_solar_bins):\n            if bin_idx in grouped.groups:\n                group = grouped.get_group(bin_idx)\n                if len(group) &gt;= min_observations:\n                    w = group[\"weight\"].values\n                    v = group[\"value\"].values\n                    if w.sum() &gt; 0:\n                        wm = np.average(v, weights=w)\n                        ws = np.sqrt(np.average((v - wm) ** 2, weights=w))\n                    else:\n                        wm, ws = np.nan, np.nan\n                    means.append(wm)\n                    stds.append(ws)\n                    n_obs.append(len(group))\n                else:\n                    means.append(np.nan)\n                    stds.append(np.nan)\n                    n_obs.append(0)\n            else:\n                means.append(np.nan)\n                stds.append(np.nan)\n                n_obs.append(0)\n\n        ds = xr.Dataset(\n            {\n                \"mean\": (\"solar_elevation\", np.array(means)),\n                \"std\": (\"solar_elevation\", np.array(stds)),\n                \"n_observations\": (\"solar_elevation\", np.array(n_obs)),\n            },\n            coords={\"solar_elevation\": bin_centers},\n        )\n        ds.attrs.update(\n            {\n                \"variable\": var_name,\n                \"grid\": self.grid_name,\n                \"n_solar_bins\": n_solar_bins,\n                \"min_observations\": min_observations,\n                \"coordinate_type\": \"solar_elevation\",\n            }\n        )\n        logger.info(\n            \"solar-binned diurnal: %d bins, mean n_obs=%.1f\",\n            n_solar_bins,\n            np.nanmean(n_obs),\n        )\n        return ds\n\n    # ------------------------------------------------------------------\n    # Solar metadata\n    # ------------------------------------------------------------------\n\n    def add_solar_metadata_to_timeseries(self, timeseries: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"Attach solar zenith, azimuth and elevation to a time-series.\n\n        Parameters\n        ----------\n        timeseries : xr.Dataset\n            Time-series dataset with an ``epoch`` coordinate.\n\n        Returns\n        -------\n        xr.Dataset\n            Copy with ``solar_zenith``, ``solar_azimuth``,\n            ``solar_elevation`` added.\n\n        Raises\n        ------\n        ValueError\n            If no solar calculator is configured.\n\n        \"\"\"\n        if self.solar_calc is None:\n            raise ValueError(\"Solar calculator not initialized\")\n\n        times = pd.to_datetime(timeseries[\"epoch\"].values)\n        zenith, azimuth = self.solar_calc.compute_solar_position(times)\n        elevation = 90 - zenith\n\n        ts_solar = timeseries.copy()\n        ts_solar[\"solar_zenith\"] = (\"epoch\", zenith)\n        ts_solar[\"solar_azimuth\"] = (\"epoch\", azimuth)\n        ts_solar[\"solar_elevation\"] = (\"epoch\", elevation)\n\n        ts_solar[\"solar_zenith\"].attrs = {\n            \"units\": \"degrees\",\n            \"description\": \"Solar zenith angle (0° = overhead)\",\n        }\n        ts_solar[\"solar_azimuth\"].attrs = {\n            \"units\": \"degrees\",\n            \"description\": \"Solar azimuth angle (0° = North, 90° = East)\",\n        }\n        ts_solar[\"solar_elevation\"].attrs = {\n            \"units\": \"degrees\",\n            \"description\": \"Solar elevation angle (0° = horizon, 90° = overhead)\",\n        }\n        return ts_solar\n\n    # ------------------------------------------------------------------\n    # Plotting\n    # ------------------------------------------------------------------\n\n    def plot_timeseries(\n        self,\n        timeseries: xr.Dataset,\n        smooth_window: int = 0,\n        show_uncertainty: bool = True,\n        show_n_cells: bool = False,\n        ax: plt.Axes | None = None,\n        **style_kwargs: Any,\n    ) -&gt; tuple[plt.Figure, plt.Axes]:\n        \"\"\"Plot a time-series with optional Savitzky-Golay smoothing.\n\n        Parameters\n        ----------\n        timeseries : xr.Dataset\n            Output of :meth:`compute_timeseries`.\n        smooth_window : int, optional\n            Savitzky-Golay window length (0 = off; forced odd internally).\n        show_uncertainty : bool, optional\n            Draw ±1 std band.\n        show_n_cells : bool, optional\n            Secondary y-axis showing cell count.\n        ax : plt.Axes or None, optional\n            Axes to draw on; created if ``None``.\n        **style_kwargs\n            ``ylabel``, ``title``, ``figsize`` forwarded to matplotlib.\n\n        Returns\n        -------\n        fig, ax : plt.Figure, plt.Axes\n\n        \"\"\"\n        if ax is None:\n            figsize = style_kwargs.pop(\"figsize\", (12, 6))\n            fig, ax = plt.subplots(figsize=figsize)\n        else:\n            fig = ax.figure\n\n        time = timeseries[\"epoch\"].values\n        mean = timeseries[\"mean\"].values\n        std = timeseries[\"std\"].values\n\n        if smooth_window &gt; 0:\n            if smooth_window % 2 == 0:\n                smooth_window += 1\n            valid = np.isfinite(mean)\n            if np.sum(valid) &gt; smooth_window:\n                mean_smooth = mean.copy()\n                mean_smooth[valid] = savgol_filter(\n                    mean[valid], smooth_window, polyorder=2\n                )\n                ax.plot(\n                    time,\n                    mean,\n                    \"o\",\n                    alpha=0.3,\n                    label=\"Raw\",\n                    markersize=3,\n                    color=\"gray\",\n                )\n                ax.plot(\n                    time,\n                    mean_smooth,\n                    \"-\",\n                    label=f\"Smoothed (window={smooth_window})\",\n                    linewidth=2,\n                )\n                mean_plot = mean_smooth\n            else:\n                ax.plot(time, mean, \"o-\", label=\"Mean\")\n                mean_plot = mean\n        else:\n            ax.plot(time, mean, \"o-\", label=\"Mean\", markersize=4)\n            mean_plot = mean\n\n        if show_uncertainty and \"std\" in timeseries:\n            ax.fill_between(\n                time,\n                mean_plot - std,\n                mean_plot + std,\n                alpha=0.2,\n                label=\"±1 std\",\n            )\n\n        if show_n_cells and \"n_cells\" in timeseries:\n            ax2 = ax.twinx()\n            ax2.plot(\n                time,\n                timeseries[\"n_cells\"].values,\n                \"--\",\n                color=\"orange\",\n                alpha=0.5,\n                label=\"N cells\",\n            )\n            ax2.set_ylabel(\"Number of cells\", color=\"orange\")\n            ax2.tick_params(axis=\"y\", labelcolor=\"orange\")\n\n        ax.set_xlabel(\"Time\")\n        ax.set_ylabel(style_kwargs.get(\"ylabel\", \"Value\"))\n        ax.set_title(style_kwargs.get(\"title\", \"Timeseries\"))\n        ax.legend(loc=\"best\")\n        ax.grid(True, alpha=0.3)\n        fig.tight_layout()\n        return fig, ax\n\n    def plot_diurnal_cycle(\n        self,\n        diurnal: xr.Dataset,\n        show_confidence: bool = True,\n        ax: plt.Axes | None = None,\n        **style_kwargs: Any,\n    ) -&gt; tuple[plt.Figure, plt.Axes]:\n        \"\"\"Plot a clock-time diurnal cycle.\n\n        Parameters\n        ----------\n        diurnal : xr.Dataset\n            Output of :meth:`compute_diurnal_cycle`.\n        show_confidence : bool, optional\n            Draw ±1 std band.\n        ax : plt.Axes or None, optional\n            Axes to draw on; created if ``None``.\n        **style_kwargs\n            ``ylabel``, ``title``, ``figsize``.\n\n        Returns\n        -------\n        fig, ax : plt.Figure, plt.Axes\n\n        \"\"\"\n        if ax is None:\n            figsize = style_kwargs.pop(\"figsize\", (10, 6))\n            fig, ax = plt.subplots(figsize=figsize)\n        else:\n            fig = ax.figure\n\n        hours = diurnal[\"hour\"].values\n        mean = diurnal[\"mean\"].values\n        std = diurnal[\"std\"].values\n\n        ax.plot(hours, mean, \"o-\", linewidth=2, markersize=6, label=\"Mean\")\n        if show_confidence:\n            ax.fill_between(\n                hours,\n                mean - std,\n                mean + std,\n                alpha=0.2,\n                label=\"±1 std\",\n            )\n\n        ax.set_xlabel(\"Hour of Day\")\n        ax.set_ylabel(style_kwargs.get(\"ylabel\", \"Value\"))\n        ax.set_title(style_kwargs.get(\"title\", \"Diurnal Cycle\"))\n        ax.set_xlim(0, 24)\n        ax.set_xticks(np.arange(0, 25, 3))\n        ax.legend(loc=\"best\")\n        ax.grid(True, alpha=0.3)\n        fig.tight_layout()\n        return fig, ax\n\n    def plot_diurnal_cycle_comparison(\n        self,\n        diurnal_clock: xr.Dataset,\n        diurnal_solar: xr.Dataset,\n        figsize: tuple[float, float] = (14, 6),\n        **style_kwargs: Any,\n    ) -&gt; tuple[plt.Figure, np.ndarray]:\n        \"\"\"Side-by-side clock-time vs solar-time diurnal cycle plots.\n\n        Parameters\n        ----------\n        diurnal_clock : xr.Dataset\n            Output of :meth:`compute_diurnal_cycle`.\n        diurnal_solar : xr.Dataset\n            Output of :meth:`compute_diurnal_cycle_solar`.\n        figsize : tuple, optional\n            Figure size.\n        **style_kwargs\n            ``ylabel``, ``title``.\n\n        Returns\n        -------\n        fig, axes : plt.Figure, np.ndarray of plt.Axes\n\n        \"\"\"\n        fig, axes = plt.subplots(1, 2, figsize=figsize)\n\n        # Clock-time panel\n        hours = diurnal_clock[\"hour\"].values\n        mean_c = diurnal_clock[\"mean\"].values\n        std_c = diurnal_clock[\"std\"].values\n\n        axes[0].plot(hours, mean_c, \"o-\", linewidth=2, markersize=6)\n        axes[0].fill_between(hours, mean_c - std_c, mean_c + std_c, alpha=0.2)\n        axes[0].set_xlabel(\"Hour of Day\")\n        axes[0].set_ylabel(style_kwargs.get(\"ylabel\", \"Value\"))\n        axes[0].set_title(\"Clock-Time Diurnal Cycle\")\n        axes[0].set_xlim(0, 24)\n        axes[0].set_xticks(np.arange(0, 25, 3))\n        axes[0].grid(True, alpha=0.3)\n\n        # Solar-elevation panel\n        solar_elev = diurnal_solar[\"solar_elevation\"].values\n        mean_s = diurnal_solar[\"mean\"].values\n        std_s = diurnal_solar[\"std\"].values\n\n        axes[1].plot(\n            solar_elev,\n            mean_s,\n            \"o-\",\n            linewidth=2,\n            markersize=6,\n            color=\"orange\",\n        )\n        axes[1].fill_between(\n            solar_elev,\n            mean_s - std_s,\n            mean_s + std_s,\n            alpha=0.2,\n            color=\"orange\",\n        )\n        axes[1].axvline(0, color=\"k\", linestyle=\"--\", alpha=0.3, label=\"Horizon\")\n        axes[1].set_xlabel(\"Solar Elevation (°)\")\n        axes[1].set_ylabel(style_kwargs.get(\"ylabel\", \"Value\"))\n        axes[1].set_title(\"Solar-Time Diurnal Cycle\")\n        axes[1].grid(True, alpha=0.3)\n        axes[1].legend()\n\n        fig.suptitle(\n            style_kwargs.get(\"title\", \"Diurnal Cycle Comparison\"), fontsize=14, y=1.02\n        )\n        fig.tight_layout()\n        return fig, axes\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.__init__","level":3,"title":"<code>__init__(vod_ds, grid, grid_name, site_lat=None, site_lon=None, site_elevation=0.0)</code>","text":"<p>Initialize the temporal analysis helper.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.__init__--parameters","level":5,"title":"Parameters","text":"<p>vod_ds : xr.Dataset     VOD dataset containing cell IDs. grid : GridData     Grid instance. grid_name : str     Grid name suffix for cell IDs. site_lat : float | None, optional     Site latitude in degrees. site_lon : float | None, optional     Site longitude in degrees. site_elevation : float, default 0.0     Site elevation in metres.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/temporal.py</code> <pre><code>def __init__(\n    self,\n    vod_ds: xr.Dataset,\n    grid: GridData,\n    grid_name: str,\n    site_lat: float | None = None,\n    site_lon: float | None = None,\n    site_elevation: float = 0.0,\n) -&gt; None:\n    \"\"\"Initialize the temporal analysis helper.\n\n    Parameters\n    ----------\n    vod_ds : xr.Dataset\n        VOD dataset containing cell IDs.\n    grid : GridData\n        Grid instance.\n    grid_name : str\n        Grid name suffix for cell IDs.\n    site_lat : float | None, optional\n        Site latitude in degrees.\n    site_lon : float | None, optional\n        Site longitude in degrees.\n    site_elevation : float, default 0.0\n        Site elevation in metres.\n\n    \"\"\"\n    self.vod_ds = vod_ds\n    self.grid = grid\n    self.grid_name = grid_name\n    self.cell_id_var = f\"cell_id_{grid_name}\"\n\n    # Solar calculator (optional)\n    if site_lat is not None and site_lon is not None:\n        self.solar_calc: SolarPositionCalculator | None = SolarPositionCalculator(\n            lat=site_lat, lon=site_lon, elevation=site_elevation\n        )\n        logger.info(\n            \"solar calculator enabled for (%.4f°, %.4f°)\",\n            site_lat,\n            site_lon,\n        )\n    else:\n        self.solar_calc = None\n\n    # Validate dataset\n    if self.cell_id_var not in vod_ds:\n        available = [v for v in vod_ds.data_vars if v.startswith(\"cell_id_\")]\n        raise ValueError(\n            f\"Cell ID variable '{self.cell_id_var}' not found in dataset. \"\n            f\"Available: {available}\"\n        )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_timeseries","level":3,"title":"<code>compute_timeseries(var_name='VOD', spatial_mask=None, weights=None, aggregate='1D', min_cells=1)</code>","text":"<p>Compute a weighted time-series aggregated over space.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_timeseries--parameters","level":5,"title":"Parameters","text":"<p>var_name : str, optional     Data variable to aggregate. spatial_mask : np.ndarray or None, optional     Boolean mask of shape <code>(grid.ncells,)</code>; <code>True</code> = include. weights : np.ndarray or None, optional     Cell weights of shape <code>(grid.ncells,)</code>; normalised     internally.  <code>None</code> → uniform weights. aggregate : str, optional     Pandas-compatible frequency string for temporal resampling. min_cells : int, optional     Minimum unique cells required per time bin.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_timeseries--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Variables: <code>mean</code>, <code>std</code>, <code>n_cells</code>,     <code>n_observations</code>, <code>sum_weights</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_timeseries--raises","level":5,"title":"Raises","text":"<p>ValueError     If var_name is missing or mask/weight shapes are wrong.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/temporal.py</code> <pre><code>def compute_timeseries(\n    self,\n    var_name: str = \"VOD\",\n    spatial_mask: np.ndarray | None = None,\n    weights: np.ndarray | None = None,\n    aggregate: str = \"1D\",\n    min_cells: int = 1,\n) -&gt; xr.Dataset:\n    \"\"\"Compute a weighted time-series aggregated over space.\n\n    Parameters\n    ----------\n    var_name : str, optional\n        Data variable to aggregate.\n    spatial_mask : np.ndarray or None, optional\n        Boolean mask of shape ``(grid.ncells,)``; ``True`` = include.\n    weights : np.ndarray or None, optional\n        Cell weights of shape ``(grid.ncells,)``; normalised\n        internally.  ``None`` → uniform weights.\n    aggregate : str, optional\n        Pandas-compatible frequency string for temporal resampling.\n    min_cells : int, optional\n        Minimum unique cells required per time bin.\n\n    Returns\n    -------\n    xr.Dataset\n        Variables: ``mean``, ``std``, ``n_cells``,\n        ``n_observations``, ``sum_weights``.\n\n    Raises\n    ------\n    ValueError\n        If *var_name* is missing or mask/weight shapes are wrong.\n\n    \"\"\"\n    if var_name not in self.vod_ds:\n        raise ValueError(f\"Variable '{var_name}' not found in dataset\")\n\n    logger.info(\n        \"compute_timeseries: var=%s aggregate=%s mask=%s weights=%s\",\n        var_name,\n        aggregate,\n        spatial_mask is not None,\n        weights is not None,\n    )\n\n    var_data = self.vod_ds[var_name]\n    cell_ids = self.vod_ds[self.cell_id_var]\n\n    # Apply spatial mask\n    if spatial_mask is not None:\n        if spatial_mask.shape != (self.grid.ncells,):\n            raise ValueError(\n                f\"Spatial mask shape {spatial_mask.shape} doesn't match \"\n                f\"grid size ({self.grid.ncells},)\"\n            )\n        data_mask = xr.zeros_like(cell_ids, dtype=bool)\n        for cell_id in np.where(spatial_mask)[0]:\n            data_mask = data_mask | (cell_ids == cell_id)\n        var_data = var_data.where(data_mask)\n        logger.debug(\n            \"spatial mask applied: %d/%d cells\",\n            spatial_mask.sum(),\n            self.grid.ncells,\n        )\n\n    # Prepare weights\n    weights = self._prepare_weights(weights)\n\n    return self._compute_weighted_timeseries(\n        var_data, cell_ids, weights, aggregate, min_cells\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_timeseries_solar_corrected","level":3,"title":"<code>compute_timeseries_solar_corrected(var_name='VOD', spatial_mask=None, weights=None, aggregate='1D', min_cells=1, solar_correction='normalize', reference_zenith=45.0, daytime_only=False, twilight_angle=-6.0)</code>","text":"<p>Compute a time-series after applying a solar correction.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_timeseries_solar_corrected--parameters","level":5,"title":"Parameters","text":"<p>var_name : str, optional     Data variable to correct and aggregate. spatial_mask : np.ndarray or None, optional     Cell selection mask. weights : np.ndarray or None, optional     Cell weights. aggregate : str, optional     Temporal resampling frequency. min_cells : int, optional     Minimum cells per time bin. solar_correction : {'normalize', 'residual', 'cos_correction'}     Correction method passed to     :meth:<code>SolarPositionCalculator.apply_solar_correction</code>. reference_zenith : float, optional     Reference zenith for normalisation (degrees). daytime_only : bool, optional     If <code>True</code>, mask out nighttime epochs. twilight_angle : float, optional     Solar-elevation threshold for daytime (degrees).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_timeseries_solar_corrected--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Solar-corrected time-series with additional metadata attrs.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_timeseries_solar_corrected--raises","level":5,"title":"Raises","text":"<p>ValueError     If no solar calculator is configured.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/temporal.py</code> <pre><code>def compute_timeseries_solar_corrected(\n    self,\n    var_name: str = \"VOD\",\n    spatial_mask: np.ndarray | None = None,\n    weights: np.ndarray | None = None,\n    aggregate: str = \"1D\",\n    min_cells: int = 1,\n    solar_correction: Literal[\n        \"normalize\", \"residual\", \"cos_correction\"\n    ] = \"normalize\",\n    reference_zenith: float = 45.0,\n    daytime_only: bool = False,\n    twilight_angle: float = -6.0,\n) -&gt; xr.Dataset:\n    \"\"\"Compute a time-series after applying a solar correction.\n\n    Parameters\n    ----------\n    var_name : str, optional\n        Data variable to correct and aggregate.\n    spatial_mask : np.ndarray or None, optional\n        Cell selection mask.\n    weights : np.ndarray or None, optional\n        Cell weights.\n    aggregate : str, optional\n        Temporal resampling frequency.\n    min_cells : int, optional\n        Minimum cells per time bin.\n    solar_correction : {'normalize', 'residual', 'cos_correction'}\n        Correction method passed to\n        :meth:`SolarPositionCalculator.apply_solar_correction`.\n    reference_zenith : float, optional\n        Reference zenith for normalisation (degrees).\n    daytime_only : bool, optional\n        If ``True``, mask out nighttime epochs.\n    twilight_angle : float, optional\n        Solar-elevation threshold for daytime (degrees).\n\n    Returns\n    -------\n    xr.Dataset\n        Solar-corrected time-series with additional metadata attrs.\n\n    Raises\n    ------\n    ValueError\n        If no solar calculator is configured.\n\n    \"\"\"\n    if self.solar_calc is None:\n        raise ValueError(\n            \"Solar calculator not initialized. \"\n            \"Provide site_lat and site_lon to TemporalAnalysis constructor.\"\n        )\n\n    logger.info(\n        \"solar-corrected timeseries: correction=%s daytime_only=%s\",\n        solar_correction,\n        daytime_only,\n    )\n\n    var_data = self.vod_ds[var_name]\n\n    # Apply solar correction\n    var_data_corrected = self.solar_calc.apply_solar_correction(\n        var_data, method=solar_correction, reference_zenith=reference_zenith\n    )\n\n    # Daytime filter\n    if daytime_only:\n        times = pd.to_datetime(var_data[\"epoch\"].values)\n        is_day = self.solar_calc.is_daytime(times, twilight_angle)\n        is_day_da = xr.DataArray(\n            is_day, coords={\"epoch\": var_data_corrected[\"epoch\"]}, dims=[\"epoch\"]\n        )\n        var_data_corrected = var_data_corrected.where(is_day_da)\n        logger.debug(\n            \"daytime filter: %d/%d timesteps kept\",\n            is_day.sum(),\n            len(is_day),\n        )\n\n    # Temporary dataset with corrected variable\n    corrected_name = f\"{var_name}_solar_corrected\"\n    ds_temp = self.vod_ds.copy()\n    ds_temp[corrected_name] = var_data_corrected\n\n    # Reuse compute_timeseries via a lightweight temporary instance\n    analysis_temp = TemporalAnalysis.__new__(TemporalAnalysis)\n    analysis_temp.vod_ds = ds_temp\n    analysis_temp.grid = self.grid\n    analysis_temp.grid_name = self.grid_name\n    analysis_temp.cell_id_var = self.cell_id_var\n    analysis_temp.solar_calc = self.solar_calc\n\n    ts = analysis_temp.compute_timeseries(\n        var_name=corrected_name,\n        spatial_mask=spatial_mask,\n        weights=weights,\n        aggregate=aggregate,\n        min_cells=min_cells,\n    )\n\n    # Solar metadata\n    ts.attrs[\"solar_correction\"] = solar_correction\n    ts.attrs[\"reference_zenith\"] = reference_zenith\n    ts.attrs[\"daytime_only\"] = daytime_only\n    if daytime_only:\n        ts.attrs[\"twilight_angle\"] = twilight_angle\n\n    return ts\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_diurnal_cycle","level":3,"title":"<code>compute_diurnal_cycle(var_name='VOD', spatial_mask=None, weights=None, hour_bins=24, min_observations=10)</code>","text":"<p>Compute clock-time diurnal cycle (hour-of-day statistics).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_diurnal_cycle--parameters","level":5,"title":"Parameters","text":"<p>var_name : str, optional     Data variable to bin. spatial_mask : np.ndarray or None, optional     Cell selection mask. weights : np.ndarray or None, optional     Cell weights. hour_bins : int, optional     Number of equal-width hour bins over [0, 24). min_observations : int, optional     Minimum observations required per bin.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_diurnal_cycle--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     <code>mean</code>, <code>std</code>, <code>n_observations</code> on the <code>hour</code>     coordinate.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/temporal.py</code> <pre><code>def compute_diurnal_cycle(\n    self,\n    var_name: str = \"VOD\",\n    spatial_mask: np.ndarray | None = None,\n    weights: np.ndarray | None = None,\n    hour_bins: int = 24,\n    min_observations: int = 10,\n) -&gt; xr.Dataset:\n    \"\"\"Compute clock-time diurnal cycle (hour-of-day statistics).\n\n    Parameters\n    ----------\n    var_name : str, optional\n        Data variable to bin.\n    spatial_mask : np.ndarray or None, optional\n        Cell selection mask.\n    weights : np.ndarray or None, optional\n        Cell weights.\n    hour_bins : int, optional\n        Number of equal-width hour bins over [0, 24).\n    min_observations : int, optional\n        Minimum observations required per bin.\n\n    Returns\n    -------\n    xr.Dataset\n        ``mean``, ``std``, ``n_observations`` on the ``hour``\n        coordinate.\n\n    \"\"\"\n    if var_name not in self.vod_ds:\n        raise ValueError(f\"Variable '{var_name}' not found in dataset\")\n\n    logger.info(\"compute_diurnal_cycle: var=%s hour_bins=%d\", var_name, hour_bins)\n\n    var_data = self.vod_ds[var_name]\n    cell_ids = self.vod_ds[self.cell_id_var]\n\n    # Spatial mask\n    if spatial_mask is not None:\n        data_mask = xr.zeros_like(cell_ids, dtype=bool)\n        for cell_id in np.where(spatial_mask)[0]:\n            data_mask = data_mask | (cell_ids == cell_id)\n        var_data = var_data.where(data_mask)\n\n    weights = self._prepare_weights(weights)\n\n    # Hour of day (fractional)\n    times = pd.to_datetime(var_data[\"epoch\"].values)\n    hours = times.hour + times.minute / 60.0\n\n    n_sid = var_data.sizes.get(\"sid\", 1)\n    hour_edges = np.linspace(0, 24, hour_bins + 1)\n    hour_centers = (hour_edges[:-1] + hour_edges[1:]) / 2\n\n    # Flatten\n    values = var_data.values.ravel()\n    cells = cell_ids.values.ravel()\n    hours_flat = np.repeat(hours, n_sid)\n\n    valid = np.isfinite(values) &amp; np.isfinite(cells)\n    df = pd.DataFrame(\n        {\n            \"hour\": hours_flat[valid],\n            \"value\": values[valid],\n            \"cell_id\": cells[valid].astype(int),\n        }\n    )\n    df[\"weight\"] = df[\"cell_id\"].map(\n        lambda cid: weights[cid] if cid &lt; len(weights) else 0.0\n    )\n    df[\"hour_bin\"] = pd.cut(\n        df[\"hour\"], bins=hour_edges, labels=hour_centers, include_lowest=True\n    )\n\n    grouped = df.groupby(\"hour_bin\")\n\n    means, stds, n_obs = [], [], []\n    for hc in hour_centers:\n        if hc in grouped.groups:\n            group = grouped.get_group(hc)\n            if len(group) &gt;= min_observations:\n                w = group[\"weight\"].values\n                v = group[\"value\"].values\n                if w.sum() &gt; 0:\n                    wm = np.average(v, weights=w)\n                    ws = np.sqrt(np.average((v - wm) ** 2, weights=w))\n                else:\n                    wm, ws = np.nan, np.nan\n                means.append(wm)\n                stds.append(ws)\n                n_obs.append(len(group))\n            else:\n                means.append(np.nan)\n                stds.append(np.nan)\n                n_obs.append(0)\n        else:\n            means.append(np.nan)\n            stds.append(np.nan)\n            n_obs.append(0)\n\n    ds = xr.Dataset(\n        {\n            \"mean\": (\"hour\", np.array(means)),\n            \"std\": (\"hour\", np.array(stds)),\n            \"n_observations\": (\"hour\", np.array(n_obs)),\n        },\n        coords={\"hour\": hour_centers},\n    )\n    ds.attrs.update(\n        {\n            \"variable\": var_name,\n            \"grid\": self.grid_name,\n            \"hour_bins\": hour_bins,\n            \"min_observations\": min_observations,\n        }\n    )\n    logger.info(\n        \"diurnal cycle: %d bins, mean n_obs=%.1f\",\n        hour_bins,\n        np.nanmean(n_obs),\n    )\n    return ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_diurnal_cycle_solar","level":3,"title":"<code>compute_diurnal_cycle_solar(var_name='VOD', spatial_mask=None, weights=None, n_solar_bins=12, min_observations=10)</code>","text":"<p>Diurnal cycle binned by solar elevation instead of clock time.</p> <p>Accounts for seasonal variation in solar position, producing a more physically meaningful diurnal pattern.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_diurnal_cycle_solar--parameters","level":5,"title":"Parameters","text":"<p>var_name : str, optional     Data variable to bin. spatial_mask : np.ndarray or None, optional     Cell selection mask. weights : np.ndarray or None, optional     Cell weights. n_solar_bins : int, optional     Number of equal-width bins over [-20°, 90°]. min_observations : int, optional     Minimum observations per bin.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_diurnal_cycle_solar--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     <code>mean</code>, <code>std</code>, <code>n_observations</code> on the     <code>solar_elevation</code> coordinate.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.compute_diurnal_cycle_solar--raises","level":5,"title":"Raises","text":"<p>ValueError     If no solar calculator is configured.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/temporal.py</code> <pre><code>def compute_diurnal_cycle_solar(\n    self,\n    var_name: str = \"VOD\",\n    spatial_mask: np.ndarray | None = None,\n    weights: np.ndarray | None = None,\n    n_solar_bins: int = 12,\n    min_observations: int = 10,\n) -&gt; xr.Dataset:\n    \"\"\"Diurnal cycle binned by solar elevation instead of clock time.\n\n    Accounts for seasonal variation in solar position, producing a\n    more physically meaningful diurnal pattern.\n\n    Parameters\n    ----------\n    var_name : str, optional\n        Data variable to bin.\n    spatial_mask : np.ndarray or None, optional\n        Cell selection mask.\n    weights : np.ndarray or None, optional\n        Cell weights.\n    n_solar_bins : int, optional\n        Number of equal-width bins over [-20°, 90°].\n    min_observations : int, optional\n        Minimum observations per bin.\n\n    Returns\n    -------\n    xr.Dataset\n        ``mean``, ``std``, ``n_observations`` on the\n        ``solar_elevation`` coordinate.\n\n    Raises\n    ------\n    ValueError\n        If no solar calculator is configured.\n\n    \"\"\"\n    if self.solar_calc is None:\n        raise ValueError(\n            \"Solar calculator not initialized. \"\n            \"Provide site_lat and site_lon to TemporalAnalysis constructor.\"\n        )\n\n    logger.info(\"solar-binned diurnal cycle: n_bins=%d\", n_solar_bins)\n\n    var_data = self.vod_ds[var_name]\n    cell_ids = self.vod_ds[self.cell_id_var]\n\n    # Spatial mask\n    if spatial_mask is not None:\n        data_mask = xr.zeros_like(cell_ids, dtype=bool)\n        for cell_id in np.where(spatial_mask)[0]:\n            data_mask = data_mask | (cell_ids == cell_id)\n        var_data = var_data.where(data_mask)\n\n    weights = self._prepare_weights(weights)\n\n    # Solar bins per epoch\n    times = pd.to_datetime(var_data[\"epoch\"].values)\n    solar_bins = self.solar_calc.compute_solar_bins(times, n_bins=n_solar_bins)\n    bin_edges = np.linspace(-20, 90, n_solar_bins + 1)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    n_sid = var_data.sizes.get(\"sid\", 1)\n\n    # Flatten\n    values = var_data.values.ravel()\n    cells = cell_ids.values.ravel()\n    bins_flat = np.repeat(solar_bins, n_sid)\n\n    valid = np.isfinite(values) &amp; np.isfinite(cells)\n    df = pd.DataFrame(\n        {\n            \"solar_bin\": bins_flat[valid],\n            \"value\": values[valid],\n            \"cell_id\": cells[valid].astype(int),\n        }\n    )\n    df[\"weight\"] = df[\"cell_id\"].map(\n        lambda cid: weights[cid] if cid &lt; len(weights) else 0.0\n    )\n\n    grouped = df.groupby(\"solar_bin\")\n\n    means, stds, n_obs = [], [], []\n    for bin_idx in range(n_solar_bins):\n        if bin_idx in grouped.groups:\n            group = grouped.get_group(bin_idx)\n            if len(group) &gt;= min_observations:\n                w = group[\"weight\"].values\n                v = group[\"value\"].values\n                if w.sum() &gt; 0:\n                    wm = np.average(v, weights=w)\n                    ws = np.sqrt(np.average((v - wm) ** 2, weights=w))\n                else:\n                    wm, ws = np.nan, np.nan\n                means.append(wm)\n                stds.append(ws)\n                n_obs.append(len(group))\n            else:\n                means.append(np.nan)\n                stds.append(np.nan)\n                n_obs.append(0)\n        else:\n            means.append(np.nan)\n            stds.append(np.nan)\n            n_obs.append(0)\n\n    ds = xr.Dataset(\n        {\n            \"mean\": (\"solar_elevation\", np.array(means)),\n            \"std\": (\"solar_elevation\", np.array(stds)),\n            \"n_observations\": (\"solar_elevation\", np.array(n_obs)),\n        },\n        coords={\"solar_elevation\": bin_centers},\n    )\n    ds.attrs.update(\n        {\n            \"variable\": var_name,\n            \"grid\": self.grid_name,\n            \"n_solar_bins\": n_solar_bins,\n            \"min_observations\": min_observations,\n            \"coordinate_type\": \"solar_elevation\",\n        }\n    )\n    logger.info(\n        \"solar-binned diurnal: %d bins, mean n_obs=%.1f\",\n        n_solar_bins,\n        np.nanmean(n_obs),\n    )\n    return ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.add_solar_metadata_to_timeseries","level":3,"title":"<code>add_solar_metadata_to_timeseries(timeseries)</code>","text":"<p>Attach solar zenith, azimuth and elevation to a time-series.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.add_solar_metadata_to_timeseries--parameters","level":5,"title":"Parameters","text":"<p>timeseries : xr.Dataset     Time-series dataset with an <code>epoch</code> coordinate.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.add_solar_metadata_to_timeseries--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Copy with <code>solar_zenith</code>, <code>solar_azimuth</code>,     <code>solar_elevation</code> added.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.add_solar_metadata_to_timeseries--raises","level":5,"title":"Raises","text":"<p>ValueError     If no solar calculator is configured.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/temporal.py</code> <pre><code>def add_solar_metadata_to_timeseries(self, timeseries: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Attach solar zenith, azimuth and elevation to a time-series.\n\n    Parameters\n    ----------\n    timeseries : xr.Dataset\n        Time-series dataset with an ``epoch`` coordinate.\n\n    Returns\n    -------\n    xr.Dataset\n        Copy with ``solar_zenith``, ``solar_azimuth``,\n        ``solar_elevation`` added.\n\n    Raises\n    ------\n    ValueError\n        If no solar calculator is configured.\n\n    \"\"\"\n    if self.solar_calc is None:\n        raise ValueError(\"Solar calculator not initialized\")\n\n    times = pd.to_datetime(timeseries[\"epoch\"].values)\n    zenith, azimuth = self.solar_calc.compute_solar_position(times)\n    elevation = 90 - zenith\n\n    ts_solar = timeseries.copy()\n    ts_solar[\"solar_zenith\"] = (\"epoch\", zenith)\n    ts_solar[\"solar_azimuth\"] = (\"epoch\", azimuth)\n    ts_solar[\"solar_elevation\"] = (\"epoch\", elevation)\n\n    ts_solar[\"solar_zenith\"].attrs = {\n        \"units\": \"degrees\",\n        \"description\": \"Solar zenith angle (0° = overhead)\",\n    }\n    ts_solar[\"solar_azimuth\"].attrs = {\n        \"units\": \"degrees\",\n        \"description\": \"Solar azimuth angle (0° = North, 90° = East)\",\n    }\n    ts_solar[\"solar_elevation\"].attrs = {\n        \"units\": \"degrees\",\n        \"description\": \"Solar elevation angle (0° = horizon, 90° = overhead)\",\n    }\n    return ts_solar\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.plot_timeseries","level":3,"title":"<code>plot_timeseries(timeseries, smooth_window=0, show_uncertainty=True, show_n_cells=False, ax=None, **style_kwargs)</code>","text":"<p>Plot a time-series with optional Savitzky-Golay smoothing.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.plot_timeseries--parameters","level":5,"title":"Parameters","text":"<p>timeseries : xr.Dataset     Output of :meth:<code>compute_timeseries</code>. smooth_window : int, optional     Savitzky-Golay window length (0 = off; forced odd internally). show_uncertainty : bool, optional     Draw ±1 std band. show_n_cells : bool, optional     Secondary y-axis showing cell count. ax : plt.Axes or None, optional     Axes to draw on; created if <code>None</code>. **style_kwargs     <code>ylabel</code>, <code>title</code>, <code>figsize</code> forwarded to matplotlib.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.plot_timeseries--returns","level":5,"title":"Returns","text":"<p>fig, ax : plt.Figure, plt.Axes</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/temporal.py</code> <pre><code>def plot_timeseries(\n    self,\n    timeseries: xr.Dataset,\n    smooth_window: int = 0,\n    show_uncertainty: bool = True,\n    show_n_cells: bool = False,\n    ax: plt.Axes | None = None,\n    **style_kwargs: Any,\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"Plot a time-series with optional Savitzky-Golay smoothing.\n\n    Parameters\n    ----------\n    timeseries : xr.Dataset\n        Output of :meth:`compute_timeseries`.\n    smooth_window : int, optional\n        Savitzky-Golay window length (0 = off; forced odd internally).\n    show_uncertainty : bool, optional\n        Draw ±1 std band.\n    show_n_cells : bool, optional\n        Secondary y-axis showing cell count.\n    ax : plt.Axes or None, optional\n        Axes to draw on; created if ``None``.\n    **style_kwargs\n        ``ylabel``, ``title``, ``figsize`` forwarded to matplotlib.\n\n    Returns\n    -------\n    fig, ax : plt.Figure, plt.Axes\n\n    \"\"\"\n    if ax is None:\n        figsize = style_kwargs.pop(\"figsize\", (12, 6))\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.figure\n\n    time = timeseries[\"epoch\"].values\n    mean = timeseries[\"mean\"].values\n    std = timeseries[\"std\"].values\n\n    if smooth_window &gt; 0:\n        if smooth_window % 2 == 0:\n            smooth_window += 1\n        valid = np.isfinite(mean)\n        if np.sum(valid) &gt; smooth_window:\n            mean_smooth = mean.copy()\n            mean_smooth[valid] = savgol_filter(\n                mean[valid], smooth_window, polyorder=2\n            )\n            ax.plot(\n                time,\n                mean,\n                \"o\",\n                alpha=0.3,\n                label=\"Raw\",\n                markersize=3,\n                color=\"gray\",\n            )\n            ax.plot(\n                time,\n                mean_smooth,\n                \"-\",\n                label=f\"Smoothed (window={smooth_window})\",\n                linewidth=2,\n            )\n            mean_plot = mean_smooth\n        else:\n            ax.plot(time, mean, \"o-\", label=\"Mean\")\n            mean_plot = mean\n    else:\n        ax.plot(time, mean, \"o-\", label=\"Mean\", markersize=4)\n        mean_plot = mean\n\n    if show_uncertainty and \"std\" in timeseries:\n        ax.fill_between(\n            time,\n            mean_plot - std,\n            mean_plot + std,\n            alpha=0.2,\n            label=\"±1 std\",\n        )\n\n    if show_n_cells and \"n_cells\" in timeseries:\n        ax2 = ax.twinx()\n        ax2.plot(\n            time,\n            timeseries[\"n_cells\"].values,\n            \"--\",\n            color=\"orange\",\n            alpha=0.5,\n            label=\"N cells\",\n        )\n        ax2.set_ylabel(\"Number of cells\", color=\"orange\")\n        ax2.tick_params(axis=\"y\", labelcolor=\"orange\")\n\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(style_kwargs.get(\"ylabel\", \"Value\"))\n    ax.set_title(style_kwargs.get(\"title\", \"Timeseries\"))\n    ax.legend(loc=\"best\")\n    ax.grid(True, alpha=0.3)\n    fig.tight_layout()\n    return fig, ax\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.plot_diurnal_cycle","level":3,"title":"<code>plot_diurnal_cycle(diurnal, show_confidence=True, ax=None, **style_kwargs)</code>","text":"<p>Plot a clock-time diurnal cycle.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.plot_diurnal_cycle--parameters","level":5,"title":"Parameters","text":"<p>diurnal : xr.Dataset     Output of :meth:<code>compute_diurnal_cycle</code>. show_confidence : bool, optional     Draw ±1 std band. ax : plt.Axes or None, optional     Axes to draw on; created if <code>None</code>. **style_kwargs     <code>ylabel</code>, <code>title</code>, <code>figsize</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.plot_diurnal_cycle--returns","level":5,"title":"Returns","text":"<p>fig, ax : plt.Figure, plt.Axes</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/temporal.py</code> <pre><code>def plot_diurnal_cycle(\n    self,\n    diurnal: xr.Dataset,\n    show_confidence: bool = True,\n    ax: plt.Axes | None = None,\n    **style_kwargs: Any,\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"Plot a clock-time diurnal cycle.\n\n    Parameters\n    ----------\n    diurnal : xr.Dataset\n        Output of :meth:`compute_diurnal_cycle`.\n    show_confidence : bool, optional\n        Draw ±1 std band.\n    ax : plt.Axes or None, optional\n        Axes to draw on; created if ``None``.\n    **style_kwargs\n        ``ylabel``, ``title``, ``figsize``.\n\n    Returns\n    -------\n    fig, ax : plt.Figure, plt.Axes\n\n    \"\"\"\n    if ax is None:\n        figsize = style_kwargs.pop(\"figsize\", (10, 6))\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.figure\n\n    hours = diurnal[\"hour\"].values\n    mean = diurnal[\"mean\"].values\n    std = diurnal[\"std\"].values\n\n    ax.plot(hours, mean, \"o-\", linewidth=2, markersize=6, label=\"Mean\")\n    if show_confidence:\n        ax.fill_between(\n            hours,\n            mean - std,\n            mean + std,\n            alpha=0.2,\n            label=\"±1 std\",\n        )\n\n    ax.set_xlabel(\"Hour of Day\")\n    ax.set_ylabel(style_kwargs.get(\"ylabel\", \"Value\"))\n    ax.set_title(style_kwargs.get(\"title\", \"Diurnal Cycle\"))\n    ax.set_xlim(0, 24)\n    ax.set_xticks(np.arange(0, 25, 3))\n    ax.legend(loc=\"best\")\n    ax.grid(True, alpha=0.3)\n    fig.tight_layout()\n    return fig, ax\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.plot_diurnal_cycle_comparison","level":3,"title":"<code>plot_diurnal_cycle_comparison(diurnal_clock, diurnal_solar, figsize=(14, 6), **style_kwargs)</code>","text":"<p>Side-by-side clock-time vs solar-time diurnal cycle plots.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.plot_diurnal_cycle_comparison--parameters","level":5,"title":"Parameters","text":"<p>diurnal_clock : xr.Dataset     Output of :meth:<code>compute_diurnal_cycle</code>. diurnal_solar : xr.Dataset     Output of :meth:<code>compute_diurnal_cycle_solar</code>. figsize : tuple, optional     Figure size. **style_kwargs     <code>ylabel</code>, <code>title</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.temporal.TemporalAnalysis.plot_diurnal_cycle_comparison--returns","level":5,"title":"Returns","text":"<p>fig, axes : plt.Figure, np.ndarray of plt.Axes</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/temporal.py</code> <pre><code>def plot_diurnal_cycle_comparison(\n    self,\n    diurnal_clock: xr.Dataset,\n    diurnal_solar: xr.Dataset,\n    figsize: tuple[float, float] = (14, 6),\n    **style_kwargs: Any,\n) -&gt; tuple[plt.Figure, np.ndarray]:\n    \"\"\"Side-by-side clock-time vs solar-time diurnal cycle plots.\n\n    Parameters\n    ----------\n    diurnal_clock : xr.Dataset\n        Output of :meth:`compute_diurnal_cycle`.\n    diurnal_solar : xr.Dataset\n        Output of :meth:`compute_diurnal_cycle_solar`.\n    figsize : tuple, optional\n        Figure size.\n    **style_kwargs\n        ``ylabel``, ``title``.\n\n    Returns\n    -------\n    fig, axes : plt.Figure, np.ndarray of plt.Axes\n\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=figsize)\n\n    # Clock-time panel\n    hours = diurnal_clock[\"hour\"].values\n    mean_c = diurnal_clock[\"mean\"].values\n    std_c = diurnal_clock[\"std\"].values\n\n    axes[0].plot(hours, mean_c, \"o-\", linewidth=2, markersize=6)\n    axes[0].fill_between(hours, mean_c - std_c, mean_c + std_c, alpha=0.2)\n    axes[0].set_xlabel(\"Hour of Day\")\n    axes[0].set_ylabel(style_kwargs.get(\"ylabel\", \"Value\"))\n    axes[0].set_title(\"Clock-Time Diurnal Cycle\")\n    axes[0].set_xlim(0, 24)\n    axes[0].set_xticks(np.arange(0, 25, 3))\n    axes[0].grid(True, alpha=0.3)\n\n    # Solar-elevation panel\n    solar_elev = diurnal_solar[\"solar_elevation\"].values\n    mean_s = diurnal_solar[\"mean\"].values\n    std_s = diurnal_solar[\"std\"].values\n\n    axes[1].plot(\n        solar_elev,\n        mean_s,\n        \"o-\",\n        linewidth=2,\n        markersize=6,\n        color=\"orange\",\n    )\n    axes[1].fill_between(\n        solar_elev,\n        mean_s - std_s,\n        mean_s + std_s,\n        alpha=0.2,\n        color=\"orange\",\n    )\n    axes[1].axvline(0, color=\"k\", linestyle=\"--\", alpha=0.3, label=\"Horizon\")\n    axes[1].set_xlabel(\"Solar Elevation (°)\")\n    axes[1].set_ylabel(style_kwargs.get(\"ylabel\", \"Value\"))\n    axes[1].set_title(\"Solar-Time Diurnal Cycle\")\n    axes[1].grid(True, alpha=0.3)\n    axes[1].legend()\n\n    fig.suptitle(\n        style_kwargs.get(\"title\", \"Diurnal Cycle Comparison\"), fontsize=14, y=1.02\n    )\n    fig.tight_layout()\n    return fig, axes\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#spatial-analysis","level":3,"title":"Spatial Analysis","text":"<p>Spatial analysis of gridded VOD data.</p> <p>Per-cell statistical aggregation and basic comparative plotting for VOD datasets with pre-assigned cell IDs.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial--classes","level":3,"title":"Classes","text":"<p><code>VODSpatialAnalyzer</code>     Computes per-cell temporal statistics and provides simple     histogram-based comparisons between filtering variants.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial--notes","level":3,"title":"Notes","text":"<ul> <li>Spatial visualisation (hemisphere maps, 3-D projections) is   handled by the <code>canvod-viz</code> package.  This module is limited to   the statistical aggregation and lightweight comparison plots that   do not require a hemisphere renderer.</li> <li><code>compute_spatial_statistics</code> returns both a grid-aligned array   (length <code>grid.ncells</code>, NaN for empty cells) and a compact   patch-aligned array containing only cells with observations.  Use   the grid-aligned form for masking / weighting; use the patch-aligned   form when passing data to <code>canvod-viz</code> renderers.</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer","level":2,"title":"<code>VODSpatialAnalyzer</code>","text":"<p>Per-cell spatial analysis of a VOD dataset.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer--parameters","level":4,"title":"Parameters","text":"<p>vod_data : xr.Dataset     Dataset with a <code>cell_id_&lt;grid_name&gt;</code> variable already     assigned. grid : GridData     Grid instance (must expose <code>.ncells</code>). grid_name : str, optional     Suffix for the cell-ID variable.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer--raises","level":4,"title":"Raises","text":"<p>ValueError     If the expected cell-ID variable is missing.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/spatial.py</code> <pre><code>class VODSpatialAnalyzer:\n    \"\"\"Per-cell spatial analysis of a VOD dataset.\n\n    Parameters\n    ----------\n    vod_data : xr.Dataset\n        Dataset with a ``cell_id_&lt;grid_name&gt;`` variable already\n        assigned.\n    grid : GridData\n        Grid instance (must expose ``.ncells``).\n    grid_name : str, optional\n        Suffix for the cell-ID variable.\n\n    Raises\n    ------\n    ValueError\n        If the expected cell-ID variable is missing.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        vod_data: xr.Dataset,\n        grid: GridData,\n        grid_name: str = \"equal_area_2deg\",\n    ) -&gt; None:\n        \"\"\"Initialize the spatial analyzer.\n\n        Parameters\n        ----------\n        vod_data : xr.Dataset\n            Dataset with cell IDs.\n        grid : GridData\n            Grid instance.\n        grid_name : str, default \"equal_area_2deg\"\n            Grid name suffix for cell IDs.\n\n        \"\"\"\n        self.vod_data = vod_data\n        self.grid = grid\n        self.grid_name = grid_name\n        self.cell_id_var = f\"cell_id_{grid_name}\"\n\n        if self.cell_id_var not in vod_data:\n            available = [v for v in vod_data.data_vars if v.startswith(\"cell_id_\")]\n            raise ValueError(\n                f\"Cell ID variable '{self.cell_id_var}' not found. \"\n                f\"Available: {available}\"\n            )\n\n        logger.info(\n            \"VODSpatialAnalyzer: grid=%s ncells=%d shape=%s\",\n            grid_name,\n            grid.ncells,\n            dict(vod_data.sizes),\n        )\n\n    def compute_spatial_statistics(\n        self,\n        var_name: str = \"VOD\",\n        time_agg: str = \"mean\",\n    ) -&gt; dict:\n        \"\"\"Compute per-cell temporal statistics.\n\n        Parameters\n        ----------\n        var_name : str, optional\n            Data variable to aggregate over time.\n        time_agg : {'mean', 'std', 'count', 'median'}\n            Aggregation function applied per cell.\n\n        Returns\n        -------\n        dict\n            Keys:\n\n            ``grid_aligned`` : np.ndarray\n                Shape ``(grid.ncells,)``.  NaN for cells without data.\n                Use for masking, weighting, or any grid-indexed operation.\n            ``patch_aligned`` : np.ndarray\n                Compact array containing only cells with observations.\n                Use when passing data to ``canvod-viz`` renderers.\n            ``cell_ids_with_data`` : np.ndarray\n                Integer cell IDs corresponding to ``patch_aligned``.\n            ``metadata`` : dict\n                ``valid_cells``, ``total_cells``, ``coverage_percent``,\n                ``variable``, ``aggregation``.\n\n        Raises\n        ------\n        ValueError\n            If *var_name* is not in the dataset or *time_agg* is\n            unrecognised.\n\n        \"\"\"\n        if var_name not in self.vod_data:\n            raise ValueError(\n                f\"Variable '{var_name}' not found. \"\n                f\"Available: {list(self.vod_data.data_vars)}\"\n            )\n\n        _AGG_FUNCS = {\"mean\", \"std\", \"count\", \"median\"}\n        if time_agg not in _AGG_FUNCS:\n            raise ValueError(\n                f\"Unknown aggregation '{time_agg}'; expected one of {_AGG_FUNCS}\"\n            )\n\n        logger.info(\"spatial statistics: var=%s agg=%s\", var_name, time_agg)\n\n        var_data = self.vod_data[var_name]\n        cell_ids = self.vod_data[self.cell_id_var]\n\n        df = pd.DataFrame(\n            {\n                \"cell_id\": cell_ids.values.ravel(),\n                \"value\": var_data.values.ravel(),\n            }\n        )\n        df = df.dropna()\n\n        if len(df) == 0:\n            logger.warning(\"no valid data after removing NaN values\")\n            return {\n                \"grid_aligned\": np.full(self.grid.ncells, np.nan),\n                \"patch_aligned\": np.array([]),\n                \"cell_ids_with_data\": np.array([], dtype=int),\n                \"metadata\": {\n                    \"valid_cells\": 0,\n                    \"total_cells\": self.grid.ncells,\n                    \"coverage_percent\": 0.0,\n                    \"variable\": var_name,\n                    \"aggregation\": time_agg,\n                },\n            }\n\n        logger.debug(\n            \"%d valid observations across %d cells\",\n            len(df),\n            df[\"cell_id\"].nunique(),\n        )\n\n        cell_stats = getattr(df.groupby(\"cell_id\")[\"value\"], time_agg)()\n\n        grid_aligned = np.full(self.grid.ncells, np.nan)\n        patch_aligned: list[float] = []\n        cell_ids_with_data: list[int] = []\n\n        for cell_id, value in cell_stats.items():\n            cell_id_int = int(cell_id)\n            if 0 &lt;= cell_id_int &lt; self.grid.ncells:\n                grid_aligned[cell_id_int] = value\n                patch_aligned.append(value)\n                cell_ids_with_data.append(cell_id_int)\n\n        n_valid = len(patch_aligned)\n        logger.info(\"spatial stats: %d/%d cells with data\", n_valid, self.grid.ncells)\n\n        return {\n            \"grid_aligned\": grid_aligned,\n            \"patch_aligned\": np.array(patch_aligned),\n            \"cell_ids_with_data\": np.array(cell_ids_with_data, dtype=int),\n            \"metadata\": {\n                \"valid_cells\": n_valid,\n                \"total_cells\": self.grid.ncells,\n                \"coverage_percent\": (n_valid / self.grid.ncells) * 100,\n                \"variable\": var_name,\n                \"aggregation\": time_agg,\n            },\n        }\n\n    def compare_filtering_methods(\n        self,\n        original_var: str = \"VOD\",\n        filtered_var: str = \"VOD_filtered\",\n        figsize: tuple = (16, 6),\n        ax: tuple | None = None,\n    ) -&gt; tuple[plt.Figure, np.ndarray]:\n        \"\"\"Histogram comparison of original vs filtered VOD distributions.\n\n        Parameters\n        ----------\n        original_var : str, optional\n            Unfiltered variable name.\n        filtered_var : str, optional\n            Filtered variable name.\n        figsize : tuple, optional\n            Figure size when *ax* is ``None``.\n        ax : tuple of two plt.Axes or None, optional\n            Pre-existing axes pair.  Created if ``None``.\n\n        Returns\n        -------\n        fig, axes : plt.Figure, np.ndarray of plt.Axes\n\n        \"\"\"\n        if ax is None:\n            fig, axes = plt.subplots(1, 2, figsize=figsize)\n        else:\n            axes = ax\n            fig = axes[0].figure\n\n        stats_orig = self.compute_spatial_statistics(original_var, \"mean\")\n        stats_filt = self.compute_spatial_statistics(filtered_var, \"mean\")\n\n        data_orig = stats_orig[\"grid_aligned\"]\n        data_filt = stats_filt[\"grid_aligned\"]\n\n        axes[0].hist(\n            data_orig[np.isfinite(data_orig)], bins=50, alpha=0.7, label=\"Original\"\n        )\n        axes[0].set_title(\"Original VOD Distribution\")\n        axes[0].set_xlabel(\"VOD\")\n        axes[0].set_ylabel(\"Frequency\")\n\n        axes[1].hist(\n            data_filt[np.isfinite(data_filt)],\n            bins=50,\n            alpha=0.7,\n            label=\"Filtered\",\n            color=\"green\",\n        )\n        axes[1].set_title(\"Filtered VOD Distribution\")\n        axes[1].set_xlabel(\"VOD\")\n        axes[1].set_ylabel(\"Frequency\")\n\n        fig.tight_layout()\n        return fig, axes\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer.__init__","level":3,"title":"<code>__init__(vod_data, grid, grid_name='equal_area_2deg')</code>","text":"<p>Initialize the spatial analyzer.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer.__init__--parameters","level":5,"title":"Parameters","text":"<p>vod_data : xr.Dataset     Dataset with cell IDs. grid : GridData     Grid instance. grid_name : str, default \"equal_area_2deg\"     Grid name suffix for cell IDs.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/spatial.py</code> <pre><code>def __init__(\n    self,\n    vod_data: xr.Dataset,\n    grid: GridData,\n    grid_name: str = \"equal_area_2deg\",\n) -&gt; None:\n    \"\"\"Initialize the spatial analyzer.\n\n    Parameters\n    ----------\n    vod_data : xr.Dataset\n        Dataset with cell IDs.\n    grid : GridData\n        Grid instance.\n    grid_name : str, default \"equal_area_2deg\"\n        Grid name suffix for cell IDs.\n\n    \"\"\"\n    self.vod_data = vod_data\n    self.grid = grid\n    self.grid_name = grid_name\n    self.cell_id_var = f\"cell_id_{grid_name}\"\n\n    if self.cell_id_var not in vod_data:\n        available = [v for v in vod_data.data_vars if v.startswith(\"cell_id_\")]\n        raise ValueError(\n            f\"Cell ID variable '{self.cell_id_var}' not found. \"\n            f\"Available: {available}\"\n        )\n\n    logger.info(\n        \"VODSpatialAnalyzer: grid=%s ncells=%d shape=%s\",\n        grid_name,\n        grid.ncells,\n        dict(vod_data.sizes),\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer.compute_spatial_statistics","level":3,"title":"<code>compute_spatial_statistics(var_name='VOD', time_agg='mean')</code>","text":"<p>Compute per-cell temporal statistics.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer.compute_spatial_statistics--parameters","level":5,"title":"Parameters","text":"<p>var_name : str, optional     Data variable to aggregate over time. time_agg : {'mean', 'std', 'count', 'median'}     Aggregation function applied per cell.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer.compute_spatial_statistics--returns","level":5,"title":"Returns","text":"<p>dict     Keys:</p> <pre><code>``grid_aligned`` : np.ndarray\n    Shape ``(grid.ncells,)``.  NaN for cells without data.\n    Use for masking, weighting, or any grid-indexed operation.\n``patch_aligned`` : np.ndarray\n    Compact array containing only cells with observations.\n    Use when passing data to ``canvod-viz`` renderers.\n``cell_ids_with_data`` : np.ndarray\n    Integer cell IDs corresponding to ``patch_aligned``.\n``metadata`` : dict\n    ``valid_cells``, ``total_cells``, ``coverage_percent``,\n    ``variable``, ``aggregation``.\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer.compute_spatial_statistics--raises","level":5,"title":"Raises","text":"<p>ValueError     If var_name is not in the dataset or time_agg is     unrecognised.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/spatial.py</code> <pre><code>def compute_spatial_statistics(\n    self,\n    var_name: str = \"VOD\",\n    time_agg: str = \"mean\",\n) -&gt; dict:\n    \"\"\"Compute per-cell temporal statistics.\n\n    Parameters\n    ----------\n    var_name : str, optional\n        Data variable to aggregate over time.\n    time_agg : {'mean', 'std', 'count', 'median'}\n        Aggregation function applied per cell.\n\n    Returns\n    -------\n    dict\n        Keys:\n\n        ``grid_aligned`` : np.ndarray\n            Shape ``(grid.ncells,)``.  NaN for cells without data.\n            Use for masking, weighting, or any grid-indexed operation.\n        ``patch_aligned`` : np.ndarray\n            Compact array containing only cells with observations.\n            Use when passing data to ``canvod-viz`` renderers.\n        ``cell_ids_with_data`` : np.ndarray\n            Integer cell IDs corresponding to ``patch_aligned``.\n        ``metadata`` : dict\n            ``valid_cells``, ``total_cells``, ``coverage_percent``,\n            ``variable``, ``aggregation``.\n\n    Raises\n    ------\n    ValueError\n        If *var_name* is not in the dataset or *time_agg* is\n        unrecognised.\n\n    \"\"\"\n    if var_name not in self.vod_data:\n        raise ValueError(\n            f\"Variable '{var_name}' not found. \"\n            f\"Available: {list(self.vod_data.data_vars)}\"\n        )\n\n    _AGG_FUNCS = {\"mean\", \"std\", \"count\", \"median\"}\n    if time_agg not in _AGG_FUNCS:\n        raise ValueError(\n            f\"Unknown aggregation '{time_agg}'; expected one of {_AGG_FUNCS}\"\n        )\n\n    logger.info(\"spatial statistics: var=%s agg=%s\", var_name, time_agg)\n\n    var_data = self.vod_data[var_name]\n    cell_ids = self.vod_data[self.cell_id_var]\n\n    df = pd.DataFrame(\n        {\n            \"cell_id\": cell_ids.values.ravel(),\n            \"value\": var_data.values.ravel(),\n        }\n    )\n    df = df.dropna()\n\n    if len(df) == 0:\n        logger.warning(\"no valid data after removing NaN values\")\n        return {\n            \"grid_aligned\": np.full(self.grid.ncells, np.nan),\n            \"patch_aligned\": np.array([]),\n            \"cell_ids_with_data\": np.array([], dtype=int),\n            \"metadata\": {\n                \"valid_cells\": 0,\n                \"total_cells\": self.grid.ncells,\n                \"coverage_percent\": 0.0,\n                \"variable\": var_name,\n                \"aggregation\": time_agg,\n            },\n        }\n\n    logger.debug(\n        \"%d valid observations across %d cells\",\n        len(df),\n        df[\"cell_id\"].nunique(),\n    )\n\n    cell_stats = getattr(df.groupby(\"cell_id\")[\"value\"], time_agg)()\n\n    grid_aligned = np.full(self.grid.ncells, np.nan)\n    patch_aligned: list[float] = []\n    cell_ids_with_data: list[int] = []\n\n    for cell_id, value in cell_stats.items():\n        cell_id_int = int(cell_id)\n        if 0 &lt;= cell_id_int &lt; self.grid.ncells:\n            grid_aligned[cell_id_int] = value\n            patch_aligned.append(value)\n            cell_ids_with_data.append(cell_id_int)\n\n    n_valid = len(patch_aligned)\n    logger.info(\"spatial stats: %d/%d cells with data\", n_valid, self.grid.ncells)\n\n    return {\n        \"grid_aligned\": grid_aligned,\n        \"patch_aligned\": np.array(patch_aligned),\n        \"cell_ids_with_data\": np.array(cell_ids_with_data, dtype=int),\n        \"metadata\": {\n            \"valid_cells\": n_valid,\n            \"total_cells\": self.grid.ncells,\n            \"coverage_percent\": (n_valid / self.grid.ncells) * 100,\n            \"variable\": var_name,\n            \"aggregation\": time_agg,\n        },\n    }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer.compare_filtering_methods","level":3,"title":"<code>compare_filtering_methods(original_var='VOD', filtered_var='VOD_filtered', figsize=(16, 6), ax=None)</code>","text":"<p>Histogram comparison of original vs filtered VOD distributions.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer.compare_filtering_methods--parameters","level":5,"title":"Parameters","text":"<p>original_var : str, optional     Unfiltered variable name. filtered_var : str, optional     Filtered variable name. figsize : tuple, optional     Figure size when ax is <code>None</code>. ax : tuple of two plt.Axes or None, optional     Pre-existing axes pair.  Created if <code>None</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.spatial.VODSpatialAnalyzer.compare_filtering_methods--returns","level":5,"title":"Returns","text":"<p>fig, axes : plt.Figure, np.ndarray of plt.Axes</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/spatial.py</code> <pre><code>def compare_filtering_methods(\n    self,\n    original_var: str = \"VOD\",\n    filtered_var: str = \"VOD_filtered\",\n    figsize: tuple = (16, 6),\n    ax: tuple | None = None,\n) -&gt; tuple[plt.Figure, np.ndarray]:\n    \"\"\"Histogram comparison of original vs filtered VOD distributions.\n\n    Parameters\n    ----------\n    original_var : str, optional\n        Unfiltered variable name.\n    filtered_var : str, optional\n        Filtered variable name.\n    figsize : tuple, optional\n        Figure size when *ax* is ``None``.\n    ax : tuple of two plt.Axes or None, optional\n        Pre-existing axes pair.  Created if ``None``.\n\n    Returns\n    -------\n    fig, axes : plt.Figure, np.ndarray of plt.Axes\n\n    \"\"\"\n    if ax is None:\n        fig, axes = plt.subplots(1, 2, figsize=figsize)\n    else:\n        axes = ax\n        fig = axes[0].figure\n\n    stats_orig = self.compute_spatial_statistics(original_var, \"mean\")\n    stats_filt = self.compute_spatial_statistics(filtered_var, \"mean\")\n\n    data_orig = stats_orig[\"grid_aligned\"]\n    data_filt = stats_filt[\"grid_aligned\"]\n\n    axes[0].hist(\n        data_orig[np.isfinite(data_orig)], bins=50, alpha=0.7, label=\"Original\"\n    )\n    axes[0].set_title(\"Original VOD Distribution\")\n    axes[0].set_xlabel(\"VOD\")\n    axes[0].set_ylabel(\"Frequency\")\n\n    axes[1].hist(\n        data_filt[np.isfinite(data_filt)],\n        bins=50,\n        alpha=0.7,\n        label=\"Filtered\",\n        color=\"green\",\n    )\n    axes[1].set_title(\"Filtered VOD Distribution\")\n    axes[1].set_xlabel(\"VOD\")\n    axes[1].set_ylabel(\"Frequency\")\n\n    fig.tight_layout()\n    return fig, axes\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#per-cell-vod-analysis","level":3,"title":"Per-Cell VOD Analysis","text":"<p>Per-cell VOD analysis and plotting.</p> <p>Statistical aggregation, diurnal dynamics, radial distributions, and theta-time heatmaps for per-cell VOD datasets.  Handles single datasets or lists of datasets with configurable multi-dataset modes (separate vs averaged).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis--classes","level":3,"title":"Classes","text":"<p><code>PerCellVODAnalyzer</code>     Main analysis class.  Accepts one or more per-cell datasets     (each must expose <code>cell_timeseries</code>, <code>cell_theta</code>,     <code>cell_phi</code>).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis--utility-functions","level":3,"title":"Utility functions","text":"<p><code>extract_percell_stats</code>            – temporal statistic per cell. <code>percell_to_grid_counts</code>           – total observation counts per cell. <code>extract_percell_temporal_stats</code>   – range / trend / CV per cell. <code>extract_percell_coverage</code>         – data-coverage percentage per cell. <code>percell_to_grid_data</code>             – thin wrapper around                                        :func:<code>extract_percell_stats</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis--notes","level":3,"title":"Notes","text":"<ul> <li>Per-cell datasets are expected to have dimensions   <code>(cell, time)</code> and variables <code>cell_timeseries</code>, <code>cell_theta</code>,   <code>cell_phi</code>, and optionally <code>cell_weights</code> and <code>cell_counts</code>.</li> <li>Spatial visualisation (hemisphere maps) lives in <code>canvod-viz</code>.</li> </ul>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.PerCellVODAnalyzer","level":2,"title":"<code>PerCellVODAnalyzer</code>","text":"<p>Multi-dataset per-cell VOD analyzer.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.PerCellVODAnalyzer--parameters","level":4,"title":"Parameters","text":"<p>datasets : xr.Dataset or list of xr.Dataset     Per-cell dataset(s).  Each must contain <code>cell_timeseries</code>,     <code>cell_theta</code>, and <code>cell_phi</code>. labels : list of str or None, optional     Human-readable labels for each dataset.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.PerCellVODAnalyzer--raises","level":4,"title":"Raises","text":"<p>ValueError     If any dataset is missing required variables.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_analysis.py</code> <pre><code>class PerCellVODAnalyzer:\n    \"\"\"Multi-dataset per-cell VOD analyzer.\n\n    Parameters\n    ----------\n    datasets : xr.Dataset or list of xr.Dataset\n        Per-cell dataset(s).  Each must contain ``cell_timeseries``,\n        ``cell_theta``, and ``cell_phi``.\n    labels : list of str or None, optional\n        Human-readable labels for each dataset.\n\n    Raises\n    ------\n    ValueError\n        If any dataset is missing required variables.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        datasets: xr.Dataset | list[xr.Dataset],\n        labels: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the analyzer.\n\n        Parameters\n        ----------\n        datasets : xr.Dataset | list[xr.Dataset]\n            Per-cell dataset(s).\n        labels : list[str] | None, optional\n            Labels for each dataset.\n\n        \"\"\"\n        if isinstance(datasets, xr.Dataset):\n            self.datasets = [datasets]\n            self.labels = [\"Dataset\"] if labels is None else [labels[0]]\n        else:\n            self.datasets = datasets\n            self.labels = labels or [f\"Dataset {i + 1}\" for i in range(len(datasets))]\n\n        self._validate_datasets()\n        logger.info(\n            \"PerCellVODAnalyzer: %d dataset(s), shapes=%s\",\n            len(self.datasets),\n            [ds.cell_timeseries.shape for ds in self.datasets],\n        )\n\n    def _validate_datasets(self) -&gt; None:\n        \"\"\"Raise if any dataset is missing required variables.\"\"\"\n        required = {\"cell_timeseries\", \"cell_theta\", \"cell_phi\"}\n        for i, ds in enumerate(self.datasets):\n            missing = required - set(ds.data_vars)\n            if missing:\n                raise ValueError(f\"Dataset {i + 1} missing variables: {missing}\")\n\n    # ------------------------------------------------------------------\n    # Computation helpers\n    # ------------------------------------------------------------------\n\n    @staticmethod\n    def _generate_dataset_hash(ds: xr.Dataset) -&gt; str:\n        \"\"\"Deterministic hash for a per-cell dataset (for optional caching).\n\n        Samples up to 10 000 values from ``cell_timeseries`` for\n        efficiency on large datasets.\n        \"\"\"\n        parts: list[str] = []\n\n        parts.append(hashlib.md5(ds.time.values.tobytes()).hexdigest()[:8])\n\n        cell_data = ds.cell_timeseries.values\n        if cell_data.size &gt; 10_000:\n            idx = np.linspace(0, cell_data.size - 1, 10_000, dtype=int)\n            sample = cell_data.flat[idx]\n        else:\n            sample = cell_data.flatten()\n\n        valid = sample[np.isfinite(sample)]\n        if len(valid) &gt; 0:\n            parts.append(hashlib.md5(valid.tobytes()).hexdigest()[:8])\n\n        shape_str = f\"{ds.cell_timeseries.shape}_{len(ds.time)}\"\n        parts.append(hashlib.md5(shape_str.encode()).hexdigest()[:8])\n\n        return hashlib.md5(\"_\".join(parts).encode()).hexdigest()[:16]\n\n    def _compute_diurnal_dynamics(self, ds: xr.Dataset) -&gt; dict:\n        \"\"\"Hourly diurnal statistics (mean, std, count) from one dataset.\n\n        Uses ``cell_weights`` when available.\n        \"\"\"\n        cell_ts = ds.cell_timeseries\n        hours = ds.time.dt.hour.values\n        unique_hours = np.unique(hours)\n\n        means, stds, counts = [], [], []\n\n        for hour in unique_hours:\n            mask = hours == hour\n            if not np.any(mask):\n                means.append(np.nan)\n                stds.append(np.nan)\n                counts.append(0)\n                continue\n\n            hour_data = cell_ts[:, mask]\n\n            if \"cell_weights\" in ds.data_vars:\n                w = ds.cell_weights[:, mask]\n                w_sum = np.nansum(w, axis=0)\n                w_sum[w_sum == 0] = np.nan\n                weighted_means = np.nansum(hour_data * w, axis=0) / w_sum\n                means.append(float(np.nanmean(weighted_means)))\n                stds.append(float(np.nanstd(weighted_means)))\n            else:\n                means.append(float(np.nanmean(hour_data)))\n                stds.append(float(np.nanstd(hour_data)))\n\n            counts.append(int(np.sum(np.isfinite(hour_data))))\n\n        return {\n            \"hours\": unique_hours,\n            \"means\": np.array(means),\n            \"stds\": np.array(stds),\n            \"counts\": np.array(counts),\n        }\n\n    def _compute_diurnal_dynamics_30min(self, ds: xr.Dataset) -&gt; dict:\n        \"\"\"30-minute-resolution diurnal statistics from one dataset.\"\"\"\n        cell_ts = ds.cell_timeseries\n        hours = ds.time.dt.hour.values\n        minutes = ds.time.dt.minute.values\n        bins_30 = hours * 2 + (minutes &gt;= 30).astype(int)\n        unique_bins = np.unique(bins_30)\n\n        means, stds, counts, labels = [], [], [], []\n\n        for b in unique_bins:\n            mask = bins_30 == b\n            if not np.any(mask):\n                means.append(np.nan)\n                stds.append(np.nan)\n                counts.append(0)\n            else:\n                bin_data = cell_ts[:, mask]\n                if \"cell_weights\" in ds.data_vars:\n                    w = ds.cell_weights[:, mask]\n                    w_sum = np.nansum(w, axis=0)\n                    w_sum[w_sum == 0] = np.nan\n                    wm = np.nansum(bin_data * w, axis=0) / w_sum\n                    means.append(float(np.nanmean(wm)))\n                    stds.append(float(np.nanstd(wm)))\n                else:\n                    means.append(float(np.nanmean(bin_data)))\n                    stds.append(float(np.nanstd(bin_data)))\n                counts.append(int(np.sum(np.isfinite(bin_data))))\n\n            h, m = int(b) // 2, 30 if (int(b) % 2) else 0\n            labels.append(f\"{h:02d}:{m:02d}\")\n\n        return {\n            \"time_bins\": unique_bins,\n            \"time_labels\": labels,\n            \"means\": np.array(means),\n            \"stds\": np.array(stds),\n            \"counts\": np.array(counts),\n        }\n\n    def _compute_averaged_diurnal_dynamics(self) -&gt; dict:\n        \"\"\"Average diurnal dynamics across all datasets.\n\n        Uncertainty is propagated as σ_avg = √(Σσᵢ²) / N.\n        \"\"\"\n        all_data = [self._compute_diurnal_dynamics(ds) for ds in self.datasets]\n        all_hours = [d[\"hours\"] for d in all_data]\n        common = sorted(set.intersection(*[set(h) for h in all_hours]))\n\n        avg_means, avg_stds, avg_counts = [], [], []\n        for hour in common:\n            h_means, h_stds, h_counts = [], [], []\n            for d in all_data:\n                idx = np.where(d[\"hours\"] == hour)[0]\n                if len(idx):\n                    h_means.append(d[\"means\"][idx[0]])\n                    h_stds.append(d[\"stds\"][idx[0]])\n                    h_counts.append(d[\"counts\"][idx[0]])\n            if h_means:\n                avg_means.append(np.nanmean(h_means))\n                avg_stds.append(np.sqrt(np.nansum(np.array(h_stds) ** 2)) / len(h_stds))\n                avg_counts.append(int(np.sum(h_counts)))\n\n        return {\n            \"hours\": np.array(common),\n            \"means\": np.array(avg_means),\n            \"stds\": np.array(avg_stds),\n            \"counts\": np.array(avg_counts),\n        }\n\n    def _compute_combined_diurnal_distributions(self) -&gt; dict[int, list[float]]:\n        \"\"\"Collect all hourly value distributions across all datasets.\"\"\"\n        combined: dict[int, list[float]] = {}\n        for hour in range(24):\n            vals: list[float] = []\n            for ds in self.datasets:\n                mask = ds.time.dt.hour.values == hour\n                if np.any(mask):\n                    raw = ds.cell_timeseries[:, mask].values.flatten()\n                    vals.extend(raw[np.isfinite(raw)].tolist())\n            if vals:\n                combined[hour] = vals\n        return combined\n\n    def _compute_radial_distribution(self, ds: xr.Dataset) -&gt; dict:\n        \"\"\"Bin mean VOD by polar angle (5° bins, 0–90°).\"\"\"\n        polar = 90.0 - ds.cell_theta.values\n        mean_vod = ds.cell_timeseries.mean(dim=\"time\").values\n\n        edges = np.arange(0, 95, 5)\n        centers = edges[:-1] + 2.5\n        indices = np.digitize(polar, edges)\n\n        binned, labels = [], []\n        for i, c in enumerate(centers):\n            mask = indices == i + 1\n            if np.any(mask):\n                clean = mean_vod[mask]\n                clean = clean[np.isfinite(clean)]\n                if len(clean) &gt; 0:\n                    binned.append(clean)\n                    labels.append(f\"{edges[i]:.0f}-{edges[i + 1]:.0f}°\")\n\n        return {\n            \"binned_data\": binned,\n            \"bin_labels\": labels,\n            \"bin_centers\": centers[: len(binned)],\n        }\n\n    def _compute_averaged_radial_distribution(self) -&gt; dict:\n        \"\"\"Average radial distributions across all datasets.\"\"\"\n        all_radial = [self._compute_radial_distribution(ds) for ds in self.datasets]\n        all_centers = [d[\"bin_centers\"] for d in all_radial]\n        common = sorted(set.intersection(*[set(c) for c in all_centers]))\n\n        avg_binned, avg_labels = [], []\n        for c in common:\n            combined: list[float] = []\n            for d in all_radial:\n                idx = np.where(np.abs(d[\"bin_centers\"] - c) &lt; 0.1)[0]\n                if len(idx):\n                    combined.extend(d[\"binned_data\"][idx[0]])\n            if combined:\n                avg_binned.append(combined)\n                avg_labels.append(f\"{c - 2.5:.0f}-{c + 2.5:.0f}°\")\n\n        return {\n            \"binned_data\": avg_binned,\n            \"bin_labels\": avg_labels,\n            \"bin_centers\": np.array(common),\n        }\n\n    def _compute_single_theta_time_heatmap(\n        self, ds: xr.Dataset, time_aggregation: str, theta_bins: int\n    ) -&gt; tuple[np.ndarray, dict]:\n        \"\"\"Compute one theta × time heatmap.\"\"\"\n        polar = 90.0 - ds.cell_theta.values\n        cell_ts = ds.cell_timeseries.values\n        time_coord = ds.time.values\n\n        edges = np.linspace(0, 90, theta_bins + 1)\n        time_info = self._process_time_aggregation(time_coord, time_aggregation)\n\n        heatmap = np.full((theta_bins, time_info[\"n_time_bins\"]), np.nan)\n\n        for i in range(theta_bins):\n            theta_mask = (polar &gt;= edges[i]) &amp; (polar &lt; edges[i + 1])\n            if not np.any(theta_mask):\n                continue\n            bin_ts = cell_ts[theta_mask, :]\n            for t in range(time_info[\"n_time_bins\"]):\n                t_mask = time_info[\"time_groups\"] == t\n                if np.any(t_mask):\n                    heatmap[i, t] = np.nanmean(bin_ts[:, t_mask])\n\n        return heatmap, time_info\n\n    def _compute_averaged_theta_time_heatmap(\n        self, time_aggregation: str, theta_bins: int\n    ) -&gt; tuple[np.ndarray, dict]:\n        \"\"\"Average theta-time heatmaps across all datasets.\"\"\"\n        heatmaps, time_info = [], None\n        for ds in self.datasets:\n            h, ti = self._compute_single_theta_time_heatmap(\n                ds,\n                time_aggregation,\n                theta_bins,\n            )\n            heatmaps.append(h)\n            if time_info is None:\n                time_info = ti\n        return np.nanmean(np.stack(heatmaps, axis=0), axis=0), time_info\n\n    @staticmethod\n    def _process_time_aggregation(\n        time_coord: np.ndarray,\n        time_aggregation: str,\n    ) -&gt; dict:\n        \"\"\"Map raw timestamps to integer group indices.\n\n        Returns\n        -------\n        dict\n            ``time_groups``, ``n_time_bins``, ``time_ticks``,\n            ``time_tick_labels``, ``x_label``.\n\n        \"\"\"\n        if time_aggregation == \"diurnal\":\n            groups = pd.to_datetime(time_coord).hour.values\n            n = 24\n            ticks = np.arange(0, 24, 4)\n            tick_labels = [f\"{h:02d}:00\" for h in ticks]\n            xlabel = \"Hour of Day (UTC)\"\n\n        elif time_aggregation == \"daily\":\n            groups = np.arange(len(time_coord))\n            n = len(time_coord)\n            ticks = np.arange(0, n, max(1, n // 10))\n            tick_labels = (\n                pd.to_datetime(time_coord)[ticks].strftime(\"%Y-%m-%d\").tolist()\n            )\n            xlabel = \"Date\"\n\n        elif time_aggregation == \"weekly\":\n            dates = pd.to_datetime(time_coord)\n            weeks = dates.isocalendar().week\n            unique_weeks = np.unique(weeks)\n            groups = np.searchsorted(unique_weeks, weeks)\n            n = len(unique_weeks)\n            ticks = np.arange(0, n, max(1, n // 10))\n            tick_labels = [f\"Week {unique_weeks[i]}\" for i in ticks]\n            xlabel = \"Week\"\n\n        elif time_aggregation == \"monthly\":\n            dates = pd.to_datetime(time_coord)\n            months = dates.month\n            unique_months = np.unique(months)\n            groups = np.searchsorted(unique_months, months)\n            n = len(unique_months)\n            ticks = np.arange(0, n)\n            tick_labels = [f\"Month {unique_months[i]}\" for i in ticks]\n            xlabel = \"Month\"\n\n        else:\n            raise ValueError(f\"Unknown time_aggregation: '{time_aggregation}'\")\n\n        return {\n            \"time_groups\": groups,\n            \"n_time_bins\": n,\n            \"time_ticks\": ticks,\n            \"time_tick_labels\": tick_labels,\n            \"x_label\": xlabel,\n        }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.PerCellVODAnalyzer.__init__","level":3,"title":"<code>__init__(datasets, labels=None)</code>","text":"<p>Initialize the analyzer.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.PerCellVODAnalyzer.__init__--parameters","level":5,"title":"Parameters","text":"<p>datasets : xr.Dataset | list[xr.Dataset]     Per-cell dataset(s). labels : list[str] | None, optional     Labels for each dataset.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_analysis.py</code> <pre><code>def __init__(\n    self,\n    datasets: xr.Dataset | list[xr.Dataset],\n    labels: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the analyzer.\n\n    Parameters\n    ----------\n    datasets : xr.Dataset | list[xr.Dataset]\n        Per-cell dataset(s).\n    labels : list[str] | None, optional\n        Labels for each dataset.\n\n    \"\"\"\n    if isinstance(datasets, xr.Dataset):\n        self.datasets = [datasets]\n        self.labels = [\"Dataset\"] if labels is None else [labels[0]]\n    else:\n        self.datasets = datasets\n        self.labels = labels or [f\"Dataset {i + 1}\" for i in range(len(datasets))]\n\n    self._validate_datasets()\n    logger.info(\n        \"PerCellVODAnalyzer: %d dataset(s), shapes=%s\",\n        len(self.datasets),\n        [ds.cell_timeseries.shape for ds in self.datasets],\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.extract_percell_stats","level":2,"title":"<code>extract_percell_stats(percell_ds, stat='median')</code>","text":"<p>Compute a single temporal statistic for every cell.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.extract_percell_stats--parameters","level":4,"title":"Parameters","text":"<p>percell_ds : xr.Dataset     Per-cell dataset with a <code>cell_timeseries</code> variable of shape     <code>(cell, time)</code>. stat : {\"mean\", \"median\", \"std\"}     Statistic to reduce across the <code>time</code> dimension.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.extract_percell_stats--returns","level":4,"title":"Returns","text":"<p>np.ndarray     1-D array of length <code>n_cells</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_analysis.py</code> <pre><code>def extract_percell_stats(\n    percell_ds: xr.Dataset,\n    stat: Literal[\"mean\", \"median\", \"std\"] = \"median\",\n) -&gt; np.ndarray:\n    \"\"\"Compute a single temporal statistic for every cell.\n\n    Parameters\n    ----------\n    percell_ds : xr.Dataset\n        Per-cell dataset with a ``cell_timeseries`` variable of shape\n        ``(cell, time)``.\n    stat : {\"mean\", \"median\", \"std\"}\n        Statistic to reduce across the ``time`` dimension.\n\n    Returns\n    -------\n    np.ndarray\n        1-D array of length ``n_cells``.\n\n    \"\"\"\n    cell_timeseries = percell_ds.cell_timeseries\n\n    if stat == \"mean\":\n        cell_values = cell_timeseries.mean(dim=\"time\")\n    elif stat == \"median\":\n        cell_values = cell_timeseries.median(dim=\"time\")\n    elif stat == \"std\":\n        cell_values = cell_timeseries.std(dim=\"time\")\n    else:\n        raise ValueError(f\"Unsupported stat: {stat}\")\n\n    return cell_values.values\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.percell_to_grid_counts","level":2,"title":"<code>percell_to_grid_counts(percell_ds)</code>","text":"<p>Sum observation counts across time for each cell.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.percell_to_grid_counts--parameters","level":4,"title":"Parameters","text":"<p>percell_ds : xr.Dataset     Per-cell dataset with a <code>cell_counts</code> variable of shape     <code>(cell, time)</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.percell_to_grid_counts--returns","level":4,"title":"Returns","text":"<p>np.ndarray     1-D array of total counts per cell.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_analysis.py</code> <pre><code>def percell_to_grid_counts(percell_ds: xr.Dataset) -&gt; np.ndarray:\n    \"\"\"Sum observation counts across time for each cell.\n\n    Parameters\n    ----------\n    percell_ds : xr.Dataset\n        Per-cell dataset with a ``cell_counts`` variable of shape\n        ``(cell, time)``.\n\n    Returns\n    -------\n    np.ndarray\n        1-D array of total counts per cell.\n\n    \"\"\"\n    cell_counts = percell_ds.cell_counts\n    return cell_counts.sum(dim=\"time\").values\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.extract_percell_temporal_stats","level":2,"title":"<code>extract_percell_temporal_stats(percell_ds, stat='range')</code>","text":"<p>Compute a temporal-characteristic statistic for every cell.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.extract_percell_temporal_stats--parameters","level":4,"title":"Parameters","text":"<p>percell_ds : xr.Dataset     Per-cell dataset with <code>cell_timeseries</code>. stat : {\"range\", \"trend\", \"cv\"}     Statistic:</p> <pre><code>* ``\"range\"`` – max − min over time.\n* ``\"trend\"`` – linear-regression slope (requires ≥ 4 valid points;\n  otherwise NaN).  Uses :mod:`scipy.stats`.\n* ``\"cv\"``    – coefficient of variation (std / mean).\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.extract_percell_temporal_stats--returns","level":4,"title":"Returns","text":"<p>np.ndarray     1-D array of length <code>n_cells</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_analysis.py</code> <pre><code>def extract_percell_temporal_stats(\n    percell_ds: xr.Dataset,\n    stat: Literal[\"range\", \"trend\", \"cv\"] = \"range\",\n) -&gt; np.ndarray:\n    \"\"\"Compute a temporal-characteristic statistic for every cell.\n\n    Parameters\n    ----------\n    percell_ds : xr.Dataset\n        Per-cell dataset with ``cell_timeseries``.\n    stat : {\"range\", \"trend\", \"cv\"}\n        Statistic:\n\n        * ``\"range\"`` – max − min over time.\n        * ``\"trend\"`` – linear-regression slope (requires ≥ 4 valid points;\n          otherwise NaN).  Uses :mod:`scipy.stats`.\n        * ``\"cv\"``    – coefficient of variation (std / mean).\n\n    Returns\n    -------\n    np.ndarray\n        1-D array of length ``n_cells``.\n\n    \"\"\"\n    from scipy import stats\n\n    cell_timeseries = percell_ds.cell_timeseries\n\n    if stat == \"range\":\n        cell_max = cell_timeseries.max(dim=\"time\")\n        cell_min = cell_timeseries.min(dim=\"time\")\n        result = cell_max - cell_min\n\n    elif stat == \"trend\":\n        trends = []\n        for i in range(cell_timeseries.shape[0]):\n            cell_data = cell_timeseries.values[i, :]\n            valid_mask = np.isfinite(cell_data)\n\n            if np.sum(valid_mask) &gt; 3:\n                time_indices = np.arange(len(cell_data))[valid_mask]\n                values = cell_data[valid_mask]\n                slope, _, _, _, _ = stats.linregress(time_indices, values)\n                trends.append(slope)\n            else:\n                trends.append(np.nan)\n\n        result = xr.DataArray(trends, dims=[\"cell\"])\n\n    elif stat == \"cv\":\n        cell_mean = cell_timeseries.mean(dim=\"time\")\n        cell_std = cell_timeseries.std(dim=\"time\")\n        result = cell_std / cell_mean\n\n    else:\n        raise ValueError(f\"Unsupported temporal stat: {stat}\")\n\n    return result.values\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.extract_percell_coverage","level":2,"title":"<code>extract_percell_coverage(percell_ds)</code>","text":"<p>Compute the fraction of valid (non-NaN) observations per cell.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.extract_percell_coverage--parameters","level":4,"title":"Parameters","text":"<p>percell_ds : xr.Dataset     Per-cell dataset with <code>cell_timeseries</code>.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.extract_percell_coverage--returns","level":4,"title":"Returns","text":"<p>np.ndarray     1-D array of coverage percentages (0–100) per cell.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_analysis.py</code> <pre><code>def extract_percell_coverage(percell_ds: xr.Dataset) -&gt; np.ndarray:\n    \"\"\"Compute the fraction of valid (non-NaN) observations per cell.\n\n    Parameters\n    ----------\n    percell_ds : xr.Dataset\n        Per-cell dataset with ``cell_timeseries``.\n\n    Returns\n    -------\n    np.ndarray\n        1-D array of coverage percentages (0–100) per cell.\n\n    \"\"\"\n    cell_timeseries = percell_ds.cell_timeseries\n\n    valid_count = np.isfinite(cell_timeseries).sum(dim=\"time\")\n    total_count = cell_timeseries.sizes[\"time\"]\n\n    coverage_pct = (valid_count / total_count) * 100\n    return coverage_pct.values\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.percell_to_grid_data","level":2,"title":"<code>percell_to_grid_data(percell_ds, stat='median')</code>","text":"<p>Thin wrapper around :func:<code>extract_percell_stats</code>.</p> <p>Provided for symmetry with <code>aggregate_data_to_grid</code> workflows.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.percell_to_grid_data--parameters","level":4,"title":"Parameters","text":"<p>percell_ds : xr.Dataset     Per-cell dataset. stat : {\"mean\", \"median\", \"std\"}     Statistic to compute.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.per_cell_analysis.percell_to_grid_data--returns","level":4,"title":"Returns","text":"<p>np.ndarray     1-D array compatible with grid visualisation.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/per_cell_analysis.py</code> <pre><code>def percell_to_grid_data(\n    percell_ds: xr.Dataset,\n    stat: Literal[\"mean\", \"median\", \"std\"] = \"median\",\n) -&gt; np.ndarray:\n    \"\"\"Thin wrapper around :func:`extract_percell_stats`.\n\n    Provided for symmetry with ``aggregate_data_to_grid`` workflows.\n\n    Parameters\n    ----------\n    percell_ds : xr.Dataset\n        Per-cell dataset.\n    stat : {\"mean\", \"median\", \"std\"}\n        Statistic to compute.\n\n    Returns\n    -------\n    np.ndarray\n        1-D array compatible with grid visualisation.\n\n    \"\"\"\n    return extract_percell_stats(percell_ds, stat=stat)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#analysis-storage","level":3,"title":"Analysis Storage","text":"<p>Persistent storage for precomputed analysis results via Icechunk.</p> <p>Stores dataset+grid-specific analysis outputs in an Icechunk repository under <code>metadata/{dataset_name}/{grid_name}/</code>:</p> <ul> <li>weights – per-cell weight arrays (<code>ncells,</code>).</li> <li>filter_masks – per-observation statistical masks (<code>epoch × sid</code>).</li> <li>spatial_masks – per-cell geometric selection masks (<code>ncells,</code>).</li> <li>statistics – per-cell aggregated statistics (<code>ncells,</code>).</li> </ul> <p>Depends on <code>canvod-store</code> at runtime.  Install the package first::</p> <pre><code>uv add canvod-store\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage--classes","level":3,"title":"Classes","text":"<p><code>AnalysisStorage</code>     Read / write / delete analysis metadata for a single Icechunk store.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage","level":2,"title":"<code>AnalysisStorage</code>","text":"<p>Manage persistent storage of analysis results for dataset+grid pairs.</p> <p>Storage layout inside the Icechunk repository::</p> <pre><code>metadata/{dataset_name}/{grid_name}/\n├── weights/              # (ncells,)\n│   ├── observation_count\n│   ├── solid_angle\n│   └── combined\n├── filter_masks/         # (epoch, sid)\n│   ├── mask_iqr\n│   └── mask_zscore\n├── spatial_masks/        # (ncells,)\n│   ├── mask_north\n│   └── mask_high_elevation\n└── statistics/           # (ncells,)\n    ├── obs_count\n    ├── mean_vod\n    └── std_vod\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage--parameters","level":4,"title":"Parameters","text":"<p>store_path : Path or str     Path to the VOD Icechunk store directory.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>class AnalysisStorage:\n    \"\"\"Manage persistent storage of analysis results for dataset+grid pairs.\n\n    Storage layout inside the Icechunk repository::\n\n        metadata/{dataset_name}/{grid_name}/\n        ├── weights/              # (ncells,)\n        │   ├── observation_count\n        │   ├── solid_angle\n        │   └── combined\n        ├── filter_masks/         # (epoch, sid)\n        │   ├── mask_iqr\n        │   └── mask_zscore\n        ├── spatial_masks/        # (ncells,)\n        │   ├── mask_north\n        │   └── mask_high_elevation\n        └── statistics/           # (ncells,)\n            ├── obs_count\n            ├── mean_vod\n            └── std_vod\n\n    Parameters\n    ----------\n    store_path : Path or str\n        Path to the VOD Icechunk store directory.\n\n    \"\"\"\n\n    def __init__(self, store_path: Path | str) -&gt; None:\n        \"\"\"Initialize the storage manager.\n\n        Parameters\n        ----------\n        store_path : Path | str\n            Path to the VOD Icechunk store directory.\n\n        \"\"\"\n        self.store_path = Path(store_path)\n        self.store: MyIcechunkStore = _get_store(self.store_path)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the developer-facing representation.\n\n        Returns\n        -------\n        str\n            Representation string.\n\n        \"\"\"\n        return f\"AnalysisStorage(store_path={self.store_path})\"\n\n    # ------------------------------------------------------------------\n    # Weights\n    # ------------------------------------------------------------------\n\n    def store_weights(\n        self,\n        dataset_name: str,\n        grid_name: str,\n        weights: dict[str, np.ndarray],\n        weight_params: dict[str, dict] | None = None,\n        overwrite: bool = False,\n    ) -&gt; str:\n        \"\"\"Store per-cell weight arrays.\n\n        Parameters\n        ----------\n        dataset_name : str\n            Dataset identifier (e.g. ``'reference_01_canopy_01'``).\n        grid_name : str\n            Grid identifier (e.g. ``'equal_area_2deg'``).\n        weights : dict\n            ``{name: array}`` – all arrays must have shape ``(ncells,)``.\n        weight_params : dict, optional\n            Parameters used to compute each weight type.\n        overwrite : bool\n            Overwrite existing weights.\n\n        Returns\n        -------\n        str\n            Icechunk snapshot ID.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/weights\"\n        logger.info(\"Storing weights to %s\", group_path)\n\n        # Validate shapes\n        ncells = len(next(iter(weights.values())))\n        for name, arr in weights.items():\n            if arr.shape != (ncells,):\n                raise ValueError(\n                    f\"Weight '{name}' has shape {arr.shape}, expected ({ncells},)\"\n                )\n\n        # Build xarray dataset\n        weight_vars = {\n            name: ([\"cell\"], arr.astype(np.float32)) for name, arr in weights.items()\n        }\n        ds_weights = xr.Dataset(\n            weight_vars, coords={\"cell\": np.arange(ncells, dtype=np.int32)}\n        )\n\n        attrs: dict[str, Any] = {\n            \"created_at\": datetime.now().isoformat(),\n            \"dataset\": dataset_name,\n            \"grid\": grid_name,\n            \"weight_types\": list(weights.keys()),\n            \"ncells\": ncells,\n        }\n        if weight_params:\n            attrs[\"weight_parameters\"] = str(weight_params)\n        ds_weights.attrs.update(attrs)\n\n        # Persist\n        with self.store.writable_session() as session:\n            from icechunk.xarray import to_icechunk\n\n            mode = \"w\" if overwrite else \"w-\"\n            to_icechunk(ds_weights, session, group=group_path, mode=mode)\n            snapshot_id: str = session.commit(\n                f\"Stored weights for {dataset_name}/{grid_name}\"\n            )\n\n        logger.info(\"Weights stored (snapshot: %s)\", snapshot_id[:8])\n        return snapshot_id\n\n    def load_weights(\n        self,\n        dataset_name: str,\n        grid_name: str,\n        weight_type: str | None = None,\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Load stored weight arrays.\n\n        Parameters\n        ----------\n        dataset_name : str\n            Dataset identifier.\n        grid_name : str\n            Grid identifier.\n        weight_type : str, optional\n            Load only this weight.  ``None`` loads all.\n\n        Returns\n        -------\n        dict\n            ``{name: ndarray}`` of loaded weights.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/weights\"\n\n        try:\n            with self.store.readonly_session() as session:\n                ds_weights = xr.open_zarr(\n                    session.store, group=group_path, consolidated=False\n                )\n\n            if weight_type:\n                if weight_type not in ds_weights:\n                    raise ValueError(\n                        f\"Weight '{weight_type}' not found. \"\n                        f\"Available: {list(ds_weights.data_vars)}\"\n                    )\n                return {weight_type: ds_weights[weight_type].values}\n            return {var: ds_weights[var].values for var in ds_weights.data_vars}\n\n        except Exception:\n            logger.error(\"Failed to load weights from %s\", group_path, exc_info=True)\n            raise\n\n    def has_weights(self, dataset_name: str, grid_name: str) -&gt; bool:\n        \"\"\"Return ``True`` if weights exist for the dataset+grid pair.\"\"\"\n        try:\n            self.load_weights(dataset_name, grid_name)\n            return True\n        except Exception:\n            return False\n\n    # ------------------------------------------------------------------\n    # Filter masks (per observation)\n    # ------------------------------------------------------------------\n\n    def store_filter_masks(\n        self,\n        dataset_name: str,\n        grid_name: str,\n        masks: dict[str, xr.DataArray],\n        filter_params: dict[str, dict] | None = None,\n        overwrite: bool = False,\n    ) -&gt; str:\n        \"\"\"Store per-observation filter masks at native resolution.\n\n        Parameters\n        ----------\n        dataset_name : str\n            Dataset identifier.\n        grid_name : str\n            Grid identifier.\n        masks : dict\n            ``{filter_name: DataArray}`` – all must share the same\n            ``(epoch, sid)`` shape.\n        filter_params : dict, optional\n            Parameters used for each filter.\n        overwrite : bool\n            Overwrite existing masks.\n\n        Returns\n        -------\n        str\n            Icechunk snapshot ID.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/filter_masks\"\n        logger.info(\"Storing filter masks to %s\", group_path)\n\n        first_mask = next(iter(masks.values()))\n        shape = first_mask.shape\n\n        mask_vars: dict[str, xr.DataArray] = {}\n        for name, mask_array in masks.items():\n            if not isinstance(mask_array, xr.DataArray):\n                raise TypeError(f\"Mask '{name}' must be xr.DataArray\")\n            if mask_array.shape != shape:\n                raise ValueError(\"All masks must have same shape\")\n            mask_vars[f\"mask_{name}\"] = mask_array.astype(np.int8)\n\n        ds_masks = xr.Dataset(mask_vars)\n\n        attrs: dict[str, Any] = {\n            \"created_at\": datetime.now().isoformat(),\n            \"dataset\": dataset_name,\n            \"grid\": grid_name,\n            \"filter_types\": list(masks.keys()),\n            \"shape\": str(shape),\n            \"coordinate_source\": f\"/{dataset_name}/\",\n        }\n        if filter_params:\n            attrs[\"filter_parameters\"] = str(filter_params)\n        ds_masks.attrs.update(attrs)\n\n        # Rechunk for efficient columnar storage\n        logger.info(\"Rechunking masks for efficient storage\")\n        ds_masks = ds_masks.chunk({\"epoch\": 10000, \"sid\": -1})\n\n        with self.store.writable_session() as session:\n            import dask\n            from icechunk.xarray import to_icechunk\n\n            logger.info(\"Writing masks (this may take a few minutes)\")\n            with dask.config.set(scheduler=\"threads\", num_workers=4):\n                mode = \"w\" if overwrite else \"w-\"\n                to_icechunk(ds_masks, session, group=group_path, mode=mode)\n\n            logger.info(\"Committing\")\n            snapshot_id = session.commit(\n                f\"Stored filter masks for {dataset_name}/{grid_name}\"\n            )\n\n        logger.info(\"Filter masks stored (snapshot: %s)\", snapshot_id[:8])\n        return snapshot_id\n\n    def load_filter_mask(\n        self,\n        dataset_name: str,\n        grid_name: str,\n        filter_type: str,\n        attach_coords: bool = True,\n    ) -&gt; xr.DataArray:\n        \"\"\"Load a single filter mask.\n\n        Parameters\n        ----------\n        dataset_name : str\n            Dataset identifier.\n        grid_name : str\n            Grid identifier.\n        filter_type : str\n            Filter name (e.g. ``'iqr'``, ``'zscore'``).\n        attach_coords : bool\n            Re-attach ``epoch`` / ``sid`` coordinates from the source\n            dataset group.  Set ``False`` for faster loading when\n            coordinates are not needed.\n\n        Returns\n        -------\n        xr.DataArray\n            Boolean mask with shape ``(epoch, sid)``.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/filter_masks\"\n\n        try:\n            with self.store.readonly_session() as session:\n                ds_masks = xr.open_zarr(\n                    session.store, group=group_path, consolidated=False\n                )\n\n            mask_var = f\"mask_{filter_type}\"\n            if mask_var not in ds_masks:\n                available = [v.replace(\"mask_\", \"\") for v in ds_masks.data_vars]\n                raise ValueError(\n                    f\"Filter mask '{filter_type}' not found. Available: {available}\"\n                )\n\n            mask = ds_masks[mask_var].astype(bool)\n\n            if attach_coords:\n                coord_source = ds_masks.attrs.get(\n                    \"coordinate_source\", f\"/{dataset_name}/\"\n                )\n                with self.store.readonly_session() as session:\n                    ds_source = xr.open_zarr(\n                        session.store,\n                        group=coord_source.strip(\"/\"),\n                        consolidated=False,\n                    )\n\n                mask = mask.assign_coords(\n                    {\"epoch\": ds_source[\"epoch\"], \"sid\": ds_source[\"sid\"]}\n                )\n                for coord in [\n                    \"band\",\n                    \"code\",\n                    \"sv\",\n                    \"system\",\n                    \"freq_min\",\n                    \"freq_max\",\n                    \"freq_center\",\n                ]:\n                    if coord in ds_source.coords:\n                        mask = mask.assign_coords({coord: ds_source[coord]})\n\n            return mask\n\n        except Exception:\n            logger.error(\"Failed to load filter mask\", exc_info=True)\n            raise\n\n    def load_all_filter_masks(\n        self, dataset_name: str, grid_name: str\n    ) -&gt; dict[str, xr.DataArray]:\n        \"\"\"Load all stored filter masks.\n\n        Returns\n        -------\n        dict\n            ``{filter_name: DataArray}`` – boolean masks.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/filter_masks\"\n\n        try:\n            with self.store.readonly_session() as session:\n                ds_masks = xr.open_zarr(\n                    session.store, group=group_path, consolidated=False\n                )\n            return {\n                var.replace(\"mask_\", \"\"): ds_masks[var].astype(bool)\n                for var in ds_masks.data_vars\n            }\n        except Exception:\n            logger.error(\n                \"Failed to load filter masks from %s\", group_path, exc_info=True\n            )\n            raise\n\n    def has_filter_masks(self, dataset_name: str, grid_name: str) -&gt; bool:\n        \"\"\"Return ``True`` if filter masks exist for the dataset+grid pair.\"\"\"\n        try:\n            self.load_all_filter_masks(dataset_name, grid_name)\n            return True\n        except Exception:\n            return False\n\n    # ------------------------------------------------------------------\n    # Spatial masks (per cell)\n    # ------------------------------------------------------------------\n\n    def store_spatial_masks(\n        self,\n        dataset_name: str,\n        grid_name: str,\n        masks: dict[str, np.ndarray],\n        mask_descriptions: dict[str, str] | None = None,\n        overwrite: bool = False,\n    ) -&gt; str:\n        \"\"\"Store per-cell geometric selection masks.\n\n        Parameters\n        ----------\n        dataset_name : str\n            Dataset identifier.\n        grid_name : str\n            Grid identifier.\n        masks : dict\n            ``{name: bool_array}`` – all arrays must have shape ``(ncells,)``\n            and boolean dtype.\n        mask_descriptions : dict, optional\n            Human-readable description for each mask (stored as variable attrs).\n        overwrite : bool\n            Overwrite existing masks.\n\n        Returns\n        -------\n        str\n            Icechunk snapshot ID.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/spatial_masks\"\n        logger.info(\"Storing spatial masks to %s\", group_path)\n\n        ncells = len(next(iter(masks.values())))\n        for name, arr in masks.items():\n            if arr.shape != (ncells,):\n                raise ValueError(\n                    f\"Mask '{name}' has shape {arr.shape}, expected ({ncells},)\"\n                )\n            if arr.dtype != bool:\n                raise ValueError(\n                    f\"Mask '{name}' must be boolean dtype, got {arr.dtype}\"\n                )\n\n        mask_vars = {\n            f\"mask_{name}\": ([\"cell\"], arr.astype(np.int8))\n            for name, arr in masks.items()\n        }\n        ds_masks = xr.Dataset(\n            mask_vars, coords={\"cell\": np.arange(ncells, dtype=np.int32)}\n        )\n\n        # Per-variable metadata\n        for name, arr in masks.items():\n            n_sel = int(arr.sum())\n            ds_masks[f\"mask_{name}\"].attrs[\"n_cells_selected\"] = n_sel\n            ds_masks[f\"mask_{name}\"].attrs[\"fraction_selected\"] = float(n_sel / ncells)\n        if mask_descriptions:\n            for name, desc in mask_descriptions.items():\n                if name in masks:\n                    ds_masks[f\"mask_{name}\"].attrs[\"description\"] = desc\n\n        ds_masks.attrs.update(\n            {\n                \"created_at\": datetime.now().isoformat(),\n                \"dataset\": dataset_name,\n                \"grid\": grid_name,\n                \"mask_types\": list(masks.keys()),\n                \"ncells\": ncells,\n            }\n        )\n\n        with self.store.writable_session() as session:\n            from icechunk.xarray import to_icechunk\n\n            mode = \"w\" if overwrite else \"w-\"\n            to_icechunk(ds_masks, session, group=group_path, mode=mode)\n            snapshot_id = session.commit(\n                f\"Stored spatial masks for {dataset_name}/{grid_name}\"\n            )\n\n        logger.info(\"Spatial masks stored (snapshot: %s)\", snapshot_id[:8])\n        return snapshot_id\n\n    def load_spatial_mask(\n        self, dataset_name: str, grid_name: str, mask_name: str\n    ) -&gt; np.ndarray:\n        \"\"\"Load a single spatial mask.\n\n        Parameters\n        ----------\n        dataset_name : str\n            Dataset identifier.\n        grid_name : str\n            Grid identifier.\n        mask_name : str\n            Mask name (e.g. ``'north'``, ``'high_elevation'``).\n\n        Returns\n        -------\n        np.ndarray\n            Boolean array with shape ``(ncells,)``.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/spatial_masks\"\n\n        try:\n            with self.store.readonly_session() as session:\n                ds_masks = xr.open_zarr(\n                    session.store, group=group_path, consolidated=False\n                )\n\n            mask_var = f\"mask_{mask_name}\"\n            if mask_var not in ds_masks:\n                available = [v.replace(\"mask_\", \"\") for v in ds_masks.data_vars]\n                raise ValueError(\n                    f\"Spatial mask '{mask_name}' not found. Available: {available}\"\n                )\n            return ds_masks[mask_var].values.astype(bool)\n\n        except Exception:\n            logger.error(\n                \"Failed to load spatial mask from %s\", group_path, exc_info=True\n            )\n            raise\n\n    def load_all_spatial_masks(\n        self, dataset_name: str, grid_name: str\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Load all spatial masks.\n\n        Returns\n        -------\n        dict\n            ``{name: bool_ndarray}``.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/spatial_masks\"\n\n        try:\n            with self.store.readonly_session() as session:\n                ds_masks = xr.open_zarr(\n                    session.store, group=group_path, consolidated=False\n                )\n            return {\n                var.replace(\"mask_\", \"\"): ds_masks[var].values.astype(bool)\n                for var in ds_masks.data_vars\n            }\n        except Exception:\n            logger.error(\n                \"Failed to load spatial masks from %s\", group_path, exc_info=True\n            )\n            raise\n\n    def has_spatial_masks(self, dataset_name: str, grid_name: str) -&gt; bool:\n        \"\"\"Return ``True`` if spatial masks exist for the dataset+grid pair.\"\"\"\n        try:\n            self.load_all_spatial_masks(dataset_name, grid_name)\n            return True\n        except Exception:\n            return False\n\n    # ------------------------------------------------------------------\n    # Statistics\n    # ------------------------------------------------------------------\n\n    def store_statistics(\n        self,\n        dataset_name: str,\n        grid_name: str,\n        stats: dict[str, np.ndarray],\n        overwrite: bool = False,\n    ) -&gt; str:\n        \"\"\"Store pre-computed per-cell statistics.\n\n        Parameters\n        ----------\n        dataset_name : str\n            Dataset identifier.\n        grid_name : str\n            Grid identifier.\n        stats : dict\n            ``{name: array}`` – all arrays must have shape ``(ncells,)``.\n            Variables whose name ends with ``'_count'`` or equals\n            ``'obs_count'`` are stored as ``int64``; everything else as\n            ``float32``.\n        overwrite : bool\n            Overwrite existing statistics.\n\n        Returns\n        -------\n        str\n            Icechunk snapshot ID.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/statistics\"\n        logger.info(\"Storing statistics to %s\", group_path)\n\n        ncells = len(next(iter(stats.values())))\n        for name, arr in stats.items():\n            if arr.shape != (ncells,):\n                raise ValueError(\n                    f\"Statistic '{name}' has shape {arr.shape}, expected ({ncells},)\"\n                )\n\n        stat_vars = {}\n        for name, arr in stats.items():\n            dtype = (\n                np.int64\n                if (name.endswith(\"_count\") or name == \"obs_count\")\n                else np.float32\n            )\n            stat_vars[name] = ([\"cell\"], arr.astype(dtype))\n\n        ds_stats = xr.Dataset(\n            stat_vars, coords={\"cell\": np.arange(ncells, dtype=np.int32)}\n        )\n\n        # Summary statistics per variable (stored as variable attrs)\n        for name, arr in stats.items():\n            valid = np.isfinite(arr)\n            if np.any(valid):\n                ds_stats[name].attrs.update(\n                    {\n                        \"min\": float(np.nanmin(arr)),\n                        \"max\": float(np.nanmax(arr)),\n                        \"mean\": float(np.nanmean(arr)),\n                        \"n_valid\": int(np.sum(valid)),\n                    }\n                )\n\n        ds_stats.attrs.update(\n            {\n                \"created_at\": datetime.now().isoformat(),\n                \"dataset\": dataset_name,\n                \"grid\": grid_name,\n                \"statistics\": list(stats.keys()),\n                \"ncells\": ncells,\n            }\n        )\n\n        with self.store.writable_session() as session:\n            from icechunk.xarray import to_icechunk\n\n            mode = \"w\" if overwrite else \"w-\"\n            to_icechunk(ds_stats, session, group=group_path, mode=mode)\n            snapshot_id = session.commit(\n                f\"Stored statistics for {dataset_name}/{grid_name}\"\n            )\n\n        logger.info(\"Statistics stored (snapshot: %s)\", snapshot_id[:8])\n        return snapshot_id\n\n    def load_statistics(\n        self,\n        dataset_name: str,\n        grid_name: str,\n        stat_name: str | None = None,\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Load pre-computed per-cell statistics.\n\n        Parameters\n        ----------\n        dataset_name : str\n            Dataset identifier.\n        grid_name : str\n            Grid identifier.\n        stat_name : str, optional\n            Load only this statistic.  ``None`` loads all.\n\n        Returns\n        -------\n        dict\n            ``{name: ndarray}``.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/statistics\"\n\n        try:\n            with self.store.readonly_session() as session:\n                ds_stats = xr.open_zarr(\n                    session.store, group=group_path, consolidated=False\n                )\n\n            if stat_name:\n                if stat_name not in ds_stats:\n                    raise ValueError(\n                        f\"Statistic '{stat_name}' not found. \"\n                        f\"Available: {list(ds_stats.data_vars)}\"\n                    )\n                return {stat_name: ds_stats[stat_name].values}\n            return {var: ds_stats[var].values for var in ds_stats.data_vars}\n\n        except Exception:\n            logger.error(\"Failed to load statistics from %s\", group_path, exc_info=True)\n            raise\n\n    def has_statistics(self, dataset_name: str, grid_name: str) -&gt; bool:\n        \"\"\"Return ``True`` if statistics exist for the dataset+grid pair.\"\"\"\n        try:\n            self.load_statistics(dataset_name, grid_name)\n            return True\n        except Exception:\n            return False\n\n    # ------------------------------------------------------------------\n    # Utilities\n    # ------------------------------------------------------------------\n\n    def list_available_metadata(\n        self, dataset_name: str, grid_name: str\n    ) -&gt; dict[str, bool]:\n        \"\"\"Check which metadata categories are stored.\n\n        Returns\n        -------\n        dict\n            ``{category: bool}`` for weights, filter_masks, spatial_masks,\n            statistics.\n\n        \"\"\"\n        return {\n            \"weights\": self.has_weights(dataset_name, grid_name),\n            \"filter_masks\": self.has_filter_masks(dataset_name, grid_name),\n            \"spatial_masks\": self.has_spatial_masks(dataset_name, grid_name),\n            \"statistics\": self.has_statistics(dataset_name, grid_name),\n        }\n\n    def get_metadata_summary(self, dataset_name: str, grid_name: str) -&gt; dict[str, Any]:\n        \"\"\"Detailed summary of all stored metadata for a dataset+grid pair.\n\n        Returns\n        -------\n        dict\n            Nested summary with availability flags and per-category details.\n\n        \"\"\"\n        summary: dict[str, Any] = {\n            \"dataset\": dataset_name,\n            \"grid\": grid_name,\n            \"available\": self.list_available_metadata(dataset_name, grid_name),\n        }\n\n        if summary[\"available\"][\"weights\"]:\n            weights = self.load_weights(dataset_name, grid_name)\n            summary[\"weights\"] = {\n                \"types\": list(weights.keys()),\n                \"ncells\": len(next(iter(weights.values()))),\n            }\n\n        if summary[\"available\"][\"filter_masks\"]:\n            masks = self.load_all_filter_masks(dataset_name, grid_name)\n            summary[\"filter_masks\"] = {\n                \"types\": list(masks.keys()),\n                \"shape\": next(iter(masks.values())).shape,\n            }\n\n        if summary[\"available\"][\"spatial_masks\"]:\n            masks = self.load_all_spatial_masks(dataset_name, grid_name)\n            summary[\"spatial_masks\"] = {\n                \"types\": list(masks.keys()),\n                \"ncells\": len(next(iter(masks.values()))),\n            }\n\n        if summary[\"available\"][\"statistics\"]:\n            stats = self.load_statistics(dataset_name, grid_name)\n            summary[\"statistics\"] = {\n                \"types\": list(stats.keys()),\n                \"ncells\": len(next(iter(stats.values()))),\n            }\n\n        return summary\n\n    # ------------------------------------------------------------------\n    # Deletion\n    # ------------------------------------------------------------------\n\n    def _delete_group(self, group_path: str, label: str) -&gt; str:\n        \"\"\"Generic group deletion helper.\n\n        Parameters\n        ----------\n        group_path : str\n            Zarr group path inside the store.\n        label : str\n            Human-readable label for log / commit messages.\n\n        Returns\n        -------\n        str\n            Snapshot ID.\n\n        Raises\n        ------\n        ValueError\n            If the group does not exist.\n\n        \"\"\"\n        logger.info(\"Deleting %s at %s\", label, group_path)\n\n        with self.store.writable_session() as session:\n            import zarr\n\n            store = zarr.open(session.store, mode=\"r+\")\n            if group_path in store:\n                del store[group_path]\n                snapshot_id = session.commit(f\"Deleted {label}\")\n                logger.info(\"%s deleted (snapshot: %s)\", label, snapshot_id[:8])\n                return snapshot_id\n            raise ValueError(f\"Group {group_path} does not exist\")\n\n    def delete_weights(self, dataset_name: str, grid_name: str) -&gt; str:\n        \"\"\"Delete all weights for a dataset+grid pair.\"\"\"\n        return self._delete_group(\n            f\"metadata/{dataset_name}/{grid_name}/weights\",\n            f\"weights for {dataset_name}/{grid_name}\",\n        )\n\n    def delete_filter_masks(self, dataset_name: str, grid_name: str) -&gt; str:\n        \"\"\"Delete all filter masks for a dataset+grid pair.\"\"\"\n        return self._delete_group(\n            f\"metadata/{dataset_name}/{grid_name}/filter_masks\",\n            f\"filter masks for {dataset_name}/{grid_name}\",\n        )\n\n    def delete_spatial_masks(self, dataset_name: str, grid_name: str) -&gt; str:\n        \"\"\"Delete all spatial masks for a dataset+grid pair.\"\"\"\n        return self._delete_group(\n            f\"metadata/{dataset_name}/{grid_name}/spatial_masks\",\n            f\"spatial masks for {dataset_name}/{grid_name}\",\n        )\n\n    def delete_statistics(self, dataset_name: str, grid_name: str) -&gt; str:\n        \"\"\"Delete all statistics for a dataset+grid pair.\"\"\"\n        return self._delete_group(\n            f\"metadata/{dataset_name}/{grid_name}/statistics\",\n            f\"statistics for {dataset_name}/{grid_name}\",\n        )\n\n    def delete_all_metadata(self, dataset_name: str, grid_name: str) -&gt; str:\n        \"\"\"Delete the entire metadata subtree for a dataset+grid pair.\"\"\"\n        return self._delete_group(\n            f\"metadata/{dataset_name}/{grid_name}\",\n            f\"all metadata for {dataset_name}/{grid_name}\",\n        )\n\n    def delete_specific_weight(\n        self, dataset_name: str, grid_name: str, weight_name: str\n    ) -&gt; str:\n        \"\"\"Delete a single weight variable from an existing weights group.\n\n        Parameters\n        ----------\n        dataset_name : str\n            Dataset identifier.\n        grid_name : str\n            Grid identifier.\n        weight_name : str\n            Weight variable name (e.g. ``'observation_count'``).\n\n        Returns\n        -------\n        str\n            Snapshot ID.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/weights\"\n        logger.info(\"Deleting weight '%s' from %s\", weight_name, group_path)\n\n        with self.store.writable_session() as session:\n            import zarr\n\n            group = zarr.open(session.store, path=group_path, mode=\"r+\")\n            if weight_name in group:\n                del group[weight_name]\n                snapshot_id = session.commit(\n                    f\"Deleted weight '{weight_name}' for {dataset_name}/{grid_name}\"\n                )\n                logger.info(\n                    \"Weight '%s' deleted (snapshot: %s)\",\n                    weight_name,\n                    snapshot_id[:8],\n                )\n                return snapshot_id\n            raise ValueError(f\"Weight '{weight_name}' does not exist in {group_path}\")\n\n    def delete_specific_filter_mask(\n        self, dataset_name: str, grid_name: str, filter_type: str\n    ) -&gt; str:\n        \"\"\"Delete a single filter mask variable from an existing filter_masks group.\n\n        Parameters\n        ----------\n        dataset_name : str\n            Dataset identifier.\n        grid_name : str\n            Grid identifier.\n        filter_type : str\n            Filter type name (e.g. ``'iqr'``).\n\n        Returns\n        -------\n        str\n            Snapshot ID.\n\n        \"\"\"\n        group_path = f\"metadata/{dataset_name}/{grid_name}/filter_masks\"\n        mask_var = f\"mask_{filter_type}\"\n        logger.info(\"Deleting filter mask '%s' from %s\", filter_type, group_path)\n\n        with self.store.writable_session() as session:\n            import zarr\n\n            group = zarr.open(session.store, path=group_path, mode=\"r+\")\n            if mask_var in group:\n                del group[mask_var]\n                commit_msg = (\n                    f\"Deleted filter mask '{filter_type}' for \"\n                    f\"{dataset_name}/{grid_name}\"\n                )\n                snapshot_id = session.commit(commit_msg)\n                logger.info(\n                    \"Filter mask '%s' deleted (snapshot: %s)\",\n                    filter_type,\n                    snapshot_id[:8],\n                )\n                return snapshot_id\n            raise ValueError(\n                f\"Filter mask '{filter_type}' does not exist in {group_path}\"\n            )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.__init__","level":3,"title":"<code>__init__(store_path)</code>","text":"<p>Initialize the storage manager.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.__init__--parameters","level":5,"title":"Parameters","text":"<p>store_path : Path | str     Path to the VOD Icechunk store directory.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def __init__(self, store_path: Path | str) -&gt; None:\n    \"\"\"Initialize the storage manager.\n\n    Parameters\n    ----------\n    store_path : Path | str\n        Path to the VOD Icechunk store directory.\n\n    \"\"\"\n    self.store_path = Path(store_path)\n    self.store: MyIcechunkStore = _get_store(self.store_path)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return the developer-facing representation.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.__repr__--returns","level":5,"title":"Returns","text":"<p>str     Representation string.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the developer-facing representation.\n\n    Returns\n    -------\n    str\n        Representation string.\n\n    \"\"\"\n    return f\"AnalysisStorage(store_path={self.store_path})\"\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_weights","level":3,"title":"<code>store_weights(dataset_name, grid_name, weights, weight_params=None, overwrite=False)</code>","text":"<p>Store per-cell weight arrays.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_weights--parameters","level":5,"title":"Parameters","text":"<p>dataset_name : str     Dataset identifier (e.g. <code>'reference_01_canopy_01'</code>). grid_name : str     Grid identifier (e.g. <code>'equal_area_2deg'</code>). weights : dict     <code>{name: array}</code> – all arrays must have shape <code>(ncells,)</code>. weight_params : dict, optional     Parameters used to compute each weight type. overwrite : bool     Overwrite existing weights.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_weights--returns","level":5,"title":"Returns","text":"<p>str     Icechunk snapshot ID.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def store_weights(\n    self,\n    dataset_name: str,\n    grid_name: str,\n    weights: dict[str, np.ndarray],\n    weight_params: dict[str, dict] | None = None,\n    overwrite: bool = False,\n) -&gt; str:\n    \"\"\"Store per-cell weight arrays.\n\n    Parameters\n    ----------\n    dataset_name : str\n        Dataset identifier (e.g. ``'reference_01_canopy_01'``).\n    grid_name : str\n        Grid identifier (e.g. ``'equal_area_2deg'``).\n    weights : dict\n        ``{name: array}`` – all arrays must have shape ``(ncells,)``.\n    weight_params : dict, optional\n        Parameters used to compute each weight type.\n    overwrite : bool\n        Overwrite existing weights.\n\n    Returns\n    -------\n    str\n        Icechunk snapshot ID.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/weights\"\n    logger.info(\"Storing weights to %s\", group_path)\n\n    # Validate shapes\n    ncells = len(next(iter(weights.values())))\n    for name, arr in weights.items():\n        if arr.shape != (ncells,):\n            raise ValueError(\n                f\"Weight '{name}' has shape {arr.shape}, expected ({ncells},)\"\n            )\n\n    # Build xarray dataset\n    weight_vars = {\n        name: ([\"cell\"], arr.astype(np.float32)) for name, arr in weights.items()\n    }\n    ds_weights = xr.Dataset(\n        weight_vars, coords={\"cell\": np.arange(ncells, dtype=np.int32)}\n    )\n\n    attrs: dict[str, Any] = {\n        \"created_at\": datetime.now().isoformat(),\n        \"dataset\": dataset_name,\n        \"grid\": grid_name,\n        \"weight_types\": list(weights.keys()),\n        \"ncells\": ncells,\n    }\n    if weight_params:\n        attrs[\"weight_parameters\"] = str(weight_params)\n    ds_weights.attrs.update(attrs)\n\n    # Persist\n    with self.store.writable_session() as session:\n        from icechunk.xarray import to_icechunk\n\n        mode = \"w\" if overwrite else \"w-\"\n        to_icechunk(ds_weights, session, group=group_path, mode=mode)\n        snapshot_id: str = session.commit(\n            f\"Stored weights for {dataset_name}/{grid_name}\"\n        )\n\n    logger.info(\"Weights stored (snapshot: %s)\", snapshot_id[:8])\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_weights","level":3,"title":"<code>load_weights(dataset_name, grid_name, weight_type=None)</code>","text":"<p>Load stored weight arrays.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_weights--parameters","level":5,"title":"Parameters","text":"<p>dataset_name : str     Dataset identifier. grid_name : str     Grid identifier. weight_type : str, optional     Load only this weight.  <code>None</code> loads all.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_weights--returns","level":5,"title":"Returns","text":"<p>dict     <code>{name: ndarray}</code> of loaded weights.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def load_weights(\n    self,\n    dataset_name: str,\n    grid_name: str,\n    weight_type: str | None = None,\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Load stored weight arrays.\n\n    Parameters\n    ----------\n    dataset_name : str\n        Dataset identifier.\n    grid_name : str\n        Grid identifier.\n    weight_type : str, optional\n        Load only this weight.  ``None`` loads all.\n\n    Returns\n    -------\n    dict\n        ``{name: ndarray}`` of loaded weights.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/weights\"\n\n    try:\n        with self.store.readonly_session() as session:\n            ds_weights = xr.open_zarr(\n                session.store, group=group_path, consolidated=False\n            )\n\n        if weight_type:\n            if weight_type not in ds_weights:\n                raise ValueError(\n                    f\"Weight '{weight_type}' not found. \"\n                    f\"Available: {list(ds_weights.data_vars)}\"\n                )\n            return {weight_type: ds_weights[weight_type].values}\n        return {var: ds_weights[var].values for var in ds_weights.data_vars}\n\n    except Exception:\n        logger.error(\"Failed to load weights from %s\", group_path, exc_info=True)\n        raise\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.has_weights","level":3,"title":"<code>has_weights(dataset_name, grid_name)</code>","text":"<p>Return <code>True</code> if weights exist for the dataset+grid pair.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def has_weights(self, dataset_name: str, grid_name: str) -&gt; bool:\n    \"\"\"Return ``True`` if weights exist for the dataset+grid pair.\"\"\"\n    try:\n        self.load_weights(dataset_name, grid_name)\n        return True\n    except Exception:\n        return False\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_filter_masks","level":3,"title":"<code>store_filter_masks(dataset_name, grid_name, masks, filter_params=None, overwrite=False)</code>","text":"<p>Store per-observation filter masks at native resolution.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_filter_masks--parameters","level":5,"title":"Parameters","text":"<p>dataset_name : str     Dataset identifier. grid_name : str     Grid identifier. masks : dict     <code>{filter_name: DataArray}</code> – all must share the same     <code>(epoch, sid)</code> shape. filter_params : dict, optional     Parameters used for each filter. overwrite : bool     Overwrite existing masks.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_filter_masks--returns","level":5,"title":"Returns","text":"<p>str     Icechunk snapshot ID.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def store_filter_masks(\n    self,\n    dataset_name: str,\n    grid_name: str,\n    masks: dict[str, xr.DataArray],\n    filter_params: dict[str, dict] | None = None,\n    overwrite: bool = False,\n) -&gt; str:\n    \"\"\"Store per-observation filter masks at native resolution.\n\n    Parameters\n    ----------\n    dataset_name : str\n        Dataset identifier.\n    grid_name : str\n        Grid identifier.\n    masks : dict\n        ``{filter_name: DataArray}`` – all must share the same\n        ``(epoch, sid)`` shape.\n    filter_params : dict, optional\n        Parameters used for each filter.\n    overwrite : bool\n        Overwrite existing masks.\n\n    Returns\n    -------\n    str\n        Icechunk snapshot ID.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/filter_masks\"\n    logger.info(\"Storing filter masks to %s\", group_path)\n\n    first_mask = next(iter(masks.values()))\n    shape = first_mask.shape\n\n    mask_vars: dict[str, xr.DataArray] = {}\n    for name, mask_array in masks.items():\n        if not isinstance(mask_array, xr.DataArray):\n            raise TypeError(f\"Mask '{name}' must be xr.DataArray\")\n        if mask_array.shape != shape:\n            raise ValueError(\"All masks must have same shape\")\n        mask_vars[f\"mask_{name}\"] = mask_array.astype(np.int8)\n\n    ds_masks = xr.Dataset(mask_vars)\n\n    attrs: dict[str, Any] = {\n        \"created_at\": datetime.now().isoformat(),\n        \"dataset\": dataset_name,\n        \"grid\": grid_name,\n        \"filter_types\": list(masks.keys()),\n        \"shape\": str(shape),\n        \"coordinate_source\": f\"/{dataset_name}/\",\n    }\n    if filter_params:\n        attrs[\"filter_parameters\"] = str(filter_params)\n    ds_masks.attrs.update(attrs)\n\n    # Rechunk for efficient columnar storage\n    logger.info(\"Rechunking masks for efficient storage\")\n    ds_masks = ds_masks.chunk({\"epoch\": 10000, \"sid\": -1})\n\n    with self.store.writable_session() as session:\n        import dask\n        from icechunk.xarray import to_icechunk\n\n        logger.info(\"Writing masks (this may take a few minutes)\")\n        with dask.config.set(scheduler=\"threads\", num_workers=4):\n            mode = \"w\" if overwrite else \"w-\"\n            to_icechunk(ds_masks, session, group=group_path, mode=mode)\n\n        logger.info(\"Committing\")\n        snapshot_id = session.commit(\n            f\"Stored filter masks for {dataset_name}/{grid_name}\"\n        )\n\n    logger.info(\"Filter masks stored (snapshot: %s)\", snapshot_id[:8])\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_filter_mask","level":3,"title":"<code>load_filter_mask(dataset_name, grid_name, filter_type, attach_coords=True)</code>","text":"<p>Load a single filter mask.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_filter_mask--parameters","level":5,"title":"Parameters","text":"<p>dataset_name : str     Dataset identifier. grid_name : str     Grid identifier. filter_type : str     Filter name (e.g. <code>'iqr'</code>, <code>'zscore'</code>). attach_coords : bool     Re-attach <code>epoch</code> / <code>sid</code> coordinates from the source     dataset group.  Set <code>False</code> for faster loading when     coordinates are not needed.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_filter_mask--returns","level":5,"title":"Returns","text":"<p>xr.DataArray     Boolean mask with shape <code>(epoch, sid)</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def load_filter_mask(\n    self,\n    dataset_name: str,\n    grid_name: str,\n    filter_type: str,\n    attach_coords: bool = True,\n) -&gt; xr.DataArray:\n    \"\"\"Load a single filter mask.\n\n    Parameters\n    ----------\n    dataset_name : str\n        Dataset identifier.\n    grid_name : str\n        Grid identifier.\n    filter_type : str\n        Filter name (e.g. ``'iqr'``, ``'zscore'``).\n    attach_coords : bool\n        Re-attach ``epoch`` / ``sid`` coordinates from the source\n        dataset group.  Set ``False`` for faster loading when\n        coordinates are not needed.\n\n    Returns\n    -------\n    xr.DataArray\n        Boolean mask with shape ``(epoch, sid)``.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/filter_masks\"\n\n    try:\n        with self.store.readonly_session() as session:\n            ds_masks = xr.open_zarr(\n                session.store, group=group_path, consolidated=False\n            )\n\n        mask_var = f\"mask_{filter_type}\"\n        if mask_var not in ds_masks:\n            available = [v.replace(\"mask_\", \"\") for v in ds_masks.data_vars]\n            raise ValueError(\n                f\"Filter mask '{filter_type}' not found. Available: {available}\"\n            )\n\n        mask = ds_masks[mask_var].astype(bool)\n\n        if attach_coords:\n            coord_source = ds_masks.attrs.get(\n                \"coordinate_source\", f\"/{dataset_name}/\"\n            )\n            with self.store.readonly_session() as session:\n                ds_source = xr.open_zarr(\n                    session.store,\n                    group=coord_source.strip(\"/\"),\n                    consolidated=False,\n                )\n\n            mask = mask.assign_coords(\n                {\"epoch\": ds_source[\"epoch\"], \"sid\": ds_source[\"sid\"]}\n            )\n            for coord in [\n                \"band\",\n                \"code\",\n                \"sv\",\n                \"system\",\n                \"freq_min\",\n                \"freq_max\",\n                \"freq_center\",\n            ]:\n                if coord in ds_source.coords:\n                    mask = mask.assign_coords({coord: ds_source[coord]})\n\n        return mask\n\n    except Exception:\n        logger.error(\"Failed to load filter mask\", exc_info=True)\n        raise\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_all_filter_masks","level":3,"title":"<code>load_all_filter_masks(dataset_name, grid_name)</code>","text":"<p>Load all stored filter masks.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_all_filter_masks--returns","level":5,"title":"Returns","text":"<p>dict     <code>{filter_name: DataArray}</code> – boolean masks.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def load_all_filter_masks(\n    self, dataset_name: str, grid_name: str\n) -&gt; dict[str, xr.DataArray]:\n    \"\"\"Load all stored filter masks.\n\n    Returns\n    -------\n    dict\n        ``{filter_name: DataArray}`` – boolean masks.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/filter_masks\"\n\n    try:\n        with self.store.readonly_session() as session:\n            ds_masks = xr.open_zarr(\n                session.store, group=group_path, consolidated=False\n            )\n        return {\n            var.replace(\"mask_\", \"\"): ds_masks[var].astype(bool)\n            for var in ds_masks.data_vars\n        }\n    except Exception:\n        logger.error(\n            \"Failed to load filter masks from %s\", group_path, exc_info=True\n        )\n        raise\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.has_filter_masks","level":3,"title":"<code>has_filter_masks(dataset_name, grid_name)</code>","text":"<p>Return <code>True</code> if filter masks exist for the dataset+grid pair.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def has_filter_masks(self, dataset_name: str, grid_name: str) -&gt; bool:\n    \"\"\"Return ``True`` if filter masks exist for the dataset+grid pair.\"\"\"\n    try:\n        self.load_all_filter_masks(dataset_name, grid_name)\n        return True\n    except Exception:\n        return False\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_spatial_masks","level":3,"title":"<code>store_spatial_masks(dataset_name, grid_name, masks, mask_descriptions=None, overwrite=False)</code>","text":"<p>Store per-cell geometric selection masks.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_spatial_masks--parameters","level":5,"title":"Parameters","text":"<p>dataset_name : str     Dataset identifier. grid_name : str     Grid identifier. masks : dict     <code>{name: bool_array}</code> – all arrays must have shape <code>(ncells,)</code>     and boolean dtype. mask_descriptions : dict, optional     Human-readable description for each mask (stored as variable attrs). overwrite : bool     Overwrite existing masks.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_spatial_masks--returns","level":5,"title":"Returns","text":"<p>str     Icechunk snapshot ID.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def store_spatial_masks(\n    self,\n    dataset_name: str,\n    grid_name: str,\n    masks: dict[str, np.ndarray],\n    mask_descriptions: dict[str, str] | None = None,\n    overwrite: bool = False,\n) -&gt; str:\n    \"\"\"Store per-cell geometric selection masks.\n\n    Parameters\n    ----------\n    dataset_name : str\n        Dataset identifier.\n    grid_name : str\n        Grid identifier.\n    masks : dict\n        ``{name: bool_array}`` – all arrays must have shape ``(ncells,)``\n        and boolean dtype.\n    mask_descriptions : dict, optional\n        Human-readable description for each mask (stored as variable attrs).\n    overwrite : bool\n        Overwrite existing masks.\n\n    Returns\n    -------\n    str\n        Icechunk snapshot ID.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/spatial_masks\"\n    logger.info(\"Storing spatial masks to %s\", group_path)\n\n    ncells = len(next(iter(masks.values())))\n    for name, arr in masks.items():\n        if arr.shape != (ncells,):\n            raise ValueError(\n                f\"Mask '{name}' has shape {arr.shape}, expected ({ncells},)\"\n            )\n        if arr.dtype != bool:\n            raise ValueError(\n                f\"Mask '{name}' must be boolean dtype, got {arr.dtype}\"\n            )\n\n    mask_vars = {\n        f\"mask_{name}\": ([\"cell\"], arr.astype(np.int8))\n        for name, arr in masks.items()\n    }\n    ds_masks = xr.Dataset(\n        mask_vars, coords={\"cell\": np.arange(ncells, dtype=np.int32)}\n    )\n\n    # Per-variable metadata\n    for name, arr in masks.items():\n        n_sel = int(arr.sum())\n        ds_masks[f\"mask_{name}\"].attrs[\"n_cells_selected\"] = n_sel\n        ds_masks[f\"mask_{name}\"].attrs[\"fraction_selected\"] = float(n_sel / ncells)\n    if mask_descriptions:\n        for name, desc in mask_descriptions.items():\n            if name in masks:\n                ds_masks[f\"mask_{name}\"].attrs[\"description\"] = desc\n\n    ds_masks.attrs.update(\n        {\n            \"created_at\": datetime.now().isoformat(),\n            \"dataset\": dataset_name,\n            \"grid\": grid_name,\n            \"mask_types\": list(masks.keys()),\n            \"ncells\": ncells,\n        }\n    )\n\n    with self.store.writable_session() as session:\n        from icechunk.xarray import to_icechunk\n\n        mode = \"w\" if overwrite else \"w-\"\n        to_icechunk(ds_masks, session, group=group_path, mode=mode)\n        snapshot_id = session.commit(\n            f\"Stored spatial masks for {dataset_name}/{grid_name}\"\n        )\n\n    logger.info(\"Spatial masks stored (snapshot: %s)\", snapshot_id[:8])\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_spatial_mask","level":3,"title":"<code>load_spatial_mask(dataset_name, grid_name, mask_name)</code>","text":"<p>Load a single spatial mask.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_spatial_mask--parameters","level":5,"title":"Parameters","text":"<p>dataset_name : str     Dataset identifier. grid_name : str     Grid identifier. mask_name : str     Mask name (e.g. <code>'north'</code>, <code>'high_elevation'</code>).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_spatial_mask--returns","level":5,"title":"Returns","text":"<p>np.ndarray     Boolean array with shape <code>(ncells,)</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def load_spatial_mask(\n    self, dataset_name: str, grid_name: str, mask_name: str\n) -&gt; np.ndarray:\n    \"\"\"Load a single spatial mask.\n\n    Parameters\n    ----------\n    dataset_name : str\n        Dataset identifier.\n    grid_name : str\n        Grid identifier.\n    mask_name : str\n        Mask name (e.g. ``'north'``, ``'high_elevation'``).\n\n    Returns\n    -------\n    np.ndarray\n        Boolean array with shape ``(ncells,)``.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/spatial_masks\"\n\n    try:\n        with self.store.readonly_session() as session:\n            ds_masks = xr.open_zarr(\n                session.store, group=group_path, consolidated=False\n            )\n\n        mask_var = f\"mask_{mask_name}\"\n        if mask_var not in ds_masks:\n            available = [v.replace(\"mask_\", \"\") for v in ds_masks.data_vars]\n            raise ValueError(\n                f\"Spatial mask '{mask_name}' not found. Available: {available}\"\n            )\n        return ds_masks[mask_var].values.astype(bool)\n\n    except Exception:\n        logger.error(\n            \"Failed to load spatial mask from %s\", group_path, exc_info=True\n        )\n        raise\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_all_spatial_masks","level":3,"title":"<code>load_all_spatial_masks(dataset_name, grid_name)</code>","text":"<p>Load all spatial masks.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_all_spatial_masks--returns","level":5,"title":"Returns","text":"<p>dict     <code>{name: bool_ndarray}</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def load_all_spatial_masks(\n    self, dataset_name: str, grid_name: str\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Load all spatial masks.\n\n    Returns\n    -------\n    dict\n        ``{name: bool_ndarray}``.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/spatial_masks\"\n\n    try:\n        with self.store.readonly_session() as session:\n            ds_masks = xr.open_zarr(\n                session.store, group=group_path, consolidated=False\n            )\n        return {\n            var.replace(\"mask_\", \"\"): ds_masks[var].values.astype(bool)\n            for var in ds_masks.data_vars\n        }\n    except Exception:\n        logger.error(\n            \"Failed to load spatial masks from %s\", group_path, exc_info=True\n        )\n        raise\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.has_spatial_masks","level":3,"title":"<code>has_spatial_masks(dataset_name, grid_name)</code>","text":"<p>Return <code>True</code> if spatial masks exist for the dataset+grid pair.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def has_spatial_masks(self, dataset_name: str, grid_name: str) -&gt; bool:\n    \"\"\"Return ``True`` if spatial masks exist for the dataset+grid pair.\"\"\"\n    try:\n        self.load_all_spatial_masks(dataset_name, grid_name)\n        return True\n    except Exception:\n        return False\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_statistics","level":3,"title":"<code>store_statistics(dataset_name, grid_name, stats, overwrite=False)</code>","text":"<p>Store pre-computed per-cell statistics.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_statistics--parameters","level":5,"title":"Parameters","text":"<p>dataset_name : str     Dataset identifier. grid_name : str     Grid identifier. stats : dict     <code>{name: array}</code> – all arrays must have shape <code>(ncells,)</code>.     Variables whose name ends with <code>'_count'</code> or equals     <code>'obs_count'</code> are stored as <code>int64</code>; everything else as     <code>float32</code>. overwrite : bool     Overwrite existing statistics.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.store_statistics--returns","level":5,"title":"Returns","text":"<p>str     Icechunk snapshot ID.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def store_statistics(\n    self,\n    dataset_name: str,\n    grid_name: str,\n    stats: dict[str, np.ndarray],\n    overwrite: bool = False,\n) -&gt; str:\n    \"\"\"Store pre-computed per-cell statistics.\n\n    Parameters\n    ----------\n    dataset_name : str\n        Dataset identifier.\n    grid_name : str\n        Grid identifier.\n    stats : dict\n        ``{name: array}`` – all arrays must have shape ``(ncells,)``.\n        Variables whose name ends with ``'_count'`` or equals\n        ``'obs_count'`` are stored as ``int64``; everything else as\n        ``float32``.\n    overwrite : bool\n        Overwrite existing statistics.\n\n    Returns\n    -------\n    str\n        Icechunk snapshot ID.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/statistics\"\n    logger.info(\"Storing statistics to %s\", group_path)\n\n    ncells = len(next(iter(stats.values())))\n    for name, arr in stats.items():\n        if arr.shape != (ncells,):\n            raise ValueError(\n                f\"Statistic '{name}' has shape {arr.shape}, expected ({ncells},)\"\n            )\n\n    stat_vars = {}\n    for name, arr in stats.items():\n        dtype = (\n            np.int64\n            if (name.endswith(\"_count\") or name == \"obs_count\")\n            else np.float32\n        )\n        stat_vars[name] = ([\"cell\"], arr.astype(dtype))\n\n    ds_stats = xr.Dataset(\n        stat_vars, coords={\"cell\": np.arange(ncells, dtype=np.int32)}\n    )\n\n    # Summary statistics per variable (stored as variable attrs)\n    for name, arr in stats.items():\n        valid = np.isfinite(arr)\n        if np.any(valid):\n            ds_stats[name].attrs.update(\n                {\n                    \"min\": float(np.nanmin(arr)),\n                    \"max\": float(np.nanmax(arr)),\n                    \"mean\": float(np.nanmean(arr)),\n                    \"n_valid\": int(np.sum(valid)),\n                }\n            )\n\n    ds_stats.attrs.update(\n        {\n            \"created_at\": datetime.now().isoformat(),\n            \"dataset\": dataset_name,\n            \"grid\": grid_name,\n            \"statistics\": list(stats.keys()),\n            \"ncells\": ncells,\n        }\n    )\n\n    with self.store.writable_session() as session:\n        from icechunk.xarray import to_icechunk\n\n        mode = \"w\" if overwrite else \"w-\"\n        to_icechunk(ds_stats, session, group=group_path, mode=mode)\n        snapshot_id = session.commit(\n            f\"Stored statistics for {dataset_name}/{grid_name}\"\n        )\n\n    logger.info(\"Statistics stored (snapshot: %s)\", snapshot_id[:8])\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_statistics","level":3,"title":"<code>load_statistics(dataset_name, grid_name, stat_name=None)</code>","text":"<p>Load pre-computed per-cell statistics.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_statistics--parameters","level":5,"title":"Parameters","text":"<p>dataset_name : str     Dataset identifier. grid_name : str     Grid identifier. stat_name : str, optional     Load only this statistic.  <code>None</code> loads all.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.load_statistics--returns","level":5,"title":"Returns","text":"<p>dict     <code>{name: ndarray}</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def load_statistics(\n    self,\n    dataset_name: str,\n    grid_name: str,\n    stat_name: str | None = None,\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Load pre-computed per-cell statistics.\n\n    Parameters\n    ----------\n    dataset_name : str\n        Dataset identifier.\n    grid_name : str\n        Grid identifier.\n    stat_name : str, optional\n        Load only this statistic.  ``None`` loads all.\n\n    Returns\n    -------\n    dict\n        ``{name: ndarray}``.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/statistics\"\n\n    try:\n        with self.store.readonly_session() as session:\n            ds_stats = xr.open_zarr(\n                session.store, group=group_path, consolidated=False\n            )\n\n        if stat_name:\n            if stat_name not in ds_stats:\n                raise ValueError(\n                    f\"Statistic '{stat_name}' not found. \"\n                    f\"Available: {list(ds_stats.data_vars)}\"\n                )\n            return {stat_name: ds_stats[stat_name].values}\n        return {var: ds_stats[var].values for var in ds_stats.data_vars}\n\n    except Exception:\n        logger.error(\"Failed to load statistics from %s\", group_path, exc_info=True)\n        raise\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.has_statistics","level":3,"title":"<code>has_statistics(dataset_name, grid_name)</code>","text":"<p>Return <code>True</code> if statistics exist for the dataset+grid pair.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def has_statistics(self, dataset_name: str, grid_name: str) -&gt; bool:\n    \"\"\"Return ``True`` if statistics exist for the dataset+grid pair.\"\"\"\n    try:\n        self.load_statistics(dataset_name, grid_name)\n        return True\n    except Exception:\n        return False\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.list_available_metadata","level":3,"title":"<code>list_available_metadata(dataset_name, grid_name)</code>","text":"<p>Check which metadata categories are stored.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.list_available_metadata--returns","level":5,"title":"Returns","text":"<p>dict     <code>{category: bool}</code> for weights, filter_masks, spatial_masks,     statistics.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def list_available_metadata(\n    self, dataset_name: str, grid_name: str\n) -&gt; dict[str, bool]:\n    \"\"\"Check which metadata categories are stored.\n\n    Returns\n    -------\n    dict\n        ``{category: bool}`` for weights, filter_masks, spatial_masks,\n        statistics.\n\n    \"\"\"\n    return {\n        \"weights\": self.has_weights(dataset_name, grid_name),\n        \"filter_masks\": self.has_filter_masks(dataset_name, grid_name),\n        \"spatial_masks\": self.has_spatial_masks(dataset_name, grid_name),\n        \"statistics\": self.has_statistics(dataset_name, grid_name),\n    }\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.get_metadata_summary","level":3,"title":"<code>get_metadata_summary(dataset_name, grid_name)</code>","text":"<p>Detailed summary of all stored metadata for a dataset+grid pair.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.get_metadata_summary--returns","level":5,"title":"Returns","text":"<p>dict     Nested summary with availability flags and per-category details.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def get_metadata_summary(self, dataset_name: str, grid_name: str) -&gt; dict[str, Any]:\n    \"\"\"Detailed summary of all stored metadata for a dataset+grid pair.\n\n    Returns\n    -------\n    dict\n        Nested summary with availability flags and per-category details.\n\n    \"\"\"\n    summary: dict[str, Any] = {\n        \"dataset\": dataset_name,\n        \"grid\": grid_name,\n        \"available\": self.list_available_metadata(dataset_name, grid_name),\n    }\n\n    if summary[\"available\"][\"weights\"]:\n        weights = self.load_weights(dataset_name, grid_name)\n        summary[\"weights\"] = {\n            \"types\": list(weights.keys()),\n            \"ncells\": len(next(iter(weights.values()))),\n        }\n\n    if summary[\"available\"][\"filter_masks\"]:\n        masks = self.load_all_filter_masks(dataset_name, grid_name)\n        summary[\"filter_masks\"] = {\n            \"types\": list(masks.keys()),\n            \"shape\": next(iter(masks.values())).shape,\n        }\n\n    if summary[\"available\"][\"spatial_masks\"]:\n        masks = self.load_all_spatial_masks(dataset_name, grid_name)\n        summary[\"spatial_masks\"] = {\n            \"types\": list(masks.keys()),\n            \"ncells\": len(next(iter(masks.values()))),\n        }\n\n    if summary[\"available\"][\"statistics\"]:\n        stats = self.load_statistics(dataset_name, grid_name)\n        summary[\"statistics\"] = {\n            \"types\": list(stats.keys()),\n            \"ncells\": len(next(iter(stats.values()))),\n        }\n\n    return summary\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.delete_weights","level":3,"title":"<code>delete_weights(dataset_name, grid_name)</code>","text":"<p>Delete all weights for a dataset+grid pair.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def delete_weights(self, dataset_name: str, grid_name: str) -&gt; str:\n    \"\"\"Delete all weights for a dataset+grid pair.\"\"\"\n    return self._delete_group(\n        f\"metadata/{dataset_name}/{grid_name}/weights\",\n        f\"weights for {dataset_name}/{grid_name}\",\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.delete_filter_masks","level":3,"title":"<code>delete_filter_masks(dataset_name, grid_name)</code>","text":"<p>Delete all filter masks for a dataset+grid pair.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def delete_filter_masks(self, dataset_name: str, grid_name: str) -&gt; str:\n    \"\"\"Delete all filter masks for a dataset+grid pair.\"\"\"\n    return self._delete_group(\n        f\"metadata/{dataset_name}/{grid_name}/filter_masks\",\n        f\"filter masks for {dataset_name}/{grid_name}\",\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.delete_spatial_masks","level":3,"title":"<code>delete_spatial_masks(dataset_name, grid_name)</code>","text":"<p>Delete all spatial masks for a dataset+grid pair.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def delete_spatial_masks(self, dataset_name: str, grid_name: str) -&gt; str:\n    \"\"\"Delete all spatial masks for a dataset+grid pair.\"\"\"\n    return self._delete_group(\n        f\"metadata/{dataset_name}/{grid_name}/spatial_masks\",\n        f\"spatial masks for {dataset_name}/{grid_name}\",\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.delete_statistics","level":3,"title":"<code>delete_statistics(dataset_name, grid_name)</code>","text":"<p>Delete all statistics for a dataset+grid pair.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def delete_statistics(self, dataset_name: str, grid_name: str) -&gt; str:\n    \"\"\"Delete all statistics for a dataset+grid pair.\"\"\"\n    return self._delete_group(\n        f\"metadata/{dataset_name}/{grid_name}/statistics\",\n        f\"statistics for {dataset_name}/{grid_name}\",\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.delete_all_metadata","level":3,"title":"<code>delete_all_metadata(dataset_name, grid_name)</code>","text":"<p>Delete the entire metadata subtree for a dataset+grid pair.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def delete_all_metadata(self, dataset_name: str, grid_name: str) -&gt; str:\n    \"\"\"Delete the entire metadata subtree for a dataset+grid pair.\"\"\"\n    return self._delete_group(\n        f\"metadata/{dataset_name}/{grid_name}\",\n        f\"all metadata for {dataset_name}/{grid_name}\",\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.delete_specific_weight","level":3,"title":"<code>delete_specific_weight(dataset_name, grid_name, weight_name)</code>","text":"<p>Delete a single weight variable from an existing weights group.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.delete_specific_weight--parameters","level":5,"title":"Parameters","text":"<p>dataset_name : str     Dataset identifier. grid_name : str     Grid identifier. weight_name : str     Weight variable name (e.g. <code>'observation_count'</code>).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.delete_specific_weight--returns","level":5,"title":"Returns","text":"<p>str     Snapshot ID.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def delete_specific_weight(\n    self, dataset_name: str, grid_name: str, weight_name: str\n) -&gt; str:\n    \"\"\"Delete a single weight variable from an existing weights group.\n\n    Parameters\n    ----------\n    dataset_name : str\n        Dataset identifier.\n    grid_name : str\n        Grid identifier.\n    weight_name : str\n        Weight variable name (e.g. ``'observation_count'``).\n\n    Returns\n    -------\n    str\n        Snapshot ID.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/weights\"\n    logger.info(\"Deleting weight '%s' from %s\", weight_name, group_path)\n\n    with self.store.writable_session() as session:\n        import zarr\n\n        group = zarr.open(session.store, path=group_path, mode=\"r+\")\n        if weight_name in group:\n            del group[weight_name]\n            snapshot_id = session.commit(\n                f\"Deleted weight '{weight_name}' for {dataset_name}/{grid_name}\"\n            )\n            logger.info(\n                \"Weight '%s' deleted (snapshot: %s)\",\n                weight_name,\n                snapshot_id[:8],\n            )\n            return snapshot_id\n        raise ValueError(f\"Weight '{weight_name}' does not exist in {group_path}\")\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.delete_specific_filter_mask","level":3,"title":"<code>delete_specific_filter_mask(dataset_name, grid_name, filter_type)</code>","text":"<p>Delete a single filter mask variable from an existing filter_masks group.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.delete_specific_filter_mask--parameters","level":5,"title":"Parameters","text":"<p>dataset_name : str     Dataset identifier. grid_name : str     Grid identifier. filter_type : str     Filter type name (e.g. <code>'iqr'</code>).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.analysis.analysis_storage.AnalysisStorage.delete_specific_filter_mask--returns","level":5,"title":"Returns","text":"<p>str     Snapshot ID.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/analysis/analysis_storage.py</code> <pre><code>def delete_specific_filter_mask(\n    self, dataset_name: str, grid_name: str, filter_type: str\n) -&gt; str:\n    \"\"\"Delete a single filter mask variable from an existing filter_masks group.\n\n    Parameters\n    ----------\n    dataset_name : str\n        Dataset identifier.\n    grid_name : str\n        Grid identifier.\n    filter_type : str\n        Filter type name (e.g. ``'iqr'``).\n\n    Returns\n    -------\n    str\n        Snapshot ID.\n\n    \"\"\"\n    group_path = f\"metadata/{dataset_name}/{grid_name}/filter_masks\"\n    mask_var = f\"mask_{filter_type}\"\n    logger.info(\"Deleting filter mask '%s' from %s\", filter_type, group_path)\n\n    with self.store.writable_session() as session:\n        import zarr\n\n        group = zarr.open(session.store, path=group_path, mode=\"r+\")\n        if mask_var in group:\n            del group[mask_var]\n            commit_msg = (\n                f\"Deleted filter mask '{filter_type}' for \"\n                f\"{dataset_name}/{grid_name}\"\n            )\n            snapshot_id = session.commit(commit_msg)\n            logger.info(\n                \"Filter mask '%s' deleted (snapshot: %s)\",\n                filter_type,\n                snapshot_id[:8],\n            )\n            return snapshot_id\n        raise ValueError(\n            f\"Filter mask '{filter_type}' does not exist in {group_path}\"\n        )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#workflows","level":2,"title":"Workflows","text":"<p>Core adapted VOD workflow.</p> <p>Orchestrates the full VOD analysis pipeline: loading data from an Icechunk store, applying cell-SID Hampel filtering (vectorised or parallelised), and persisting results back to a <code>processing</code> branch.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow--classes","level":3,"title":"Classes","text":"<p><code>AdaptedVODWorkflow</code>     Main workflow entry-point.  Lazily imports <code>canvod-store</code> so the     package can be imported even when the store is not installed.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow--functions","level":3,"title":"Functions","text":"<p><code>get_workflow_for_store</code>     Convenience factory. <code>check_processed_data_status</code>     Introspect the store for previously filtered data.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow","level":2,"title":"<code>AdaptedVODWorkflow</code>","text":"<p>Core VOD analysis workflow with polars-optimised loading and refined temporal matching.</p> <p>All heavy lifting (filtering, grid operations) is delegated to <code>canvod.grids.analysis</code>.  This class is responsible only for Icechunk I/O and orchestration.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow--parameters","level":4,"title":"Parameters","text":"<p>vod_store_path : Path or str     Path to the VOD Icechunk store directory.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>class AdaptedVODWorkflow:\n    \"\"\"Core VOD analysis workflow with polars-optimised loading and refined\n    temporal matching.\n\n    All heavy lifting (filtering, grid operations) is delegated to\n    ``canvod.grids.analysis``.  This class is responsible only for\n    Icechunk I/O and orchestration.\n\n    Parameters\n    ----------\n    vod_store_path : Path or str\n        Path to the VOD Icechunk store directory.\n\n    \"\"\"\n\n    def __init__(self, vod_store_path: Path | str) -&gt; None:\n        \"\"\"Initialize the workflow.\n\n        Parameters\n        ----------\n        vod_store_path : Path | str\n            Path to the VOD Icechunk store directory.\n\n        \"\"\"\n        self.vod_store_path = Path(vod_store_path)\n        self.vod_store: MyIcechunkStore = _get_store(self.vod_store_path)\n\n    # ------------------------------------------------------------------\n    # Data loading\n    # ------------------------------------------------------------------\n\n    def load_vod_data(\n        self,\n        group_name: str = \"reference_01_canopy_01\",\n        branch: str = \"main\",\n    ) -&gt; xr.Dataset:\n        \"\"\"Load a VOD dataset from the store.\n\n        Parameters\n        ----------\n        group_name : str\n            Zarr group path inside the store.\n        branch : str\n            Icechunk branch to read from.\n\n        Returns\n        -------\n        xr.Dataset\n            Lazy-loaded VOD dataset.\n\n        \"\"\"\n        logger.info(\"Loading VOD data from branch=%s group=%s\", branch, group_name)\n        with self.vod_store.readonly_session(branch=branch) as session:\n            vod_ds = xr.open_zarr(session.store, group=group_name, consolidated=False)\n        logger.info(\"Loaded VOD dataset: %s\", dict(vod_ds.sizes))\n        return vod_ds\n\n    # ------------------------------------------------------------------\n    # Temporal coverage checks\n    # ------------------------------------------------------------------\n\n    def check_temporal_coverage_compatibility(\n        self,\n        main_ds: xr.Dataset,\n        processed_ds: xr.Dataset,\n        requested_time_range: tuple[datetime.date, datetime.date] | None = None,\n    ) -&gt; tuple[bool, dict[str, Any]]:\n        \"\"\"Check whether *processed_ds* adequately covers a time range.\n\n        When *requested_time_range* is ``None`` the method checks that the\n        processed dataset covers at least 70 % of the main dataset's span.\n        When a range is given it verifies that both endpoints fall within the\n        processed dataset (with a 1-day tolerance).\n\n        Parameters\n        ----------\n        main_ds : xr.Dataset\n            Reference (unfiltered) dataset.\n        processed_ds : xr.Dataset\n            Filtered dataset to validate.\n        requested_time_range : tuple of date, optional\n            ``(start, end)`` to check against.\n\n        Returns\n        -------\n        compatible : bool\n        coverage_info : dict\n            Diagnostic information with ``main_range``, ``processed_range``,\n            and ``requested_range``.\n\n        \"\"\"\n\n        def _date_range(ds: xr.Dataset) -&gt; tuple[datetime.date, datetime.date]:\n            \"\"\"Return the date range for a dataset.\n\n            Parameters\n            ----------\n            ds : xr.Dataset\n                Dataset with an epoch coordinate.\n\n            Returns\n            -------\n            tuple[datetime.date, datetime.date]\n                Start and end dates.\n\n            \"\"\"\n            return (\n                pd.to_datetime(ds.epoch.min().values).date(),\n                pd.to_datetime(ds.epoch.max().values).date(),\n            )\n\n        main_start, main_end = _date_range(main_ds)\n        proc_start, proc_end = _date_range(processed_ds)\n\n        coverage_info: dict[str, Any] = {\n            \"main_range\": (main_start, main_end),\n            \"processed_range\": (proc_start, proc_end),\n            \"requested_range\": requested_time_range,\n        }\n\n        if requested_time_range is None:\n            main_days = (main_end - main_start).days\n            proc_days = (proc_end - proc_start).days\n            ratio = proc_days / main_days if main_days &gt; 0 else 0.0\n            logger.info(\n                \"Coverage check: main=%d days, processed=%d days, ratio=%.1f%%\",\n                main_days,\n                proc_days,\n                ratio * 100,\n            )\n            return ratio &gt;= 0.7, coverage_info\n\n        req_start, req_end = requested_time_range\n        one_day = datetime.timedelta(days=1)\n        start_ok = proc_start &lt;= req_start &lt;= proc_end + one_day\n        end_ok = proc_start - one_day &lt;= req_end &lt;= proc_end\n        compatible = start_ok and end_ok\n\n        if not compatible:\n            logger.warning(\n                \"Temporal coverage mismatch: processed=%s→%s, requested=%s→%s\",\n                proc_start,\n                proc_end,\n                req_start,\n                req_end,\n            )\n        return compatible, coverage_info\n\n    # ------------------------------------------------------------------\n    # Filtering entry-points\n    # ------------------------------------------------------------------\n\n    def create_processed_data_fast_hampel_complete(\n        self,\n        start_date: datetime.date | datetime.datetime,\n        end_date: datetime.date | datetime.datetime,\n        force_recreate: bool = False,\n        window_hours: float = 1.0,\n        sigma_threshold: float = 3.0,\n        min_points: int = 5,\n        ultra_fast_mode: bool = False,\n        cell_batch_size: int = 200,\n        n_workers: int | None = None,\n    ) -&gt; str | None:\n        \"\"\"Run the vectorised / ultra-fast Hampel pipeline end-to-end.\n\n        Delegates the actual filtering to\n        :func:`canvod.grids.analysis.sigma_clip_filter.astropy_hampel_vectorized_fast`\n        (or its ultra-fast variant) and persists the result on a\n        ``processing`` branch.\n\n        Parameters\n        ----------\n        start_date, end_date : date or datetime\n            Temporal extent to process.\n        force_recreate : bool\n            Overwrite existing filtered data.\n        window_hours : float\n            Hampel temporal window in hours.\n        sigma_threshold : float\n            MAD-based outlier threshold.\n        min_points : int\n            Minimum observations required per window.\n        ultra_fast_mode : bool\n            Use the pure-NumPy sigma-clip path (faster, less precise).\n        cell_batch_size : int\n            Number of cells per spatial batch.\n        n_workers : int, optional\n            Parallel workers.  ``None`` → auto-detect.\n\n        Returns\n        -------\n        str or None\n            Icechunk snapshot ID, or ``None`` if existing data was kept.\n\n        \"\"\"\n        return _create_processed_data_fast_hampel(\n            workflow_instance=self,\n            start_date=start_date,\n            end_date=end_date,\n            force_recreate=force_recreate,\n            window_hours=window_hours,\n            sigma_threshold=sigma_threshold,\n            min_points=min_points,\n            ultra_fast_mode=ultra_fast_mode,\n            cell_batch_size=cell_batch_size,\n            n_workers=n_workers,\n        )\n\n    def create_processed_data_hampel_parallel_complete(\n        self,\n        start_date: datetime.date | datetime.datetime,\n        end_date: datetime.date | datetime.datetime,\n        force_recreate: bool = False,\n        threshold: float = 3.0,\n        min_obs_per_sid: int = 20,\n        spatial_batch_size: int = 500,\n        n_workers: int | None = None,\n        temporal_agg: str | None = None,\n        agg_method: str | None = None,\n    ) -&gt; str | None:\n        \"\"\"Run the parallelised cell-SID Hampel pipeline end-to-end.\n\n        Loads the complete requested time range (no temporal chunking) and\n        applies\n        :func:`canvod.grids.analysis.hampel_filtering.aggr_hampel_cell_sid_parallelized`\n        with spatial batching.\n\n        Parameters\n        ----------\n        start_date, end_date : date or datetime\n            Temporal extent to process.\n        force_recreate : bool\n            Overwrite existing filtered data.\n        threshold : float\n            MAD-based outlier threshold.\n        min_obs_per_sid : int\n            Minimum observations per cell-SID combination.\n        spatial_batch_size : int\n            Cells per spatial batch.\n        n_workers : int, optional\n            Parallel workers.  ``None`` → auto-detect.\n        temporal_agg : str, optional\n            Post-filtering aggregation frequency (e.g. ``'1H'``, ``'1D'``).\n        agg_method : str, optional\n            Aggregation method (e.g. ``'mean'``).\n\n        Returns\n        -------\n        str or None\n            Icechunk snapshot ID, or ``None`` if existing data was kept.\n\n        \"\"\"\n        from canvod.grids import create_hemigrid\n        from canvod.grids.analysis.hampel_filtering import (\n            aggr_hampel_cell_sid_parallelized,\n        )\n        from canvod.grids.operations import add_cell_ids_to_ds_fast\n\n        logger.info(\"=\" * 60)\n        logger.info(\"PARALLEL HAMPEL — complete temporal coverage\")\n        logger.info(\n            \"Range: %s → %s | threshold=%.1f | min_obs=%d | batch=%d | workers=%s\",\n            start_date,\n            end_date,\n            threshold,\n            min_obs_per_sid,\n            spatial_batch_size,\n            n_workers or \"auto\",\n        )\n\n        # --- guard: existing data ---\n        if not self._force_or_skip(\"processing\", force_recreate):\n            return None\n\n        # --- load complete time range ---\n        logger.info(\"Loading complete time range for parallel processing\")\n        with self.vod_store.readonly_session(branch=\"main\") as session:\n            vod_ds = xr.open_zarr(session.store, group=\"reference_01_canopy_01\")\n\n        vod_ds_complete = vod_ds.sel(epoch=slice(start_date, end_date))\n\n        if \"cell_id_equal_area_2deg\" not in vod_ds_complete:\n            grid = create_hemigrid(grid_type=\"equal_area\", angular_resolution=2)\n            vod_ds_complete = add_cell_ids_to_ds_fast(\n                vod_ds_complete, grid, \"equal_area_2deg\", data_var=\"VOD\"\n            )\n\n        logger.info(\"Dataset loaded: %s\", dict(vod_ds_complete.sizes))\n\n        # --- filter ---\n        t0 = time.time()\n        vod_ds_filtered = aggr_hampel_cell_sid_parallelized(\n            vod_ds_complete,\n            threshold=threshold,\n            min_obs_per_sid=min_obs_per_sid,\n            spatial_batch_size=spatial_batch_size,\n            n_workers=n_workers,\n            temporal_agg=temporal_agg,\n            agg_method=agg_method,\n        )\n        logger.info(\"Parallel filtering completed in %.1f s\", time.time() - t0)\n\n        # --- persist ---\n        snapshot_id = self._persist_filtered(\n            vod_ds_filtered,\n            \"parallel Cell-SID Hampel\",\n        )\n        logger.info(\"Parallel Hampel complete. Snapshot: %s\", snapshot_id)\n        return snapshot_id\n\n    # ------------------------------------------------------------------\n    # High-level orchestration\n    # ------------------------------------------------------------------\n\n    def run_complete_workflow(\n        self,\n        group_name: str = \"reference_01_canopy_01\",\n        branch: str = \"auto\",\n        time_range: tuple[datetime.date, datetime.date] | None = None,\n        **kwargs: Any,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Orchestrate a complete analysis run.\n\n        Auto-detection logic (``branch='auto'``) looks for Hampel-filtered\n        data on the ``processing`` branch first.  If found and temporally\n        compatible it is used directly; otherwise raw data from ``main`` is\n        returned.\n\n        Parameters\n        ----------\n        group_name : str\n            Zarr group for the raw VOD data.\n        branch : str\n            ``'auto'`` for detection, or an explicit branch name.\n        time_range : tuple of date, optional\n            ``(start, end)`` to select.\n\n        Returns\n        -------\n        dict\n            Keys: ``final_data`` (Dataset), ``source_branch``,\n            ``pre_filtered`` (bool), ``filter_type``.\n\n        \"\"\"\n        logger.info(\"=\" * 60)\n        logger.info(\"HAMPEL-FILTERED VOD ANALYSIS WORKFLOW\")\n        logger.info(\"branch=%s group=%s time_range=%s\", branch, group_name, time_range)\n\n        results: dict[str, Any] = {}\n\n        if branch == \"auto\":\n            hampel_ds = self._try_load_hampel()\n\n            if hampel_ds is not None:\n                # Validate temporal coverage when a range is requested\n                if time_range is not None:\n                    dataset_start = pd.to_datetime(hampel_ds.epoch.min().values).date()\n                    dataset_end = pd.to_datetime(hampel_ds.epoch.max().values).date()\n\n                    start_ok = normalize_datetime_for_comparison(\n                        time_range[0]\n                    ) &gt;= normalize_datetime_for_comparison(dataset_start)\n                    end_ok = normalize_datetime_for_comparison(\n                        time_range[1]\n                    ) &lt;= normalize_datetime_for_comparison(dataset_end)\n\n                    if start_ok and end_ok:\n                        hampel_ds = hampel_ds.sel(\n                            epoch=slice(time_range[0], time_range[1])\n                        )\n                    else:\n                        logger.warning(\n                            \"Hampel data (%s→%s) does not cover requested \"\n                            \"range (%s→%s); falling back to main branch\",\n                            dataset_start,\n                            dataset_end,\n                            time_range[0],\n                            time_range[1],\n                        )\n                        hampel_ds = None\n\n                if hampel_ds is not None:\n                    logger.info(\"Using Hampel filtered data from processing branch\")\n                    return {\n                        \"final_data\": hampel_ds,\n                        \"source_branch\": \"processing\",\n                        \"pre_filtered\": True,\n                        \"filter_type\": \"hampel\",\n                    }\n\n            logger.info(\"No usable Hampel data found; using raw data from main branch\")\n            branch = \"main\"\n\n        # --- main branch (raw) ---\n        logger.info(\"Loading raw data from branch=%s\", branch)\n        vod_ds = self.load_vod_data(group_name, branch)\n\n        if time_range is not None:\n            vod_ds = vod_ds.sel(epoch=slice(time_range[0], time_range[1]))\n\n        results = {\n            \"final_data\": vod_ds,\n            \"source_branch\": branch,\n            \"pre_filtered\": False,\n            \"filter_type\": \"none\",\n        }\n        logger.info(\"Workflow complete — source=%s\", branch)\n        return results\n\n    # ------------------------------------------------------------------\n    # Private helpers\n    # ------------------------------------------------------------------\n\n    def _try_load_hampel(self) -&gt; xr.Dataset | None:\n        \"\"\"Attempt to load Hampel-filtered data from the processing branch.\"\"\"\n        try:\n            with self.vod_store.readonly_session(branch=\"processing\") as session:\n                ds = xr.open_zarr(\n                    session.store,\n                    group=\"reference_01_canopy_01_hampel_filtered\",\n                    consolidated=False,\n                )\n            logger.info(\"Found Hampel filtered data on processing branch\")\n            return ds\n        except Exception:\n            logger.debug(\"No Hampel data on processing branch\", exc_info=True)\n            return None\n\n    def _force_or_skip(self, branch: str, force_recreate: bool) -&gt; bool:\n        \"\"\"Guard pattern: return ``True`` to proceed, ``False`` to skip.\n\n        If filtered data already exists and *force_recreate* is ``False``\n        the method logs a warning and returns ``False``.  When\n        *force_recreate* is ``True`` it deletes the branch first.\n        \"\"\"\n        exists = self._try_load_hampel() is not None\n        if exists and not force_recreate:\n            logger.warning(\n                \"Filtered data already exists. Pass force_recreate=True to overwrite.\"\n            )\n            return False\n        if exists and force_recreate:\n            try:\n                self.vod_store.delete_branch(branch)\n                logger.info(\"Deleted existing %s branch\", branch)\n            except Exception:\n                logger.warning(\"Could not delete branch %s\", branch, exc_info=True)\n        return True\n\n    def _persist_filtered(\n        self,\n        ds: xr.Dataset,\n        label: str,\n        target_group: str = \"reference_01_canopy_01_hampel_filtered\",\n    ) -&gt; str:\n        \"\"\"Write a filtered dataset to the ``processing`` branch.\n\n        Rechunks variables along the ``epoch`` dimension (max 50 000 epochs\n        per chunk) before writing.\n\n        Returns the Icechunk snapshot ID.\n        \"\"\"\n        from icechunk.xarray import to_icechunk\n\n        # Ensure processing branch exists\n        try:\n            current_snapshot = next(self.vod_store.repo.ancestry(branch=\"main\")).id\n            self.vod_store.repo.create_branch(\"processing\", current_snapshot)\n        except Exception:\n            pass  # branch may already exist\n\n        with self.vod_store.writable_session(\"processing\") as session:\n            logger.info(\"Persisting filtered data (%s)\", label)\n            for var_name in ds.data_vars:\n                if \"epoch\" in ds[var_name].dims:\n                    epoch_size = ds[var_name].sizes[\"epoch\"]\n                    ds[var_name] = ds[var_name].chunk(\n                        {\"epoch\": min(epoch_size, 50000), \"sid\": -1}\n                    )\n            to_icechunk(ds, session, group=target_group, mode=\"w\", safe_chunks=False)\n            snapshot_id: str = session.commit(label)\n\n        return snapshot_id\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.__init__","level":3,"title":"<code>__init__(vod_store_path)</code>","text":"<p>Initialize the workflow.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.__init__--parameters","level":5,"title":"Parameters","text":"<p>vod_store_path : Path | str     Path to the VOD Icechunk store directory.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def __init__(self, vod_store_path: Path | str) -&gt; None:\n    \"\"\"Initialize the workflow.\n\n    Parameters\n    ----------\n    vod_store_path : Path | str\n        Path to the VOD Icechunk store directory.\n\n    \"\"\"\n    self.vod_store_path = Path(vod_store_path)\n    self.vod_store: MyIcechunkStore = _get_store(self.vod_store_path)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.load_vod_data","level":3,"title":"<code>load_vod_data(group_name='reference_01_canopy_01', branch='main')</code>","text":"<p>Load a VOD dataset from the store.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.load_vod_data--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Zarr group path inside the store. branch : str     Icechunk branch to read from.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.load_vod_data--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Lazy-loaded VOD dataset.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def load_vod_data(\n    self,\n    group_name: str = \"reference_01_canopy_01\",\n    branch: str = \"main\",\n) -&gt; xr.Dataset:\n    \"\"\"Load a VOD dataset from the store.\n\n    Parameters\n    ----------\n    group_name : str\n        Zarr group path inside the store.\n    branch : str\n        Icechunk branch to read from.\n\n    Returns\n    -------\n    xr.Dataset\n        Lazy-loaded VOD dataset.\n\n    \"\"\"\n    logger.info(\"Loading VOD data from branch=%s group=%s\", branch, group_name)\n    with self.vod_store.readonly_session(branch=branch) as session:\n        vod_ds = xr.open_zarr(session.store, group=group_name, consolidated=False)\n    logger.info(\"Loaded VOD dataset: %s\", dict(vod_ds.sizes))\n    return vod_ds\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.check_temporal_coverage_compatibility","level":3,"title":"<code>check_temporal_coverage_compatibility(main_ds, processed_ds, requested_time_range=None)</code>","text":"<p>Check whether processed_ds adequately covers a time range.</p> <p>When requested_time_range is <code>None</code> the method checks that the processed dataset covers at least 70 % of the main dataset's span. When a range is given it verifies that both endpoints fall within the processed dataset (with a 1-day tolerance).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.check_temporal_coverage_compatibility--parameters","level":5,"title":"Parameters","text":"<p>main_ds : xr.Dataset     Reference (unfiltered) dataset. processed_ds : xr.Dataset     Filtered dataset to validate. requested_time_range : tuple of date, optional     <code>(start, end)</code> to check against.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.check_temporal_coverage_compatibility--returns","level":5,"title":"Returns","text":"<p>compatible : bool coverage_info : dict     Diagnostic information with <code>main_range</code>, <code>processed_range</code>,     and <code>requested_range</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def check_temporal_coverage_compatibility(\n    self,\n    main_ds: xr.Dataset,\n    processed_ds: xr.Dataset,\n    requested_time_range: tuple[datetime.date, datetime.date] | None = None,\n) -&gt; tuple[bool, dict[str, Any]]:\n    \"\"\"Check whether *processed_ds* adequately covers a time range.\n\n    When *requested_time_range* is ``None`` the method checks that the\n    processed dataset covers at least 70 % of the main dataset's span.\n    When a range is given it verifies that both endpoints fall within the\n    processed dataset (with a 1-day tolerance).\n\n    Parameters\n    ----------\n    main_ds : xr.Dataset\n        Reference (unfiltered) dataset.\n    processed_ds : xr.Dataset\n        Filtered dataset to validate.\n    requested_time_range : tuple of date, optional\n        ``(start, end)`` to check against.\n\n    Returns\n    -------\n    compatible : bool\n    coverage_info : dict\n        Diagnostic information with ``main_range``, ``processed_range``,\n        and ``requested_range``.\n\n    \"\"\"\n\n    def _date_range(ds: xr.Dataset) -&gt; tuple[datetime.date, datetime.date]:\n        \"\"\"Return the date range for a dataset.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset with an epoch coordinate.\n\n        Returns\n        -------\n        tuple[datetime.date, datetime.date]\n            Start and end dates.\n\n        \"\"\"\n        return (\n            pd.to_datetime(ds.epoch.min().values).date(),\n            pd.to_datetime(ds.epoch.max().values).date(),\n        )\n\n    main_start, main_end = _date_range(main_ds)\n    proc_start, proc_end = _date_range(processed_ds)\n\n    coverage_info: dict[str, Any] = {\n        \"main_range\": (main_start, main_end),\n        \"processed_range\": (proc_start, proc_end),\n        \"requested_range\": requested_time_range,\n    }\n\n    if requested_time_range is None:\n        main_days = (main_end - main_start).days\n        proc_days = (proc_end - proc_start).days\n        ratio = proc_days / main_days if main_days &gt; 0 else 0.0\n        logger.info(\n            \"Coverage check: main=%d days, processed=%d days, ratio=%.1f%%\",\n            main_days,\n            proc_days,\n            ratio * 100,\n        )\n        return ratio &gt;= 0.7, coverage_info\n\n    req_start, req_end = requested_time_range\n    one_day = datetime.timedelta(days=1)\n    start_ok = proc_start &lt;= req_start &lt;= proc_end + one_day\n    end_ok = proc_start - one_day &lt;= req_end &lt;= proc_end\n    compatible = start_ok and end_ok\n\n    if not compatible:\n        logger.warning(\n            \"Temporal coverage mismatch: processed=%s→%s, requested=%s→%s\",\n            proc_start,\n            proc_end,\n            req_start,\n            req_end,\n        )\n    return compatible, coverage_info\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.create_processed_data_fast_hampel_complete","level":3,"title":"<code>create_processed_data_fast_hampel_complete(start_date, end_date, force_recreate=False, window_hours=1.0, sigma_threshold=3.0, min_points=5, ultra_fast_mode=False, cell_batch_size=200, n_workers=None)</code>","text":"<p>Run the vectorised / ultra-fast Hampel pipeline end-to-end.</p> <p>Delegates the actual filtering to :func:<code>canvod.grids.analysis.sigma_clip_filter.astropy_hampel_vectorized_fast</code> (or its ultra-fast variant) and persists the result on a <code>processing</code> branch.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.create_processed_data_fast_hampel_complete--parameters","level":5,"title":"Parameters","text":"<p>start_date, end_date : date or datetime     Temporal extent to process. force_recreate : bool     Overwrite existing filtered data. window_hours : float     Hampel temporal window in hours. sigma_threshold : float     MAD-based outlier threshold. min_points : int     Minimum observations required per window. ultra_fast_mode : bool     Use the pure-NumPy sigma-clip path (faster, less precise). cell_batch_size : int     Number of cells per spatial batch. n_workers : int, optional     Parallel workers.  <code>None</code> → auto-detect.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.create_processed_data_fast_hampel_complete--returns","level":5,"title":"Returns","text":"<p>str or None     Icechunk snapshot ID, or <code>None</code> if existing data was kept.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def create_processed_data_fast_hampel_complete(\n    self,\n    start_date: datetime.date | datetime.datetime,\n    end_date: datetime.date | datetime.datetime,\n    force_recreate: bool = False,\n    window_hours: float = 1.0,\n    sigma_threshold: float = 3.0,\n    min_points: int = 5,\n    ultra_fast_mode: bool = False,\n    cell_batch_size: int = 200,\n    n_workers: int | None = None,\n) -&gt; str | None:\n    \"\"\"Run the vectorised / ultra-fast Hampel pipeline end-to-end.\n\n    Delegates the actual filtering to\n    :func:`canvod.grids.analysis.sigma_clip_filter.astropy_hampel_vectorized_fast`\n    (or its ultra-fast variant) and persists the result on a\n    ``processing`` branch.\n\n    Parameters\n    ----------\n    start_date, end_date : date or datetime\n        Temporal extent to process.\n    force_recreate : bool\n        Overwrite existing filtered data.\n    window_hours : float\n        Hampel temporal window in hours.\n    sigma_threshold : float\n        MAD-based outlier threshold.\n    min_points : int\n        Minimum observations required per window.\n    ultra_fast_mode : bool\n        Use the pure-NumPy sigma-clip path (faster, less precise).\n    cell_batch_size : int\n        Number of cells per spatial batch.\n    n_workers : int, optional\n        Parallel workers.  ``None`` → auto-detect.\n\n    Returns\n    -------\n    str or None\n        Icechunk snapshot ID, or ``None`` if existing data was kept.\n\n    \"\"\"\n    return _create_processed_data_fast_hampel(\n        workflow_instance=self,\n        start_date=start_date,\n        end_date=end_date,\n        force_recreate=force_recreate,\n        window_hours=window_hours,\n        sigma_threshold=sigma_threshold,\n        min_points=min_points,\n        ultra_fast_mode=ultra_fast_mode,\n        cell_batch_size=cell_batch_size,\n        n_workers=n_workers,\n    )\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.create_processed_data_hampel_parallel_complete","level":3,"title":"<code>create_processed_data_hampel_parallel_complete(start_date, end_date, force_recreate=False, threshold=3.0, min_obs_per_sid=20, spatial_batch_size=500, n_workers=None, temporal_agg=None, agg_method=None)</code>","text":"<p>Run the parallelised cell-SID Hampel pipeline end-to-end.</p> <p>Loads the complete requested time range (no temporal chunking) and applies :func:<code>canvod.grids.analysis.hampel_filtering.aggr_hampel_cell_sid_parallelized</code> with spatial batching.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.create_processed_data_hampel_parallel_complete--parameters","level":5,"title":"Parameters","text":"<p>start_date, end_date : date or datetime     Temporal extent to process. force_recreate : bool     Overwrite existing filtered data. threshold : float     MAD-based outlier threshold. min_obs_per_sid : int     Minimum observations per cell-SID combination. spatial_batch_size : int     Cells per spatial batch. n_workers : int, optional     Parallel workers.  <code>None</code> → auto-detect. temporal_agg : str, optional     Post-filtering aggregation frequency (e.g. <code>'1H'</code>, <code>'1D'</code>). agg_method : str, optional     Aggregation method (e.g. <code>'mean'</code>).</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.create_processed_data_hampel_parallel_complete--returns","level":5,"title":"Returns","text":"<p>str or None     Icechunk snapshot ID, or <code>None</code> if existing data was kept.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def create_processed_data_hampel_parallel_complete(\n    self,\n    start_date: datetime.date | datetime.datetime,\n    end_date: datetime.date | datetime.datetime,\n    force_recreate: bool = False,\n    threshold: float = 3.0,\n    min_obs_per_sid: int = 20,\n    spatial_batch_size: int = 500,\n    n_workers: int | None = None,\n    temporal_agg: str | None = None,\n    agg_method: str | None = None,\n) -&gt; str | None:\n    \"\"\"Run the parallelised cell-SID Hampel pipeline end-to-end.\n\n    Loads the complete requested time range (no temporal chunking) and\n    applies\n    :func:`canvod.grids.analysis.hampel_filtering.aggr_hampel_cell_sid_parallelized`\n    with spatial batching.\n\n    Parameters\n    ----------\n    start_date, end_date : date or datetime\n        Temporal extent to process.\n    force_recreate : bool\n        Overwrite existing filtered data.\n    threshold : float\n        MAD-based outlier threshold.\n    min_obs_per_sid : int\n        Minimum observations per cell-SID combination.\n    spatial_batch_size : int\n        Cells per spatial batch.\n    n_workers : int, optional\n        Parallel workers.  ``None`` → auto-detect.\n    temporal_agg : str, optional\n        Post-filtering aggregation frequency (e.g. ``'1H'``, ``'1D'``).\n    agg_method : str, optional\n        Aggregation method (e.g. ``'mean'``).\n\n    Returns\n    -------\n    str or None\n        Icechunk snapshot ID, or ``None`` if existing data was kept.\n\n    \"\"\"\n    from canvod.grids import create_hemigrid\n    from canvod.grids.analysis.hampel_filtering import (\n        aggr_hampel_cell_sid_parallelized,\n    )\n    from canvod.grids.operations import add_cell_ids_to_ds_fast\n\n    logger.info(\"=\" * 60)\n    logger.info(\"PARALLEL HAMPEL — complete temporal coverage\")\n    logger.info(\n        \"Range: %s → %s | threshold=%.1f | min_obs=%d | batch=%d | workers=%s\",\n        start_date,\n        end_date,\n        threshold,\n        min_obs_per_sid,\n        spatial_batch_size,\n        n_workers or \"auto\",\n    )\n\n    # --- guard: existing data ---\n    if not self._force_or_skip(\"processing\", force_recreate):\n        return None\n\n    # --- load complete time range ---\n    logger.info(\"Loading complete time range for parallel processing\")\n    with self.vod_store.readonly_session(branch=\"main\") as session:\n        vod_ds = xr.open_zarr(session.store, group=\"reference_01_canopy_01\")\n\n    vod_ds_complete = vod_ds.sel(epoch=slice(start_date, end_date))\n\n    if \"cell_id_equal_area_2deg\" not in vod_ds_complete:\n        grid = create_hemigrid(grid_type=\"equal_area\", angular_resolution=2)\n        vod_ds_complete = add_cell_ids_to_ds_fast(\n            vod_ds_complete, grid, \"equal_area_2deg\", data_var=\"VOD\"\n        )\n\n    logger.info(\"Dataset loaded: %s\", dict(vod_ds_complete.sizes))\n\n    # --- filter ---\n    t0 = time.time()\n    vod_ds_filtered = aggr_hampel_cell_sid_parallelized(\n        vod_ds_complete,\n        threshold=threshold,\n        min_obs_per_sid=min_obs_per_sid,\n        spatial_batch_size=spatial_batch_size,\n        n_workers=n_workers,\n        temporal_agg=temporal_agg,\n        agg_method=agg_method,\n    )\n    logger.info(\"Parallel filtering completed in %.1f s\", time.time() - t0)\n\n    # --- persist ---\n    snapshot_id = self._persist_filtered(\n        vod_ds_filtered,\n        \"parallel Cell-SID Hampel\",\n    )\n    logger.info(\"Parallel Hampel complete. Snapshot: %s\", snapshot_id)\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.run_complete_workflow","level":3,"title":"<code>run_complete_workflow(group_name='reference_01_canopy_01', branch='auto', time_range=None, **kwargs)</code>","text":"<p>Orchestrate a complete analysis run.</p> <p>Auto-detection logic (<code>branch='auto'</code>) looks for Hampel-filtered data on the <code>processing</code> branch first.  If found and temporally compatible it is used directly; otherwise raw data from <code>main</code> is returned.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.run_complete_workflow--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Zarr group for the raw VOD data. branch : str     <code>'auto'</code> for detection, or an explicit branch name. time_range : tuple of date, optional     <code>(start, end)</code> to select.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.AdaptedVODWorkflow.run_complete_workflow--returns","level":5,"title":"Returns","text":"<p>dict     Keys: <code>final_data</code> (Dataset), <code>source_branch</code>,     <code>pre_filtered</code> (bool), <code>filter_type</code>.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def run_complete_workflow(\n    self,\n    group_name: str = \"reference_01_canopy_01\",\n    branch: str = \"auto\",\n    time_range: tuple[datetime.date, datetime.date] | None = None,\n    **kwargs: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Orchestrate a complete analysis run.\n\n    Auto-detection logic (``branch='auto'``) looks for Hampel-filtered\n    data on the ``processing`` branch first.  If found and temporally\n    compatible it is used directly; otherwise raw data from ``main`` is\n    returned.\n\n    Parameters\n    ----------\n    group_name : str\n        Zarr group for the raw VOD data.\n    branch : str\n        ``'auto'`` for detection, or an explicit branch name.\n    time_range : tuple of date, optional\n        ``(start, end)`` to select.\n\n    Returns\n    -------\n    dict\n        Keys: ``final_data`` (Dataset), ``source_branch``,\n        ``pre_filtered`` (bool), ``filter_type``.\n\n    \"\"\"\n    logger.info(\"=\" * 60)\n    logger.info(\"HAMPEL-FILTERED VOD ANALYSIS WORKFLOW\")\n    logger.info(\"branch=%s group=%s time_range=%s\", branch, group_name, time_range)\n\n    results: dict[str, Any] = {}\n\n    if branch == \"auto\":\n        hampel_ds = self._try_load_hampel()\n\n        if hampel_ds is not None:\n            # Validate temporal coverage when a range is requested\n            if time_range is not None:\n                dataset_start = pd.to_datetime(hampel_ds.epoch.min().values).date()\n                dataset_end = pd.to_datetime(hampel_ds.epoch.max().values).date()\n\n                start_ok = normalize_datetime_for_comparison(\n                    time_range[0]\n                ) &gt;= normalize_datetime_for_comparison(dataset_start)\n                end_ok = normalize_datetime_for_comparison(\n                    time_range[1]\n                ) &lt;= normalize_datetime_for_comparison(dataset_end)\n\n                if start_ok and end_ok:\n                    hampel_ds = hampel_ds.sel(\n                        epoch=slice(time_range[0], time_range[1])\n                    )\n                else:\n                    logger.warning(\n                        \"Hampel data (%s→%s) does not cover requested \"\n                        \"range (%s→%s); falling back to main branch\",\n                        dataset_start,\n                        dataset_end,\n                        time_range[0],\n                        time_range[1],\n                    )\n                    hampel_ds = None\n\n            if hampel_ds is not None:\n                logger.info(\"Using Hampel filtered data from processing branch\")\n                return {\n                    \"final_data\": hampel_ds,\n                    \"source_branch\": \"processing\",\n                    \"pre_filtered\": True,\n                    \"filter_type\": \"hampel\",\n                }\n\n        logger.info(\"No usable Hampel data found; using raw data from main branch\")\n        branch = \"main\"\n\n    # --- main branch (raw) ---\n    logger.info(\"Loading raw data from branch=%s\", branch)\n    vod_ds = self.load_vod_data(group_name, branch)\n\n    if time_range is not None:\n        vod_ds = vod_ds.sel(epoch=slice(time_range[0], time_range[1]))\n\n    results = {\n        \"final_data\": vod_ds,\n        \"source_branch\": branch,\n        \"pre_filtered\": False,\n        \"filter_type\": \"none\",\n    }\n    logger.info(\"Workflow complete — source=%s\", branch)\n    return results\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.normalize_datetime_for_comparison","level":2,"title":"<code>normalize_datetime_for_comparison(dt)</code>","text":"<p>Coerce date to datetime at midnight for safe comparison.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.normalize_datetime_for_comparison--parameters","level":4,"title":"Parameters","text":"<p>dt : datetime.date | datetime.datetime     Date-like input.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.normalize_datetime_for_comparison--returns","level":4,"title":"Returns","text":"<p>datetime.datetime     Datetime at midnight.</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def normalize_datetime_for_comparison(\n    dt: datetime.date | datetime.datetime,\n) -&gt; datetime.datetime:\n    \"\"\"Coerce date to datetime at midnight for safe comparison.\n\n    Parameters\n    ----------\n    dt : datetime.date | datetime.datetime\n        Date-like input.\n\n    Returns\n    -------\n    datetime.datetime\n        Datetime at midnight.\n\n    \"\"\"\n    if isinstance(dt, datetime.date) and not isinstance(dt, datetime.datetime):\n        return datetime.datetime.combine(dt, datetime.time.min)\n    return dt\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.get_workflow_for_store","level":2,"title":"<code>get_workflow_for_store(vod_store_path)</code>","text":"<p>Create an :class:<code>AdaptedVODWorkflow</code> for the given store path.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.get_workflow_for_store--parameters","level":4,"title":"Parameters","text":"<p>vod_store_path : Path or str     Path to VOD Icechunk store.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.get_workflow_for_store--returns","level":4,"title":"Returns","text":"<p>AdaptedVODWorkflow</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def get_workflow_for_store(vod_store_path: Path | str) -&gt; AdaptedVODWorkflow:\n    \"\"\"Create an :class:`AdaptedVODWorkflow` for the given store path.\n\n    Parameters\n    ----------\n    vod_store_path : Path or str\n        Path to VOD Icechunk store.\n\n    Returns\n    -------\n    AdaptedVODWorkflow\n\n    \"\"\"\n    return AdaptedVODWorkflow(vod_store_path)\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.check_processed_data_status","level":2,"title":"<code>check_processed_data_status(vod_store_path)</code>","text":"<p>Introspect the store for Hampel-filtered data.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.check_processed_data_status--parameters","level":4,"title":"Parameters","text":"<p>vod_store_path : Path or str     Path to VOD Icechunk store.</p>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-grids/#canvod.grids.workflows.adapted_workflow.check_processed_data_status--returns","level":4,"title":"Returns","text":"<p>dict     Keys: <code>has_processing_branch</code>, <code>has_hampel_data</code>,     <code>temporal_coverage</code> (tuple of dates or <code>None</code>),     <code>data_size</code> (dict or <code>None</code>).</p> Source code in <code>packages/canvod-grids/src/canvod/grids/workflows/adapted_workflow.py</code> <pre><code>def check_processed_data_status(vod_store_path: Path | str) -&gt; dict[str, Any]:\n    \"\"\"Introspect the store for Hampel-filtered data.\n\n    Parameters\n    ----------\n    vod_store_path : Path or str\n        Path to VOD Icechunk store.\n\n    Returns\n    -------\n    dict\n        Keys: ``has_processing_branch``, ``has_hampel_data``,\n        ``temporal_coverage`` (tuple of dates or ``None``),\n        ``data_size`` (dict or ``None``).\n\n    \"\"\"\n    workflow = AdaptedVODWorkflow(vod_store_path)\n\n    status: dict[str, Any] = {\n        \"has_processing_branch\": False,\n        \"has_hampel_data\": False,\n        \"temporal_coverage\": None,\n        \"data_size\": None,\n    }\n\n    try:\n        with workflow.vod_store.readonly_session(branch=\"processing\") as session:\n            hampel_ds = xr.open_zarr(\n                session.store,\n                group=\"reference_01_canopy_01_hampel_filtered\",\n                consolidated=False,\n            )\n\n        status[\"has_processing_branch\"] = True\n        status[\"has_hampel_data\"] = True\n        status[\"temporal_coverage\"] = (\n            pd.to_datetime(hampel_ds.epoch.min().values).date(),\n            pd.to_datetime(hampel_ds.epoch.max().values).date(),\n        )\n        status[\"data_size\"] = dict(hampel_ds.sizes)\n\n    except Exception as exc:\n        status[\"error\"] = str(exc)\n\n    return status\n</code></pre>","path":["API Reference","canvod.grids API Reference"],"tags":[]},{"location":"api/canvod-readers/","level":1,"title":"canvod.readers API Reference","text":"<p>RINEX observation file parsing with validation and GNSS signal specifications.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#package","level":2,"title":"Package","text":"<p>GNSS data format readers.</p> <p>This package provides readers for various GNSS data formats, all implementing a common interface for seamless integration with processing pipelines.</p> <p>Supported formats: - RINEX v3.04 (GNSS observations) - More formats coming soon...</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers--quick-start","level":3,"title":"Quick Start","text":"<pre><code>from canvod.readers import Rnxv3Obs\n\n# Read RINEX v3 file\nreader = Rnxv3Obs(fpath=\"station.24o\")\ndataset = reader.to_ds()\n</code></pre> <p>Or use the factory for automatic format detection: <pre><code>from canvod.readers import ReaderFactory\n\n# Auto-detects format\nreader = ReaderFactory.create(\"station.24o\")\ndataset = reader.to_ds()\n</code></pre></p> <p>Directory Matching: <pre><code>from canvod.readers import DataDirMatcher\n\n# Find dates with RINEX files in both receivers\nmatcher = DataDirMatcher(root=Path(\"/data/01_Rosalia\"))\nfor matched_dirs in matcher:\n    print(matched_dirs.yyyydoy)\n    # Load RINEX files from matched_dirs.canopy_data_dir\n</code></pre></p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader","level":2,"title":"<code>GNSSDataReader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all GNSS data format readers.</p> <p>All readers must: 1. Inherit from this class 2. Implement all abstract methods 3. Return xarray.Dataset that passes DatasetStructureValidator 4. Provide file hash for deduplication</p> <p>This ensures compatibility with: - canvod-vod: VOD calculation - canvod-store: MyIcechunkStore storage - canvod-grids: Grid projection operations</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader--examples","level":4,"title":"Examples","text":"<p>class Rnxv3Obs(GNSSDataReader): ...     def to_ds(self, **kwargs) -&gt; xr.Dataset: ...         # Implementation ...         return dataset ... reader = Rnxv3Obs(fpath=\"station.24o\") ds = reader.to_ds() reader.validate_output(ds)  # Automatic validation</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader--notes","level":4,"title":"Notes","text":"<p>This class uses <code>ABC</code> and defines abstract methods and properties for reader implementations.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>class GNSSDataReader(ABC):\n    \"\"\"Abstract base class for all GNSS data format readers.\n\n    All readers must:\n    1. Inherit from this class\n    2. Implement all abstract methods\n    3. Return xarray.Dataset that passes DatasetStructureValidator\n    4. Provide file hash for deduplication\n\n    This ensures compatibility with:\n    - canvod-vod: VOD calculation\n    - canvod-store: MyIcechunkStore storage\n    - canvod-grids: Grid projection operations\n\n    Examples\n    --------\n    &gt;&gt;&gt; class Rnxv3Obs(GNSSDataReader):\n    ...     def to_ds(self, **kwargs) -&gt; xr.Dataset:\n    ...         # Implementation\n    ...         return dataset\n    ...\n    &gt;&gt;&gt; reader = Rnxv3Obs(fpath=\"station.24o\")\n    &gt;&gt;&gt; ds = reader.to_ds()\n    &gt;&gt;&gt; reader.validate_output(ds)  # Automatic validation\n\n    Notes\n    -----\n    This class uses ``ABC`` and defines abstract methods and properties\n    for reader implementations.\n\n    \"\"\"\n\n    # Note: fpath is not @abstractmethod because Pydantic models define it as a field\n    # which provides the same interface\n    fpath: Path\n\n    @property\n    @abstractmethod\n    def file_hash(self) -&gt; str:\n        \"\"\"Return SHA256 hash of file for deduplication.\n\n        Used by MyIcechunkStore to avoid duplicate ingestion.\n        Must be deterministic and reproducible.\n\n        Returns\n        -------\n        str\n            Short hash (16 chars) or full hash of file content\n\n        \"\"\"\n\n    @abstractmethod\n    def to_ds(\n        self,\n        keep_rnx_data_vars: list[str] | None = None,\n        **kwargs: object,\n    ) -&gt; xr.Dataset:\n        \"\"\"Convert data to xarray.Dataset.\n\n        Must return Dataset with structure:\n        - Dims: (epoch, sid)\n        - Coords: epoch, sid, sv, system, band, code, freq_*\n        - Data vars: At minimum SNR, Phase\n        - Attrs: Must include \"RINEX File Hash\"\n\n        Parameters\n        ----------\n        keep_rnx_data_vars : list of str, optional\n            Data variables to include. If None, includes all available.\n        **kwargs\n            Implementation-specific parameters\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset that passes DatasetStructureValidator.\n\n        \"\"\"\n\n    @abstractmethod\n    def iter_epochs(self) -&gt; Iterator[object]:\n        \"\"\"Iterate over epochs in the file.\n\n        Returns\n        -------\n        Generator\n            Generator yielding Epoch objects.\n\n        Yields\n        ------\n        Epoch\n            Parsed epoch with satellites and observations.\n\n        \"\"\"\n\n    def validate_output(\n        self, dataset: xr.Dataset, required_vars: list[str] | None = None\n    ) -&gt; None:\n        \"\"\"Validate output Dataset structure.\n\n        Called automatically by to_ds() to ensure compatibility.\n        Can be called manually for testing.\n\n        Parameters\n        ----------\n        dataset : xr.Dataset\n            Dataset to validate\n        required_vars : list of str, optional\n            Required data variables. If None, uses minimum set.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If Dataset doesn't meet requirements.\n\n        \"\"\"\n        validator = DatasetStructureValidator(dataset=dataset)\n        validator.validate_all(required_vars=required_vars)\n\n    @property\n    @abstractmethod\n    def start_time(self) -&gt; datetime:\n        \"\"\"Return start time of observations.\n\n        Returns\n        -------\n        datetime\n            First observation timestamp in the file.\n\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def end_time(self) -&gt; datetime:\n        \"\"\"Return end time of observations.\n\n        Returns\n        -------\n        datetime\n            Last observation timestamp in the file.\n\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def systems(self) -&gt; list[str]:\n        \"\"\"Return list of GNSS systems in file.\n\n        Returns\n        -------\n        list of str\n            System identifiers: 'G', 'R', 'E', 'C', 'J', 'S', 'I'\n\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def num_epochs(self) -&gt; int:\n        \"\"\"Return number of epochs in file.\n\n        Returns\n        -------\n        int\n            Total number of observation epochs.\n\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def num_satellites(self) -&gt; int:\n        \"\"\"Return total number of unique satellites observed.\n\n        Returns\n        -------\n        int\n            Count of unique satellite vehicles across all systems.\n\n        \"\"\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation.\"\"\"\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"file='{self.fpath.name}', \"\n            f\"systems={self.systems}, \"\n            f\"epochs={self.num_epochs})\"\n        )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.file_hash","level":3,"title":"<code>file_hash</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return SHA256 hash of file for deduplication.</p> <p>Used by MyIcechunkStore to avoid duplicate ingestion. Must be deterministic and reproducible.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.file_hash--returns","level":5,"title":"Returns","text":"<p>str     Short hash (16 chars) or full hash of file content</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.start_time","level":3,"title":"<code>start_time</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return start time of observations.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.start_time--returns","level":5,"title":"Returns","text":"<p>datetime     First observation timestamp in the file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.end_time","level":3,"title":"<code>end_time</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return end time of observations.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.end_time--returns","level":5,"title":"Returns","text":"<p>datetime     Last observation timestamp in the file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.systems","level":3,"title":"<code>systems</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return list of GNSS systems in file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.systems--returns","level":5,"title":"Returns","text":"<p>list of str     System identifiers: 'G', 'R', 'E', 'C', 'J', 'S', 'I'</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.num_epochs","level":3,"title":"<code>num_epochs</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return number of epochs in file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.num_epochs--returns","level":5,"title":"Returns","text":"<p>int     Total number of observation epochs.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.num_satellites","level":3,"title":"<code>num_satellites</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return total number of unique satellites observed.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.num_satellites--returns","level":5,"title":"Returns","text":"<p>int     Count of unique satellite vehicles across all systems.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.to_ds","level":3,"title":"<code>to_ds(keep_rnx_data_vars=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Convert data to xarray.Dataset.</p> <p>Must return Dataset with structure: - Dims: (epoch, sid) - Coords: epoch, sid, sv, system, band, code, freq_* - Data vars: At minimum SNR, Phase - Attrs: Must include \"RINEX File Hash\"</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.to_ds--parameters","level":5,"title":"Parameters","text":"<p>keep_rnx_data_vars : list of str, optional     Data variables to include. If None, includes all available. **kwargs     Implementation-specific parameters</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.to_ds--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset that passes DatasetStructureValidator.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>@abstractmethod\ndef to_ds(\n    self,\n    keep_rnx_data_vars: list[str] | None = None,\n    **kwargs: object,\n) -&gt; xr.Dataset:\n    \"\"\"Convert data to xarray.Dataset.\n\n    Must return Dataset with structure:\n    - Dims: (epoch, sid)\n    - Coords: epoch, sid, sv, system, band, code, freq_*\n    - Data vars: At minimum SNR, Phase\n    - Attrs: Must include \"RINEX File Hash\"\n\n    Parameters\n    ----------\n    keep_rnx_data_vars : list of str, optional\n        Data variables to include. If None, includes all available.\n    **kwargs\n        Implementation-specific parameters\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset that passes DatasetStructureValidator.\n\n    \"\"\"\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.iter_epochs","level":3,"title":"<code>iter_epochs()</code>  <code>abstractmethod</code>","text":"<p>Iterate over epochs in the file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.iter_epochs--returns","level":5,"title":"Returns","text":"<p>Generator     Generator yielding Epoch objects.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.iter_epochs--yields","level":5,"title":"Yields","text":"<p>Epoch     Parsed epoch with satellites and observations.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>@abstractmethod\ndef iter_epochs(self) -&gt; Iterator[object]:\n    \"\"\"Iterate over epochs in the file.\n\n    Returns\n    -------\n    Generator\n        Generator yielding Epoch objects.\n\n    Yields\n    ------\n    Epoch\n        Parsed epoch with satellites and observations.\n\n    \"\"\"\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.validate_output","level":3,"title":"<code>validate_output(dataset, required_vars=None)</code>","text":"<p>Validate output Dataset structure.</p> <p>Called automatically by to_ds() to ensure compatibility. Can be called manually for testing.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.validate_output--parameters","level":5,"title":"Parameters","text":"<p>dataset : xr.Dataset     Dataset to validate required_vars : list of str, optional     Required data variables. If None, uses minimum set.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.validate_output--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.validate_output--raises","level":5,"title":"Raises","text":"<p>ValueError     If Dataset doesn't meet requirements.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_output(\n    self, dataset: xr.Dataset, required_vars: list[str] | None = None\n) -&gt; None:\n    \"\"\"Validate output Dataset structure.\n\n    Called automatically by to_ds() to ensure compatibility.\n    Can be called manually for testing.\n\n    Parameters\n    ----------\n    dataset : xr.Dataset\n        Dataset to validate\n    required_vars : list of str, optional\n        Required data variables. If None, uses minimum set.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If Dataset doesn't meet requirements.\n\n    \"\"\"\n    validator = DatasetStructureValidator(dataset=dataset)\n    validator.validate_all(required_vars=required_vars)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.GNSSDataReader.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return the string representation.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation.\"\"\"\n    return (\n        f\"{self.__class__.__name__}(\"\n        f\"file='{self.fpath.name}', \"\n        f\"systems={self.systems}, \"\n        f\"epochs={self.num_epochs})\"\n    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs","level":2,"title":"<code>Rnxv3Obs</code>","text":"<p>               Bases: <code>GNSSDataReader</code>, <code>BaseModel</code></p> <p>RINEX v3.04 observation reader.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs--attributes","level":4,"title":"Attributes","text":"<p>fpath : Path     Path to the RINEX observation file. polarization : str, default \"RHCP\"     Polarization label for observables. completeness_mode : {\"strict\", \"warn\", \"off\"}, default \"strict\"     Behavior when epoch completeness checks fail. expected_dump_interval : str or pint.Quantity, optional     Expected file dump interval for completeness validation. expected_sampling_interval : str or pint.Quantity, optional     Expected sampling interval for completeness validation. include_auxiliary : bool, default False     Whether to include auxiliary observations (e.g., X1). apply_overlap_filter : bool, default False     Whether to filter overlapping signal groups. overlap_preferences : dict[str, str], optional     Preferred signals for overlap resolution. aggregate_glonass_fdma : bool, optional     Whether to aggregate GLONASS FDMA channels.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs--notes","level":4,"title":"Notes","text":"<p>This class inherits from <code>GNSSDataReader</code> and is a Pydantic <code>BaseModel</code> configured with <code>ConfigDict</code> (frozen, arbitrary_types_allowed).</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>class Rnxv3Obs(GNSSDataReader, BaseModel):\n    \"\"\"RINEX v3.04 observation reader.\n\n    Attributes\n    ----------\n    fpath : Path\n        Path to the RINEX observation file.\n    polarization : str, default \"RHCP\"\n        Polarization label for observables.\n    completeness_mode : {\"strict\", \"warn\", \"off\"}, default \"strict\"\n        Behavior when epoch completeness checks fail.\n    expected_dump_interval : str or pint.Quantity, optional\n        Expected file dump interval for completeness validation.\n    expected_sampling_interval : str or pint.Quantity, optional\n        Expected sampling interval for completeness validation.\n    include_auxiliary : bool, default False\n        Whether to include auxiliary observations (e.g., X1).\n    apply_overlap_filter : bool, default False\n        Whether to filter overlapping signal groups.\n    overlap_preferences : dict[str, str], optional\n        Preferred signals for overlap resolution.\n    aggregate_glonass_fdma : bool, optional\n        Whether to aggregate GLONASS FDMA channels.\n\n    Notes\n    -----\n    This class inherits from `GNSSDataReader` and is a Pydantic `BaseModel`\n    configured with `ConfigDict` (frozen, arbitrary_types_allowed).\n\n    \"\"\"\n\n    fpath: Path\n    polarization: str = \"RHCP\"\n\n    completeness_mode: Literal[\"strict\", \"warn\", \"off\"] = \"strict\"\n    expected_dump_interval: str | pint.Quantity | None = None\n    expected_sampling_interval: str | pint.Quantity | None = None\n\n    include_auxiliary: bool = False\n    apply_overlap_filter: bool = False\n    overlap_preferences: dict[str, str] | None = None\n\n    aggregate_glonass_fdma: bool = True\n\n    _header: Rnxv3Header = PrivateAttr()\n    _signal_mapper: \"SignalIDMapper\" = PrivateAttr()\n\n    _lines: list[str] = PrivateAttr()\n    _file_hash: str = PrivateAttr()\n\n    model_config = ConfigDict(\n        frozen=True,\n        arbitrary_types_allowed=True,\n    )\n\n    @model_validator(mode=\"after\")\n    def _post_init(self) -&gt; Self:\n        \"\"\"Initialize derived state after validation.\"\"\"\n        # Load header once\n        self._header = Rnxv3Header.from_file(self.fpath)\n\n        # Initialize signal mapper\n        self._signal_mapper = SignalIDMapper(\n            aggregate_glonass_fdma=self.aggregate_glonass_fdma\n        )\n\n        # Optionally auto-check completeness\n        if self.completeness_mode != \"off\":\n            try:\n                self.validate_epoch_completeness(\n                    dump_interval=self.expected_dump_interval,\n                    sampling_interval=self.expected_sampling_interval,\n                )\n            except MissingEpochError as e:\n                if self.completeness_mode == \"strict\":\n                    raise\n                warnings.warn(str(e), RuntimeWarning, stacklevel=2)\n\n        # Cache file lines\n        self._lines = self._load_file()\n\n        return self\n\n    @property\n    def header(self) -&gt; Rnxv3Header:\n        \"\"\"Expose validated header (read-only).\n\n        Returns\n        -------\n        Rnxv3Header\n            Parsed and validated RINEX header.\n\n        \"\"\"\n        return self._header\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable summary.\"\"\"\n        return (\n            f\"{self.__class__.__name__}:\\n\"\n            f\"  File Path: {self.fpath}\\n\"\n            f\"  Header: {self.header}\\n\"\n            f\"  Polarization: {self.polarization}\\n\"\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a concise representation for debugging.\"\"\"\n        return f\"{self.__class__.__name__}(fpath={self.fpath})\"\n\n    def _load_file(self) -&gt; list[str]:\n        \"\"\"Read file once, cache lines, and compute hash.\n\n        Returns\n        -------\n        list[str]\n            File contents split into lines.\n\n        \"\"\"\n        if not hasattr(self, \"_lines\"):\n            h = hashlib.sha256()\n            with self.fpath.open(\"rb\") as f:  # binary mode for consistent hash\n                data = f.read()\n                h.update(data)\n                self._lines = data.decode(\"utf-8\", errors=\"replace\").splitlines()\n            self._file_hash = h.hexdigest()[:16]  # short hash for storage\n        return self._lines\n\n    @property\n    def file_hash(self) -&gt; str:\n        \"\"\"Return cached SHA256 short hash of the file content.\n\n        Returns\n        -------\n        str\n            16-character short hash for deduplication.\n\n        \"\"\"\n        return self._file_hash\n\n    @property\n    def start_time(self) -&gt; datetime:\n        \"\"\"Return start time of observations from header.\n\n        Returns\n        -------\n        datetime\n            First observation timestamp.\n\n        \"\"\"\n        return min(self.header.t0.values())\n\n    @property\n    def end_time(self) -&gt; datetime:\n        \"\"\"Return end time of observations from last epoch.\n\n        Returns\n        -------\n        datetime\n            Last observation timestamp.\n\n        \"\"\"\n        last_epoch = None\n        for epoch in self.iter_epochs():\n            last_epoch = epoch\n        if last_epoch:\n            return self.get_datetime_from_epoch_record_info(last_epoch.info)\n        return self.start_time\n\n    @property\n    def systems(self) -&gt; list[str]:\n        \"\"\"Return list of GNSS systems in file.\n\n        Returns\n        -------\n        list of str\n            System identifiers (G, R, E, C, J, S, I).\n\n        \"\"\"\n        if self.header.systems == \"M\":\n            return list(self.header.obs_codes_per_system.keys())\n        return [self.header.systems]\n\n    @property\n    def num_epochs(self) -&gt; int:\n        \"\"\"Return number of epochs in file.\n\n        Returns\n        -------\n        int\n            Total epoch count.\n\n        \"\"\"\n        return len(list(self.get_epoch_record_batches()))\n\n    @property\n    def num_satellites(self) -&gt; int:\n        \"\"\"Return total number of unique satellites observed.\n\n        Returns\n        -------\n        int\n            Count of unique satellite vehicles across all systems.\n\n        \"\"\"\n        satellites = set()\n        for epoch in self.iter_epochs():\n            for sat in epoch.data:\n                satellites.add(sat.sv)\n        return len(satellites)\n\n    def get_epoch_record_batches(\n        self, epoch_record_indicator: str = EPOCH_RECORD_INDICATOR\n    ) -&gt; list[tuple[int, int]]:\n        \"\"\"Get the start and end line numbers for each epoch in the file.\n\n        Parameters\n        ----------\n        epoch_record_indicator : str, default '&gt;'\n            Character marking epoch record lines.\n\n        Returns\n        -------\n        list of tuple of int\n            List of (start_line, end_line) pairs for each epoch.\n\n        \"\"\"\n        starts = [\n            i\n            for i, line in enumerate(self._load_file())\n            if line.startswith(epoch_record_indicator)\n        ]\n        starts.append(len(self._load_file()))  # Add EOF\n        return [\n            (start, starts[i + 1])\n            for i, start in enumerate(starts)\n            if i + 1 &lt; len(starts)\n        ]\n\n    def parse_observation_slice(\n        self,\n        slice_text: str,\n    ) -&gt; tuple[float | None, int | None, int | None]:\n        \"\"\"Parse a RINEX observation slice into value, LLI, and SSI.\n\n        Enhanced to handle both standard 16-character format and\n        variable-length records.\n\n        Parameters\n        ----------\n        slice_text : str\n            Observation slice to parse.\n\n        Returns\n        -------\n        tuple[float | None, int | None, int | None]\n            Parsed (value, LLI, SSI) tuple.\n\n        \"\"\"\n        if not slice_text or not slice_text.strip():\n            return None, None, None\n\n        try:\n            # Method 1: Standard RINEX format with decimal at position -6\n            if (\n                len(slice_text) &gt;= OBS_SLICE_MIN_LEN\n                and len(slice_text) &lt;= OBS_SLICE_MAX_LEN\n                and slice_text[OBS_SLICE_DECIMAL_POS] == \".\"\n            ):\n                slice_chars = list(slice_text)\n                ssi = slice_chars.pop(-1) if len(slice_chars) &gt; 0 else \"\"\n                lli = slice_chars.pop(-1) if len(slice_chars) &gt; 0 else \"\"\n\n                # Convert LLI and SSI\n                lli = int(lli) if lli.strip() and lli.isdigit() else None\n                ssi = int(ssi) if ssi.strip() and ssi.isdigit() else None\n\n                # Convert value\n                value_str = \"\".join(slice_chars).strip()\n                if value_str:\n                    value = float(value_str)\n                    return value, lli, ssi\n\n        except (ValueError, IndexError):\n            pass\n\n        try:\n            # Method 2: Flexible parsing for variable-length records\n            slice_trimmed = slice_text.strip()\n            if not slice_trimmed:\n                return None, None, None\n\n            # Look for a decimal point to identify the numeric value\n            if \".\" in slice_trimmed:\n                # Find the main numeric value (supports negative numbers)\n                number_match = re.search(r\"(-?\\d+\\.\\d+)\", slice_trimmed)\n\n                if number_match:\n                    value = float(number_match.group(1))\n\n                    # Check for LLI/SSI indicators after the number\n                    remaining_part = slice_trimmed[number_match.end() :].strip()\n                    lli = None\n                    ssi = None\n\n                    # Parse remaining characters as potential LLI/SSI\n                    if remaining_part:\n                        # Could be just SSI, or LLI followed by SSI\n                        if len(remaining_part) == 1:\n                            # Just one indicator - assume it's SSI\n                            if remaining_part.isdigit():\n                                ssi = int(remaining_part)\n                        elif len(remaining_part) &gt;= LLI_SSI_PAIR_LEN:\n                            # Two or more characters - take last two as LLI, SSI\n                            lli_char = remaining_part[-2]\n                            ssi_char = remaining_part[-1]\n\n                            if lli_char.isdigit():\n                                lli = int(lli_char)\n                            if ssi_char.isdigit():\n                                ssi = int(ssi_char)\n\n                    return value, lli, ssi\n\n        except (ValueError, IndexError):\n            pass\n\n        # Method 3: Last resort - try simple float parsing\n        try:\n            simple_value = float(slice_text.strip())\n            return simple_value, None, None\n        except ValueError:\n            pass\n\n        return None, None, None\n\n    def process_satellite_data(self, s: str) -&gt; Satellite:\n        \"\"\"Process satellite data line into a Satellite object with observations.\n\n        Handles variable-length observation records correctly by adaptively parsing\n        based on the actual line length and content.\n        \"\"\"\n        sv = s[:3].strip()\n        satellite = Satellite(sv=sv)\n        bands_tbe = [f\"{sv}|{b}\" for b in self.header.obs_codes_per_system[sv[0]]]\n\n        # Get the data part (after sv identifier)\n        data_part = s[3:]\n\n        # Process each observation adaptively\n        for i, band in enumerate(bands_tbe):\n            start_idx = i * 16\n            end_idx = start_idx + 16\n\n            # Check if we have enough data for this observation\n            if start_idx &gt;= len(data_part):\n                # No more data available - create empty observation\n                observation = Observation(\n                    observation_freq_tag=band,\n                    obs_type=band.split(\"|\")[1][0],\n                    value=None,\n                    lli=None,\n                    ssi=None,\n                )\n                satellite.add_observation(observation)\n                continue\n\n            # Extract the slice, but handle variable length\n            if end_idx &lt;= len(data_part):\n                # Full 16-character slice available\n                slice_data = data_part[start_idx:end_idx]\n            else:\n                # Partial slice - pad with spaces to maintain consistency\n                available_slice = data_part[start_idx:]\n                slice_data = available_slice.ljust(16)  # Pad with spaces if needed\n\n            value, lli, ssi = self.parse_observation_slice(slice_data)\n\n            observation = Observation(\n                observation_freq_tag=band,\n                obs_type=band.split(\"|\")[1][0],\n                value=value,\n                lli=lli,\n                ssi=ssi,\n            )\n            satellite.add_observation(observation)\n\n        return satellite\n\n    @property\n    def epochs(self) -&gt; list[Rnxv3ObsEpochRecord]:\n        \"\"\"Materialize all epochs (legacy compatibility).\n\n        Returns\n        -------\n        list of Rnxv3ObsEpochRecord\n            All epochs in memory (use iter_epochs for efficiency)\n\n        \"\"\"\n        return list(self.iter_epochs())\n\n    def iter_epochs(self) -&gt; Iterable[Rnxv3ObsEpochRecord]:\n        \"\"\"Yield epochs one by one instead of materializing the whole list.\n\n        Returns\n        -------\n        Generator\n            Generator yielding Rnxv3ObsEpochRecord objects\n\n        Yields\n        ------\n        Rnxv3ObsEpochRecord\n            Each epoch with timestamp and satellite observations\n\n        \"\"\"\n        for start, end in self.get_epoch_record_batches():\n            try:\n                info = Rnxv3ObsEpochRecordLineModel(epoch=self._lines[start])\n                data = self._lines[start + 1 : end]\n                epoch = Rnxv3ObsEpochRecord(\n                    info=info,\n                    data=(\n                        self.process_satellite_data(line) for line in data\n                    ),  # generator here too\n                )\n                yield epoch\n            except (InvalidEpochError, IncompleteEpochError):\n                # Skip unexpected errors silently\n                pass\n\n    def iter_epochs_in_range(\n        self,\n        start: datetime,\n        end: datetime,\n    ) -&gt; Iterable[Rnxv3ObsEpochRecord]:\n        \"\"\"Yield epochs lazily that fall into the given datetime range.\n\n        Parameters\n        ----------\n        start : datetime\n            Start of time range (inclusive)\n        end : datetime\n            End of time range (inclusive)\n\n        Returns\n        -------\n        Generator\n            Generator yielding epochs in the specified range\n\n        Yields\n        ------\n        Rnxv3ObsEpochRecord\n            Epochs within the time range\n\n        \"\"\"\n        for epoch in self.iter_epochs():\n            dt = self.get_datetime_from_epoch_record_info(epoch.info)\n            if start &lt;= dt &lt;= end:\n                yield epoch\n\n    def get_datetime_from_epoch_record_info(\n        self,\n        epoch_record_info: Rnxv3ObsEpochRecordLineModel,\n    ) -&gt; datetime:\n        \"\"\"Convert epoch record info to datetime object.\n\n        Parameters\n        ----------\n        epoch_record_info : Rnxv3ObsEpochRecordLineModel\n            Parsed epoch record line\n\n        Returns\n        -------\n        datetime\n            Timestamp from epoch record\n\n        \"\"\"\n        return datetime(\n            year=int(epoch_record_info.year),\n            month=int(epoch_record_info.month),\n            day=int(epoch_record_info.day),\n            hour=int(epoch_record_info.hour),\n            minute=int(epoch_record_info.minute),\n            second=int(epoch_record_info.seconds),\n            tzinfo=timezone.utc,\n        )\n\n    @staticmethod\n    def epochrecordinfo_dt_to_numpy_dt(\n        epch: Rnxv3ObsEpochRecord,\n    ) -&gt; np.datetime64:\n        \"\"\"Convert Python datetime to numpy datetime64[ns].\n\n        Parameters\n        ----------\n        epch : Rnxv3ObsEpochRecord\n            Epoch record containing timestamp info\n\n        Returns\n        -------\n        np.datetime64\n            Numpy datetime64 with nanosecond precision\n\n        \"\"\"\n        dt = datetime(\n            year=int(epch.info.year),\n            month=int(epch.info.month),\n            day=int(epch.info.day),\n            hour=int(epch.info.hour),\n            minute=int(epch.info.minute),\n            second=int(epch.info.seconds),\n            tzinfo=timezone.utc,\n        )\n        # np.datetime64 doesn't support timezone info, but datetime is already UTC\n        # Convert to naive datetime (UTC) to avoid warning\n        return np.datetime64(dt.replace(tzinfo=None), \"ns\")\n\n    def _epoch_datetimes(self) -&gt; list[datetime]:\n        \"\"\"Extract epoch datetimes from the file.\n\n        Uses the same epoch parsing logic already implemented.\n        \"\"\"\n        dts: list[datetime] = []\n\n        for start, _end in self.get_epoch_record_batches():\n            info = Rnxv3ObsEpochRecordLineModel(epoch=self._lines[start])\n            dts.append(\n                datetime(\n                    year=int(info.year),\n                    month=int(info.month),\n                    day=int(info.day),\n                    hour=int(info.hour),\n                    minute=int(info.minute),\n                    second=int(info.seconds),\n                    tzinfo=timezone.utc,\n                )\n            )\n        return dts\n\n    def infer_sampling_interval(self) -&gt; pint.Quantity | None:\n        \"\"\"Infer sampling interval from consecutive epoch deltas.\n\n        Returns\n        -------\n        pint.Quantity or None\n            Sampling interval in seconds, or None if cannot be inferred\n\n        \"\"\"\n        dts = self._epoch_datetimes()\n        if len(dts) &lt; MIN_EPOCHS_FOR_INTERVAL:\n            return None\n        # Compute deltas\n        deltas: list[timedelta] = [b - a for a, b in pairwise(dts) if b &gt;= a]\n        if not deltas:\n            return None\n        # Pick the most common delta (robust to an occasional missing epoch)\n        seconds = Counter(\n            int(dt.total_seconds()) for dt in deltas if dt.total_seconds() &gt; 0\n        )\n        if not seconds:\n            return None\n        mode_seconds, _ = seconds.most_common(1)[0]\n        return (mode_seconds * UREG.second).to(UREG.seconds)\n\n    def infer_dump_interval(\n        self, sampling_interval: pint.Quantity | None = None\n    ) -&gt; pint.Quantity | None:\n        \"\"\"Infer the intended dump interval for the RINEX file.\n\n        Parameters\n        ----------\n        sampling_interval : pint.Quantity, optional\n            Known sampling interval. If provided, returns (#epochs * sampling_interval)\n\n        Returns\n        -------\n        pint.Quantity or None\n            Dump interval in seconds, or None if cannot be inferred\n\n        \"\"\"\n        idx = self.get_epoch_record_batches()\n        n_epochs = len(idx)\n        if n_epochs == 0:\n            return None\n\n        if sampling_interval is not None:\n            return (n_epochs * sampling_interval).to(UREG.seconds)\n\n        # Fallback: time coverage inclusive (last - first) + typical step\n        dts = self._epoch_datetimes()\n        if len(dts) == 0:\n            return None\n        if len(dts) == 1:\n            # single epoch: treat as 1 * unknown step (cannot infer)\n            return None\n\n        # Estimate step from data\n        est_step = self.infer_sampling_interval()\n        if est_step is None:\n            return None\n\n        # Inclusive coverage often equals (n_epochs - 1) * step; intended\n        # dump interval is n_epochs * step.\n        return (n_epochs * est_step.to(UREG.seconds)).to(UREG.seconds)\n\n    def validate_epoch_completeness(\n        self,\n        dump_interval: str | pint.Quantity | None = None,\n        sampling_interval: str | pint.Quantity | None = None,\n    ) -&gt; None:\n        \"\"\"Validate that the number of epochs matches the expected dump interval.\n\n        Parameters\n        ----------\n        dump_interval : str or pint.Quantity, optional\n            Expected file dump interval. If None, inferred from epochs.\n        sampling_interval : str or pint.Quantity, optional\n            Expected sampling interval. If None, inferred from epochs.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        MissingEpochError\n            If total sampling time doesn't match dump interval\n        ValueError\n            If intervals cannot be inferred\n\n        \"\"\"\n        # Normalize/Infer sampling interval\n        if sampling_interval is None:\n            inferred = self.infer_sampling_interval()\n            if inferred is None:\n                msg = \"Could not infer sampling interval from epochs\"\n                raise ValueError(msg)\n            sampling_interval = inferred\n        # normalize to pint\n        elif not isinstance(sampling_interval, pint.Quantity):\n            sampling_interval = UREG.Quantity(sampling_interval).to(UREG.seconds)\n\n        # Normalize/Infer dump interval\n        if dump_interval is None:\n            inferred_dump = self.infer_dump_interval(\n                sampling_interval=sampling_interval\n            )\n            if inferred_dump is None:\n                msg = \"Could not infer dump interval from file\"\n                raise ValueError(msg)\n            dump_interval = inferred_dump\n        elif not isinstance(dump_interval, pint.Quantity):\n            # Accept '15 min', '1h', etc.\n            dump_interval = UREG.Quantity(dump_interval).to(UREG.seconds)\n\n        # Build inputs for the validator model\n        epoch_indices = self.get_epoch_record_batches()\n\n        # This throws MissingEpochError automatically if inconsistent\n        Rnxv3ObsEpochRecordCompletenessModel(\n            epoch_records_indeces=epoch_indices,\n            rnx_file_dump_interval=dump_interval,\n            sampling_interval=sampling_interval,\n        )\n\n    def filter_by_overlapping_groups(\n        self,\n        ds: xr.Dataset,\n        group_preference: dict[str, str] | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"Filter overlapping bands using per-group preferences.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset with `sid` dimension and signal properties.\n        group_preference : dict[str, str], optional\n            Mapping of overlap group to preferred band.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset filtered to preferred overlapping bands.\n\n        \"\"\"\n        if group_preference is None:\n            group_preference = {\n                \"L1_E1_B1I\": \"L1\",\n                \"L5_E5a\": \"L5\",\n                \"L2_E5b_B2b\": \"L2\",\n            }\n\n        keep = []\n        for sid in ds.sid.values:\n            _sv, band, _code = self._signal_mapper.parse_signal_id(str(sid))\n            group = self._signal_mapper.get_overlapping_group(band)\n            if group and group in group_preference:\n                if band == group_preference[group]:\n                    keep.append(sid)\n            else:\n                keep.append(sid)\n        return ds.sel(sid=keep)\n\n    def create_rinex_netcdf_with_signal_id(\n        self,\n        analyze_conflicts: bool = False,\n        analyze_systems: bool = False,\n        start: datetime | None = None,\n        end: datetime | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"Create a NetCDF dataset with signal IDs.\n\n        Can optionally restrict to epochs within a datetime range.\n\n        \"\"\"\n        if analyze_conflicts:\n            print(\"\\nNote: Conflict analysis will be adapted for sid structure.\")\n        if analyze_systems:\n            print(\"\\nNote: System analysis will be adapted for sid structure.\")\n\n        signal_ids = set()\n        signal_id_to_properties: dict[str, dict[str, object]] = {}\n        timestamps: list[np.datetime64] = []\n\n        # pick generator depending on range\n        if start and end:\n            epoch_iter = self.iter_epochs_in_range(start, end)\n        else:\n            epoch_iter = self.iter_epochs()\n\n        for epoch in epoch_iter:\n            dt = self.epochrecordinfo_dt_to_numpy_dt(epoch)\n            timestamps.append(np.datetime64(dt, \"ns\"))\n\n            for sat in epoch.data:\n                sv = sat.sv\n                for obs in sat.observations:\n                    if not self.include_auxiliary and obs.observation_freq_tag.endswith(\n                        \"|X1\"\n                    ):\n                        continue\n\n                    sid = self._signal_mapper.create_signal_id(\n                        sv, obs.observation_freq_tag\n                    )\n                    signal_ids.add(sid)\n\n                    if sid not in signal_id_to_properties:\n                        sv_part, band, code = self._signal_mapper.parse_signal_id(sid)\n                        system = sv_part[0]\n                        center_frequency = self._signal_mapper.get_band_frequency(band)\n                        bandwidth = self._signal_mapper.get_band_bandwidth(band)\n                        overlapping_group = self._signal_mapper.get_overlapping_group(\n                            band\n                        )\n\n                        if center_frequency is not None and bandwidth is not None:\n                            # Extract bandwidth value\n                            bw = (\n                                bandwidth[0]\n                                if isinstance(bandwidth, list)\n                                else bandwidth\n                            )\n\n                            # Ensure both are pint quantities\n                            if not hasattr(center_frequency, \"m_as\"):\n                                center_frequency = center_frequency * UREG.MHz\n                            if not hasattr(bw, \"m_as\"):\n                                bw = bw * UREG.MHz\n\n                            # Calculate frequency range\n                            freq_min = center_frequency - (bw / 2.0)\n                            freq_max = center_frequency + (bw / 2.0)\n\n                            # Extract magnitudes to ensure float64 dtype\n                            center_frequency = float(center_frequency.m_as(UREG.MHz))\n                            freq_min = float(freq_min.m_as(UREG.MHz))\n                            freq_max = float(freq_max.m_as(UREG.MHz))\n                            bw = float(bw.m_as(UREG.MHz))\n                        else:\n                            print(\n                                f\"WARNING: No frequency data for sid={sid}, \"\n                                f\"band={band}, sv={sv_part}\"\n                            )\n                            center_frequency = np.nan\n                            freq_min = np.nan\n                            freq_max = np.nan\n                            bw = np.nan\n\n                        signal_id_to_properties[sid] = {\n                            \"sv\": sv_part,\n                            \"system\": system,\n                            \"band\": band,\n                            \"code\": code,\n                            \"freq_center\": center_frequency,\n                            \"freq_min\": freq_min,\n                            \"freq_max\": freq_max,\n                            \"bandwidth\": bw,\n                            \"overlapping_group\": overlapping_group,\n                        }\n\n        # Inconsistent integration of the Septentrio X1 obs. code, filtering\n        # out here again.\n        signal_ids = {sid for sid in signal_ids if \"|X1|\" not in sid}\n        signal_id_to_properties = {\n            sid: props\n            for sid, props in signal_id_to_properties.items()\n            if \"|X1|\" not in sid\n        }\n\n        sorted_signal_ids = sorted(signal_ids)\n        n_epochs = len(timestamps)\n        n_signals = len(sorted_signal_ids)\n\n        data_arrays = {\n            \"SNR\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"SNR\"]),\n            \"Pseudorange\": np.full(\n                (n_epochs, n_signals), np.nan, dtype=DTYPES[\"Pseudorange\"]\n            ),\n            \"Phase\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"Phase\"]),\n            \"Doppler\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"Doppler\"]),\n            \"LLI\": np.full((n_epochs, n_signals), -1, dtype=DTYPES[\"LLI\"]),\n            \"SSI\": np.full((n_epochs, n_signals), -1, dtype=DTYPES[\"SSI\"]),\n        }\n        sid_to_idx = {sid: i for i, sid in enumerate(sorted_signal_ids)}\n\n        # second pass to fill arrays\n        if start and end:\n            epoch_iter = self.iter_epochs_in_range(start, end)\n        else:\n            epoch_iter = self.iter_epochs()\n\n        for t_idx, epoch in enumerate(epoch_iter):\n            for sat in epoch.data:\n                sv = sat.sv\n                for obs in sat.observations:\n                    if not self.include_auxiliary and obs.observation_freq_tag.endswith(\n                        \"|X1\"\n                    ):\n                        continue\n                    if obs.value is None:\n                        continue\n                    sid = self._signal_mapper.create_signal_id(\n                        sv, obs.observation_freq_tag\n                    )\n                    if sid not in sid_to_idx:\n                        continue\n                    s_idx = sid_to_idx[sid]\n\n                    ot = obs.obs_type\n                    if ot == \"S\" and obs.value != 0:\n                        data_arrays[\"SNR\"][t_idx, s_idx] = obs.value\n                    elif ot == \"C\":\n                        data_arrays[\"Pseudorange\"][t_idx, s_idx] = obs.value\n                    elif ot == \"L\":\n                        data_arrays[\"Phase\"][t_idx, s_idx] = obs.value\n                    elif ot == \"D\":\n                        data_arrays[\"Doppler\"][t_idx, s_idx] = obs.value\n                    elif ot == \"X\":\n                        data_arrays.setdefault(\n                            \"Auxiliary\",\n                            np.full((n_epochs, n_signals), np.nan, dtype=np.float32),\n                        )\n                        data_arrays[\"Auxiliary\"][t_idx, s_idx] = obs.value\n\n                    if obs.lli is not None:\n                        data_arrays[\"LLI\"][t_idx, s_idx] = obs.lli\n                    if obs.ssi is not None:\n                        data_arrays[\"SSI\"][t_idx, s_idx] = obs.ssi\n\n        signal_id_coord = xr.DataArray(\n            sorted_signal_ids, dims=[\"sid\"], attrs=COORDS_METADATA[\"sid\"]\n        )\n        sv_list = [signal_id_to_properties[sid][\"sv\"] for sid in sorted_signal_ids]\n        constellation_list = [\n            signal_id_to_properties[sid][\"system\"] for sid in sorted_signal_ids\n        ]\n        band_list = [signal_id_to_properties[sid][\"band\"] for sid in sorted_signal_ids]\n        code_list = [signal_id_to_properties[sid][\"code\"] for sid in sorted_signal_ids]\n        freq_center_list = [\n            signal_id_to_properties[sid][\"freq_center\"] for sid in sorted_signal_ids\n        ]\n        freq_min_list = [\n            signal_id_to_properties[sid][\"freq_min\"] for sid in sorted_signal_ids\n        ]\n        freq_max_list = [\n            signal_id_to_properties[sid][\"freq_max\"] for sid in sorted_signal_ids\n        ]\n\n        coords = {\n            \"epoch\": (\"epoch\", timestamps, COORDS_METADATA[\"epoch\"]),\n            \"sid\": signal_id_coord,\n            \"sv\": (\"sid\", sv_list, COORDS_METADATA[\"sv\"]),\n            \"system\": (\"sid\", constellation_list, COORDS_METADATA[\"system\"]),\n            \"band\": (\"sid\", band_list, COORDS_METADATA[\"band\"]),\n            \"code\": (\"sid\", code_list, COORDS_METADATA[\"code\"]),\n            \"freq_center\": (\n                \"sid\",\n                np.asarray(freq_center_list, dtype=DTYPES[\"freq_center\"]),\n                COORDS_METADATA[\"freq_center\"],\n            ),\n            \"freq_min\": (\n                \"sid\",\n                np.asarray(freq_min_list, dtype=DTYPES[\"freq_min\"]),\n                COORDS_METADATA[\"freq_min\"],\n            ),\n            \"freq_max\": (\n                \"sid\",\n                np.asarray(freq_max_list, dtype=DTYPES[\"freq_max\"]),\n                COORDS_METADATA[\"freq_max\"],\n            ),\n        }\n\n        if self.header.signal_strength_unit == UREG.dBHz:\n            snr_meta = CN0_METADATA\n        else:\n            snr_meta = SNR_METADATA\n\n        ds = xr.Dataset(\n            data_vars={\n                \"SNR\": ([\"epoch\", \"sid\"], data_arrays[\"SNR\"], snr_meta),\n                \"Pseudorange\": (\n                    [\"epoch\", \"sid\"],\n                    data_arrays[\"Pseudorange\"],\n                    OBSERVABLES_METADATA[\"Pseudorange\"],\n                ),\n                \"Phase\": (\n                    [\"epoch\", \"sid\"],\n                    data_arrays[\"Phase\"],\n                    OBSERVABLES_METADATA[\"Phase\"],\n                ),\n                \"Doppler\": (\n                    [\"epoch\", \"sid\"],\n                    data_arrays[\"Doppler\"],\n                    OBSERVABLES_METADATA[\"Doppler\"],\n                ),\n                \"LLI\": (\n                    [\"epoch\", \"sid\"],\n                    data_arrays[\"LLI\"],\n                    OBSERVABLES_METADATA[\"LLI\"],\n                ),\n                \"SSI\": (\n                    [\"epoch\", \"sid\"],\n                    data_arrays[\"SSI\"],\n                    OBSERVABLES_METADATA[\"SSI\"],\n                ),\n            },\n            coords=coords,\n            attrs={**self._create_basic_attrs()},\n        )\n\n        if \"Auxiliary\" in data_arrays:\n            ds[\"Auxiliary\"] = (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"Auxiliary\"],\n                OBSERVABLES_METADATA[\"Auxiliary\"],\n            )\n\n        if self.apply_overlap_filter:\n            ds = self.filter_by_overlapping_groups(ds, self.overlap_preferences)\n\n        return ds\n\n    def to_ds(\n        self,\n        outname: Path | str | None = None,\n        keep_rnx_data_vars: list[str] | None = None,\n        write_global_attrs: bool = False,\n        pad_global_sid: bool = True,\n        strip_fillval: bool = True,\n        add_future_datavars: bool = True,\n        keep_sids: list[str] | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"Convert RINEX observations to xarray.Dataset with signal ID structure.\n\n        Parameters\n        ----------\n        outname : Path or str, optional\n            If provided, saves dataset to this file path\n        keep_rnx_data_vars : list of str or None, optional\n            Data variables to include in dataset. Defaults to config value.\n        write_global_attrs : bool, default False\n            If True, adds comprehensive global attributes\n        pad_global_sid : bool, default True\n            If True, pads to global signal ID space\n        strip_fillval : bool, default True\n            If True, removes fill values\n        add_future_datavars : bool, default True\n            If True, adds placeholder variables for future data\n        keep_sids : list of str or None, default None\n            If provided, filters/pads dataset to these specific SIDs.\n            If None and pad_global_sid=True, pads to all possible SIDs.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with dimensions (epoch, sid) and requested data variables\n\n        \"\"\"\n        if keep_rnx_data_vars is None:\n            from canvod.utils.config import load_config\n\n            keep_rnx_data_vars = load_config().processing.processing.keep_rnx_vars\n\n        ds = self.create_rinex_netcdf_with_signal_id()\n\n        # drop unwanted vars\n        for var in list(ds.data_vars):\n            if var not in keep_rnx_data_vars:\n                ds = ds.drop_vars(var)\n\n        if pad_global_sid:\n            from canvod.auxiliary.preprocessing import pad_to_global_sid\n\n            # Pad/filter to specified sids or all possible sids\n            ds = pad_to_global_sid(ds, keep_sids=keep_sids)\n\n        if strip_fillval:\n            from canvod.auxiliary.preprocessing import strip_fillvalue\n\n            ds = strip_fillvalue(ds)\n\n        if add_future_datavars:\n            pass\n\n        if write_global_attrs:\n            ds.attrs.update(self._create_comprehensive_attrs())\n\n        ds.attrs[\"RINEX File Hash\"] = self.file_hash\n\n        if outname:\n            from canvod.utils.config import load_config as _load_config\n\n            comp = _load_config().processing.compression\n            encoding = {\n                var: {\"zlib\": comp.zlib, \"complevel\": comp.complevel}\n                for var in ds.data_vars\n            }\n            ds.to_netcdf(str(outname), encoding=encoding)\n\n        # Validate output structure for pipeline compatibility\n        self.validate_output(ds, required_vars=keep_rnx_data_vars)\n\n        return ds\n\n    def validate_rinex_304_compliance(\n        self,\n        ds: xr.Dataset | None = None,\n        strict: bool = False,\n        print_report: bool = True,\n    ) -&gt; dict[str, list[str]]:\n        \"\"\"Run enhanced RINEX 3.04 specification validation.\n\n        Validates:\n        1. System-specific observation codes\n        2. GLONASS mandatory fields (slot/frequency, biases)\n        3. Phase shift records (RINEX 3.01+)\n        4. Observation value ranges\n\n        Parameters\n        ----------\n        ds : xr.Dataset, optional\n            Dataset to validate. If None, creates one from current file.\n        strict : bool\n            If True, raise ValueError on validation failures\n        print_report : bool\n            If True, print validation report to console\n\n        Returns\n        -------\n        dict[str, list[str]]\n            Validation results by category\n\n        Examples\n        --------\n        &gt;&gt;&gt; reader = Rnxv3Obs(fpath=\"station.24o\")\n        &gt;&gt;&gt; results = reader.validate_rinex_304_compliance()\n        &gt;&gt;&gt; # Or validate a specific dataset\n        &gt;&gt;&gt; ds = reader.to_ds()\n        &gt;&gt;&gt; results = reader.validate_rinex_304_compliance(ds=ds)\n\n        \"\"\"\n        if ds is None:\n            ds = self.to_ds(write_global_attrs=False)\n\n        # Prepare header dict for validators\n        header_dict = {\n            \"obs_codes_per_system\": self.header.obs_codes_per_system,\n        }\n\n        # Add GLONASS-specific headers if available\n        if hasattr(self.header, \"glonass_slot_frq\"):\n            header_dict[\"GLONASS SLOT / FRQ #\"] = self.header.glonass_slot_frq\n\n        if hasattr(self.header, \"glonass_cod_phs_bis\"):\n            header_dict[\"GLONASS COD/PHS/BIS\"] = self.header.glonass_cod_phs_bis\n\n        if hasattr(self.header, \"phase_shift\"):\n            header_dict[\"SYS / PHASE SHIFT\"] = self.header.phase_shift\n\n        # Run validation\n        results = RINEX304ComplianceValidator.validate_all(\n            ds=ds, header_dict=header_dict, strict=strict\n        )\n\n        if print_report:\n            RINEX304ComplianceValidator.print_validation_report(results)\n\n        return results\n\n    def _create_basic_attrs(self) -&gt; dict[str, object]:\n        attrs = get_global_attrs()\n        attrs[\"Created\"] = datetime.now(timezone.utc).isoformat()\n        attrs[\"Software\"] = (\n            f\"{attrs['Software']}, Version: {get_version_from_pyproject()}\"\n        )\n        return attrs\n\n    def _create_comprehensive_attrs(self) -&gt; dict[str, object]:\n        attrs = {\n            \"File Path\": str(self.fpath),\n            \"File Type\": self.header.filetype,\n            \"RINEX Version\": self.header.version,\n            \"RINEX Type\": self.header.rinextype,\n            \"Observer\": self.header.observer,\n            \"Agency\": self.header.agency,\n            \"Date\": self.header.date.isoformat(),\n            \"Marker Name\": self.header.marker_name,\n            \"Marker Number\": self.header.marker_number,\n            \"Marker Type\": self.header.marker_type,\n            \"Approximate Position\": (\n                f\"(X = {self.header.approx_position[0].magnitude} \"\n                f\"{self.header.approx_position[0].units:~}, \"\n                f\"Y = {self.header.approx_position[1].magnitude} \"\n                f\"{self.header.approx_position[1].units:~}, \"\n                f\"Z = {self.header.approx_position[2].magnitude} \"\n                f\"{self.header.approx_position[2].units:~})\"\n            ),\n            \"Receiver Type\": self.header.receiver_type,\n            \"Receiver Version\": self.header.receiver_version,\n            \"Receiver Number\": self.header.receiver_number,\n            \"Antenna Type\": self.header.antenna_type,\n            \"Antenna Number\": self.header.antenna_number,\n            \"Antenna Position\": (\n                f\"(X = {self.header.antenna_position[0].magnitude} \"\n                f\"{self.header.antenna_position[0].units:~}, \"\n                f\"Y = {self.header.antenna_position[1].magnitude} \"\n                f\"{self.header.antenna_position[1].units:~}, \"\n                f\"Z = {self.header.antenna_position[2].magnitude} \"\n                f\"{self.header.antenna_position[2].units:~})\"\n            ),\n            \"Program\": self.header.pgm,\n            \"Run By\": self.header.run_by,\n            \"Time of First Observation\": json.dumps(\n                {k: v.isoformat() for k, v in self.header.t0.items()}\n            ),\n            \"GLONASS COD\": self.header.glonass_cod,\n            \"GLONASS PHS\": self.header.glonass_phs,\n            \"GLONASS BIS\": self.header.glonass_bis,\n            \"GLONASS Slot Frequency Dict\": json.dumps(\n                self.header.glonass_slot_freq_dict\n            ),\n            \"Leap Seconds\": f\"{self.header.leap_seconds:~}\",\n        }\n        return attrs\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.header","level":3,"title":"<code>header</code>  <code>property</code>","text":"<p>Expose validated header (read-only).</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.header--returns","level":5,"title":"Returns","text":"<p>Rnxv3Header     Parsed and validated RINEX header.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.file_hash","level":3,"title":"<code>file_hash</code>  <code>property</code>","text":"<p>Return cached SHA256 short hash of the file content.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.file_hash--returns","level":5,"title":"Returns","text":"<p>str     16-character short hash for deduplication.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.start_time","level":3,"title":"<code>start_time</code>  <code>property</code>","text":"<p>Return start time of observations from header.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.start_time--returns","level":5,"title":"Returns","text":"<p>datetime     First observation timestamp.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.end_time","level":3,"title":"<code>end_time</code>  <code>property</code>","text":"<p>Return end time of observations from last epoch.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.end_time--returns","level":5,"title":"Returns","text":"<p>datetime     Last observation timestamp.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.systems","level":3,"title":"<code>systems</code>  <code>property</code>","text":"<p>Return list of GNSS systems in file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.systems--returns","level":5,"title":"Returns","text":"<p>list of str     System identifiers (G, R, E, C, J, S, I).</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.num_epochs","level":3,"title":"<code>num_epochs</code>  <code>property</code>","text":"<p>Return number of epochs in file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.num_epochs--returns","level":5,"title":"Returns","text":"<p>int     Total epoch count.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.num_satellites","level":3,"title":"<code>num_satellites</code>  <code>property</code>","text":"<p>Return total number of unique satellites observed.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.num_satellites--returns","level":5,"title":"Returns","text":"<p>int     Count of unique satellite vehicles across all systems.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.epochs","level":3,"title":"<code>epochs</code>  <code>property</code>","text":"<p>Materialize all epochs (legacy compatibility).</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.epochs--returns","level":5,"title":"Returns","text":"<p>list of Rnxv3ObsEpochRecord     All epochs in memory (use iter_epochs for efficiency)</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.__str__","level":3,"title":"<code>__str__()</code>","text":"<p>Return a human-readable summary.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable summary.\"\"\"\n    return (\n        f\"{self.__class__.__name__}:\\n\"\n        f\"  File Path: {self.fpath}\\n\"\n        f\"  Header: {self.header}\\n\"\n        f\"  Polarization: {self.polarization}\\n\"\n    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return a concise representation for debugging.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a concise representation for debugging.\"\"\"\n    return f\"{self.__class__.__name__}(fpath={self.fpath})\"\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.get_epoch_record_batches","level":3,"title":"<code>get_epoch_record_batches(epoch_record_indicator=EPOCH_RECORD_INDICATOR)</code>","text":"<p>Get the start and end line numbers for each epoch in the file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.get_epoch_record_batches--parameters","level":5,"title":"Parameters","text":"<p>epoch_record_indicator : str, default '&gt;'     Character marking epoch record lines.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.get_epoch_record_batches--returns","level":5,"title":"Returns","text":"<p>list of tuple of int     List of (start_line, end_line) pairs for each epoch.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def get_epoch_record_batches(\n    self, epoch_record_indicator: str = EPOCH_RECORD_INDICATOR\n) -&gt; list[tuple[int, int]]:\n    \"\"\"Get the start and end line numbers for each epoch in the file.\n\n    Parameters\n    ----------\n    epoch_record_indicator : str, default '&gt;'\n        Character marking epoch record lines.\n\n    Returns\n    -------\n    list of tuple of int\n        List of (start_line, end_line) pairs for each epoch.\n\n    \"\"\"\n    starts = [\n        i\n        for i, line in enumerate(self._load_file())\n        if line.startswith(epoch_record_indicator)\n    ]\n    starts.append(len(self._load_file()))  # Add EOF\n    return [\n        (start, starts[i + 1])\n        for i, start in enumerate(starts)\n        if i + 1 &lt; len(starts)\n    ]\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.parse_observation_slice","level":3,"title":"<code>parse_observation_slice(slice_text)</code>","text":"<p>Parse a RINEX observation slice into value, LLI, and SSI.</p> <p>Enhanced to handle both standard 16-character format and variable-length records.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.parse_observation_slice--parameters","level":5,"title":"Parameters","text":"<p>slice_text : str     Observation slice to parse.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.parse_observation_slice--returns","level":5,"title":"Returns","text":"<p>tuple[float | None, int | None, int | None]     Parsed (value, LLI, SSI) tuple.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def parse_observation_slice(\n    self,\n    slice_text: str,\n) -&gt; tuple[float | None, int | None, int | None]:\n    \"\"\"Parse a RINEX observation slice into value, LLI, and SSI.\n\n    Enhanced to handle both standard 16-character format and\n    variable-length records.\n\n    Parameters\n    ----------\n    slice_text : str\n        Observation slice to parse.\n\n    Returns\n    -------\n    tuple[float | None, int | None, int | None]\n        Parsed (value, LLI, SSI) tuple.\n\n    \"\"\"\n    if not slice_text or not slice_text.strip():\n        return None, None, None\n\n    try:\n        # Method 1: Standard RINEX format with decimal at position -6\n        if (\n            len(slice_text) &gt;= OBS_SLICE_MIN_LEN\n            and len(slice_text) &lt;= OBS_SLICE_MAX_LEN\n            and slice_text[OBS_SLICE_DECIMAL_POS] == \".\"\n        ):\n            slice_chars = list(slice_text)\n            ssi = slice_chars.pop(-1) if len(slice_chars) &gt; 0 else \"\"\n            lli = slice_chars.pop(-1) if len(slice_chars) &gt; 0 else \"\"\n\n            # Convert LLI and SSI\n            lli = int(lli) if lli.strip() and lli.isdigit() else None\n            ssi = int(ssi) if ssi.strip() and ssi.isdigit() else None\n\n            # Convert value\n            value_str = \"\".join(slice_chars).strip()\n            if value_str:\n                value = float(value_str)\n                return value, lli, ssi\n\n    except (ValueError, IndexError):\n        pass\n\n    try:\n        # Method 2: Flexible parsing for variable-length records\n        slice_trimmed = slice_text.strip()\n        if not slice_trimmed:\n            return None, None, None\n\n        # Look for a decimal point to identify the numeric value\n        if \".\" in slice_trimmed:\n            # Find the main numeric value (supports negative numbers)\n            number_match = re.search(r\"(-?\\d+\\.\\d+)\", slice_trimmed)\n\n            if number_match:\n                value = float(number_match.group(1))\n\n                # Check for LLI/SSI indicators after the number\n                remaining_part = slice_trimmed[number_match.end() :].strip()\n                lli = None\n                ssi = None\n\n                # Parse remaining characters as potential LLI/SSI\n                if remaining_part:\n                    # Could be just SSI, or LLI followed by SSI\n                    if len(remaining_part) == 1:\n                        # Just one indicator - assume it's SSI\n                        if remaining_part.isdigit():\n                            ssi = int(remaining_part)\n                    elif len(remaining_part) &gt;= LLI_SSI_PAIR_LEN:\n                        # Two or more characters - take last two as LLI, SSI\n                        lli_char = remaining_part[-2]\n                        ssi_char = remaining_part[-1]\n\n                        if lli_char.isdigit():\n                            lli = int(lli_char)\n                        if ssi_char.isdigit():\n                            ssi = int(ssi_char)\n\n                return value, lli, ssi\n\n    except (ValueError, IndexError):\n        pass\n\n    # Method 3: Last resort - try simple float parsing\n    try:\n        simple_value = float(slice_text.strip())\n        return simple_value, None, None\n    except ValueError:\n        pass\n\n    return None, None, None\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.process_satellite_data","level":3,"title":"<code>process_satellite_data(s)</code>","text":"<p>Process satellite data line into a Satellite object with observations.</p> <p>Handles variable-length observation records correctly by adaptively parsing based on the actual line length and content.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def process_satellite_data(self, s: str) -&gt; Satellite:\n    \"\"\"Process satellite data line into a Satellite object with observations.\n\n    Handles variable-length observation records correctly by adaptively parsing\n    based on the actual line length and content.\n    \"\"\"\n    sv = s[:3].strip()\n    satellite = Satellite(sv=sv)\n    bands_tbe = [f\"{sv}|{b}\" for b in self.header.obs_codes_per_system[sv[0]]]\n\n    # Get the data part (after sv identifier)\n    data_part = s[3:]\n\n    # Process each observation adaptively\n    for i, band in enumerate(bands_tbe):\n        start_idx = i * 16\n        end_idx = start_idx + 16\n\n        # Check if we have enough data for this observation\n        if start_idx &gt;= len(data_part):\n            # No more data available - create empty observation\n            observation = Observation(\n                observation_freq_tag=band,\n                obs_type=band.split(\"|\")[1][0],\n                value=None,\n                lli=None,\n                ssi=None,\n            )\n            satellite.add_observation(observation)\n            continue\n\n        # Extract the slice, but handle variable length\n        if end_idx &lt;= len(data_part):\n            # Full 16-character slice available\n            slice_data = data_part[start_idx:end_idx]\n        else:\n            # Partial slice - pad with spaces to maintain consistency\n            available_slice = data_part[start_idx:]\n            slice_data = available_slice.ljust(16)  # Pad with spaces if needed\n\n        value, lli, ssi = self.parse_observation_slice(slice_data)\n\n        observation = Observation(\n            observation_freq_tag=band,\n            obs_type=band.split(\"|\")[1][0],\n            value=value,\n            lli=lli,\n            ssi=ssi,\n        )\n        satellite.add_observation(observation)\n\n    return satellite\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.iter_epochs","level":3,"title":"<code>iter_epochs()</code>","text":"<p>Yield epochs one by one instead of materializing the whole list.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.iter_epochs--returns","level":5,"title":"Returns","text":"<p>Generator     Generator yielding Rnxv3ObsEpochRecord objects</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.iter_epochs--yields","level":5,"title":"Yields","text":"<p>Rnxv3ObsEpochRecord     Each epoch with timestamp and satellite observations</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def iter_epochs(self) -&gt; Iterable[Rnxv3ObsEpochRecord]:\n    \"\"\"Yield epochs one by one instead of materializing the whole list.\n\n    Returns\n    -------\n    Generator\n        Generator yielding Rnxv3ObsEpochRecord objects\n\n    Yields\n    ------\n    Rnxv3ObsEpochRecord\n        Each epoch with timestamp and satellite observations\n\n    \"\"\"\n    for start, end in self.get_epoch_record_batches():\n        try:\n            info = Rnxv3ObsEpochRecordLineModel(epoch=self._lines[start])\n            data = self._lines[start + 1 : end]\n            epoch = Rnxv3ObsEpochRecord(\n                info=info,\n                data=(\n                    self.process_satellite_data(line) for line in data\n                ),  # generator here too\n            )\n            yield epoch\n        except (InvalidEpochError, IncompleteEpochError):\n            # Skip unexpected errors silently\n            pass\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.iter_epochs_in_range","level":3,"title":"<code>iter_epochs_in_range(start, end)</code>","text":"<p>Yield epochs lazily that fall into the given datetime range.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.iter_epochs_in_range--parameters","level":5,"title":"Parameters","text":"<p>start : datetime     Start of time range (inclusive) end : datetime     End of time range (inclusive)</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.iter_epochs_in_range--returns","level":5,"title":"Returns","text":"<p>Generator     Generator yielding epochs in the specified range</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.iter_epochs_in_range--yields","level":5,"title":"Yields","text":"<p>Rnxv3ObsEpochRecord     Epochs within the time range</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def iter_epochs_in_range(\n    self,\n    start: datetime,\n    end: datetime,\n) -&gt; Iterable[Rnxv3ObsEpochRecord]:\n    \"\"\"Yield epochs lazily that fall into the given datetime range.\n\n    Parameters\n    ----------\n    start : datetime\n        Start of time range (inclusive)\n    end : datetime\n        End of time range (inclusive)\n\n    Returns\n    -------\n    Generator\n        Generator yielding epochs in the specified range\n\n    Yields\n    ------\n    Rnxv3ObsEpochRecord\n        Epochs within the time range\n\n    \"\"\"\n    for epoch in self.iter_epochs():\n        dt = self.get_datetime_from_epoch_record_info(epoch.info)\n        if start &lt;= dt &lt;= end:\n            yield epoch\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.get_datetime_from_epoch_record_info","level":3,"title":"<code>get_datetime_from_epoch_record_info(epoch_record_info)</code>","text":"<p>Convert epoch record info to datetime object.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.get_datetime_from_epoch_record_info--parameters","level":5,"title":"Parameters","text":"<p>epoch_record_info : Rnxv3ObsEpochRecordLineModel     Parsed epoch record line</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.get_datetime_from_epoch_record_info--returns","level":5,"title":"Returns","text":"<p>datetime     Timestamp from epoch record</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def get_datetime_from_epoch_record_info(\n    self,\n    epoch_record_info: Rnxv3ObsEpochRecordLineModel,\n) -&gt; datetime:\n    \"\"\"Convert epoch record info to datetime object.\n\n    Parameters\n    ----------\n    epoch_record_info : Rnxv3ObsEpochRecordLineModel\n        Parsed epoch record line\n\n    Returns\n    -------\n    datetime\n        Timestamp from epoch record\n\n    \"\"\"\n    return datetime(\n        year=int(epoch_record_info.year),\n        month=int(epoch_record_info.month),\n        day=int(epoch_record_info.day),\n        hour=int(epoch_record_info.hour),\n        minute=int(epoch_record_info.minute),\n        second=int(epoch_record_info.seconds),\n        tzinfo=timezone.utc,\n    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.epochrecordinfo_dt_to_numpy_dt","level":3,"title":"<code>epochrecordinfo_dt_to_numpy_dt(epch)</code>  <code>staticmethod</code>","text":"<p>Convert Python datetime to numpy datetime64[ns].</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.epochrecordinfo_dt_to_numpy_dt--parameters","level":5,"title":"Parameters","text":"<p>epch : Rnxv3ObsEpochRecord     Epoch record containing timestamp info</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.epochrecordinfo_dt_to_numpy_dt--returns","level":5,"title":"Returns","text":"<p>np.datetime64     Numpy datetime64 with nanosecond precision</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>@staticmethod\ndef epochrecordinfo_dt_to_numpy_dt(\n    epch: Rnxv3ObsEpochRecord,\n) -&gt; np.datetime64:\n    \"\"\"Convert Python datetime to numpy datetime64[ns].\n\n    Parameters\n    ----------\n    epch : Rnxv3ObsEpochRecord\n        Epoch record containing timestamp info\n\n    Returns\n    -------\n    np.datetime64\n        Numpy datetime64 with nanosecond precision\n\n    \"\"\"\n    dt = datetime(\n        year=int(epch.info.year),\n        month=int(epch.info.month),\n        day=int(epch.info.day),\n        hour=int(epch.info.hour),\n        minute=int(epch.info.minute),\n        second=int(epch.info.seconds),\n        tzinfo=timezone.utc,\n    )\n    # np.datetime64 doesn't support timezone info, but datetime is already UTC\n    # Convert to naive datetime (UTC) to avoid warning\n    return np.datetime64(dt.replace(tzinfo=None), \"ns\")\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.infer_sampling_interval","level":3,"title":"<code>infer_sampling_interval()</code>","text":"<p>Infer sampling interval from consecutive epoch deltas.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.infer_sampling_interval--returns","level":5,"title":"Returns","text":"<p>pint.Quantity or None     Sampling interval in seconds, or None if cannot be inferred</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def infer_sampling_interval(self) -&gt; pint.Quantity | None:\n    \"\"\"Infer sampling interval from consecutive epoch deltas.\n\n    Returns\n    -------\n    pint.Quantity or None\n        Sampling interval in seconds, or None if cannot be inferred\n\n    \"\"\"\n    dts = self._epoch_datetimes()\n    if len(dts) &lt; MIN_EPOCHS_FOR_INTERVAL:\n        return None\n    # Compute deltas\n    deltas: list[timedelta] = [b - a for a, b in pairwise(dts) if b &gt;= a]\n    if not deltas:\n        return None\n    # Pick the most common delta (robust to an occasional missing epoch)\n    seconds = Counter(\n        int(dt.total_seconds()) for dt in deltas if dt.total_seconds() &gt; 0\n    )\n    if not seconds:\n        return None\n    mode_seconds, _ = seconds.most_common(1)[0]\n    return (mode_seconds * UREG.second).to(UREG.seconds)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.infer_dump_interval","level":3,"title":"<code>infer_dump_interval(sampling_interval=None)</code>","text":"<p>Infer the intended dump interval for the RINEX file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.infer_dump_interval--parameters","level":5,"title":"Parameters","text":"<p>sampling_interval : pint.Quantity, optional     Known sampling interval. If provided, returns (#epochs * sampling_interval)</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.infer_dump_interval--returns","level":5,"title":"Returns","text":"<p>pint.Quantity or None     Dump interval in seconds, or None if cannot be inferred</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def infer_dump_interval(\n    self, sampling_interval: pint.Quantity | None = None\n) -&gt; pint.Quantity | None:\n    \"\"\"Infer the intended dump interval for the RINEX file.\n\n    Parameters\n    ----------\n    sampling_interval : pint.Quantity, optional\n        Known sampling interval. If provided, returns (#epochs * sampling_interval)\n\n    Returns\n    -------\n    pint.Quantity or None\n        Dump interval in seconds, or None if cannot be inferred\n\n    \"\"\"\n    idx = self.get_epoch_record_batches()\n    n_epochs = len(idx)\n    if n_epochs == 0:\n        return None\n\n    if sampling_interval is not None:\n        return (n_epochs * sampling_interval).to(UREG.seconds)\n\n    # Fallback: time coverage inclusive (last - first) + typical step\n    dts = self._epoch_datetimes()\n    if len(dts) == 0:\n        return None\n    if len(dts) == 1:\n        # single epoch: treat as 1 * unknown step (cannot infer)\n        return None\n\n    # Estimate step from data\n    est_step = self.infer_sampling_interval()\n    if est_step is None:\n        return None\n\n    # Inclusive coverage often equals (n_epochs - 1) * step; intended\n    # dump interval is n_epochs * step.\n    return (n_epochs * est_step.to(UREG.seconds)).to(UREG.seconds)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.validate_epoch_completeness","level":3,"title":"<code>validate_epoch_completeness(dump_interval=None, sampling_interval=None)</code>","text":"<p>Validate that the number of epochs matches the expected dump interval.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.validate_epoch_completeness--parameters","level":5,"title":"Parameters","text":"<p>dump_interval : str or pint.Quantity, optional     Expected file dump interval. If None, inferred from epochs. sampling_interval : str or pint.Quantity, optional     Expected sampling interval. If None, inferred from epochs.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.validate_epoch_completeness--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.validate_epoch_completeness--raises","level":5,"title":"Raises","text":"<p>MissingEpochError     If total sampling time doesn't match dump interval ValueError     If intervals cannot be inferred</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def validate_epoch_completeness(\n    self,\n    dump_interval: str | pint.Quantity | None = None,\n    sampling_interval: str | pint.Quantity | None = None,\n) -&gt; None:\n    \"\"\"Validate that the number of epochs matches the expected dump interval.\n\n    Parameters\n    ----------\n    dump_interval : str or pint.Quantity, optional\n        Expected file dump interval. If None, inferred from epochs.\n    sampling_interval : str or pint.Quantity, optional\n        Expected sampling interval. If None, inferred from epochs.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    MissingEpochError\n        If total sampling time doesn't match dump interval\n    ValueError\n        If intervals cannot be inferred\n\n    \"\"\"\n    # Normalize/Infer sampling interval\n    if sampling_interval is None:\n        inferred = self.infer_sampling_interval()\n        if inferred is None:\n            msg = \"Could not infer sampling interval from epochs\"\n            raise ValueError(msg)\n        sampling_interval = inferred\n    # normalize to pint\n    elif not isinstance(sampling_interval, pint.Quantity):\n        sampling_interval = UREG.Quantity(sampling_interval).to(UREG.seconds)\n\n    # Normalize/Infer dump interval\n    if dump_interval is None:\n        inferred_dump = self.infer_dump_interval(\n            sampling_interval=sampling_interval\n        )\n        if inferred_dump is None:\n            msg = \"Could not infer dump interval from file\"\n            raise ValueError(msg)\n        dump_interval = inferred_dump\n    elif not isinstance(dump_interval, pint.Quantity):\n        # Accept '15 min', '1h', etc.\n        dump_interval = UREG.Quantity(dump_interval).to(UREG.seconds)\n\n    # Build inputs for the validator model\n    epoch_indices = self.get_epoch_record_batches()\n\n    # This throws MissingEpochError automatically if inconsistent\n    Rnxv3ObsEpochRecordCompletenessModel(\n        epoch_records_indeces=epoch_indices,\n        rnx_file_dump_interval=dump_interval,\n        sampling_interval=sampling_interval,\n    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.filter_by_overlapping_groups","level":3,"title":"<code>filter_by_overlapping_groups(ds, group_preference=None)</code>","text":"<p>Filter overlapping bands using per-group preferences.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.filter_by_overlapping_groups--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with <code>sid</code> dimension and signal properties. group_preference : dict[str, str], optional     Mapping of overlap group to preferred band.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.filter_by_overlapping_groups--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset filtered to preferred overlapping bands.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def filter_by_overlapping_groups(\n    self,\n    ds: xr.Dataset,\n    group_preference: dict[str, str] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Filter overlapping bands using per-group preferences.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with `sid` dimension and signal properties.\n    group_preference : dict[str, str], optional\n        Mapping of overlap group to preferred band.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset filtered to preferred overlapping bands.\n\n    \"\"\"\n    if group_preference is None:\n        group_preference = {\n            \"L1_E1_B1I\": \"L1\",\n            \"L5_E5a\": \"L5\",\n            \"L2_E5b_B2b\": \"L2\",\n        }\n\n    keep = []\n    for sid in ds.sid.values:\n        _sv, band, _code = self._signal_mapper.parse_signal_id(str(sid))\n        group = self._signal_mapper.get_overlapping_group(band)\n        if group and group in group_preference:\n            if band == group_preference[group]:\n                keep.append(sid)\n        else:\n            keep.append(sid)\n    return ds.sel(sid=keep)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.create_rinex_netcdf_with_signal_id","level":3,"title":"<code>create_rinex_netcdf_with_signal_id(analyze_conflicts=False, analyze_systems=False, start=None, end=None)</code>","text":"<p>Create a NetCDF dataset with signal IDs.</p> <p>Can optionally restrict to epochs within a datetime range.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def create_rinex_netcdf_with_signal_id(\n    self,\n    analyze_conflicts: bool = False,\n    analyze_systems: bool = False,\n    start: datetime | None = None,\n    end: datetime | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Create a NetCDF dataset with signal IDs.\n\n    Can optionally restrict to epochs within a datetime range.\n\n    \"\"\"\n    if analyze_conflicts:\n        print(\"\\nNote: Conflict analysis will be adapted for sid structure.\")\n    if analyze_systems:\n        print(\"\\nNote: System analysis will be adapted for sid structure.\")\n\n    signal_ids = set()\n    signal_id_to_properties: dict[str, dict[str, object]] = {}\n    timestamps: list[np.datetime64] = []\n\n    # pick generator depending on range\n    if start and end:\n        epoch_iter = self.iter_epochs_in_range(start, end)\n    else:\n        epoch_iter = self.iter_epochs()\n\n    for epoch in epoch_iter:\n        dt = self.epochrecordinfo_dt_to_numpy_dt(epoch)\n        timestamps.append(np.datetime64(dt, \"ns\"))\n\n        for sat in epoch.data:\n            sv = sat.sv\n            for obs in sat.observations:\n                if not self.include_auxiliary and obs.observation_freq_tag.endswith(\n                    \"|X1\"\n                ):\n                    continue\n\n                sid = self._signal_mapper.create_signal_id(\n                    sv, obs.observation_freq_tag\n                )\n                signal_ids.add(sid)\n\n                if sid not in signal_id_to_properties:\n                    sv_part, band, code = self._signal_mapper.parse_signal_id(sid)\n                    system = sv_part[0]\n                    center_frequency = self._signal_mapper.get_band_frequency(band)\n                    bandwidth = self._signal_mapper.get_band_bandwidth(band)\n                    overlapping_group = self._signal_mapper.get_overlapping_group(\n                        band\n                    )\n\n                    if center_frequency is not None and bandwidth is not None:\n                        # Extract bandwidth value\n                        bw = (\n                            bandwidth[0]\n                            if isinstance(bandwidth, list)\n                            else bandwidth\n                        )\n\n                        # Ensure both are pint quantities\n                        if not hasattr(center_frequency, \"m_as\"):\n                            center_frequency = center_frequency * UREG.MHz\n                        if not hasattr(bw, \"m_as\"):\n                            bw = bw * UREG.MHz\n\n                        # Calculate frequency range\n                        freq_min = center_frequency - (bw / 2.0)\n                        freq_max = center_frequency + (bw / 2.0)\n\n                        # Extract magnitudes to ensure float64 dtype\n                        center_frequency = float(center_frequency.m_as(UREG.MHz))\n                        freq_min = float(freq_min.m_as(UREG.MHz))\n                        freq_max = float(freq_max.m_as(UREG.MHz))\n                        bw = float(bw.m_as(UREG.MHz))\n                    else:\n                        print(\n                            f\"WARNING: No frequency data for sid={sid}, \"\n                            f\"band={band}, sv={sv_part}\"\n                        )\n                        center_frequency = np.nan\n                        freq_min = np.nan\n                        freq_max = np.nan\n                        bw = np.nan\n\n                    signal_id_to_properties[sid] = {\n                        \"sv\": sv_part,\n                        \"system\": system,\n                        \"band\": band,\n                        \"code\": code,\n                        \"freq_center\": center_frequency,\n                        \"freq_min\": freq_min,\n                        \"freq_max\": freq_max,\n                        \"bandwidth\": bw,\n                        \"overlapping_group\": overlapping_group,\n                    }\n\n    # Inconsistent integration of the Septentrio X1 obs. code, filtering\n    # out here again.\n    signal_ids = {sid for sid in signal_ids if \"|X1|\" not in sid}\n    signal_id_to_properties = {\n        sid: props\n        for sid, props in signal_id_to_properties.items()\n        if \"|X1|\" not in sid\n    }\n\n    sorted_signal_ids = sorted(signal_ids)\n    n_epochs = len(timestamps)\n    n_signals = len(sorted_signal_ids)\n\n    data_arrays = {\n        \"SNR\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"SNR\"]),\n        \"Pseudorange\": np.full(\n            (n_epochs, n_signals), np.nan, dtype=DTYPES[\"Pseudorange\"]\n        ),\n        \"Phase\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"Phase\"]),\n        \"Doppler\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"Doppler\"]),\n        \"LLI\": np.full((n_epochs, n_signals), -1, dtype=DTYPES[\"LLI\"]),\n        \"SSI\": np.full((n_epochs, n_signals), -1, dtype=DTYPES[\"SSI\"]),\n    }\n    sid_to_idx = {sid: i for i, sid in enumerate(sorted_signal_ids)}\n\n    # second pass to fill arrays\n    if start and end:\n        epoch_iter = self.iter_epochs_in_range(start, end)\n    else:\n        epoch_iter = self.iter_epochs()\n\n    for t_idx, epoch in enumerate(epoch_iter):\n        for sat in epoch.data:\n            sv = sat.sv\n            for obs in sat.observations:\n                if not self.include_auxiliary and obs.observation_freq_tag.endswith(\n                    \"|X1\"\n                ):\n                    continue\n                if obs.value is None:\n                    continue\n                sid = self._signal_mapper.create_signal_id(\n                    sv, obs.observation_freq_tag\n                )\n                if sid not in sid_to_idx:\n                    continue\n                s_idx = sid_to_idx[sid]\n\n                ot = obs.obs_type\n                if ot == \"S\" and obs.value != 0:\n                    data_arrays[\"SNR\"][t_idx, s_idx] = obs.value\n                elif ot == \"C\":\n                    data_arrays[\"Pseudorange\"][t_idx, s_idx] = obs.value\n                elif ot == \"L\":\n                    data_arrays[\"Phase\"][t_idx, s_idx] = obs.value\n                elif ot == \"D\":\n                    data_arrays[\"Doppler\"][t_idx, s_idx] = obs.value\n                elif ot == \"X\":\n                    data_arrays.setdefault(\n                        \"Auxiliary\",\n                        np.full((n_epochs, n_signals), np.nan, dtype=np.float32),\n                    )\n                    data_arrays[\"Auxiliary\"][t_idx, s_idx] = obs.value\n\n                if obs.lli is not None:\n                    data_arrays[\"LLI\"][t_idx, s_idx] = obs.lli\n                if obs.ssi is not None:\n                    data_arrays[\"SSI\"][t_idx, s_idx] = obs.ssi\n\n    signal_id_coord = xr.DataArray(\n        sorted_signal_ids, dims=[\"sid\"], attrs=COORDS_METADATA[\"sid\"]\n    )\n    sv_list = [signal_id_to_properties[sid][\"sv\"] for sid in sorted_signal_ids]\n    constellation_list = [\n        signal_id_to_properties[sid][\"system\"] for sid in sorted_signal_ids\n    ]\n    band_list = [signal_id_to_properties[sid][\"band\"] for sid in sorted_signal_ids]\n    code_list = [signal_id_to_properties[sid][\"code\"] for sid in sorted_signal_ids]\n    freq_center_list = [\n        signal_id_to_properties[sid][\"freq_center\"] for sid in sorted_signal_ids\n    ]\n    freq_min_list = [\n        signal_id_to_properties[sid][\"freq_min\"] for sid in sorted_signal_ids\n    ]\n    freq_max_list = [\n        signal_id_to_properties[sid][\"freq_max\"] for sid in sorted_signal_ids\n    ]\n\n    coords = {\n        \"epoch\": (\"epoch\", timestamps, COORDS_METADATA[\"epoch\"]),\n        \"sid\": signal_id_coord,\n        \"sv\": (\"sid\", sv_list, COORDS_METADATA[\"sv\"]),\n        \"system\": (\"sid\", constellation_list, COORDS_METADATA[\"system\"]),\n        \"band\": (\"sid\", band_list, COORDS_METADATA[\"band\"]),\n        \"code\": (\"sid\", code_list, COORDS_METADATA[\"code\"]),\n        \"freq_center\": (\n            \"sid\",\n            np.asarray(freq_center_list, dtype=DTYPES[\"freq_center\"]),\n            COORDS_METADATA[\"freq_center\"],\n        ),\n        \"freq_min\": (\n            \"sid\",\n            np.asarray(freq_min_list, dtype=DTYPES[\"freq_min\"]),\n            COORDS_METADATA[\"freq_min\"],\n        ),\n        \"freq_max\": (\n            \"sid\",\n            np.asarray(freq_max_list, dtype=DTYPES[\"freq_max\"]),\n            COORDS_METADATA[\"freq_max\"],\n        ),\n    }\n\n    if self.header.signal_strength_unit == UREG.dBHz:\n        snr_meta = CN0_METADATA\n    else:\n        snr_meta = SNR_METADATA\n\n    ds = xr.Dataset(\n        data_vars={\n            \"SNR\": ([\"epoch\", \"sid\"], data_arrays[\"SNR\"], snr_meta),\n            \"Pseudorange\": (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"Pseudorange\"],\n                OBSERVABLES_METADATA[\"Pseudorange\"],\n            ),\n            \"Phase\": (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"Phase\"],\n                OBSERVABLES_METADATA[\"Phase\"],\n            ),\n            \"Doppler\": (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"Doppler\"],\n                OBSERVABLES_METADATA[\"Doppler\"],\n            ),\n            \"LLI\": (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"LLI\"],\n                OBSERVABLES_METADATA[\"LLI\"],\n            ),\n            \"SSI\": (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"SSI\"],\n                OBSERVABLES_METADATA[\"SSI\"],\n            ),\n        },\n        coords=coords,\n        attrs={**self._create_basic_attrs()},\n    )\n\n    if \"Auxiliary\" in data_arrays:\n        ds[\"Auxiliary\"] = (\n            [\"epoch\", \"sid\"],\n            data_arrays[\"Auxiliary\"],\n            OBSERVABLES_METADATA[\"Auxiliary\"],\n        )\n\n    if self.apply_overlap_filter:\n        ds = self.filter_by_overlapping_groups(ds, self.overlap_preferences)\n\n    return ds\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.to_ds","level":3,"title":"<code>to_ds(outname=None, keep_rnx_data_vars=None, write_global_attrs=False, pad_global_sid=True, strip_fillval=True, add_future_datavars=True, keep_sids=None)</code>","text":"<p>Convert RINEX observations to xarray.Dataset with signal ID structure.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.to_ds--parameters","level":5,"title":"Parameters","text":"<p>outname : Path or str, optional     If provided, saves dataset to this file path keep_rnx_data_vars : list of str or None, optional     Data variables to include in dataset. Defaults to config value. write_global_attrs : bool, default False     If True, adds comprehensive global attributes pad_global_sid : bool, default True     If True, pads to global signal ID space strip_fillval : bool, default True     If True, removes fill values add_future_datavars : bool, default True     If True, adds placeholder variables for future data keep_sids : list of str or None, default None     If provided, filters/pads dataset to these specific SIDs.     If None and pad_global_sid=True, pads to all possible SIDs.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.to_ds--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with dimensions (epoch, sid) and requested data variables</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def to_ds(\n    self,\n    outname: Path | str | None = None,\n    keep_rnx_data_vars: list[str] | None = None,\n    write_global_attrs: bool = False,\n    pad_global_sid: bool = True,\n    strip_fillval: bool = True,\n    add_future_datavars: bool = True,\n    keep_sids: list[str] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Convert RINEX observations to xarray.Dataset with signal ID structure.\n\n    Parameters\n    ----------\n    outname : Path or str, optional\n        If provided, saves dataset to this file path\n    keep_rnx_data_vars : list of str or None, optional\n        Data variables to include in dataset. Defaults to config value.\n    write_global_attrs : bool, default False\n        If True, adds comprehensive global attributes\n    pad_global_sid : bool, default True\n        If True, pads to global signal ID space\n    strip_fillval : bool, default True\n        If True, removes fill values\n    add_future_datavars : bool, default True\n        If True, adds placeholder variables for future data\n    keep_sids : list of str or None, default None\n        If provided, filters/pads dataset to these specific SIDs.\n        If None and pad_global_sid=True, pads to all possible SIDs.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with dimensions (epoch, sid) and requested data variables\n\n    \"\"\"\n    if keep_rnx_data_vars is None:\n        from canvod.utils.config import load_config\n\n        keep_rnx_data_vars = load_config().processing.processing.keep_rnx_vars\n\n    ds = self.create_rinex_netcdf_with_signal_id()\n\n    # drop unwanted vars\n    for var in list(ds.data_vars):\n        if var not in keep_rnx_data_vars:\n            ds = ds.drop_vars(var)\n\n    if pad_global_sid:\n        from canvod.auxiliary.preprocessing import pad_to_global_sid\n\n        # Pad/filter to specified sids or all possible sids\n        ds = pad_to_global_sid(ds, keep_sids=keep_sids)\n\n    if strip_fillval:\n        from canvod.auxiliary.preprocessing import strip_fillvalue\n\n        ds = strip_fillvalue(ds)\n\n    if add_future_datavars:\n        pass\n\n    if write_global_attrs:\n        ds.attrs.update(self._create_comprehensive_attrs())\n\n    ds.attrs[\"RINEX File Hash\"] = self.file_hash\n\n    if outname:\n        from canvod.utils.config import load_config as _load_config\n\n        comp = _load_config().processing.compression\n        encoding = {\n            var: {\"zlib\": comp.zlib, \"complevel\": comp.complevel}\n            for var in ds.data_vars\n        }\n        ds.to_netcdf(str(outname), encoding=encoding)\n\n    # Validate output structure for pipeline compatibility\n    self.validate_output(ds, required_vars=keep_rnx_data_vars)\n\n    return ds\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.validate_rinex_304_compliance","level":3,"title":"<code>validate_rinex_304_compliance(ds=None, strict=False, print_report=True)</code>","text":"<p>Run enhanced RINEX 3.04 specification validation.</p> <p>Validates: 1. System-specific observation codes 2. GLONASS mandatory fields (slot/frequency, biases) 3. Phase shift records (RINEX 3.01+) 4. Observation value ranges</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.validate_rinex_304_compliance--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset, optional     Dataset to validate. If None, creates one from current file. strict : bool     If True, raise ValueError on validation failures print_report : bool     If True, print validation report to console</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.validate_rinex_304_compliance--returns","level":5,"title":"Returns","text":"<p>dict[str, list[str]]     Validation results by category</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.validate_rinex_304_compliance--examples","level":5,"title":"Examples","text":"<p>reader = Rnxv3Obs(fpath=\"station.24o\") results = reader.validate_rinex_304_compliance()</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def validate_rinex_304_compliance(\n    self,\n    ds: xr.Dataset | None = None,\n    strict: bool = False,\n    print_report: bool = True,\n) -&gt; dict[str, list[str]]:\n    \"\"\"Run enhanced RINEX 3.04 specification validation.\n\n    Validates:\n    1. System-specific observation codes\n    2. GLONASS mandatory fields (slot/frequency, biases)\n    3. Phase shift records (RINEX 3.01+)\n    4. Observation value ranges\n\n    Parameters\n    ----------\n    ds : xr.Dataset, optional\n        Dataset to validate. If None, creates one from current file.\n    strict : bool\n        If True, raise ValueError on validation failures\n    print_report : bool\n        If True, print validation report to console\n\n    Returns\n    -------\n    dict[str, list[str]]\n        Validation results by category\n\n    Examples\n    --------\n    &gt;&gt;&gt; reader = Rnxv3Obs(fpath=\"station.24o\")\n    &gt;&gt;&gt; results = reader.validate_rinex_304_compliance()\n    &gt;&gt;&gt; # Or validate a specific dataset\n    &gt;&gt;&gt; ds = reader.to_ds()\n    &gt;&gt;&gt; results = reader.validate_rinex_304_compliance(ds=ds)\n\n    \"\"\"\n    if ds is None:\n        ds = self.to_ds(write_global_attrs=False)\n\n    # Prepare header dict for validators\n    header_dict = {\n        \"obs_codes_per_system\": self.header.obs_codes_per_system,\n    }\n\n    # Add GLONASS-specific headers if available\n    if hasattr(self.header, \"glonass_slot_frq\"):\n        header_dict[\"GLONASS SLOT / FRQ #\"] = self.header.glonass_slot_frq\n\n    if hasattr(self.header, \"glonass_cod_phs_bis\"):\n        header_dict[\"GLONASS COD/PHS/BIS\"] = self.header.glonass_cod_phs_bis\n\n    if hasattr(self.header, \"phase_shift\"):\n        header_dict[\"SYS / PHASE SHIFT\"] = self.header.phase_shift\n\n    # Run validation\n    results = RINEX304ComplianceValidator.validate_all(\n        ds=ds, header_dict=header_dict, strict=strict\n    )\n\n    if print_report:\n        RINEX304ComplianceValidator.print_validation_report(results)\n\n    return results\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.Rnxv3Obs.validate_rinex_304_compliance--or-validate-a-specific-dataset","level":4,"title":"Or validate a specific dataset","text":"<p>ds = reader.to_ds() results = reader.validate_rinex_304_compliance(ds=ds)</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory","level":2,"title":"<code>ReaderFactory</code>","text":"<p>Factory for creating appropriate reader based on file format.</p> <p>Automatically detects format and instantiates correct reader.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory--examples","level":4,"title":"Examples","text":"<p>reader = ReaderFactory.create(\"station.24o\") isinstance(reader, Rnxv3Obs) True</p> <p>reader = ReaderFactory.create(\"station.10o\") isinstance(reader, Rnxv2Obs) True</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>class ReaderFactory:\n    \"\"\"Factory for creating appropriate reader based on file format.\n\n    Automatically detects format and instantiates correct reader.\n\n    Examples\n    --------\n    &gt;&gt;&gt; reader = ReaderFactory.create(\"station.24o\")\n    &gt;&gt;&gt; isinstance(reader, Rnxv3Obs)\n    True\n\n    &gt;&gt;&gt; reader = ReaderFactory.create(\"station.10o\")\n    &gt;&gt;&gt; isinstance(reader, Rnxv2Obs)\n    True\n\n    \"\"\"\n\n    _readers: ClassVar[dict[str, type]] = {}\n\n    @classmethod\n    def register(cls, format_name: str, reader_class: type) -&gt; None:\n        \"\"\"Register a reader class for a format.\n\n        Parameters\n        ----------\n        format_name : str\n            Format identifier (e.g., 'rinex_v3', 'rinex_v2')\n        reader_class : type\n            Reader class (must inherit from GNSSDataReader)\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        TypeError\n            If reader_class does not inherit from GNSSDataReader.\n\n        \"\"\"\n        if not issubclass(reader_class, GNSSDataReader):\n            msg = f\"{reader_class} must inherit from GNSSDataReader\"\n            raise TypeError(msg)\n        cls._readers[format_name] = reader_class\n\n    @classmethod\n    def create(\n        cls,\n        fpath: Path | str,\n        **kwargs: object,\n    ) -&gt; GNSSDataReader:\n        \"\"\"Create appropriate reader for file.\n\n        Parameters\n        ----------\n        fpath : Path or str\n            Path to data file\n        **kwargs\n            Parameters to pass to reader constructor\n\n        Returns\n        -------\n        GNSSDataReader\n            Instantiated reader.\n\n        Raises\n        ------\n        ValueError\n            If file format cannot be determined.\n\n        \"\"\"\n        fpath = Path(fpath)\n\n        if not fpath.exists():\n            msg = f\"File not found: {fpath}\"\n            raise FileNotFoundError(msg)\n\n        # Detect format from file\n        format_name = cls._detect_format(fpath)\n\n        if format_name not in cls._readers:\n            msg = (\n                f\"No reader registered for format: {format_name}. \"\n                f\"Available: {list(cls._readers.keys())}\"\n            )\n            raise ValueError(msg)\n\n        reader_class = cls._readers[format_name]\n        return reader_class(fpath=fpath, **kwargs)\n\n    @staticmethod\n    def _detect_format(fpath: Path) -&gt; str:\n        \"\"\"Detect file format.\n\n        Parameters\n        ----------\n        fpath : Path\n            Path to file\n\n        Returns\n        -------\n        str\n            Format name.\n\n        \"\"\"\n        # Check RINEX version from first line\n        with fpath.open() as f:\n            first_line = f.readline()\n\n        # RINEX version is in columns 1-9\n        try:\n            version_str = first_line[:9].strip()\n            version = float(version_str)\n        except (ValueError, IndexError) as e:\n            msg = f\"Cannot determine file format: {e}\"\n            raise ValueError(msg) from e\n\n        rinex_v2_min = 2.0\n        rinex_v3_min = 3.0\n        rinex_v4_min = 4.0\n\n        if rinex_v3_min &lt;= version &lt; rinex_v4_min:\n            return \"rinex_v3\"\n        if rinex_v2_min &lt;= version &lt; rinex_v3_min:\n            return \"rinex_v2\"\n        msg = f\"Unsupported RINEX version: {version}\"\n        raise ValueError(msg)\n\n    @classmethod\n    def list_formats(cls) -&gt; list[str]:\n        \"\"\"List available formats.\n\n        Returns\n        -------\n        list of str\n            Registered format identifiers.\n\n        \"\"\"\n        return list(cls._readers.keys())\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory.register","level":3,"title":"<code>register(format_name, reader_class)</code>  <code>classmethod</code>","text":"<p>Register a reader class for a format.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory.register--parameters","level":5,"title":"Parameters","text":"<p>format_name : str     Format identifier (e.g., 'rinex_v3', 'rinex_v2') reader_class : type     Reader class (must inherit from GNSSDataReader)</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory.register--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory.register--raises","level":5,"title":"Raises","text":"<p>TypeError     If reader_class does not inherit from GNSSDataReader.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>@classmethod\ndef register(cls, format_name: str, reader_class: type) -&gt; None:\n    \"\"\"Register a reader class for a format.\n\n    Parameters\n    ----------\n    format_name : str\n        Format identifier (e.g., 'rinex_v3', 'rinex_v2')\n    reader_class : type\n        Reader class (must inherit from GNSSDataReader)\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    TypeError\n        If reader_class does not inherit from GNSSDataReader.\n\n    \"\"\"\n    if not issubclass(reader_class, GNSSDataReader):\n        msg = f\"{reader_class} must inherit from GNSSDataReader\"\n        raise TypeError(msg)\n    cls._readers[format_name] = reader_class\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory.create","level":3,"title":"<code>create(fpath, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create appropriate reader for file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory.create--parameters","level":5,"title":"Parameters","text":"<p>fpath : Path or str     Path to data file **kwargs     Parameters to pass to reader constructor</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory.create--returns","level":5,"title":"Returns","text":"<p>GNSSDataReader     Instantiated reader.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory.create--raises","level":5,"title":"Raises","text":"<p>ValueError     If file format cannot be determined.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    fpath: Path | str,\n    **kwargs: object,\n) -&gt; GNSSDataReader:\n    \"\"\"Create appropriate reader for file.\n\n    Parameters\n    ----------\n    fpath : Path or str\n        Path to data file\n    **kwargs\n        Parameters to pass to reader constructor\n\n    Returns\n    -------\n    GNSSDataReader\n        Instantiated reader.\n\n    Raises\n    ------\n    ValueError\n        If file format cannot be determined.\n\n    \"\"\"\n    fpath = Path(fpath)\n\n    if not fpath.exists():\n        msg = f\"File not found: {fpath}\"\n        raise FileNotFoundError(msg)\n\n    # Detect format from file\n    format_name = cls._detect_format(fpath)\n\n    if format_name not in cls._readers:\n        msg = (\n            f\"No reader registered for format: {format_name}. \"\n            f\"Available: {list(cls._readers.keys())}\"\n        )\n        raise ValueError(msg)\n\n    reader_class = cls._readers[format_name]\n    return reader_class(fpath=fpath, **kwargs)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory.list_formats","level":3,"title":"<code>list_formats()</code>  <code>classmethod</code>","text":"<p>List available formats.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.ReaderFactory.list_formats--returns","level":5,"title":"Returns","text":"<p>list of str     Registered format identifiers.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>@classmethod\ndef list_formats(cls) -&gt; list[str]:\n    \"\"\"List available formats.\n\n    Returns\n    -------\n    list of str\n        Registered format identifiers.\n\n    \"\"\"\n    return list(cls._readers.keys())\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator","level":2,"title":"<code>DatasetStructureValidator</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Validates xarray.Dataset structure for pipeline compatibility.</p> <p>All readers must produce Datasets that pass this validation to ensure compatibility with downstream VOD and storage operations.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic <code>BaseModel</code> with <code>arbitrary_types_allowed=True</code>.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>class DatasetStructureValidator(BaseModel):\n    \"\"\"Validates xarray.Dataset structure for pipeline compatibility.\n\n    All readers must produce Datasets that pass this validation\n    to ensure compatibility with downstream VOD and storage operations.\n\n    Notes\n    -----\n    This is a Pydantic `BaseModel` with `arbitrary_types_allowed=True`.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    dataset: xr.Dataset\n\n    def validate_dimensions(self) -&gt; None:\n        \"\"\"Validate required dimensions exist.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If required dimensions (epoch, sid) are missing.\n\n        \"\"\"\n        required_dims = {\"epoch\", \"sid\"}\n        missing_dims = required_dims - set(self.dataset.dims)\n        if missing_dims:\n            msg = f\"Missing required dimensions: {missing_dims}\"\n            raise ValueError(msg)\n\n    def validate_coordinates(self) -&gt; None:\n        \"\"\"Validate required coordinates exist and have correct types.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If required coordinates are missing or have incorrect dtypes.\n\n        \"\"\"\n        required_coords = {\n            \"epoch\": \"datetime64[ns]\",\n            \"sid\": \"object\",  # string\n            \"sv\": \"object\",\n            \"system\": \"object\",\n            \"band\": \"object\",\n            \"code\": \"object\",\n            \"freq_center\": \"float32\",\n            \"freq_min\": \"float32\",\n            \"freq_max\": \"float32\",\n        }\n\n        for coord, expected_dtype in required_coords.items():\n            if coord not in self.dataset.coords:\n                msg = f\"Missing required coordinate: {coord}\"\n                raise ValueError(msg)\n\n            actual_dtype = str(self.dataset[coord].dtype)\n            if expected_dtype == \"object\":\n                # String coordinates can be stored as object or Unicode string (&lt;U)\n                if actual_dtype not in [\"object\"] and not actual_dtype.startswith(\"&lt;U\"):\n                    msg = (\n                        f\"Coordinate {coord} has wrong dtype: \"\n                        f\"expected string, got {actual_dtype}\"\n                    )\n                    raise ValueError(msg)\n            elif expected_dtype not in actual_dtype:\n                msg = (\n                    f\"Coordinate {coord} has wrong dtype: \"\n                    f\"expected {expected_dtype}, got {actual_dtype}\"\n                )\n                raise ValueError(msg)\n\n    def validate_data_variables(self, required_vars: list[str] | None = None) -&gt; None:\n        \"\"\"Validate required data variables exist.\n\n        Parameters\n        ----------\n        required_vars : list of str, optional\n            List of required variables. If None, uses default minimum set.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If required variables are missing or have incorrect dimensions.\n\n        \"\"\"\n        if required_vars is None:\n            # Minimum required for VOD calculation\n            required_vars = [\"SNR\", \"Phase\"]\n\n        missing_vars = set(required_vars) - set(self.dataset.data_vars)\n        if missing_vars:\n            msg = f\"Missing required data variables: {missing_vars}\"\n            raise ValueError(msg)\n\n        # Validate all data vars have (epoch, sid) dimensions\n        for var in self.dataset.data_vars:\n            expected_dims = (\"epoch\", \"sid\")\n            actual_dims = self.dataset[var].dims\n            if actual_dims != expected_dims:\n                msg = (\n                    f\"Data variable {var} has wrong dimensions: \"\n                    f\"expected {expected_dims}, got {actual_dims}\"\n                )\n                raise ValueError(msg)\n\n    def validate_attributes(self) -&gt; None:\n        \"\"\"Validate required global attributes for storage.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If required attributes (Created, Software, Institution,\n            RINEX File Hash) are missing.\n\n        \"\"\"\n        required_attrs = {\n            \"Created\",\n            \"Software\",\n            \"Institution\",\n            \"RINEX File Hash\",  # Required for MyIcechunkStore deduplication\n        }\n\n        missing_attrs = required_attrs - set(self.dataset.attrs.keys())\n        if missing_attrs:\n            msg = f\"Missing required attributes: {missing_attrs}\"\n            raise ValueError(msg)\n\n    def validate_all(self, required_vars: list[str] | None = None) -&gt; None:\n        \"\"\"Run all validations.\n\n        Parameters\n        ----------\n        required_vars : list of str, optional\n            List of required data variables. If None, uses default minimum set.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If any validation fails.\n\n        \"\"\"\n        self.validate_dimensions()\n        self.validate_coordinates()\n        self.validate_data_variables(required_vars)\n        self.validate_attributes()\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_dimensions","level":3,"title":"<code>validate_dimensions()</code>","text":"<p>Validate required dimensions exist.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_dimensions--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_dimensions--raises","level":5,"title":"Raises","text":"<p>ValueError     If required dimensions (epoch, sid) are missing.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_dimensions(self) -&gt; None:\n    \"\"\"Validate required dimensions exist.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If required dimensions (epoch, sid) are missing.\n\n    \"\"\"\n    required_dims = {\"epoch\", \"sid\"}\n    missing_dims = required_dims - set(self.dataset.dims)\n    if missing_dims:\n        msg = f\"Missing required dimensions: {missing_dims}\"\n        raise ValueError(msg)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_coordinates","level":3,"title":"<code>validate_coordinates()</code>","text":"<p>Validate required coordinates exist and have correct types.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_coordinates--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_coordinates--raises","level":5,"title":"Raises","text":"<p>ValueError     If required coordinates are missing or have incorrect dtypes.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_coordinates(self) -&gt; None:\n    \"\"\"Validate required coordinates exist and have correct types.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If required coordinates are missing or have incorrect dtypes.\n\n    \"\"\"\n    required_coords = {\n        \"epoch\": \"datetime64[ns]\",\n        \"sid\": \"object\",  # string\n        \"sv\": \"object\",\n        \"system\": \"object\",\n        \"band\": \"object\",\n        \"code\": \"object\",\n        \"freq_center\": \"float32\",\n        \"freq_min\": \"float32\",\n        \"freq_max\": \"float32\",\n    }\n\n    for coord, expected_dtype in required_coords.items():\n        if coord not in self.dataset.coords:\n            msg = f\"Missing required coordinate: {coord}\"\n            raise ValueError(msg)\n\n        actual_dtype = str(self.dataset[coord].dtype)\n        if expected_dtype == \"object\":\n            # String coordinates can be stored as object or Unicode string (&lt;U)\n            if actual_dtype not in [\"object\"] and not actual_dtype.startswith(\"&lt;U\"):\n                msg = (\n                    f\"Coordinate {coord} has wrong dtype: \"\n                    f\"expected string, got {actual_dtype}\"\n                )\n                raise ValueError(msg)\n        elif expected_dtype not in actual_dtype:\n            msg = (\n                f\"Coordinate {coord} has wrong dtype: \"\n                f\"expected {expected_dtype}, got {actual_dtype}\"\n            )\n            raise ValueError(msg)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_data_variables","level":3,"title":"<code>validate_data_variables(required_vars=None)</code>","text":"<p>Validate required data variables exist.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_data_variables--parameters","level":5,"title":"Parameters","text":"<p>required_vars : list of str, optional     List of required variables. If None, uses default minimum set.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_data_variables--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_data_variables--raises","level":5,"title":"Raises","text":"<p>ValueError     If required variables are missing or have incorrect dimensions.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_data_variables(self, required_vars: list[str] | None = None) -&gt; None:\n    \"\"\"Validate required data variables exist.\n\n    Parameters\n    ----------\n    required_vars : list of str, optional\n        List of required variables. If None, uses default minimum set.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If required variables are missing or have incorrect dimensions.\n\n    \"\"\"\n    if required_vars is None:\n        # Minimum required for VOD calculation\n        required_vars = [\"SNR\", \"Phase\"]\n\n    missing_vars = set(required_vars) - set(self.dataset.data_vars)\n    if missing_vars:\n        msg = f\"Missing required data variables: {missing_vars}\"\n        raise ValueError(msg)\n\n    # Validate all data vars have (epoch, sid) dimensions\n    for var in self.dataset.data_vars:\n        expected_dims = (\"epoch\", \"sid\")\n        actual_dims = self.dataset[var].dims\n        if actual_dims != expected_dims:\n            msg = (\n                f\"Data variable {var} has wrong dimensions: \"\n                f\"expected {expected_dims}, got {actual_dims}\"\n            )\n            raise ValueError(msg)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_attributes","level":3,"title":"<code>validate_attributes()</code>","text":"<p>Validate required global attributes for storage.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_attributes--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_attributes--raises","level":5,"title":"Raises","text":"<p>ValueError     If required attributes (Created, Software, Institution,     RINEX File Hash) are missing.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_attributes(self) -&gt; None:\n    \"\"\"Validate required global attributes for storage.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If required attributes (Created, Software, Institution,\n        RINEX File Hash) are missing.\n\n    \"\"\"\n    required_attrs = {\n        \"Created\",\n        \"Software\",\n        \"Institution\",\n        \"RINEX File Hash\",  # Required for MyIcechunkStore deduplication\n    }\n\n    missing_attrs = required_attrs - set(self.dataset.attrs.keys())\n    if missing_attrs:\n        msg = f\"Missing required attributes: {missing_attrs}\"\n        raise ValueError(msg)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_all","level":3,"title":"<code>validate_all(required_vars=None)</code>","text":"<p>Run all validations.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_all--parameters","level":5,"title":"Parameters","text":"<p>required_vars : list of str, optional     List of required data variables. If None, uses default minimum set.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_all--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DatasetStructureValidator.validate_all--raises","level":5,"title":"Raises","text":"<p>ValueError     If any validation fails.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_all(self, required_vars: list[str] | None = None) -&gt; None:\n    \"\"\"Run all validations.\n\n    Parameters\n    ----------\n    required_vars : list of str, optional\n        List of required data variables. If None, uses default minimum set.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If any validation fails.\n\n    \"\"\"\n    self.validate_dimensions()\n    self.validate_coordinates()\n    self.validate_data_variables(required_vars)\n    self.validate_attributes()\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DataDirMatcher","level":2,"title":"<code>DataDirMatcher</code>","text":"<p>Match RINEX data directories for canopy and reference receivers.</p> <p>Scans a root directory structure to find dates with RINEX files present in both canopy and reference receiver directories.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DataDirMatcher--parameters","level":4,"title":"Parameters","text":"<p>root : Path     Root directory containing receiver subdirectories reference_pattern : Path, optional     Relative path pattern for reference receiver     (default: \"01_reference/01_GNSS/01_raw\") canopy_pattern : Path, optional     Relative path pattern for canopy receiver     (default: \"02_canopy/01_GNSS/01_raw\")</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DataDirMatcher--examples","level":4,"title":"Examples","text":"<p>from pathlib import Path matcher = DataDirMatcher( ...     root=Path(\"/data/01_Rosalia\"), ...     reference_pattern=Path(\"01_reference/01_GNSS/01_raw\"), ...     canopy_pattern=Path(\"02_canopy/01_GNSS/01_raw\") ... )</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>class DataDirMatcher:\n    \"\"\"Match RINEX data directories for canopy and reference receivers.\n\n    Scans a root directory structure to find dates with RINEX files\n    present in both canopy and reference receiver directories.\n\n    Parameters\n    ----------\n    root : Path\n        Root directory containing receiver subdirectories\n    reference_pattern : Path, optional\n        Relative path pattern for reference receiver\n        (default: \"01_reference/01_GNSS/01_raw\")\n    canopy_pattern : Path, optional\n        Relative path pattern for canopy receiver\n        (default: \"02_canopy/01_GNSS/01_raw\")\n\n    Examples\n    --------\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; matcher = DataDirMatcher(\n    ...     root=Path(\"/data/01_Rosalia\"),\n    ...     reference_pattern=Path(\"01_reference/01_GNSS/01_raw\"),\n    ...     canopy_pattern=Path(\"02_canopy/01_GNSS/01_raw\")\n    ... )\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Iterate over matched directories\n    &gt;&gt;&gt; for matched_dirs in matcher:\n    ...     print(matched_dirs.yyyydoy)\n    ...     rinex_files = list(matched_dirs.canopy_data_dir.glob(\"*.25o\"))\n    ...     print(f\"  Found {len(rinex_files)} RINEX files\")\n\n    &gt;&gt;&gt; # Get list of common dates\n    &gt;&gt;&gt; dates = matcher.get_common_dates()\n    &gt;&gt;&gt; print(f\"Found {len(dates)} dates with data\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        root: Path,\n        reference_pattern: Path = Path(\"01_reference/01_GNSS/01_raw\"),\n        canopy_pattern: Path = Path(\"02_canopy/01_GNSS/01_raw\"),\n    ) -&gt; None:\n        \"\"\"Initialize matcher with directory structure.\"\"\"\n        self.root = Path(root)\n        self.reference_dir = self.root / reference_pattern\n        self.canopy_dir = self.root / canopy_pattern\n\n        # Validate directories exist\n        self._validate_directory(self.root, \"Root\")\n        self._validate_directory(self.reference_dir, \"Reference\")\n        self._validate_directory(self.canopy_dir, \"Canopy\")\n\n    def __iter__(self) -&gt; Iterator[MatchedDirs]:\n        \"\"\"Iterate over matched directory pairs with RINEX files.\n\n        Yields\n        ------\n        MatchedDirs\n            Matched directories for each date.\n\n        \"\"\"\n        for date_str in self.get_common_dates():\n            yield MatchedDirs(\n                canopy_data_dir=self.canopy_dir / date_str,\n                reference_data_dir=self.reference_dir / date_str,\n                yyyydoy=YYYYDOY.from_yydoy_str(date_str),\n            )\n\n    def get_common_dates(self) -&gt; list[str]:\n        \"\"\"Get dates with RINEX files in both receivers.\n\n        Uses parallel processing to check directories efficiently.\n\n        Returns\n        -------\n        list[str]\n            Sorted list of date strings (YYDDD format, e.g., \"25001\")\n            that have RINEX files in both canopy and reference directories.\n\n        \"\"\"\n        # Find dates with RINEX in each receiver\n        ref_dates = self._get_dates_with_rinex(self.reference_dir)\n        can_dates = self._get_dates_with_rinex(self.canopy_dir)\n\n        # Find intersection\n        common = ref_dates &amp; can_dates\n        common.discard(\"00000\")  # Remove placeholder directories\n\n        # Sort naturally (numerical order)\n        return natsorted(common)\n\n    def _get_dates_with_rinex(self, base_dir: Path) -&gt; set[str]:\n        \"\"\"Find all date directories containing RINEX files.\n\n        Uses parallel processing to check multiple directories at once.\n\n        Parameters\n        ----------\n        base_dir : Path\n            Base directory to search (e.g., canopy or reference root).\n\n        Returns\n        -------\n        set[str]\n            Set of date directory names that contain RINEX files.\n\n        \"\"\"\n        # Get all subdirectories\n        date_dirs = (d for d in base_dir.iterdir() if d.is_dir())\n\n        # Check for RINEX files in parallel\n        dates_with_rinex = set()\n\n        with ThreadPoolExecutor() as executor:\n            future_to_dir = {\n                executor.submit(self._has_rinex_files, d): d for d in date_dirs\n            }\n\n            for future in as_completed(future_to_dir):\n                directory = future_to_dir[future]\n                if future.result():\n                    dates_with_rinex.add(directory.name)\n\n        return dates_with_rinex\n\n    @staticmethod\n    def _has_rinex_files(directory: Path) -&gt; bool:\n        \"\"\"Check if directory contains RINEX observation files.\n\n        Parameters\n        ----------\n        directory : Path\n            Directory to check.\n\n        Returns\n        -------\n        bool\n            True if RINEX files found.\n\n        \"\"\"\n        return _has_rinex_files(directory)\n\n    def _validate_directory(self, path: Path, name: str) -&gt; None:\n        \"\"\"Validate directory exists.\n\n        Parameters\n        ----------\n        path : Path\n            Directory to check.\n        name : str\n            Name for error message.\n\n        Raises\n        ------\n        FileNotFoundError\n            If directory doesn't exist.\n\n        \"\"\"\n        if not path.exists():\n            msg = f\"{name} directory not found: {path}\"\n            raise FileNotFoundError(msg)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DataDirMatcher--iterate-over-matched-directories","level":3,"title":"Iterate over matched directories","text":"<p>for matched_dirs in matcher: ...     print(matched_dirs.yyyydoy) ...     rinex_files = list(matched_dirs.canopy_data_dir.glob(\"*.25o\")) ...     print(f\"  Found {len(rinex_files)} RINEX files\")</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DataDirMatcher--get-list-of-common-dates","level":3,"title":"Get list of common dates","text":"<p>dates = matcher.get_common_dates() print(f\"Found {len(dates)} dates with data\")</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DataDirMatcher.__init__","level":3,"title":"<code>__init__(root, reference_pattern=Path('01_reference/01_GNSS/01_raw'), canopy_pattern=Path('02_canopy/01_GNSS/01_raw'))</code>","text":"<p>Initialize matcher with directory structure.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>def __init__(\n    self,\n    root: Path,\n    reference_pattern: Path = Path(\"01_reference/01_GNSS/01_raw\"),\n    canopy_pattern: Path = Path(\"02_canopy/01_GNSS/01_raw\"),\n) -&gt; None:\n    \"\"\"Initialize matcher with directory structure.\"\"\"\n    self.root = Path(root)\n    self.reference_dir = self.root / reference_pattern\n    self.canopy_dir = self.root / canopy_pattern\n\n    # Validate directories exist\n    self._validate_directory(self.root, \"Root\")\n    self._validate_directory(self.reference_dir, \"Reference\")\n    self._validate_directory(self.canopy_dir, \"Canopy\")\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DataDirMatcher.__iter__","level":3,"title":"<code>__iter__()</code>","text":"<p>Iterate over matched directory pairs with RINEX files.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DataDirMatcher.__iter__--yields","level":5,"title":"Yields","text":"<p>MatchedDirs     Matched directories for each date.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>def __iter__(self) -&gt; Iterator[MatchedDirs]:\n    \"\"\"Iterate over matched directory pairs with RINEX files.\n\n    Yields\n    ------\n    MatchedDirs\n        Matched directories for each date.\n\n    \"\"\"\n    for date_str in self.get_common_dates():\n        yield MatchedDirs(\n            canopy_data_dir=self.canopy_dir / date_str,\n            reference_data_dir=self.reference_dir / date_str,\n            yyyydoy=YYYYDOY.from_yydoy_str(date_str),\n        )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DataDirMatcher.get_common_dates","level":3,"title":"<code>get_common_dates()</code>","text":"<p>Get dates with RINEX files in both receivers.</p> <p>Uses parallel processing to check directories efficiently.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.DataDirMatcher.get_common_dates--returns","level":5,"title":"Returns","text":"<p>list[str]     Sorted list of date strings (YYDDD format, e.g., \"25001\")     that have RINEX files in both canopy and reference directories.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>def get_common_dates(self) -&gt; list[str]:\n    \"\"\"Get dates with RINEX files in both receivers.\n\n    Uses parallel processing to check directories efficiently.\n\n    Returns\n    -------\n    list[str]\n        Sorted list of date strings (YYDDD format, e.g., \"25001\")\n        that have RINEX files in both canopy and reference directories.\n\n    \"\"\"\n    # Find dates with RINEX in each receiver\n    ref_dates = self._get_dates_with_rinex(self.reference_dir)\n    can_dates = self._get_dates_with_rinex(self.canopy_dir)\n\n    # Find intersection\n    common = ref_dates &amp; can_dates\n    common.discard(\"00000\")  # Remove placeholder directories\n\n    # Sort naturally (numerical order)\n    return natsorted(common)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.PairDataDirMatcher","level":2,"title":"<code>PairDataDirMatcher</code>","text":"<p>Match RINEX directories for receiver pairs across dates.</p> <p>Supports multi-receiver configurations where multiple canopy/reference pairs may exist at the same site. Requires a configuration dict specifying receiver locations and analysis pairs.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.PairDataDirMatcher--parameters","level":4,"title":"Parameters","text":"<p>base_dir : Path     Root directory containing all receiver data receivers : dict     Receiver configuration mapping receiver names to their directory paths.     The <code>directory</code> value is the full relative path from <code>base_dir</code> to the     raw RINEX data directory (before the <code>{YYDOY}</code> date folders).     Example: {\"canopy_01\": {\"directory\": \"02_canopy_01/01_GNSS/01_raw\"},               \"reference_01\": {\"directory\": \"01_reference_01/01_GNSS/01_raw\"}} analysis_pairs : dict     Analysis pair configuration specifying which receivers to match     Example: {\"pair_01\": {\"canopy_receiver\": \"canopy_01\",                            \"reference_receiver\": \"reference_01\"}}</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.PairDataDirMatcher--examples","level":4,"title":"Examples","text":"<p>receivers = { ...     \"canopy_01\": {\"directory\": \"02_canopy/01_GNSS/01_raw\"}, ...     \"reference_01\": {\"directory\": \"01_reference/01_GNSS/01_raw\"} ... } pairs = { ...     \"main_pair\": { ...         \"canopy_receiver\": \"canopy_01\", ...         \"reference_receiver\": \"reference_01\" ...     } ... }</p> <p>matcher = PairDataDirMatcher( ...     base_dir=Path(\"/data/01_Rosalia\"), ...     receivers=receivers, ...     analysis_pairs=pairs ... )</p> <p>for matched in matcher: ...     print(f\"{matched.yyyydoy}: {matched.pair_name}\") ...     print(f\"  Canopy: {matched.canopy_data_dir}\") ...     print(f\"  Reference: {matched.reference_data_dir}\")</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>class PairDataDirMatcher:\n    \"\"\"Match RINEX directories for receiver pairs across dates.\n\n    Supports multi-receiver configurations where multiple canopy/reference\n    pairs may exist at the same site. Requires a configuration dict\n    specifying receiver locations and analysis pairs.\n\n    Parameters\n    ----------\n    base_dir : Path\n        Root directory containing all receiver data\n    receivers : dict\n        Receiver configuration mapping receiver names to their directory paths.\n        The ``directory`` value is the full relative path from ``base_dir`` to the\n        raw RINEX data directory (before the ``{YYDOY}`` date folders).\n        Example: {\"canopy_01\": {\"directory\": \"02_canopy_01/01_GNSS/01_raw\"},\n                  \"reference_01\": {\"directory\": \"01_reference_01/01_GNSS/01_raw\"}}\n    analysis_pairs : dict\n        Analysis pair configuration specifying which receivers to match\n        Example: {\"pair_01\": {\"canopy_receiver\": \"canopy_01\",\n                               \"reference_receiver\": \"reference_01\"}}\n\n    Examples\n    --------\n    &gt;&gt;&gt; receivers = {\n    ...     \"canopy_01\": {\"directory\": \"02_canopy/01_GNSS/01_raw\"},\n    ...     \"reference_01\": {\"directory\": \"01_reference/01_GNSS/01_raw\"}\n    ... }\n    &gt;&gt;&gt; pairs = {\n    ...     \"main_pair\": {\n    ...         \"canopy_receiver\": \"canopy_01\",\n    ...         \"reference_receiver\": \"reference_01\"\n    ...     }\n    ... }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; matcher = PairDataDirMatcher(\n    ...     base_dir=Path(\"/data/01_Rosalia\"),\n    ...     receivers=receivers,\n    ...     analysis_pairs=pairs\n    ... )\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; for matched in matcher:\n    ...     print(f\"{matched.yyyydoy}: {matched.pair_name}\")\n    ...     print(f\"  Canopy: {matched.canopy_data_dir}\")\n    ...     print(f\"  Reference: {matched.reference_data_dir}\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        base_dir: Path,\n        receivers: dict[str, dict[str, str]],\n        analysis_pairs: dict[str, dict[str, str]],\n    ) -&gt; None:\n        \"\"\"Initialize pair matcher with receiver configuration.\"\"\"\n        self.base_dir = Path(base_dir)\n        self.receivers = receivers\n        self.analysis_pairs = analysis_pairs\n\n        # Validate receivers have directory config\n        self.receiver_dirs = self._build_receiver_dir_mapping()\n\n    def _build_receiver_dir_mapping(self) -&gt; dict[str, str]:\n        \"\"\"Map receiver names to their directory prefixes.\n\n        Returns\n        -------\n        dict[str, str]\n            Mapping of receiver name to directory path.\n\n        Raises\n        ------\n        ValueError\n            If receiver missing 'directory' in config.\n\n        \"\"\"\n        mapping = {}\n        for receiver_name, config in self.receivers.items():\n            if \"directory\" not in config:\n                msg = f\"Receiver '{receiver_name}' missing 'directory' in config\"\n                raise ValueError(msg)\n            mapping[receiver_name] = config[\"directory\"]\n        return mapping\n\n    def _get_receiver_path(self, receiver_name: str, yyyydoy: YYYYDOY) -&gt; Path:\n        \"\"\"Build full path to receiver data for a specific date.\n\n        Parameters\n        ----------\n        receiver_name : str\n            Receiver name (e.g., \"canopy_01\").\n        yyyydoy : YYYYDOY\n            Date object.\n\n        Returns\n        -------\n        Path\n            Full path to receiver's RINEX directory for the date.\n\n        \"\"\"\n        receiver_dir = self.receiver_dirs[receiver_name]\n\n        # Convert YYYYDDD to YYDDD format for directory name\n        yyddd_str = yyyydoy.yydoy\n\n        return self.base_dir / receiver_dir / yyddd_str\n\n    def _get_all_dates(self) -&gt; set[YYYYDOY]:\n        \"\"\"Find all dates that have data in any receiver directory.\n\n        Returns\n        -------\n        set[YYYYDOY]\n            Set of all dates with available data.\n\n        \"\"\"\n        all_dates = set()\n\n        for receiver_name in self.receivers:\n            receiver_dir = self.receiver_dirs[receiver_name]\n            receiver_base = self.base_dir / receiver_dir\n\n            if not receiver_base.exists():\n                continue\n\n            # Find all date directories (format: YYDDD - 5 digits)\n            for date_dir in receiver_base.iterdir():\n                if not date_dir.is_dir():\n                    continue\n\n                # Check if directory name is 5 digits\n                if len(date_dir.name) != DATE_DIR_LEN or not date_dir.name.isdigit():\n                    continue\n\n                # Skip placeholder directories\n                if date_dir.name == \"00000\":\n                    continue\n\n                try:\n                    yyyydoy = YYYYDOY.from_yydoy_str(date_dir.name)\n                    all_dates.add(yyyydoy)\n                except ValueError:\n                    continue\n\n        return all_dates\n\n    def __iter__(self) -&gt; Iterator[PairMatchedDirs]:\n        \"\"\"Iterate over all date/pair combinations with available data.\n\n        Yields\n        ------\n        PairMatchedDirs\n            Matched directories for a receiver pair on a specific date.\n\n        \"\"\"\n        all_dates = sorted(self._get_all_dates())\n\n        for yyyydoy in all_dates:\n            # For each configured analysis pair\n            for pair_name, pair_config in self.analysis_pairs.items():\n                canopy_rx = pair_config[\"canopy_receiver\"]\n                reference_rx = pair_config[\"reference_receiver\"]\n\n                # Build paths for this pair\n                canopy_path = self._get_receiver_path(canopy_rx, yyyydoy)\n                reference_path = self._get_receiver_path(reference_rx, yyyydoy)\n\n                # Check for RINEX files\n                canopy_has_files = _has_rinex_files(canopy_path)\n                reference_has_files = _has_rinex_files(reference_path)\n\n                # Only yield if both directories exist and have data\n                if canopy_has_files and reference_has_files:\n                    yield PairMatchedDirs(\n                        yyyydoy=yyyydoy,\n                        pair_name=pair_name,\n                        canopy_receiver=canopy_rx,\n                        reference_receiver=reference_rx,\n                        canopy_data_dir=canopy_path,\n                        reference_data_dir=reference_path,\n                    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.PairDataDirMatcher.__init__","level":3,"title":"<code>__init__(base_dir, receivers, analysis_pairs)</code>","text":"<p>Initialize pair matcher with receiver configuration.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>def __init__(\n    self,\n    base_dir: Path,\n    receivers: dict[str, dict[str, str]],\n    analysis_pairs: dict[str, dict[str, str]],\n) -&gt; None:\n    \"\"\"Initialize pair matcher with receiver configuration.\"\"\"\n    self.base_dir = Path(base_dir)\n    self.receivers = receivers\n    self.analysis_pairs = analysis_pairs\n\n    # Validate receivers have directory config\n    self.receiver_dirs = self._build_receiver_dir_mapping()\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.PairDataDirMatcher.__iter__","level":3,"title":"<code>__iter__()</code>","text":"<p>Iterate over all date/pair combinations with available data.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.PairDataDirMatcher.__iter__--yields","level":5,"title":"Yields","text":"<p>PairMatchedDirs     Matched directories for a receiver pair on a specific date.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>def __iter__(self) -&gt; Iterator[PairMatchedDirs]:\n    \"\"\"Iterate over all date/pair combinations with available data.\n\n    Yields\n    ------\n    PairMatchedDirs\n        Matched directories for a receiver pair on a specific date.\n\n    \"\"\"\n    all_dates = sorted(self._get_all_dates())\n\n    for yyyydoy in all_dates:\n        # For each configured analysis pair\n        for pair_name, pair_config in self.analysis_pairs.items():\n            canopy_rx = pair_config[\"canopy_receiver\"]\n            reference_rx = pair_config[\"reference_receiver\"]\n\n            # Build paths for this pair\n            canopy_path = self._get_receiver_path(canopy_rx, yyyydoy)\n            reference_path = self._get_receiver_path(reference_rx, yyyydoy)\n\n            # Check for RINEX files\n            canopy_has_files = _has_rinex_files(canopy_path)\n            reference_has_files = _has_rinex_files(reference_path)\n\n            # Only yield if both directories exist and have data\n            if canopy_has_files and reference_has_files:\n                yield PairMatchedDirs(\n                    yyyydoy=yyyydoy,\n                    pair_name=pair_name,\n                    canopy_receiver=canopy_rx,\n                    reference_receiver=reference_rx,\n                    canopy_data_dir=canopy_path,\n                    reference_data_dir=reference_path,\n                )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.MatchedDirs","level":2,"title":"<code>MatchedDirs</code>  <code>dataclass</code>","text":"<p>Matched directory paths for canopy and reference receivers.</p> <p>Immutable container representing a pair of directories containing RINEX data for the same date.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.MatchedDirs--parameters","level":4,"title":"Parameters","text":"<p>canopy_data_dir : Path     Path to canopy receiver RINEX directory. reference_data_dir : Path     Path to reference (open-sky) receiver RINEX directory. yyyydoy : YYYYDOY     Date object for this matched pair.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.MatchedDirs--examples","level":4,"title":"Examples","text":"<p>from pathlib import Path from canvod.utils.tools import YYYYDOY</p> <p>md = MatchedDirs( ...     canopy_data_dir=Path(\"/data/02_canopy/25001\"), ...     reference_data_dir=Path(\"/data/01_reference/25001\"), ...     yyyydoy=YYYYDOY.from_str(\"2025001\") ... ) md.yyyydoy.to_str() '2025001'</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/models.py</code> <pre><code>@dataclass(frozen=True)\nclass MatchedDirs:\n    \"\"\"Matched directory paths for canopy and reference receivers.\n\n    Immutable container representing a pair of directories containing\n    RINEX data for the same date.\n\n    Parameters\n    ----------\n    canopy_data_dir : Path\n        Path to canopy receiver RINEX directory.\n    reference_data_dir : Path\n        Path to reference (open-sky) receiver RINEX directory.\n    yyyydoy : YYYYDOY\n        Date object for this matched pair.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; from canvod.utils.tools import YYYYDOY\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; md = MatchedDirs(\n    ...     canopy_data_dir=Path(\"/data/02_canopy/25001\"),\n    ...     reference_data_dir=Path(\"/data/01_reference/25001\"),\n    ...     yyyydoy=YYYYDOY.from_str(\"2025001\")\n    ... )\n    &gt;&gt;&gt; md.yyyydoy.to_str()\n    '2025001'\n\n    \"\"\"\n\n    canopy_data_dir: Path\n    reference_data_dir: Path\n    yyyydoy: YYYYDOY\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.PairMatchedDirs","level":2,"title":"<code>PairMatchedDirs</code>  <code>dataclass</code>","text":"<p>Matched directories for a receiver pair on a specific date.</p> <p>Supports multi-receiver configurations where multiple canopy/reference pairs may exist at the same site.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.PairMatchedDirs--parameters","level":4,"title":"Parameters","text":"<p>yyyydoy : YYYYDOY     Date for this matched pair. pair_name : str     Identifier for this receiver pair (e.g., \"pair_01\"). canopy_receiver : str     Name of canopy receiver (e.g., \"canopy_01\"). reference_receiver : str     Name of reference receiver (e.g., \"reference_01\"). canopy_data_dir : Path     Path to canopy receiver RINEX directory. reference_data_dir : Path     Path to reference receiver RINEX directory.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.PairMatchedDirs--examples","level":4,"title":"Examples","text":"<p>pmd = PairMatchedDirs( ...     yyyydoy=YYYYDOY.from_str(\"2025001\"), ...     pair_name=\"pair_01\", ...     canopy_receiver=\"canopy_01\", ...     reference_receiver=\"reference_01\", ...     canopy_data_dir=Path(\"/data/canopy_01/25001\"), ...     reference_data_dir=Path(\"/data/reference_01/25001\") ... ) pmd.pair_name 'pair_01'</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/models.py</code> <pre><code>@dataclass\nclass PairMatchedDirs:\n    \"\"\"Matched directories for a receiver pair on a specific date.\n\n    Supports multi-receiver configurations where multiple canopy/reference\n    pairs may exist at the same site.\n\n    Parameters\n    ----------\n    yyyydoy : YYYYDOY\n        Date for this matched pair.\n    pair_name : str\n        Identifier for this receiver pair (e.g., \"pair_01\").\n    canopy_receiver : str\n        Name of canopy receiver (e.g., \"canopy_01\").\n    reference_receiver : str\n        Name of reference receiver (e.g., \"reference_01\").\n    canopy_data_dir : Path\n        Path to canopy receiver RINEX directory.\n    reference_data_dir : Path\n        Path to reference receiver RINEX directory.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pmd = PairMatchedDirs(\n    ...     yyyydoy=YYYYDOY.from_str(\"2025001\"),\n    ...     pair_name=\"pair_01\",\n    ...     canopy_receiver=\"canopy_01\",\n    ...     reference_receiver=\"reference_01\",\n    ...     canopy_data_dir=Path(\"/data/canopy_01/25001\"),\n    ...     reference_data_dir=Path(\"/data/reference_01/25001\")\n    ... )\n    &gt;&gt;&gt; pmd.pair_name\n    'pair_01'\n\n    \"\"\"\n\n    yyyydoy: YYYYDOY\n    pair_name: str\n    canopy_receiver: str\n    reference_receiver: str\n    canopy_data_dir: Path\n    reference_data_dir: Path\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#rinex-v304","level":2,"title":"RINEX v3.04","text":"<p>RINEX v3.04 observation file reader.</p> <p>Migrated from: gnssvodpy/rinexreader/rinex_reader.py</p> <p>Changes from original: - Updated imports to use canvod.readers.gnss_specs - Added structured logging for LLM-friendly diagnostics - Removed IcechunkPreprocessor calls (TODO: move to canvod-store) - Preserved all other functionality</p> <p>Classes: - Rnxv3Header: Parse RINEX v3 headers - Rnxv3Obs: Main reader class, converts RINEX to xarray Dataset</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Header","level":2,"title":"<code>Rnxv3Header</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Enhanced RINEX v3 header following the original implementation logic.</p> <p>Key changes from previous version: - date field is now datetime (like original) - Uses the original parsing logic for __get_pgm_runby_date</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Header--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic <code>BaseModel</code> configured with <code>ConfigDict</code> (frozen, validate_assignment, arbitrary_types_allowed, str_strip_whitespace). Prefer :meth:<code>from_file</code> for construction.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>class Rnxv3Header(BaseModel):\n    \"\"\"Enhanced RINEX v3 header following the original implementation logic.\n\n    Key changes from previous version:\n    - date field is now datetime (like original)\n    - Uses the original parsing logic for __get_pgm_runby_date\n\n    Notes\n    -----\n    This is a Pydantic `BaseModel` configured with `ConfigDict` (frozen,\n    validate_assignment, arbitrary_types_allowed, str_strip_whitespace). Prefer\n    :meth:`from_file` for construction.\n\n    \"\"\"\n\n    model_config = ConfigDict(\n        frozen=True,\n        validate_assignment=True,\n        arbitrary_types_allowed=True,\n        str_strip_whitespace=True,\n    )\n\n    # Required fields\n    fpath: Path\n    version: float\n    filetype: str\n    rinextype: str\n    systems: str\n    pgm: str\n    run_by: str\n    date: datetime\n    marker_name: str\n    observer: str\n    agency: str\n    receiver_number: str\n    receiver_type: str\n    receiver_version: str\n    antenna_number: str\n    antenna_type: str\n    approx_position: list[pint.Quantity]\n    antenna_position: list[pint.Quantity]\n    t0: dict[str, datetime]\n    signal_strength_unit: pint.Unit | str\n    obs_codes_per_system: dict[str, list[str]]\n\n    # Optional fields with defaults\n    comment: str | None = None\n    marker_number: int | None = None\n    marker_type: str | None = None\n    glonass_cod: str | None = None\n    glonass_phs: str | None = None\n    glonass_bis: str | None = None\n    glonass_slot_freq_dict: dict[str, int] = Field(default_factory=dict)\n    leap_seconds: pint.Quantity | None = None\n    system_phase_shift: dict[str, dict[str, float | None]] = Field(default_factory=dict)\n\n    @field_validator(\"marker_number\", mode=\"before\")\n    @classmethod\n    def parse_marker_number(cls, v: object) -&gt; int | None:\n        \"\"\"Convert empty strings to None, parse valid integers.\"\"\"\n        if v is None or (isinstance(v, str) and not v.strip()):\n            return None\n        try:\n            return int(v)\n        except (ValueError, TypeError):\n            return None\n\n    @classmethod\n    def from_file(cls, fpath: Path) -&gt; Self:\n        \"\"\"Create header from a RINEX file.\"\"\"\n        # External validation models handle file and version checks\n        _ = RnxObsFileModel(fpath=fpath)\n\n        try:\n            header = gr.rinexheader(fpath)\n        except (OSError, ValueError, TypeError) as e:\n            msg = f\"Failed to read RINEX header: {e}\"\n            raise ValueError(msg) from e\n\n        RnxVersion3Model.version_must_be_3(header[\"version\"])\n\n        # Parse and create instance using original logic\n        parsed_data = cls._parse_header_data(header, fpath)\n        return cls.model_validate(parsed_data)\n\n    @staticmethod\n    def _parse_header_data(\n        header: dict[str, Any],\n        fpath: Path,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Parse raw header into structured data using original logic.\n\n        Parameters\n        ----------\n        header : dict[str, Any]\n            Raw header dictionary returned by `georinex`.\n        fpath : Path\n            Path to the RINEX file.\n\n        Returns\n        -------\n        dict[str, Any]\n            Parsed header data suitable for model validation.\n\n        \"\"\"\n        data = {\n            \"fpath\": fpath,\n            \"version\": header.get(\"version\", 3.0),\n            \"filetype\": header.get(\"filetype\", \"\"),\n            \"rinextype\": header.get(\"rinextype\", \"\"),\n            \"systems\": header.get(\"systems\", \"\"),\n        }\n\n        if \"PGM / RUN BY / DATE\" in header:\n            pgm, run_by, date_dt = Rnxv3Header._get_pgm_runby_date(header)\n            data.update(\n                {\n                    \"pgm\": pgm,\n                    \"run_by\": run_by,\n                    \"date\": date_dt,  # This is now a datetime object\n                }\n            )\n        else:\n            data.update(\n                {\n                    \"pgm\": \"\",\n                    \"run_by\": \"\",\n                    \"date\": datetime.now(timezone.utc),  # Default to current time\n                }\n            )\n\n        if \"OBSERVER / AGENCY\" in header:\n            observer, agency = Rnxv3Header._get_observer_agency(header)\n            data.update({\"observer\": observer, \"agency\": agency})\n        else:\n            data.update({\"observer\": \"\", \"agency\": \"\"})\n\n        if \"REC # / TYPE / VERS\" in header:\n            rec_num, rec_type, rec_version = Rnxv3Header._get_receiver_num_type_version(\n                header\n            )\n            data.update(\n                {\n                    \"receiver_number\": rec_num,\n                    \"receiver_type\": rec_type,\n                    \"receiver_version\": rec_version,\n                }\n            )\n        else:\n            data.update(\n                {\"receiver_number\": \"\", \"receiver_type\": \"\", \"receiver_version\": \"\"}\n            )\n\n        if \"ANT # / TYPE\" in header:\n            ant_num, ant_type = Rnxv3Header._get_antenna_num_type(header)\n            data.update({\"antenna_number\": ant_num, \"antenna_type\": ant_type})\n        else:\n            data.update({\"antenna_number\": \"\", \"antenna_type\": \"\"})\n\n        # Parse positions with safe fallbacks\n        pos_parts = header.get(\"APPROX POSITION XYZ\", \"0 0 0\").split()\n        delta_parts = header.get(\"ANTENNA: DELTA H/E/N\", \"0 0 0\").split()\n\n        def safe_float(s: str, default: float = 0.0) -&gt; float:\n            try:\n                return float(s)\n            except (ValueError, TypeError):\n                return default\n\n        pos_y = (\n            safe_float(pos_parts[1]) * UREG.meters\n            if len(pos_parts) &gt; 1\n            else 0.0 * UREG.meters\n        )\n        pos_z = (\n            safe_float(pos_parts[2]) * UREG.meters\n            if len(pos_parts) &gt; POSITION_PARTS_MIN\n            else 0.0 * UREG.meters\n        )\n        ant_y = (\n            safe_float(delta_parts[1]) * UREG.meters\n            if len(delta_parts) &gt; 1\n            else 0.0 * UREG.meters\n        )\n        ant_z = (\n            safe_float(delta_parts[2]) * UREG.meters\n            if len(delta_parts) &gt; DELTA_PARTS_MIN\n            else 0.0 * UREG.meters\n        )\n\n        data.update(\n            {\n                \"approx_position\": [\n                    safe_float(pos_parts[0]) * UREG.meters,\n                    pos_y,\n                    pos_z,\n                ],\n                \"antenna_position\": [\n                    safe_float(delta_parts[0]) * UREG.meters,\n                    ant_y,\n                    ant_z,\n                ],\n            }\n        )\n\n        if \"TIME OF FIRST OBS\" in header:\n            data[\"t0\"] = Rnxv3Header._get_time_of_first_obs(header)\n        else:\n            now = datetime.now(timezone.utc)\n            data[\"t0\"] = {\n                \"UTC\": now.replace(tzinfo=pytz.UTC) if now.tzinfo is None else now,\n                \"GPS\": now,\n            }\n\n        # Signal strength unit\n        data[\"signal_strength_unit\"] = Rnxv3Header._get_signal_strength_unit(header)\n\n        # Basic fields\n        data.update(\n            {\n                \"comment\": header.get(\"COMMENT\"),\n                \"marker_name\": header.get(\"MARKER NAME\", \"\").strip(),\n                \"marker_number\": header.get(\"MARKER NUMBER\"),\n                \"marker_type\": header.get(\"MARKER TYPE\"),\n                \"obs_codes_per_system\": header.get(\"fields\", {}),\n            }\n        )\n\n        # Optional GLONASS fields using original methods\n        if \"GLONASS COD/PHS/BIS\" in header:\n            cod, phs, bis = Rnxv3Header._get_glonass_cod_phs_bis(header)\n            data.update({\"glonass_cod\": cod, \"glonass_phs\": phs, \"glonass_bis\": bis})\n\n        if \"GLONASS SLOT / FRQ #\" in header:\n            data[\"glonass_slot_freq_dict\"] = Rnxv3Header._get_glonass_slot_freq_num(\n                header\n            )\n\n        # Leap seconds\n        if \"LEAP SECONDS\" in header:\n            leap_parts = header[\"LEAP SECONDS\"].split()\n            if leap_parts and leap_parts[0].lstrip(\"-\").isdigit():\n                data[\"leap_seconds\"] = int(leap_parts[0]) * UREG.seconds\n\n        # System phase shift using original method\n        if \"SYS / PHASE SHIFT\" in header:\n            data[\"system_phase_shift\"] = Rnxv3Header._get_sys_phase_shift(header)\n        else:\n            data[\"system_phase_shift\"] = {}\n\n        return data\n\n    @staticmethod\n    def _get_pgm_runby_date(\n        header_dict: dict[str, Any],\n    ) -&gt; tuple[str, str, datetime]:\n        \"\"\"Parse ``PGM / RUN BY / DATE`` into program, run_by, and datetime.\n\n        Based on the original __get_pgm_runby_date method.\n        \"\"\"\n        header_value = header_dict.get(\"PGM / RUN BY / DATE\", \"\")\n        components = header_value.split()\n\n        if not components:\n            return \"\", \"\", datetime.now(timezone.utc)\n\n        pgm = components[0]\n        run_by = components[1] if len(components) &gt; PGM_RUNBY_MIN_COMPONENTS else \"\"\n\n        # Original logic for extracting date components\n        date = (\n            [components[-3], components[-2], components[-1]]\n            if len(components) &gt; 1\n            else None\n        )\n\n        if date:\n            try:\n                # Original parsing logic\n                dt = datetime.strptime(\n                    date[0] + date[1],\n                    \"%Y%m%d%H%M%S\",\n                )\n                tz = pytz.timezone(date[2])  # e.g., \"UTC\"\n                localized_date = tz.localize(dt)\n                return pgm, run_by, localized_date\n            except (ValueError, TypeError) as e:\n                print(f\"Warning: Could not parse date components {date}: {e}\")\n                return pgm, run_by, datetime.now(timezone.utc)\n        else:\n            return pgm, run_by, datetime.now(timezone.utc)\n\n    @staticmethod\n    def _get_observer_agency(header_dict: dict[str, Any]) -&gt; tuple[str, str]:\n        \"\"\"Parse ``OBSERVER / AGENCY`` record.\n\n        Parameters\n        ----------\n        header_dict : dict[str, Any]\n            Raw header dictionary.\n\n        Returns\n        -------\n        tuple[str, str]\n            (observer, agency).\n\n        \"\"\"\n        header_value = header_dict.get(\"OBSERVER / AGENCY\", \"\")\n        try:\n            observer, agency = header_value.split(maxsplit=1)\n            return observer, agency\n        except ValueError:\n            return \"\", \"\"\n\n    @staticmethod\n    def _get_receiver_num_type_version(\n        header_dict: dict[str, Any],\n    ) -&gt; tuple[str, str, str]:\n        \"\"\"Parse ``REC # / TYPE / VERS`` record.\n\n        Parameters\n        ----------\n        header_dict : dict[str, Any]\n            Raw header dictionary.\n\n        Returns\n        -------\n        tuple[str, str, str]\n            (receiver_number, receiver_type, receiver_version).\n\n        \"\"\"\n        header_value = header_dict.get(\"REC # / TYPE / VERS\", \"\")\n        components = header_value.split()\n\n        if not components:\n            return \"\", \"\", \"\"\n        if len(components) == 1:\n            return components[0], \"\", \"\"\n        if len(components) == RECEIVER_COMPONENTS_SECOND:\n            return components[0], components[1], \"\"\n        return components[0], \" \".join(components[1:-1]), components[-1]\n\n    @staticmethod\n    def _get_antenna_num_type(header_dict: dict[str, Any]) -&gt; tuple[str, str]:\n        \"\"\"Parse ``ANT # / TYPE`` record.\n\n        Parameters\n        ----------\n        header_dict : dict[str, Any]\n            Raw header dictionary.\n\n        Returns\n        -------\n        tuple[str, str]\n            (antenna_number, antenna_type).\n\n        \"\"\"\n        header_value = header_dict.get(\"ANT # / TYPE\", \"\")\n        components = header_value.split()\n\n        if not components:\n            return \"\", \"\"\n        if len(components) == 1:\n            return components[0], \"\"\n        return components[0], \" \".join(components[1:])\n\n    @staticmethod\n    def _get_time_of_first_obs(\n        header_dict: dict[str, Any],\n    ) -&gt; dict[str, datetime]:\n        \"\"\"Parse ``TIME OF FIRST OBS`` record.\n\n        Parameters\n        ----------\n        header_dict : dict[str, Any]\n            Raw header dictionary.\n\n        Returns\n        -------\n        dict[str, datetime]\n            Mapping of time system labels to datetimes.\n\n        \"\"\"\n        header_value = header_dict.get(\"TIME OF FIRST OBS\", \"\")\n        components = header_value.split()\n\n        if len(components) &lt; TIME_OF_FIRST_OBS_MIN_COMPONENTS:\n            now = datetime.now(timezone.utc)\n            return {\"UTC\": now, \"GPS\": now}\n\n        try:\n            year, month, day = map(int, components[:3])\n            hour, minute = map(int, components[3:5])\n            second = float(components[5])\n\n            dt_gps = datetime(\n                year,\n                month,\n                day,\n                hour,\n                minute,\n                int(second),\n                int((second - int(second)) * 1e6),\n                tzinfo=timezone.utc,\n            )\n\n            gps_utc_offset = timedelta(seconds=18)\n            dt_utc = dt_gps - gps_utc_offset\n            tz = pytz.timezone(\"UTC\")\n\n            return {\"UTC\": tz.localize(dt_utc), \"GPS\": dt_gps}\n\n        except (ValueError, TypeError, IndexError):\n            now = datetime.now(timezone.utc)\n            return {\"UTC\": now, \"GPS\": now}\n\n    @staticmethod\n    def _get_glonass_cod_phs_bis(\n        header_dict: dict[str, Any],\n    ) -&gt; tuple[str, str, str]:\n        \"\"\"Parse ``GLONASS COD/PHS/BIS`` record.\n\n        Parameters\n        ----------\n        header_dict : dict[str, Any]\n            Raw header dictionary.\n\n        Returns\n        -------\n        tuple[str, str, str]\n            (glonass_cod, glonass_phs, glonass_bis).\n\n        \"\"\"\n        header_value = header_dict.get(\"GLONASS COD/PHS/BIS\", \"\")\n        components = header_value.split()\n\n        if len(components) &gt;= GLONASS_COD_PHS_MIN_COMPONENTS:\n            c1c = f\"{components[0]} {components[1]}\"\n            c2c = f\"{components[2]} {components[3]}\"\n            c2p = f\"{components[4]} {components[5]}\"\n            return c1c, c2c, c2p\n        return \"\", \"\", \"\"\n\n    @staticmethod\n    def _get_glonass_slot_freq_num(\n        header_dict: dict[str, Any],\n    ) -&gt; dict[str, int]:\n        \"\"\"Parse ``GLONASS SLOT / FRQ #`` record.\n\n        Parameters\n        ----------\n        header_dict : dict[str, Any]\n            Raw header dictionary.\n\n        Returns\n        -------\n        dict[str, int]\n            Mapping of slot to frequency number.\n\n        \"\"\"\n        header_value = header_dict.get(\"GLONASS SLOT / FRQ #\", \"\")\n        components = header_value.split()\n\n        result = {}\n        for i in range(1, len(components), 2):  # Skip first component\n            if i + 1 &lt; len(components):\n                try:\n                    slot = components[i]\n                    freq_num = int(components[i + 1])\n                    result[slot] = freq_num\n                except (ValueError, IndexError):\n                    continue\n\n        return result\n\n    @staticmethod\n    def _get_sys_phase_shift(\n        header_dict: dict[str, Any],\n    ) -&gt; dict[str, dict[str, float | None]]:\n        \"\"\"Parse ``SYS / PHASE SHIFT`` records.\n\n        Parameters\n        ----------\n        header_dict : dict[str, Any]\n            Raw header dictionary.\n\n        Returns\n        -------\n        dict[str, dict[str, float | None]]\n            Mapping of system to signal phase shifts.\n\n        \"\"\"\n        header_value = header_dict.get(\"SYS / PHASE SHIFT\", \"\")\n        components = header_value.split()\n\n        sys_phase_shift_dict = defaultdict(dict)\n        i = 0\n\n        while i &lt; len(components):\n            if i &gt;= len(components):\n                break\n\n            system_abbrv = components[i]\n\n            if i + 1 &gt;= len(components):\n                break\n            signal_code = components[i + 1]\n\n            # Check if there's a phase shift value\n            phase_shift = None\n            if (\n                i + 2 &lt; len(components)\n                and components[i + 2].replace(\".\", \"\", 1).replace(\"-\", \"\", 1).isdigit()\n            ):\n                try:\n                    phase_shift = float(components[i + 2])\n                    i += 3\n                except (ValueError, TypeError):\n                    i += 2\n            else:\n                i += 2\n\n            sys_phase_shift_dict[system_abbrv][signal_code] = phase_shift\n\n        return {k: dict(v) for k, v in sys_phase_shift_dict.items()}\n\n    @staticmethod\n    def _get_signal_strength_unit(\n        header_dict: dict[str, Any],\n    ) -&gt; pint.Unit | str:\n        \"\"\"Parse ``SIGNAL STRENGTH UNIT`` record.\n\n        Parameters\n        ----------\n        header_dict : dict[str, Any]\n            Raw header dictionary.\n\n        Returns\n        -------\n        pint.Unit or str\n            Parsed unit or a default string.\n\n        \"\"\"\n        header_value = header_dict.get(\"SIGNAL STRENGTH UNIT\", \"\").strip()\n\n        # Using match statement like original\n        match header_value:\n            case \"DBHZ\":\n                return UREG.dBHz\n            case \"DB\":\n                return UREG.dB\n            case _:\n                return header_value if header_value else \"dB\"\n\n    @property\n    def is_mixed_systems(self) -&gt; bool:\n        \"\"\"Check if the RINEX file contains mixed GNSS systems.\"\"\"\n        return self.systems == \"M\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a concise representation for debugging.\"\"\"\n        return (\n            f\"Rnxv3Header(file='{self.fpath.name}', \"\n            f\"version={self.version}, \"\n            f\"systems='{self.systems}')\"\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable header summary.\"\"\"\n        systems_str = \"Mixed\" if self.systems == \"M\" else self.systems\n        return (\n            f\"RINEX v{self.version} Header\\n\"\n            f\"  File: {self.fpath.name}\\n\"\n            f\"  Marker: {self.marker_name}\\n\"\n            f\"  Systems: {systems_str}\\n\"\n            f\"  Receiver: {self.receiver_type}\\n\"\n            f\"  Date: {self.date.strftime('%Y-%m-%d %H:%M:%S %Z')}\\n\"\n        )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Header.is_mixed_systems","level":3,"title":"<code>is_mixed_systems</code>  <code>property</code>","text":"<p>Check if the RINEX file contains mixed GNSS systems.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Header.parse_marker_number","level":3,"title":"<code>parse_marker_number(v)</code>  <code>classmethod</code>","text":"<p>Convert empty strings to None, parse valid integers.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>@field_validator(\"marker_number\", mode=\"before\")\n@classmethod\ndef parse_marker_number(cls, v: object) -&gt; int | None:\n    \"\"\"Convert empty strings to None, parse valid integers.\"\"\"\n    if v is None or (isinstance(v, str) and not v.strip()):\n        return None\n    try:\n        return int(v)\n    except (ValueError, TypeError):\n        return None\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Header.from_file","level":3,"title":"<code>from_file(fpath)</code>  <code>classmethod</code>","text":"<p>Create header from a RINEX file.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>@classmethod\ndef from_file(cls, fpath: Path) -&gt; Self:\n    \"\"\"Create header from a RINEX file.\"\"\"\n    # External validation models handle file and version checks\n    _ = RnxObsFileModel(fpath=fpath)\n\n    try:\n        header = gr.rinexheader(fpath)\n    except (OSError, ValueError, TypeError) as e:\n        msg = f\"Failed to read RINEX header: {e}\"\n        raise ValueError(msg) from e\n\n    RnxVersion3Model.version_must_be_3(header[\"version\"])\n\n    # Parse and create instance using original logic\n    parsed_data = cls._parse_header_data(header, fpath)\n    return cls.model_validate(parsed_data)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Header.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return a concise representation for debugging.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a concise representation for debugging.\"\"\"\n    return (\n        f\"Rnxv3Header(file='{self.fpath.name}', \"\n        f\"version={self.version}, \"\n        f\"systems='{self.systems}')\"\n    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Header.__str__","level":3,"title":"<code>__str__()</code>","text":"<p>Return a human-readable header summary.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable header summary.\"\"\"\n    systems_str = \"Mixed\" if self.systems == \"M\" else self.systems\n    return (\n        f\"RINEX v{self.version} Header\\n\"\n        f\"  File: {self.fpath.name}\\n\"\n        f\"  Marker: {self.marker_name}\\n\"\n        f\"  Systems: {systems_str}\\n\"\n        f\"  Receiver: {self.receiver_type}\\n\"\n        f\"  Date: {self.date.strftime('%Y-%m-%d %H:%M:%S %Z')}\\n\"\n    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs","level":2,"title":"<code>Rnxv3Obs</code>","text":"<p>               Bases: <code>GNSSDataReader</code>, <code>BaseModel</code></p> <p>RINEX v3.04 observation reader.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs--attributes","level":4,"title":"Attributes","text":"<p>fpath : Path     Path to the RINEX observation file. polarization : str, default \"RHCP\"     Polarization label for observables. completeness_mode : {\"strict\", \"warn\", \"off\"}, default \"strict\"     Behavior when epoch completeness checks fail. expected_dump_interval : str or pint.Quantity, optional     Expected file dump interval for completeness validation. expected_sampling_interval : str or pint.Quantity, optional     Expected sampling interval for completeness validation. include_auxiliary : bool, default False     Whether to include auxiliary observations (e.g., X1). apply_overlap_filter : bool, default False     Whether to filter overlapping signal groups. overlap_preferences : dict[str, str], optional     Preferred signals for overlap resolution. aggregate_glonass_fdma : bool, optional     Whether to aggregate GLONASS FDMA channels.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs--notes","level":4,"title":"Notes","text":"<p>This class inherits from <code>GNSSDataReader</code> and is a Pydantic <code>BaseModel</code> configured with <code>ConfigDict</code> (frozen, arbitrary_types_allowed).</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>class Rnxv3Obs(GNSSDataReader, BaseModel):\n    \"\"\"RINEX v3.04 observation reader.\n\n    Attributes\n    ----------\n    fpath : Path\n        Path to the RINEX observation file.\n    polarization : str, default \"RHCP\"\n        Polarization label for observables.\n    completeness_mode : {\"strict\", \"warn\", \"off\"}, default \"strict\"\n        Behavior when epoch completeness checks fail.\n    expected_dump_interval : str or pint.Quantity, optional\n        Expected file dump interval for completeness validation.\n    expected_sampling_interval : str or pint.Quantity, optional\n        Expected sampling interval for completeness validation.\n    include_auxiliary : bool, default False\n        Whether to include auxiliary observations (e.g., X1).\n    apply_overlap_filter : bool, default False\n        Whether to filter overlapping signal groups.\n    overlap_preferences : dict[str, str], optional\n        Preferred signals for overlap resolution.\n    aggregate_glonass_fdma : bool, optional\n        Whether to aggregate GLONASS FDMA channels.\n\n    Notes\n    -----\n    This class inherits from `GNSSDataReader` and is a Pydantic `BaseModel`\n    configured with `ConfigDict` (frozen, arbitrary_types_allowed).\n\n    \"\"\"\n\n    fpath: Path\n    polarization: str = \"RHCP\"\n\n    completeness_mode: Literal[\"strict\", \"warn\", \"off\"] = \"strict\"\n    expected_dump_interval: str | pint.Quantity | None = None\n    expected_sampling_interval: str | pint.Quantity | None = None\n\n    include_auxiliary: bool = False\n    apply_overlap_filter: bool = False\n    overlap_preferences: dict[str, str] | None = None\n\n    aggregate_glonass_fdma: bool = True\n\n    _header: Rnxv3Header = PrivateAttr()\n    _signal_mapper: \"SignalIDMapper\" = PrivateAttr()\n\n    _lines: list[str] = PrivateAttr()\n    _file_hash: str = PrivateAttr()\n\n    model_config = ConfigDict(\n        frozen=True,\n        arbitrary_types_allowed=True,\n    )\n\n    @model_validator(mode=\"after\")\n    def _post_init(self) -&gt; Self:\n        \"\"\"Initialize derived state after validation.\"\"\"\n        # Load header once\n        self._header = Rnxv3Header.from_file(self.fpath)\n\n        # Initialize signal mapper\n        self._signal_mapper = SignalIDMapper(\n            aggregate_glonass_fdma=self.aggregate_glonass_fdma\n        )\n\n        # Optionally auto-check completeness\n        if self.completeness_mode != \"off\":\n            try:\n                self.validate_epoch_completeness(\n                    dump_interval=self.expected_dump_interval,\n                    sampling_interval=self.expected_sampling_interval,\n                )\n            except MissingEpochError as e:\n                if self.completeness_mode == \"strict\":\n                    raise\n                warnings.warn(str(e), RuntimeWarning, stacklevel=2)\n\n        # Cache file lines\n        self._lines = self._load_file()\n\n        return self\n\n    @property\n    def header(self) -&gt; Rnxv3Header:\n        \"\"\"Expose validated header (read-only).\n\n        Returns\n        -------\n        Rnxv3Header\n            Parsed and validated RINEX header.\n\n        \"\"\"\n        return self._header\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable summary.\"\"\"\n        return (\n            f\"{self.__class__.__name__}:\\n\"\n            f\"  File Path: {self.fpath}\\n\"\n            f\"  Header: {self.header}\\n\"\n            f\"  Polarization: {self.polarization}\\n\"\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a concise representation for debugging.\"\"\"\n        return f\"{self.__class__.__name__}(fpath={self.fpath})\"\n\n    def _load_file(self) -&gt; list[str]:\n        \"\"\"Read file once, cache lines, and compute hash.\n\n        Returns\n        -------\n        list[str]\n            File contents split into lines.\n\n        \"\"\"\n        if not hasattr(self, \"_lines\"):\n            h = hashlib.sha256()\n            with self.fpath.open(\"rb\") as f:  # binary mode for consistent hash\n                data = f.read()\n                h.update(data)\n                self._lines = data.decode(\"utf-8\", errors=\"replace\").splitlines()\n            self._file_hash = h.hexdigest()[:16]  # short hash for storage\n        return self._lines\n\n    @property\n    def file_hash(self) -&gt; str:\n        \"\"\"Return cached SHA256 short hash of the file content.\n\n        Returns\n        -------\n        str\n            16-character short hash for deduplication.\n\n        \"\"\"\n        return self._file_hash\n\n    @property\n    def start_time(self) -&gt; datetime:\n        \"\"\"Return start time of observations from header.\n\n        Returns\n        -------\n        datetime\n            First observation timestamp.\n\n        \"\"\"\n        return min(self.header.t0.values())\n\n    @property\n    def end_time(self) -&gt; datetime:\n        \"\"\"Return end time of observations from last epoch.\n\n        Returns\n        -------\n        datetime\n            Last observation timestamp.\n\n        \"\"\"\n        last_epoch = None\n        for epoch in self.iter_epochs():\n            last_epoch = epoch\n        if last_epoch:\n            return self.get_datetime_from_epoch_record_info(last_epoch.info)\n        return self.start_time\n\n    @property\n    def systems(self) -&gt; list[str]:\n        \"\"\"Return list of GNSS systems in file.\n\n        Returns\n        -------\n        list of str\n            System identifiers (G, R, E, C, J, S, I).\n\n        \"\"\"\n        if self.header.systems == \"M\":\n            return list(self.header.obs_codes_per_system.keys())\n        return [self.header.systems]\n\n    @property\n    def num_epochs(self) -&gt; int:\n        \"\"\"Return number of epochs in file.\n\n        Returns\n        -------\n        int\n            Total epoch count.\n\n        \"\"\"\n        return len(list(self.get_epoch_record_batches()))\n\n    @property\n    def num_satellites(self) -&gt; int:\n        \"\"\"Return total number of unique satellites observed.\n\n        Returns\n        -------\n        int\n            Count of unique satellite vehicles across all systems.\n\n        \"\"\"\n        satellites = set()\n        for epoch in self.iter_epochs():\n            for sat in epoch.data:\n                satellites.add(sat.sv)\n        return len(satellites)\n\n    def get_epoch_record_batches(\n        self, epoch_record_indicator: str = EPOCH_RECORD_INDICATOR\n    ) -&gt; list[tuple[int, int]]:\n        \"\"\"Get the start and end line numbers for each epoch in the file.\n\n        Parameters\n        ----------\n        epoch_record_indicator : str, default '&gt;'\n            Character marking epoch record lines.\n\n        Returns\n        -------\n        list of tuple of int\n            List of (start_line, end_line) pairs for each epoch.\n\n        \"\"\"\n        starts = [\n            i\n            for i, line in enumerate(self._load_file())\n            if line.startswith(epoch_record_indicator)\n        ]\n        starts.append(len(self._load_file()))  # Add EOF\n        return [\n            (start, starts[i + 1])\n            for i, start in enumerate(starts)\n            if i + 1 &lt; len(starts)\n        ]\n\n    def parse_observation_slice(\n        self,\n        slice_text: str,\n    ) -&gt; tuple[float | None, int | None, int | None]:\n        \"\"\"Parse a RINEX observation slice into value, LLI, and SSI.\n\n        Enhanced to handle both standard 16-character format and\n        variable-length records.\n\n        Parameters\n        ----------\n        slice_text : str\n            Observation slice to parse.\n\n        Returns\n        -------\n        tuple[float | None, int | None, int | None]\n            Parsed (value, LLI, SSI) tuple.\n\n        \"\"\"\n        if not slice_text or not slice_text.strip():\n            return None, None, None\n\n        try:\n            # Method 1: Standard RINEX format with decimal at position -6\n            if (\n                len(slice_text) &gt;= OBS_SLICE_MIN_LEN\n                and len(slice_text) &lt;= OBS_SLICE_MAX_LEN\n                and slice_text[OBS_SLICE_DECIMAL_POS] == \".\"\n            ):\n                slice_chars = list(slice_text)\n                ssi = slice_chars.pop(-1) if len(slice_chars) &gt; 0 else \"\"\n                lli = slice_chars.pop(-1) if len(slice_chars) &gt; 0 else \"\"\n\n                # Convert LLI and SSI\n                lli = int(lli) if lli.strip() and lli.isdigit() else None\n                ssi = int(ssi) if ssi.strip() and ssi.isdigit() else None\n\n                # Convert value\n                value_str = \"\".join(slice_chars).strip()\n                if value_str:\n                    value = float(value_str)\n                    return value, lli, ssi\n\n        except (ValueError, IndexError):\n            pass\n\n        try:\n            # Method 2: Flexible parsing for variable-length records\n            slice_trimmed = slice_text.strip()\n            if not slice_trimmed:\n                return None, None, None\n\n            # Look for a decimal point to identify the numeric value\n            if \".\" in slice_trimmed:\n                # Find the main numeric value (supports negative numbers)\n                number_match = re.search(r\"(-?\\d+\\.\\d+)\", slice_trimmed)\n\n                if number_match:\n                    value = float(number_match.group(1))\n\n                    # Check for LLI/SSI indicators after the number\n                    remaining_part = slice_trimmed[number_match.end() :].strip()\n                    lli = None\n                    ssi = None\n\n                    # Parse remaining characters as potential LLI/SSI\n                    if remaining_part:\n                        # Could be just SSI, or LLI followed by SSI\n                        if len(remaining_part) == 1:\n                            # Just one indicator - assume it's SSI\n                            if remaining_part.isdigit():\n                                ssi = int(remaining_part)\n                        elif len(remaining_part) &gt;= LLI_SSI_PAIR_LEN:\n                            # Two or more characters - take last two as LLI, SSI\n                            lli_char = remaining_part[-2]\n                            ssi_char = remaining_part[-1]\n\n                            if lli_char.isdigit():\n                                lli = int(lli_char)\n                            if ssi_char.isdigit():\n                                ssi = int(ssi_char)\n\n                    return value, lli, ssi\n\n        except (ValueError, IndexError):\n            pass\n\n        # Method 3: Last resort - try simple float parsing\n        try:\n            simple_value = float(slice_text.strip())\n            return simple_value, None, None\n        except ValueError:\n            pass\n\n        return None, None, None\n\n    def process_satellite_data(self, s: str) -&gt; Satellite:\n        \"\"\"Process satellite data line into a Satellite object with observations.\n\n        Handles variable-length observation records correctly by adaptively parsing\n        based on the actual line length and content.\n        \"\"\"\n        sv = s[:3].strip()\n        satellite = Satellite(sv=sv)\n        bands_tbe = [f\"{sv}|{b}\" for b in self.header.obs_codes_per_system[sv[0]]]\n\n        # Get the data part (after sv identifier)\n        data_part = s[3:]\n\n        # Process each observation adaptively\n        for i, band in enumerate(bands_tbe):\n            start_idx = i * 16\n            end_idx = start_idx + 16\n\n            # Check if we have enough data for this observation\n            if start_idx &gt;= len(data_part):\n                # No more data available - create empty observation\n                observation = Observation(\n                    observation_freq_tag=band,\n                    obs_type=band.split(\"|\")[1][0],\n                    value=None,\n                    lli=None,\n                    ssi=None,\n                )\n                satellite.add_observation(observation)\n                continue\n\n            # Extract the slice, but handle variable length\n            if end_idx &lt;= len(data_part):\n                # Full 16-character slice available\n                slice_data = data_part[start_idx:end_idx]\n            else:\n                # Partial slice - pad with spaces to maintain consistency\n                available_slice = data_part[start_idx:]\n                slice_data = available_slice.ljust(16)  # Pad with spaces if needed\n\n            value, lli, ssi = self.parse_observation_slice(slice_data)\n\n            observation = Observation(\n                observation_freq_tag=band,\n                obs_type=band.split(\"|\")[1][0],\n                value=value,\n                lli=lli,\n                ssi=ssi,\n            )\n            satellite.add_observation(observation)\n\n        return satellite\n\n    @property\n    def epochs(self) -&gt; list[Rnxv3ObsEpochRecord]:\n        \"\"\"Materialize all epochs (legacy compatibility).\n\n        Returns\n        -------\n        list of Rnxv3ObsEpochRecord\n            All epochs in memory (use iter_epochs for efficiency)\n\n        \"\"\"\n        return list(self.iter_epochs())\n\n    def iter_epochs(self) -&gt; Iterable[Rnxv3ObsEpochRecord]:\n        \"\"\"Yield epochs one by one instead of materializing the whole list.\n\n        Returns\n        -------\n        Generator\n            Generator yielding Rnxv3ObsEpochRecord objects\n\n        Yields\n        ------\n        Rnxv3ObsEpochRecord\n            Each epoch with timestamp and satellite observations\n\n        \"\"\"\n        for start, end in self.get_epoch_record_batches():\n            try:\n                info = Rnxv3ObsEpochRecordLineModel(epoch=self._lines[start])\n                data = self._lines[start + 1 : end]\n                epoch = Rnxv3ObsEpochRecord(\n                    info=info,\n                    data=(\n                        self.process_satellite_data(line) for line in data\n                    ),  # generator here too\n                )\n                yield epoch\n            except (InvalidEpochError, IncompleteEpochError):\n                # Skip unexpected errors silently\n                pass\n\n    def iter_epochs_in_range(\n        self,\n        start: datetime,\n        end: datetime,\n    ) -&gt; Iterable[Rnxv3ObsEpochRecord]:\n        \"\"\"Yield epochs lazily that fall into the given datetime range.\n\n        Parameters\n        ----------\n        start : datetime\n            Start of time range (inclusive)\n        end : datetime\n            End of time range (inclusive)\n\n        Returns\n        -------\n        Generator\n            Generator yielding epochs in the specified range\n\n        Yields\n        ------\n        Rnxv3ObsEpochRecord\n            Epochs within the time range\n\n        \"\"\"\n        for epoch in self.iter_epochs():\n            dt = self.get_datetime_from_epoch_record_info(epoch.info)\n            if start &lt;= dt &lt;= end:\n                yield epoch\n\n    def get_datetime_from_epoch_record_info(\n        self,\n        epoch_record_info: Rnxv3ObsEpochRecordLineModel,\n    ) -&gt; datetime:\n        \"\"\"Convert epoch record info to datetime object.\n\n        Parameters\n        ----------\n        epoch_record_info : Rnxv3ObsEpochRecordLineModel\n            Parsed epoch record line\n\n        Returns\n        -------\n        datetime\n            Timestamp from epoch record\n\n        \"\"\"\n        return datetime(\n            year=int(epoch_record_info.year),\n            month=int(epoch_record_info.month),\n            day=int(epoch_record_info.day),\n            hour=int(epoch_record_info.hour),\n            minute=int(epoch_record_info.minute),\n            second=int(epoch_record_info.seconds),\n            tzinfo=timezone.utc,\n        )\n\n    @staticmethod\n    def epochrecordinfo_dt_to_numpy_dt(\n        epch: Rnxv3ObsEpochRecord,\n    ) -&gt; np.datetime64:\n        \"\"\"Convert Python datetime to numpy datetime64[ns].\n\n        Parameters\n        ----------\n        epch : Rnxv3ObsEpochRecord\n            Epoch record containing timestamp info\n\n        Returns\n        -------\n        np.datetime64\n            Numpy datetime64 with nanosecond precision\n\n        \"\"\"\n        dt = datetime(\n            year=int(epch.info.year),\n            month=int(epch.info.month),\n            day=int(epch.info.day),\n            hour=int(epch.info.hour),\n            minute=int(epch.info.minute),\n            second=int(epch.info.seconds),\n            tzinfo=timezone.utc,\n        )\n        # np.datetime64 doesn't support timezone info, but datetime is already UTC\n        # Convert to naive datetime (UTC) to avoid warning\n        return np.datetime64(dt.replace(tzinfo=None), \"ns\")\n\n    def _epoch_datetimes(self) -&gt; list[datetime]:\n        \"\"\"Extract epoch datetimes from the file.\n\n        Uses the same epoch parsing logic already implemented.\n        \"\"\"\n        dts: list[datetime] = []\n\n        for start, _end in self.get_epoch_record_batches():\n            info = Rnxv3ObsEpochRecordLineModel(epoch=self._lines[start])\n            dts.append(\n                datetime(\n                    year=int(info.year),\n                    month=int(info.month),\n                    day=int(info.day),\n                    hour=int(info.hour),\n                    minute=int(info.minute),\n                    second=int(info.seconds),\n                    tzinfo=timezone.utc,\n                )\n            )\n        return dts\n\n    def infer_sampling_interval(self) -&gt; pint.Quantity | None:\n        \"\"\"Infer sampling interval from consecutive epoch deltas.\n\n        Returns\n        -------\n        pint.Quantity or None\n            Sampling interval in seconds, or None if cannot be inferred\n\n        \"\"\"\n        dts = self._epoch_datetimes()\n        if len(dts) &lt; MIN_EPOCHS_FOR_INTERVAL:\n            return None\n        # Compute deltas\n        deltas: list[timedelta] = [b - a for a, b in pairwise(dts) if b &gt;= a]\n        if not deltas:\n            return None\n        # Pick the most common delta (robust to an occasional missing epoch)\n        seconds = Counter(\n            int(dt.total_seconds()) for dt in deltas if dt.total_seconds() &gt; 0\n        )\n        if not seconds:\n            return None\n        mode_seconds, _ = seconds.most_common(1)[0]\n        return (mode_seconds * UREG.second).to(UREG.seconds)\n\n    def infer_dump_interval(\n        self, sampling_interval: pint.Quantity | None = None\n    ) -&gt; pint.Quantity | None:\n        \"\"\"Infer the intended dump interval for the RINEX file.\n\n        Parameters\n        ----------\n        sampling_interval : pint.Quantity, optional\n            Known sampling interval. If provided, returns (#epochs * sampling_interval)\n\n        Returns\n        -------\n        pint.Quantity or None\n            Dump interval in seconds, or None if cannot be inferred\n\n        \"\"\"\n        idx = self.get_epoch_record_batches()\n        n_epochs = len(idx)\n        if n_epochs == 0:\n            return None\n\n        if sampling_interval is not None:\n            return (n_epochs * sampling_interval).to(UREG.seconds)\n\n        # Fallback: time coverage inclusive (last - first) + typical step\n        dts = self._epoch_datetimes()\n        if len(dts) == 0:\n            return None\n        if len(dts) == 1:\n            # single epoch: treat as 1 * unknown step (cannot infer)\n            return None\n\n        # Estimate step from data\n        est_step = self.infer_sampling_interval()\n        if est_step is None:\n            return None\n\n        # Inclusive coverage often equals (n_epochs - 1) * step; intended\n        # dump interval is n_epochs * step.\n        return (n_epochs * est_step.to(UREG.seconds)).to(UREG.seconds)\n\n    def validate_epoch_completeness(\n        self,\n        dump_interval: str | pint.Quantity | None = None,\n        sampling_interval: str | pint.Quantity | None = None,\n    ) -&gt; None:\n        \"\"\"Validate that the number of epochs matches the expected dump interval.\n\n        Parameters\n        ----------\n        dump_interval : str or pint.Quantity, optional\n            Expected file dump interval. If None, inferred from epochs.\n        sampling_interval : str or pint.Quantity, optional\n            Expected sampling interval. If None, inferred from epochs.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        MissingEpochError\n            If total sampling time doesn't match dump interval\n        ValueError\n            If intervals cannot be inferred\n\n        \"\"\"\n        # Normalize/Infer sampling interval\n        if sampling_interval is None:\n            inferred = self.infer_sampling_interval()\n            if inferred is None:\n                msg = \"Could not infer sampling interval from epochs\"\n                raise ValueError(msg)\n            sampling_interval = inferred\n        # normalize to pint\n        elif not isinstance(sampling_interval, pint.Quantity):\n            sampling_interval = UREG.Quantity(sampling_interval).to(UREG.seconds)\n\n        # Normalize/Infer dump interval\n        if dump_interval is None:\n            inferred_dump = self.infer_dump_interval(\n                sampling_interval=sampling_interval\n            )\n            if inferred_dump is None:\n                msg = \"Could not infer dump interval from file\"\n                raise ValueError(msg)\n            dump_interval = inferred_dump\n        elif not isinstance(dump_interval, pint.Quantity):\n            # Accept '15 min', '1h', etc.\n            dump_interval = UREG.Quantity(dump_interval).to(UREG.seconds)\n\n        # Build inputs for the validator model\n        epoch_indices = self.get_epoch_record_batches()\n\n        # This throws MissingEpochError automatically if inconsistent\n        Rnxv3ObsEpochRecordCompletenessModel(\n            epoch_records_indeces=epoch_indices,\n            rnx_file_dump_interval=dump_interval,\n            sampling_interval=sampling_interval,\n        )\n\n    def filter_by_overlapping_groups(\n        self,\n        ds: xr.Dataset,\n        group_preference: dict[str, str] | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"Filter overlapping bands using per-group preferences.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset with `sid` dimension and signal properties.\n        group_preference : dict[str, str], optional\n            Mapping of overlap group to preferred band.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset filtered to preferred overlapping bands.\n\n        \"\"\"\n        if group_preference is None:\n            group_preference = {\n                \"L1_E1_B1I\": \"L1\",\n                \"L5_E5a\": \"L5\",\n                \"L2_E5b_B2b\": \"L2\",\n            }\n\n        keep = []\n        for sid in ds.sid.values:\n            _sv, band, _code = self._signal_mapper.parse_signal_id(str(sid))\n            group = self._signal_mapper.get_overlapping_group(band)\n            if group and group in group_preference:\n                if band == group_preference[group]:\n                    keep.append(sid)\n            else:\n                keep.append(sid)\n        return ds.sel(sid=keep)\n\n    def create_rinex_netcdf_with_signal_id(\n        self,\n        analyze_conflicts: bool = False,\n        analyze_systems: bool = False,\n        start: datetime | None = None,\n        end: datetime | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"Create a NetCDF dataset with signal IDs.\n\n        Can optionally restrict to epochs within a datetime range.\n\n        \"\"\"\n        if analyze_conflicts:\n            print(\"\\nNote: Conflict analysis will be adapted for sid structure.\")\n        if analyze_systems:\n            print(\"\\nNote: System analysis will be adapted for sid structure.\")\n\n        signal_ids = set()\n        signal_id_to_properties: dict[str, dict[str, object]] = {}\n        timestamps: list[np.datetime64] = []\n\n        # pick generator depending on range\n        if start and end:\n            epoch_iter = self.iter_epochs_in_range(start, end)\n        else:\n            epoch_iter = self.iter_epochs()\n\n        for epoch in epoch_iter:\n            dt = self.epochrecordinfo_dt_to_numpy_dt(epoch)\n            timestamps.append(np.datetime64(dt, \"ns\"))\n\n            for sat in epoch.data:\n                sv = sat.sv\n                for obs in sat.observations:\n                    if not self.include_auxiliary and obs.observation_freq_tag.endswith(\n                        \"|X1\"\n                    ):\n                        continue\n\n                    sid = self._signal_mapper.create_signal_id(\n                        sv, obs.observation_freq_tag\n                    )\n                    signal_ids.add(sid)\n\n                    if sid not in signal_id_to_properties:\n                        sv_part, band, code = self._signal_mapper.parse_signal_id(sid)\n                        system = sv_part[0]\n                        center_frequency = self._signal_mapper.get_band_frequency(band)\n                        bandwidth = self._signal_mapper.get_band_bandwidth(band)\n                        overlapping_group = self._signal_mapper.get_overlapping_group(\n                            band\n                        )\n\n                        if center_frequency is not None and bandwidth is not None:\n                            # Extract bandwidth value\n                            bw = (\n                                bandwidth[0]\n                                if isinstance(bandwidth, list)\n                                else bandwidth\n                            )\n\n                            # Ensure both are pint quantities\n                            if not hasattr(center_frequency, \"m_as\"):\n                                center_frequency = center_frequency * UREG.MHz\n                            if not hasattr(bw, \"m_as\"):\n                                bw = bw * UREG.MHz\n\n                            # Calculate frequency range\n                            freq_min = center_frequency - (bw / 2.0)\n                            freq_max = center_frequency + (bw / 2.0)\n\n                            # Extract magnitudes to ensure float64 dtype\n                            center_frequency = float(center_frequency.m_as(UREG.MHz))\n                            freq_min = float(freq_min.m_as(UREG.MHz))\n                            freq_max = float(freq_max.m_as(UREG.MHz))\n                            bw = float(bw.m_as(UREG.MHz))\n                        else:\n                            print(\n                                f\"WARNING: No frequency data for sid={sid}, \"\n                                f\"band={band}, sv={sv_part}\"\n                            )\n                            center_frequency = np.nan\n                            freq_min = np.nan\n                            freq_max = np.nan\n                            bw = np.nan\n\n                        signal_id_to_properties[sid] = {\n                            \"sv\": sv_part,\n                            \"system\": system,\n                            \"band\": band,\n                            \"code\": code,\n                            \"freq_center\": center_frequency,\n                            \"freq_min\": freq_min,\n                            \"freq_max\": freq_max,\n                            \"bandwidth\": bw,\n                            \"overlapping_group\": overlapping_group,\n                        }\n\n        # Inconsistent integration of the Septentrio X1 obs. code, filtering\n        # out here again.\n        signal_ids = {sid for sid in signal_ids if \"|X1|\" not in sid}\n        signal_id_to_properties = {\n            sid: props\n            for sid, props in signal_id_to_properties.items()\n            if \"|X1|\" not in sid\n        }\n\n        sorted_signal_ids = sorted(signal_ids)\n        n_epochs = len(timestamps)\n        n_signals = len(sorted_signal_ids)\n\n        data_arrays = {\n            \"SNR\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"SNR\"]),\n            \"Pseudorange\": np.full(\n                (n_epochs, n_signals), np.nan, dtype=DTYPES[\"Pseudorange\"]\n            ),\n            \"Phase\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"Phase\"]),\n            \"Doppler\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"Doppler\"]),\n            \"LLI\": np.full((n_epochs, n_signals), -1, dtype=DTYPES[\"LLI\"]),\n            \"SSI\": np.full((n_epochs, n_signals), -1, dtype=DTYPES[\"SSI\"]),\n        }\n        sid_to_idx = {sid: i for i, sid in enumerate(sorted_signal_ids)}\n\n        # second pass to fill arrays\n        if start and end:\n            epoch_iter = self.iter_epochs_in_range(start, end)\n        else:\n            epoch_iter = self.iter_epochs()\n\n        for t_idx, epoch in enumerate(epoch_iter):\n            for sat in epoch.data:\n                sv = sat.sv\n                for obs in sat.observations:\n                    if not self.include_auxiliary and obs.observation_freq_tag.endswith(\n                        \"|X1\"\n                    ):\n                        continue\n                    if obs.value is None:\n                        continue\n                    sid = self._signal_mapper.create_signal_id(\n                        sv, obs.observation_freq_tag\n                    )\n                    if sid not in sid_to_idx:\n                        continue\n                    s_idx = sid_to_idx[sid]\n\n                    ot = obs.obs_type\n                    if ot == \"S\" and obs.value != 0:\n                        data_arrays[\"SNR\"][t_idx, s_idx] = obs.value\n                    elif ot == \"C\":\n                        data_arrays[\"Pseudorange\"][t_idx, s_idx] = obs.value\n                    elif ot == \"L\":\n                        data_arrays[\"Phase\"][t_idx, s_idx] = obs.value\n                    elif ot == \"D\":\n                        data_arrays[\"Doppler\"][t_idx, s_idx] = obs.value\n                    elif ot == \"X\":\n                        data_arrays.setdefault(\n                            \"Auxiliary\",\n                            np.full((n_epochs, n_signals), np.nan, dtype=np.float32),\n                        )\n                        data_arrays[\"Auxiliary\"][t_idx, s_idx] = obs.value\n\n                    if obs.lli is not None:\n                        data_arrays[\"LLI\"][t_idx, s_idx] = obs.lli\n                    if obs.ssi is not None:\n                        data_arrays[\"SSI\"][t_idx, s_idx] = obs.ssi\n\n        signal_id_coord = xr.DataArray(\n            sorted_signal_ids, dims=[\"sid\"], attrs=COORDS_METADATA[\"sid\"]\n        )\n        sv_list = [signal_id_to_properties[sid][\"sv\"] for sid in sorted_signal_ids]\n        constellation_list = [\n            signal_id_to_properties[sid][\"system\"] for sid in sorted_signal_ids\n        ]\n        band_list = [signal_id_to_properties[sid][\"band\"] for sid in sorted_signal_ids]\n        code_list = [signal_id_to_properties[sid][\"code\"] for sid in sorted_signal_ids]\n        freq_center_list = [\n            signal_id_to_properties[sid][\"freq_center\"] for sid in sorted_signal_ids\n        ]\n        freq_min_list = [\n            signal_id_to_properties[sid][\"freq_min\"] for sid in sorted_signal_ids\n        ]\n        freq_max_list = [\n            signal_id_to_properties[sid][\"freq_max\"] for sid in sorted_signal_ids\n        ]\n\n        coords = {\n            \"epoch\": (\"epoch\", timestamps, COORDS_METADATA[\"epoch\"]),\n            \"sid\": signal_id_coord,\n            \"sv\": (\"sid\", sv_list, COORDS_METADATA[\"sv\"]),\n            \"system\": (\"sid\", constellation_list, COORDS_METADATA[\"system\"]),\n            \"band\": (\"sid\", band_list, COORDS_METADATA[\"band\"]),\n            \"code\": (\"sid\", code_list, COORDS_METADATA[\"code\"]),\n            \"freq_center\": (\n                \"sid\",\n                np.asarray(freq_center_list, dtype=DTYPES[\"freq_center\"]),\n                COORDS_METADATA[\"freq_center\"],\n            ),\n            \"freq_min\": (\n                \"sid\",\n                np.asarray(freq_min_list, dtype=DTYPES[\"freq_min\"]),\n                COORDS_METADATA[\"freq_min\"],\n            ),\n            \"freq_max\": (\n                \"sid\",\n                np.asarray(freq_max_list, dtype=DTYPES[\"freq_max\"]),\n                COORDS_METADATA[\"freq_max\"],\n            ),\n        }\n\n        if self.header.signal_strength_unit == UREG.dBHz:\n            snr_meta = CN0_METADATA\n        else:\n            snr_meta = SNR_METADATA\n\n        ds = xr.Dataset(\n            data_vars={\n                \"SNR\": ([\"epoch\", \"sid\"], data_arrays[\"SNR\"], snr_meta),\n                \"Pseudorange\": (\n                    [\"epoch\", \"sid\"],\n                    data_arrays[\"Pseudorange\"],\n                    OBSERVABLES_METADATA[\"Pseudorange\"],\n                ),\n                \"Phase\": (\n                    [\"epoch\", \"sid\"],\n                    data_arrays[\"Phase\"],\n                    OBSERVABLES_METADATA[\"Phase\"],\n                ),\n                \"Doppler\": (\n                    [\"epoch\", \"sid\"],\n                    data_arrays[\"Doppler\"],\n                    OBSERVABLES_METADATA[\"Doppler\"],\n                ),\n                \"LLI\": (\n                    [\"epoch\", \"sid\"],\n                    data_arrays[\"LLI\"],\n                    OBSERVABLES_METADATA[\"LLI\"],\n                ),\n                \"SSI\": (\n                    [\"epoch\", \"sid\"],\n                    data_arrays[\"SSI\"],\n                    OBSERVABLES_METADATA[\"SSI\"],\n                ),\n            },\n            coords=coords,\n            attrs={**self._create_basic_attrs()},\n        )\n\n        if \"Auxiliary\" in data_arrays:\n            ds[\"Auxiliary\"] = (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"Auxiliary\"],\n                OBSERVABLES_METADATA[\"Auxiliary\"],\n            )\n\n        if self.apply_overlap_filter:\n            ds = self.filter_by_overlapping_groups(ds, self.overlap_preferences)\n\n        return ds\n\n    def to_ds(\n        self,\n        outname: Path | str | None = None,\n        keep_rnx_data_vars: list[str] | None = None,\n        write_global_attrs: bool = False,\n        pad_global_sid: bool = True,\n        strip_fillval: bool = True,\n        add_future_datavars: bool = True,\n        keep_sids: list[str] | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"Convert RINEX observations to xarray.Dataset with signal ID structure.\n\n        Parameters\n        ----------\n        outname : Path or str, optional\n            If provided, saves dataset to this file path\n        keep_rnx_data_vars : list of str or None, optional\n            Data variables to include in dataset. Defaults to config value.\n        write_global_attrs : bool, default False\n            If True, adds comprehensive global attributes\n        pad_global_sid : bool, default True\n            If True, pads to global signal ID space\n        strip_fillval : bool, default True\n            If True, removes fill values\n        add_future_datavars : bool, default True\n            If True, adds placeholder variables for future data\n        keep_sids : list of str or None, default None\n            If provided, filters/pads dataset to these specific SIDs.\n            If None and pad_global_sid=True, pads to all possible SIDs.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with dimensions (epoch, sid) and requested data variables\n\n        \"\"\"\n        if keep_rnx_data_vars is None:\n            from canvod.utils.config import load_config\n\n            keep_rnx_data_vars = load_config().processing.processing.keep_rnx_vars\n\n        ds = self.create_rinex_netcdf_with_signal_id()\n\n        # drop unwanted vars\n        for var in list(ds.data_vars):\n            if var not in keep_rnx_data_vars:\n                ds = ds.drop_vars(var)\n\n        if pad_global_sid:\n            from canvod.auxiliary.preprocessing import pad_to_global_sid\n\n            # Pad/filter to specified sids or all possible sids\n            ds = pad_to_global_sid(ds, keep_sids=keep_sids)\n\n        if strip_fillval:\n            from canvod.auxiliary.preprocessing import strip_fillvalue\n\n            ds = strip_fillvalue(ds)\n\n        if add_future_datavars:\n            pass\n\n        if write_global_attrs:\n            ds.attrs.update(self._create_comprehensive_attrs())\n\n        ds.attrs[\"RINEX File Hash\"] = self.file_hash\n\n        if outname:\n            from canvod.utils.config import load_config as _load_config\n\n            comp = _load_config().processing.compression\n            encoding = {\n                var: {\"zlib\": comp.zlib, \"complevel\": comp.complevel}\n                for var in ds.data_vars\n            }\n            ds.to_netcdf(str(outname), encoding=encoding)\n\n        # Validate output structure for pipeline compatibility\n        self.validate_output(ds, required_vars=keep_rnx_data_vars)\n\n        return ds\n\n    def validate_rinex_304_compliance(\n        self,\n        ds: xr.Dataset | None = None,\n        strict: bool = False,\n        print_report: bool = True,\n    ) -&gt; dict[str, list[str]]:\n        \"\"\"Run enhanced RINEX 3.04 specification validation.\n\n        Validates:\n        1. System-specific observation codes\n        2. GLONASS mandatory fields (slot/frequency, biases)\n        3. Phase shift records (RINEX 3.01+)\n        4. Observation value ranges\n\n        Parameters\n        ----------\n        ds : xr.Dataset, optional\n            Dataset to validate. If None, creates one from current file.\n        strict : bool\n            If True, raise ValueError on validation failures\n        print_report : bool\n            If True, print validation report to console\n\n        Returns\n        -------\n        dict[str, list[str]]\n            Validation results by category\n\n        Examples\n        --------\n        &gt;&gt;&gt; reader = Rnxv3Obs(fpath=\"station.24o\")\n        &gt;&gt;&gt; results = reader.validate_rinex_304_compliance()\n        &gt;&gt;&gt; # Or validate a specific dataset\n        &gt;&gt;&gt; ds = reader.to_ds()\n        &gt;&gt;&gt; results = reader.validate_rinex_304_compliance(ds=ds)\n\n        \"\"\"\n        if ds is None:\n            ds = self.to_ds(write_global_attrs=False)\n\n        # Prepare header dict for validators\n        header_dict = {\n            \"obs_codes_per_system\": self.header.obs_codes_per_system,\n        }\n\n        # Add GLONASS-specific headers if available\n        if hasattr(self.header, \"glonass_slot_frq\"):\n            header_dict[\"GLONASS SLOT / FRQ #\"] = self.header.glonass_slot_frq\n\n        if hasattr(self.header, \"glonass_cod_phs_bis\"):\n            header_dict[\"GLONASS COD/PHS/BIS\"] = self.header.glonass_cod_phs_bis\n\n        if hasattr(self.header, \"phase_shift\"):\n            header_dict[\"SYS / PHASE SHIFT\"] = self.header.phase_shift\n\n        # Run validation\n        results = RINEX304ComplianceValidator.validate_all(\n            ds=ds, header_dict=header_dict, strict=strict\n        )\n\n        if print_report:\n            RINEX304ComplianceValidator.print_validation_report(results)\n\n        return results\n\n    def _create_basic_attrs(self) -&gt; dict[str, object]:\n        attrs = get_global_attrs()\n        attrs[\"Created\"] = datetime.now(timezone.utc).isoformat()\n        attrs[\"Software\"] = (\n            f\"{attrs['Software']}, Version: {get_version_from_pyproject()}\"\n        )\n        return attrs\n\n    def _create_comprehensive_attrs(self) -&gt; dict[str, object]:\n        attrs = {\n            \"File Path\": str(self.fpath),\n            \"File Type\": self.header.filetype,\n            \"RINEX Version\": self.header.version,\n            \"RINEX Type\": self.header.rinextype,\n            \"Observer\": self.header.observer,\n            \"Agency\": self.header.agency,\n            \"Date\": self.header.date.isoformat(),\n            \"Marker Name\": self.header.marker_name,\n            \"Marker Number\": self.header.marker_number,\n            \"Marker Type\": self.header.marker_type,\n            \"Approximate Position\": (\n                f\"(X = {self.header.approx_position[0].magnitude} \"\n                f\"{self.header.approx_position[0].units:~}, \"\n                f\"Y = {self.header.approx_position[1].magnitude} \"\n                f\"{self.header.approx_position[1].units:~}, \"\n                f\"Z = {self.header.approx_position[2].magnitude} \"\n                f\"{self.header.approx_position[2].units:~})\"\n            ),\n            \"Receiver Type\": self.header.receiver_type,\n            \"Receiver Version\": self.header.receiver_version,\n            \"Receiver Number\": self.header.receiver_number,\n            \"Antenna Type\": self.header.antenna_type,\n            \"Antenna Number\": self.header.antenna_number,\n            \"Antenna Position\": (\n                f\"(X = {self.header.antenna_position[0].magnitude} \"\n                f\"{self.header.antenna_position[0].units:~}, \"\n                f\"Y = {self.header.antenna_position[1].magnitude} \"\n                f\"{self.header.antenna_position[1].units:~}, \"\n                f\"Z = {self.header.antenna_position[2].magnitude} \"\n                f\"{self.header.antenna_position[2].units:~})\"\n            ),\n            \"Program\": self.header.pgm,\n            \"Run By\": self.header.run_by,\n            \"Time of First Observation\": json.dumps(\n                {k: v.isoformat() for k, v in self.header.t0.items()}\n            ),\n            \"GLONASS COD\": self.header.glonass_cod,\n            \"GLONASS PHS\": self.header.glonass_phs,\n            \"GLONASS BIS\": self.header.glonass_bis,\n            \"GLONASS Slot Frequency Dict\": json.dumps(\n                self.header.glonass_slot_freq_dict\n            ),\n            \"Leap Seconds\": f\"{self.header.leap_seconds:~}\",\n        }\n        return attrs\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.header","level":3,"title":"<code>header</code>  <code>property</code>","text":"<p>Expose validated header (read-only).</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.header--returns","level":5,"title":"Returns","text":"<p>Rnxv3Header     Parsed and validated RINEX header.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.file_hash","level":3,"title":"<code>file_hash</code>  <code>property</code>","text":"<p>Return cached SHA256 short hash of the file content.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.file_hash--returns","level":5,"title":"Returns","text":"<p>str     16-character short hash for deduplication.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.start_time","level":3,"title":"<code>start_time</code>  <code>property</code>","text":"<p>Return start time of observations from header.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.start_time--returns","level":5,"title":"Returns","text":"<p>datetime     First observation timestamp.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.end_time","level":3,"title":"<code>end_time</code>  <code>property</code>","text":"<p>Return end time of observations from last epoch.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.end_time--returns","level":5,"title":"Returns","text":"<p>datetime     Last observation timestamp.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.systems","level":3,"title":"<code>systems</code>  <code>property</code>","text":"<p>Return list of GNSS systems in file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.systems--returns","level":5,"title":"Returns","text":"<p>list of str     System identifiers (G, R, E, C, J, S, I).</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.num_epochs","level":3,"title":"<code>num_epochs</code>  <code>property</code>","text":"<p>Return number of epochs in file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.num_epochs--returns","level":5,"title":"Returns","text":"<p>int     Total epoch count.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.num_satellites","level":3,"title":"<code>num_satellites</code>  <code>property</code>","text":"<p>Return total number of unique satellites observed.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.num_satellites--returns","level":5,"title":"Returns","text":"<p>int     Count of unique satellite vehicles across all systems.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.epochs","level":3,"title":"<code>epochs</code>  <code>property</code>","text":"<p>Materialize all epochs (legacy compatibility).</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.epochs--returns","level":5,"title":"Returns","text":"<p>list of Rnxv3ObsEpochRecord     All epochs in memory (use iter_epochs for efficiency)</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.__str__","level":3,"title":"<code>__str__()</code>","text":"<p>Return a human-readable summary.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable summary.\"\"\"\n    return (\n        f\"{self.__class__.__name__}:\\n\"\n        f\"  File Path: {self.fpath}\\n\"\n        f\"  Header: {self.header}\\n\"\n        f\"  Polarization: {self.polarization}\\n\"\n    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return a concise representation for debugging.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a concise representation for debugging.\"\"\"\n    return f\"{self.__class__.__name__}(fpath={self.fpath})\"\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.get_epoch_record_batches","level":3,"title":"<code>get_epoch_record_batches(epoch_record_indicator=EPOCH_RECORD_INDICATOR)</code>","text":"<p>Get the start and end line numbers for each epoch in the file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.get_epoch_record_batches--parameters","level":5,"title":"Parameters","text":"<p>epoch_record_indicator : str, default '&gt;'     Character marking epoch record lines.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.get_epoch_record_batches--returns","level":5,"title":"Returns","text":"<p>list of tuple of int     List of (start_line, end_line) pairs for each epoch.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def get_epoch_record_batches(\n    self, epoch_record_indicator: str = EPOCH_RECORD_INDICATOR\n) -&gt; list[tuple[int, int]]:\n    \"\"\"Get the start and end line numbers for each epoch in the file.\n\n    Parameters\n    ----------\n    epoch_record_indicator : str, default '&gt;'\n        Character marking epoch record lines.\n\n    Returns\n    -------\n    list of tuple of int\n        List of (start_line, end_line) pairs for each epoch.\n\n    \"\"\"\n    starts = [\n        i\n        for i, line in enumerate(self._load_file())\n        if line.startswith(epoch_record_indicator)\n    ]\n    starts.append(len(self._load_file()))  # Add EOF\n    return [\n        (start, starts[i + 1])\n        for i, start in enumerate(starts)\n        if i + 1 &lt; len(starts)\n    ]\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.parse_observation_slice","level":3,"title":"<code>parse_observation_slice(slice_text)</code>","text":"<p>Parse a RINEX observation slice into value, LLI, and SSI.</p> <p>Enhanced to handle both standard 16-character format and variable-length records.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.parse_observation_slice--parameters","level":5,"title":"Parameters","text":"<p>slice_text : str     Observation slice to parse.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.parse_observation_slice--returns","level":5,"title":"Returns","text":"<p>tuple[float | None, int | None, int | None]     Parsed (value, LLI, SSI) tuple.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def parse_observation_slice(\n    self,\n    slice_text: str,\n) -&gt; tuple[float | None, int | None, int | None]:\n    \"\"\"Parse a RINEX observation slice into value, LLI, and SSI.\n\n    Enhanced to handle both standard 16-character format and\n    variable-length records.\n\n    Parameters\n    ----------\n    slice_text : str\n        Observation slice to parse.\n\n    Returns\n    -------\n    tuple[float | None, int | None, int | None]\n        Parsed (value, LLI, SSI) tuple.\n\n    \"\"\"\n    if not slice_text or not slice_text.strip():\n        return None, None, None\n\n    try:\n        # Method 1: Standard RINEX format with decimal at position -6\n        if (\n            len(slice_text) &gt;= OBS_SLICE_MIN_LEN\n            and len(slice_text) &lt;= OBS_SLICE_MAX_LEN\n            and slice_text[OBS_SLICE_DECIMAL_POS] == \".\"\n        ):\n            slice_chars = list(slice_text)\n            ssi = slice_chars.pop(-1) if len(slice_chars) &gt; 0 else \"\"\n            lli = slice_chars.pop(-1) if len(slice_chars) &gt; 0 else \"\"\n\n            # Convert LLI and SSI\n            lli = int(lli) if lli.strip() and lli.isdigit() else None\n            ssi = int(ssi) if ssi.strip() and ssi.isdigit() else None\n\n            # Convert value\n            value_str = \"\".join(slice_chars).strip()\n            if value_str:\n                value = float(value_str)\n                return value, lli, ssi\n\n    except (ValueError, IndexError):\n        pass\n\n    try:\n        # Method 2: Flexible parsing for variable-length records\n        slice_trimmed = slice_text.strip()\n        if not slice_trimmed:\n            return None, None, None\n\n        # Look for a decimal point to identify the numeric value\n        if \".\" in slice_trimmed:\n            # Find the main numeric value (supports negative numbers)\n            number_match = re.search(r\"(-?\\d+\\.\\d+)\", slice_trimmed)\n\n            if number_match:\n                value = float(number_match.group(1))\n\n                # Check for LLI/SSI indicators after the number\n                remaining_part = slice_trimmed[number_match.end() :].strip()\n                lli = None\n                ssi = None\n\n                # Parse remaining characters as potential LLI/SSI\n                if remaining_part:\n                    # Could be just SSI, or LLI followed by SSI\n                    if len(remaining_part) == 1:\n                        # Just one indicator - assume it's SSI\n                        if remaining_part.isdigit():\n                            ssi = int(remaining_part)\n                    elif len(remaining_part) &gt;= LLI_SSI_PAIR_LEN:\n                        # Two or more characters - take last two as LLI, SSI\n                        lli_char = remaining_part[-2]\n                        ssi_char = remaining_part[-1]\n\n                        if lli_char.isdigit():\n                            lli = int(lli_char)\n                        if ssi_char.isdigit():\n                            ssi = int(ssi_char)\n\n                return value, lli, ssi\n\n    except (ValueError, IndexError):\n        pass\n\n    # Method 3: Last resort - try simple float parsing\n    try:\n        simple_value = float(slice_text.strip())\n        return simple_value, None, None\n    except ValueError:\n        pass\n\n    return None, None, None\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.process_satellite_data","level":3,"title":"<code>process_satellite_data(s)</code>","text":"<p>Process satellite data line into a Satellite object with observations.</p> <p>Handles variable-length observation records correctly by adaptively parsing based on the actual line length and content.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def process_satellite_data(self, s: str) -&gt; Satellite:\n    \"\"\"Process satellite data line into a Satellite object with observations.\n\n    Handles variable-length observation records correctly by adaptively parsing\n    based on the actual line length and content.\n    \"\"\"\n    sv = s[:3].strip()\n    satellite = Satellite(sv=sv)\n    bands_tbe = [f\"{sv}|{b}\" for b in self.header.obs_codes_per_system[sv[0]]]\n\n    # Get the data part (after sv identifier)\n    data_part = s[3:]\n\n    # Process each observation adaptively\n    for i, band in enumerate(bands_tbe):\n        start_idx = i * 16\n        end_idx = start_idx + 16\n\n        # Check if we have enough data for this observation\n        if start_idx &gt;= len(data_part):\n            # No more data available - create empty observation\n            observation = Observation(\n                observation_freq_tag=band,\n                obs_type=band.split(\"|\")[1][0],\n                value=None,\n                lli=None,\n                ssi=None,\n            )\n            satellite.add_observation(observation)\n            continue\n\n        # Extract the slice, but handle variable length\n        if end_idx &lt;= len(data_part):\n            # Full 16-character slice available\n            slice_data = data_part[start_idx:end_idx]\n        else:\n            # Partial slice - pad with spaces to maintain consistency\n            available_slice = data_part[start_idx:]\n            slice_data = available_slice.ljust(16)  # Pad with spaces if needed\n\n        value, lli, ssi = self.parse_observation_slice(slice_data)\n\n        observation = Observation(\n            observation_freq_tag=band,\n            obs_type=band.split(\"|\")[1][0],\n            value=value,\n            lli=lli,\n            ssi=ssi,\n        )\n        satellite.add_observation(observation)\n\n    return satellite\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.iter_epochs","level":3,"title":"<code>iter_epochs()</code>","text":"<p>Yield epochs one by one instead of materializing the whole list.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.iter_epochs--returns","level":5,"title":"Returns","text":"<p>Generator     Generator yielding Rnxv3ObsEpochRecord objects</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.iter_epochs--yields","level":5,"title":"Yields","text":"<p>Rnxv3ObsEpochRecord     Each epoch with timestamp and satellite observations</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def iter_epochs(self) -&gt; Iterable[Rnxv3ObsEpochRecord]:\n    \"\"\"Yield epochs one by one instead of materializing the whole list.\n\n    Returns\n    -------\n    Generator\n        Generator yielding Rnxv3ObsEpochRecord objects\n\n    Yields\n    ------\n    Rnxv3ObsEpochRecord\n        Each epoch with timestamp and satellite observations\n\n    \"\"\"\n    for start, end in self.get_epoch_record_batches():\n        try:\n            info = Rnxv3ObsEpochRecordLineModel(epoch=self._lines[start])\n            data = self._lines[start + 1 : end]\n            epoch = Rnxv3ObsEpochRecord(\n                info=info,\n                data=(\n                    self.process_satellite_data(line) for line in data\n                ),  # generator here too\n            )\n            yield epoch\n        except (InvalidEpochError, IncompleteEpochError):\n            # Skip unexpected errors silently\n            pass\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.iter_epochs_in_range","level":3,"title":"<code>iter_epochs_in_range(start, end)</code>","text":"<p>Yield epochs lazily that fall into the given datetime range.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.iter_epochs_in_range--parameters","level":5,"title":"Parameters","text":"<p>start : datetime     Start of time range (inclusive) end : datetime     End of time range (inclusive)</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.iter_epochs_in_range--returns","level":5,"title":"Returns","text":"<p>Generator     Generator yielding epochs in the specified range</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.iter_epochs_in_range--yields","level":5,"title":"Yields","text":"<p>Rnxv3ObsEpochRecord     Epochs within the time range</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def iter_epochs_in_range(\n    self,\n    start: datetime,\n    end: datetime,\n) -&gt; Iterable[Rnxv3ObsEpochRecord]:\n    \"\"\"Yield epochs lazily that fall into the given datetime range.\n\n    Parameters\n    ----------\n    start : datetime\n        Start of time range (inclusive)\n    end : datetime\n        End of time range (inclusive)\n\n    Returns\n    -------\n    Generator\n        Generator yielding epochs in the specified range\n\n    Yields\n    ------\n    Rnxv3ObsEpochRecord\n        Epochs within the time range\n\n    \"\"\"\n    for epoch in self.iter_epochs():\n        dt = self.get_datetime_from_epoch_record_info(epoch.info)\n        if start &lt;= dt &lt;= end:\n            yield epoch\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.get_datetime_from_epoch_record_info","level":3,"title":"<code>get_datetime_from_epoch_record_info(epoch_record_info)</code>","text":"<p>Convert epoch record info to datetime object.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.get_datetime_from_epoch_record_info--parameters","level":5,"title":"Parameters","text":"<p>epoch_record_info : Rnxv3ObsEpochRecordLineModel     Parsed epoch record line</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.get_datetime_from_epoch_record_info--returns","level":5,"title":"Returns","text":"<p>datetime     Timestamp from epoch record</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def get_datetime_from_epoch_record_info(\n    self,\n    epoch_record_info: Rnxv3ObsEpochRecordLineModel,\n) -&gt; datetime:\n    \"\"\"Convert epoch record info to datetime object.\n\n    Parameters\n    ----------\n    epoch_record_info : Rnxv3ObsEpochRecordLineModel\n        Parsed epoch record line\n\n    Returns\n    -------\n    datetime\n        Timestamp from epoch record\n\n    \"\"\"\n    return datetime(\n        year=int(epoch_record_info.year),\n        month=int(epoch_record_info.month),\n        day=int(epoch_record_info.day),\n        hour=int(epoch_record_info.hour),\n        minute=int(epoch_record_info.minute),\n        second=int(epoch_record_info.seconds),\n        tzinfo=timezone.utc,\n    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.epochrecordinfo_dt_to_numpy_dt","level":3,"title":"<code>epochrecordinfo_dt_to_numpy_dt(epch)</code>  <code>staticmethod</code>","text":"<p>Convert Python datetime to numpy datetime64[ns].</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.epochrecordinfo_dt_to_numpy_dt--parameters","level":5,"title":"Parameters","text":"<p>epch : Rnxv3ObsEpochRecord     Epoch record containing timestamp info</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.epochrecordinfo_dt_to_numpy_dt--returns","level":5,"title":"Returns","text":"<p>np.datetime64     Numpy datetime64 with nanosecond precision</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>@staticmethod\ndef epochrecordinfo_dt_to_numpy_dt(\n    epch: Rnxv3ObsEpochRecord,\n) -&gt; np.datetime64:\n    \"\"\"Convert Python datetime to numpy datetime64[ns].\n\n    Parameters\n    ----------\n    epch : Rnxv3ObsEpochRecord\n        Epoch record containing timestamp info\n\n    Returns\n    -------\n    np.datetime64\n        Numpy datetime64 with nanosecond precision\n\n    \"\"\"\n    dt = datetime(\n        year=int(epch.info.year),\n        month=int(epch.info.month),\n        day=int(epch.info.day),\n        hour=int(epch.info.hour),\n        minute=int(epch.info.minute),\n        second=int(epch.info.seconds),\n        tzinfo=timezone.utc,\n    )\n    # np.datetime64 doesn't support timezone info, but datetime is already UTC\n    # Convert to naive datetime (UTC) to avoid warning\n    return np.datetime64(dt.replace(tzinfo=None), \"ns\")\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.infer_sampling_interval","level":3,"title":"<code>infer_sampling_interval()</code>","text":"<p>Infer sampling interval from consecutive epoch deltas.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.infer_sampling_interval--returns","level":5,"title":"Returns","text":"<p>pint.Quantity or None     Sampling interval in seconds, or None if cannot be inferred</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def infer_sampling_interval(self) -&gt; pint.Quantity | None:\n    \"\"\"Infer sampling interval from consecutive epoch deltas.\n\n    Returns\n    -------\n    pint.Quantity or None\n        Sampling interval in seconds, or None if cannot be inferred\n\n    \"\"\"\n    dts = self._epoch_datetimes()\n    if len(dts) &lt; MIN_EPOCHS_FOR_INTERVAL:\n        return None\n    # Compute deltas\n    deltas: list[timedelta] = [b - a for a, b in pairwise(dts) if b &gt;= a]\n    if not deltas:\n        return None\n    # Pick the most common delta (robust to an occasional missing epoch)\n    seconds = Counter(\n        int(dt.total_seconds()) for dt in deltas if dt.total_seconds() &gt; 0\n    )\n    if not seconds:\n        return None\n    mode_seconds, _ = seconds.most_common(1)[0]\n    return (mode_seconds * UREG.second).to(UREG.seconds)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.infer_dump_interval","level":3,"title":"<code>infer_dump_interval(sampling_interval=None)</code>","text":"<p>Infer the intended dump interval for the RINEX file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.infer_dump_interval--parameters","level":5,"title":"Parameters","text":"<p>sampling_interval : pint.Quantity, optional     Known sampling interval. If provided, returns (#epochs * sampling_interval)</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.infer_dump_interval--returns","level":5,"title":"Returns","text":"<p>pint.Quantity or None     Dump interval in seconds, or None if cannot be inferred</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def infer_dump_interval(\n    self, sampling_interval: pint.Quantity | None = None\n) -&gt; pint.Quantity | None:\n    \"\"\"Infer the intended dump interval for the RINEX file.\n\n    Parameters\n    ----------\n    sampling_interval : pint.Quantity, optional\n        Known sampling interval. If provided, returns (#epochs * sampling_interval)\n\n    Returns\n    -------\n    pint.Quantity or None\n        Dump interval in seconds, or None if cannot be inferred\n\n    \"\"\"\n    idx = self.get_epoch_record_batches()\n    n_epochs = len(idx)\n    if n_epochs == 0:\n        return None\n\n    if sampling_interval is not None:\n        return (n_epochs * sampling_interval).to(UREG.seconds)\n\n    # Fallback: time coverage inclusive (last - first) + typical step\n    dts = self._epoch_datetimes()\n    if len(dts) == 0:\n        return None\n    if len(dts) == 1:\n        # single epoch: treat as 1 * unknown step (cannot infer)\n        return None\n\n    # Estimate step from data\n    est_step = self.infer_sampling_interval()\n    if est_step is None:\n        return None\n\n    # Inclusive coverage often equals (n_epochs - 1) * step; intended\n    # dump interval is n_epochs * step.\n    return (n_epochs * est_step.to(UREG.seconds)).to(UREG.seconds)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.validate_epoch_completeness","level":3,"title":"<code>validate_epoch_completeness(dump_interval=None, sampling_interval=None)</code>","text":"<p>Validate that the number of epochs matches the expected dump interval.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.validate_epoch_completeness--parameters","level":5,"title":"Parameters","text":"<p>dump_interval : str or pint.Quantity, optional     Expected file dump interval. If None, inferred from epochs. sampling_interval : str or pint.Quantity, optional     Expected sampling interval. If None, inferred from epochs.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.validate_epoch_completeness--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.validate_epoch_completeness--raises","level":5,"title":"Raises","text":"<p>MissingEpochError     If total sampling time doesn't match dump interval ValueError     If intervals cannot be inferred</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def validate_epoch_completeness(\n    self,\n    dump_interval: str | pint.Quantity | None = None,\n    sampling_interval: str | pint.Quantity | None = None,\n) -&gt; None:\n    \"\"\"Validate that the number of epochs matches the expected dump interval.\n\n    Parameters\n    ----------\n    dump_interval : str or pint.Quantity, optional\n        Expected file dump interval. If None, inferred from epochs.\n    sampling_interval : str or pint.Quantity, optional\n        Expected sampling interval. If None, inferred from epochs.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    MissingEpochError\n        If total sampling time doesn't match dump interval\n    ValueError\n        If intervals cannot be inferred\n\n    \"\"\"\n    # Normalize/Infer sampling interval\n    if sampling_interval is None:\n        inferred = self.infer_sampling_interval()\n        if inferred is None:\n            msg = \"Could not infer sampling interval from epochs\"\n            raise ValueError(msg)\n        sampling_interval = inferred\n    # normalize to pint\n    elif not isinstance(sampling_interval, pint.Quantity):\n        sampling_interval = UREG.Quantity(sampling_interval).to(UREG.seconds)\n\n    # Normalize/Infer dump interval\n    if dump_interval is None:\n        inferred_dump = self.infer_dump_interval(\n            sampling_interval=sampling_interval\n        )\n        if inferred_dump is None:\n            msg = \"Could not infer dump interval from file\"\n            raise ValueError(msg)\n        dump_interval = inferred_dump\n    elif not isinstance(dump_interval, pint.Quantity):\n        # Accept '15 min', '1h', etc.\n        dump_interval = UREG.Quantity(dump_interval).to(UREG.seconds)\n\n    # Build inputs for the validator model\n    epoch_indices = self.get_epoch_record_batches()\n\n    # This throws MissingEpochError automatically if inconsistent\n    Rnxv3ObsEpochRecordCompletenessModel(\n        epoch_records_indeces=epoch_indices,\n        rnx_file_dump_interval=dump_interval,\n        sampling_interval=sampling_interval,\n    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.filter_by_overlapping_groups","level":3,"title":"<code>filter_by_overlapping_groups(ds, group_preference=None)</code>","text":"<p>Filter overlapping bands using per-group preferences.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.filter_by_overlapping_groups--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with <code>sid</code> dimension and signal properties. group_preference : dict[str, str], optional     Mapping of overlap group to preferred band.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.filter_by_overlapping_groups--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset filtered to preferred overlapping bands.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def filter_by_overlapping_groups(\n    self,\n    ds: xr.Dataset,\n    group_preference: dict[str, str] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Filter overlapping bands using per-group preferences.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with `sid` dimension and signal properties.\n    group_preference : dict[str, str], optional\n        Mapping of overlap group to preferred band.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset filtered to preferred overlapping bands.\n\n    \"\"\"\n    if group_preference is None:\n        group_preference = {\n            \"L1_E1_B1I\": \"L1\",\n            \"L5_E5a\": \"L5\",\n            \"L2_E5b_B2b\": \"L2\",\n        }\n\n    keep = []\n    for sid in ds.sid.values:\n        _sv, band, _code = self._signal_mapper.parse_signal_id(str(sid))\n        group = self._signal_mapper.get_overlapping_group(band)\n        if group and group in group_preference:\n            if band == group_preference[group]:\n                keep.append(sid)\n        else:\n            keep.append(sid)\n    return ds.sel(sid=keep)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.create_rinex_netcdf_with_signal_id","level":3,"title":"<code>create_rinex_netcdf_with_signal_id(analyze_conflicts=False, analyze_systems=False, start=None, end=None)</code>","text":"<p>Create a NetCDF dataset with signal IDs.</p> <p>Can optionally restrict to epochs within a datetime range.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def create_rinex_netcdf_with_signal_id(\n    self,\n    analyze_conflicts: bool = False,\n    analyze_systems: bool = False,\n    start: datetime | None = None,\n    end: datetime | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Create a NetCDF dataset with signal IDs.\n\n    Can optionally restrict to epochs within a datetime range.\n\n    \"\"\"\n    if analyze_conflicts:\n        print(\"\\nNote: Conflict analysis will be adapted for sid structure.\")\n    if analyze_systems:\n        print(\"\\nNote: System analysis will be adapted for sid structure.\")\n\n    signal_ids = set()\n    signal_id_to_properties: dict[str, dict[str, object]] = {}\n    timestamps: list[np.datetime64] = []\n\n    # pick generator depending on range\n    if start and end:\n        epoch_iter = self.iter_epochs_in_range(start, end)\n    else:\n        epoch_iter = self.iter_epochs()\n\n    for epoch in epoch_iter:\n        dt = self.epochrecordinfo_dt_to_numpy_dt(epoch)\n        timestamps.append(np.datetime64(dt, \"ns\"))\n\n        for sat in epoch.data:\n            sv = sat.sv\n            for obs in sat.observations:\n                if not self.include_auxiliary and obs.observation_freq_tag.endswith(\n                    \"|X1\"\n                ):\n                    continue\n\n                sid = self._signal_mapper.create_signal_id(\n                    sv, obs.observation_freq_tag\n                )\n                signal_ids.add(sid)\n\n                if sid not in signal_id_to_properties:\n                    sv_part, band, code = self._signal_mapper.parse_signal_id(sid)\n                    system = sv_part[0]\n                    center_frequency = self._signal_mapper.get_band_frequency(band)\n                    bandwidth = self._signal_mapper.get_band_bandwidth(band)\n                    overlapping_group = self._signal_mapper.get_overlapping_group(\n                        band\n                    )\n\n                    if center_frequency is not None and bandwidth is not None:\n                        # Extract bandwidth value\n                        bw = (\n                            bandwidth[0]\n                            if isinstance(bandwidth, list)\n                            else bandwidth\n                        )\n\n                        # Ensure both are pint quantities\n                        if not hasattr(center_frequency, \"m_as\"):\n                            center_frequency = center_frequency * UREG.MHz\n                        if not hasattr(bw, \"m_as\"):\n                            bw = bw * UREG.MHz\n\n                        # Calculate frequency range\n                        freq_min = center_frequency - (bw / 2.0)\n                        freq_max = center_frequency + (bw / 2.0)\n\n                        # Extract magnitudes to ensure float64 dtype\n                        center_frequency = float(center_frequency.m_as(UREG.MHz))\n                        freq_min = float(freq_min.m_as(UREG.MHz))\n                        freq_max = float(freq_max.m_as(UREG.MHz))\n                        bw = float(bw.m_as(UREG.MHz))\n                    else:\n                        print(\n                            f\"WARNING: No frequency data for sid={sid}, \"\n                            f\"band={band}, sv={sv_part}\"\n                        )\n                        center_frequency = np.nan\n                        freq_min = np.nan\n                        freq_max = np.nan\n                        bw = np.nan\n\n                    signal_id_to_properties[sid] = {\n                        \"sv\": sv_part,\n                        \"system\": system,\n                        \"band\": band,\n                        \"code\": code,\n                        \"freq_center\": center_frequency,\n                        \"freq_min\": freq_min,\n                        \"freq_max\": freq_max,\n                        \"bandwidth\": bw,\n                        \"overlapping_group\": overlapping_group,\n                    }\n\n    # Inconsistent integration of the Septentrio X1 obs. code, filtering\n    # out here again.\n    signal_ids = {sid for sid in signal_ids if \"|X1|\" not in sid}\n    signal_id_to_properties = {\n        sid: props\n        for sid, props in signal_id_to_properties.items()\n        if \"|X1|\" not in sid\n    }\n\n    sorted_signal_ids = sorted(signal_ids)\n    n_epochs = len(timestamps)\n    n_signals = len(sorted_signal_ids)\n\n    data_arrays = {\n        \"SNR\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"SNR\"]),\n        \"Pseudorange\": np.full(\n            (n_epochs, n_signals), np.nan, dtype=DTYPES[\"Pseudorange\"]\n        ),\n        \"Phase\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"Phase\"]),\n        \"Doppler\": np.full((n_epochs, n_signals), np.nan, dtype=DTYPES[\"Doppler\"]),\n        \"LLI\": np.full((n_epochs, n_signals), -1, dtype=DTYPES[\"LLI\"]),\n        \"SSI\": np.full((n_epochs, n_signals), -1, dtype=DTYPES[\"SSI\"]),\n    }\n    sid_to_idx = {sid: i for i, sid in enumerate(sorted_signal_ids)}\n\n    # second pass to fill arrays\n    if start and end:\n        epoch_iter = self.iter_epochs_in_range(start, end)\n    else:\n        epoch_iter = self.iter_epochs()\n\n    for t_idx, epoch in enumerate(epoch_iter):\n        for sat in epoch.data:\n            sv = sat.sv\n            for obs in sat.observations:\n                if not self.include_auxiliary and obs.observation_freq_tag.endswith(\n                    \"|X1\"\n                ):\n                    continue\n                if obs.value is None:\n                    continue\n                sid = self._signal_mapper.create_signal_id(\n                    sv, obs.observation_freq_tag\n                )\n                if sid not in sid_to_idx:\n                    continue\n                s_idx = sid_to_idx[sid]\n\n                ot = obs.obs_type\n                if ot == \"S\" and obs.value != 0:\n                    data_arrays[\"SNR\"][t_idx, s_idx] = obs.value\n                elif ot == \"C\":\n                    data_arrays[\"Pseudorange\"][t_idx, s_idx] = obs.value\n                elif ot == \"L\":\n                    data_arrays[\"Phase\"][t_idx, s_idx] = obs.value\n                elif ot == \"D\":\n                    data_arrays[\"Doppler\"][t_idx, s_idx] = obs.value\n                elif ot == \"X\":\n                    data_arrays.setdefault(\n                        \"Auxiliary\",\n                        np.full((n_epochs, n_signals), np.nan, dtype=np.float32),\n                    )\n                    data_arrays[\"Auxiliary\"][t_idx, s_idx] = obs.value\n\n                if obs.lli is not None:\n                    data_arrays[\"LLI\"][t_idx, s_idx] = obs.lli\n                if obs.ssi is not None:\n                    data_arrays[\"SSI\"][t_idx, s_idx] = obs.ssi\n\n    signal_id_coord = xr.DataArray(\n        sorted_signal_ids, dims=[\"sid\"], attrs=COORDS_METADATA[\"sid\"]\n    )\n    sv_list = [signal_id_to_properties[sid][\"sv\"] for sid in sorted_signal_ids]\n    constellation_list = [\n        signal_id_to_properties[sid][\"system\"] for sid in sorted_signal_ids\n    ]\n    band_list = [signal_id_to_properties[sid][\"band\"] for sid in sorted_signal_ids]\n    code_list = [signal_id_to_properties[sid][\"code\"] for sid in sorted_signal_ids]\n    freq_center_list = [\n        signal_id_to_properties[sid][\"freq_center\"] for sid in sorted_signal_ids\n    ]\n    freq_min_list = [\n        signal_id_to_properties[sid][\"freq_min\"] for sid in sorted_signal_ids\n    ]\n    freq_max_list = [\n        signal_id_to_properties[sid][\"freq_max\"] for sid in sorted_signal_ids\n    ]\n\n    coords = {\n        \"epoch\": (\"epoch\", timestamps, COORDS_METADATA[\"epoch\"]),\n        \"sid\": signal_id_coord,\n        \"sv\": (\"sid\", sv_list, COORDS_METADATA[\"sv\"]),\n        \"system\": (\"sid\", constellation_list, COORDS_METADATA[\"system\"]),\n        \"band\": (\"sid\", band_list, COORDS_METADATA[\"band\"]),\n        \"code\": (\"sid\", code_list, COORDS_METADATA[\"code\"]),\n        \"freq_center\": (\n            \"sid\",\n            np.asarray(freq_center_list, dtype=DTYPES[\"freq_center\"]),\n            COORDS_METADATA[\"freq_center\"],\n        ),\n        \"freq_min\": (\n            \"sid\",\n            np.asarray(freq_min_list, dtype=DTYPES[\"freq_min\"]),\n            COORDS_METADATA[\"freq_min\"],\n        ),\n        \"freq_max\": (\n            \"sid\",\n            np.asarray(freq_max_list, dtype=DTYPES[\"freq_max\"]),\n            COORDS_METADATA[\"freq_max\"],\n        ),\n    }\n\n    if self.header.signal_strength_unit == UREG.dBHz:\n        snr_meta = CN0_METADATA\n    else:\n        snr_meta = SNR_METADATA\n\n    ds = xr.Dataset(\n        data_vars={\n            \"SNR\": ([\"epoch\", \"sid\"], data_arrays[\"SNR\"], snr_meta),\n            \"Pseudorange\": (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"Pseudorange\"],\n                OBSERVABLES_METADATA[\"Pseudorange\"],\n            ),\n            \"Phase\": (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"Phase\"],\n                OBSERVABLES_METADATA[\"Phase\"],\n            ),\n            \"Doppler\": (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"Doppler\"],\n                OBSERVABLES_METADATA[\"Doppler\"],\n            ),\n            \"LLI\": (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"LLI\"],\n                OBSERVABLES_METADATA[\"LLI\"],\n            ),\n            \"SSI\": (\n                [\"epoch\", \"sid\"],\n                data_arrays[\"SSI\"],\n                OBSERVABLES_METADATA[\"SSI\"],\n            ),\n        },\n        coords=coords,\n        attrs={**self._create_basic_attrs()},\n    )\n\n    if \"Auxiliary\" in data_arrays:\n        ds[\"Auxiliary\"] = (\n            [\"epoch\", \"sid\"],\n            data_arrays[\"Auxiliary\"],\n            OBSERVABLES_METADATA[\"Auxiliary\"],\n        )\n\n    if self.apply_overlap_filter:\n        ds = self.filter_by_overlapping_groups(ds, self.overlap_preferences)\n\n    return ds\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.to_ds","level":3,"title":"<code>to_ds(outname=None, keep_rnx_data_vars=None, write_global_attrs=False, pad_global_sid=True, strip_fillval=True, add_future_datavars=True, keep_sids=None)</code>","text":"<p>Convert RINEX observations to xarray.Dataset with signal ID structure.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.to_ds--parameters","level":5,"title":"Parameters","text":"<p>outname : Path or str, optional     If provided, saves dataset to this file path keep_rnx_data_vars : list of str or None, optional     Data variables to include in dataset. Defaults to config value. write_global_attrs : bool, default False     If True, adds comprehensive global attributes pad_global_sid : bool, default True     If True, pads to global signal ID space strip_fillval : bool, default True     If True, removes fill values add_future_datavars : bool, default True     If True, adds placeholder variables for future data keep_sids : list of str or None, default None     If provided, filters/pads dataset to these specific SIDs.     If None and pad_global_sid=True, pads to all possible SIDs.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.to_ds--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with dimensions (epoch, sid) and requested data variables</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def to_ds(\n    self,\n    outname: Path | str | None = None,\n    keep_rnx_data_vars: list[str] | None = None,\n    write_global_attrs: bool = False,\n    pad_global_sid: bool = True,\n    strip_fillval: bool = True,\n    add_future_datavars: bool = True,\n    keep_sids: list[str] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Convert RINEX observations to xarray.Dataset with signal ID structure.\n\n    Parameters\n    ----------\n    outname : Path or str, optional\n        If provided, saves dataset to this file path\n    keep_rnx_data_vars : list of str or None, optional\n        Data variables to include in dataset. Defaults to config value.\n    write_global_attrs : bool, default False\n        If True, adds comprehensive global attributes\n    pad_global_sid : bool, default True\n        If True, pads to global signal ID space\n    strip_fillval : bool, default True\n        If True, removes fill values\n    add_future_datavars : bool, default True\n        If True, adds placeholder variables for future data\n    keep_sids : list of str or None, default None\n        If provided, filters/pads dataset to these specific SIDs.\n        If None and pad_global_sid=True, pads to all possible SIDs.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with dimensions (epoch, sid) and requested data variables\n\n    \"\"\"\n    if keep_rnx_data_vars is None:\n        from canvod.utils.config import load_config\n\n        keep_rnx_data_vars = load_config().processing.processing.keep_rnx_vars\n\n    ds = self.create_rinex_netcdf_with_signal_id()\n\n    # drop unwanted vars\n    for var in list(ds.data_vars):\n        if var not in keep_rnx_data_vars:\n            ds = ds.drop_vars(var)\n\n    if pad_global_sid:\n        from canvod.auxiliary.preprocessing import pad_to_global_sid\n\n        # Pad/filter to specified sids or all possible sids\n        ds = pad_to_global_sid(ds, keep_sids=keep_sids)\n\n    if strip_fillval:\n        from canvod.auxiliary.preprocessing import strip_fillvalue\n\n        ds = strip_fillvalue(ds)\n\n    if add_future_datavars:\n        pass\n\n    if write_global_attrs:\n        ds.attrs.update(self._create_comprehensive_attrs())\n\n    ds.attrs[\"RINEX File Hash\"] = self.file_hash\n\n    if outname:\n        from canvod.utils.config import load_config as _load_config\n\n        comp = _load_config().processing.compression\n        encoding = {\n            var: {\"zlib\": comp.zlib, \"complevel\": comp.complevel}\n            for var in ds.data_vars\n        }\n        ds.to_netcdf(str(outname), encoding=encoding)\n\n    # Validate output structure for pipeline compatibility\n    self.validate_output(ds, required_vars=keep_rnx_data_vars)\n\n    return ds\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.validate_rinex_304_compliance","level":3,"title":"<code>validate_rinex_304_compliance(ds=None, strict=False, print_report=True)</code>","text":"<p>Run enhanced RINEX 3.04 specification validation.</p> <p>Validates: 1. System-specific observation codes 2. GLONASS mandatory fields (slot/frequency, biases) 3. Phase shift records (RINEX 3.01+) 4. Observation value ranges</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.validate_rinex_304_compliance--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset, optional     Dataset to validate. If None, creates one from current file. strict : bool     If True, raise ValueError on validation failures print_report : bool     If True, print validation report to console</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.validate_rinex_304_compliance--returns","level":5,"title":"Returns","text":"<p>dict[str, list[str]]     Validation results by category</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.validate_rinex_304_compliance--examples","level":5,"title":"Examples","text":"<p>reader = Rnxv3Obs(fpath=\"station.24o\") results = reader.validate_rinex_304_compliance()</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def validate_rinex_304_compliance(\n    self,\n    ds: xr.Dataset | None = None,\n    strict: bool = False,\n    print_report: bool = True,\n) -&gt; dict[str, list[str]]:\n    \"\"\"Run enhanced RINEX 3.04 specification validation.\n\n    Validates:\n    1. System-specific observation codes\n    2. GLONASS mandatory fields (slot/frequency, biases)\n    3. Phase shift records (RINEX 3.01+)\n    4. Observation value ranges\n\n    Parameters\n    ----------\n    ds : xr.Dataset, optional\n        Dataset to validate. If None, creates one from current file.\n    strict : bool\n        If True, raise ValueError on validation failures\n    print_report : bool\n        If True, print validation report to console\n\n    Returns\n    -------\n    dict[str, list[str]]\n        Validation results by category\n\n    Examples\n    --------\n    &gt;&gt;&gt; reader = Rnxv3Obs(fpath=\"station.24o\")\n    &gt;&gt;&gt; results = reader.validate_rinex_304_compliance()\n    &gt;&gt;&gt; # Or validate a specific dataset\n    &gt;&gt;&gt; ds = reader.to_ds()\n    &gt;&gt;&gt; results = reader.validate_rinex_304_compliance(ds=ds)\n\n    \"\"\"\n    if ds is None:\n        ds = self.to_ds(write_global_attrs=False)\n\n    # Prepare header dict for validators\n    header_dict = {\n        \"obs_codes_per_system\": self.header.obs_codes_per_system,\n    }\n\n    # Add GLONASS-specific headers if available\n    if hasattr(self.header, \"glonass_slot_frq\"):\n        header_dict[\"GLONASS SLOT / FRQ #\"] = self.header.glonass_slot_frq\n\n    if hasattr(self.header, \"glonass_cod_phs_bis\"):\n        header_dict[\"GLONASS COD/PHS/BIS\"] = self.header.glonass_cod_phs_bis\n\n    if hasattr(self.header, \"phase_shift\"):\n        header_dict[\"SYS / PHASE SHIFT\"] = self.header.phase_shift\n\n    # Run validation\n    results = RINEX304ComplianceValidator.validate_all(\n        ds=ds, header_dict=header_dict, strict=strict\n    )\n\n    if print_report:\n        RINEX304ComplianceValidator.print_validation_report(results)\n\n    return results\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.Rnxv3Obs.validate_rinex_304_compliance--or-validate-a-specific-dataset","level":4,"title":"Or validate a specific dataset","text":"<p>ds = reader.to_ds() results = reader.validate_rinex_304_compliance(ds=ds)</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.adapt_existing_rnxv3obs_class","level":2,"title":"<code>adapt_existing_rnxv3obs_class(original_class_path=None)</code>","text":"<p>Provide guidance to integrate the enhanced sid functionality.</p> <p>This function provides guidance on how to modify the existing class to support the new sid structure alongside the current OFT structure.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.rinex.v3_04.adapt_existing_rnxv3obs_class--returns","level":4,"title":"Returns","text":"<p>str     Integration instructions</p> Source code in <code>packages/canvod-readers/src/canvod/readers/rinex/v3_04.py</code> <pre><code>def adapt_existing_rnxv3obs_class(original_class_path: str | None = None) -&gt; str:\n    \"\"\"Provide guidance to integrate the enhanced sid functionality.\n\n    This function provides guidance on how to modify the existing class\n    to support the new sid structure alongside the current OFT structure.\n\n    Returns\n    -------\n    str\n        Integration instructions\n\n    \"\"\"\n    _ = original_class_path\n    return \"\"\"\n    INTEGRATION GUIDE: Adapting Rnxv3Obs for sid Structure\n    ============================================================\n\n    To integrate the new sid functionality into your existing Rnxv3Obs class:\n\n    1. ADD THE SIGNAL_ID_MAPPER CLASS:\n       - Copy the SignalIDMapper class to your rinex_reader.py file\n       - This handles the mapping logic and band properties\n\n    2. ADD NEW METHODS TO Rnxv3Obs CLASS:\n\n       Method: create_rinex_netcdf_with_signal_id()\n       - Copy from EnhancedRnxv3Obs.create_rinex_netcdf_with_signal_id()\n       - This creates the new sid-based structure\n\n       Method: filter_by_overlapping_groups()\n       - Copy from EnhancedRnxv3Obs.filter_by_overlapping_groups()\n       - Handles overlapping signal filtering (Problem A solution)\n\n       Method: to_ds()\n       - Copy from EnhancedRnxv3Obs.to_ds()\n       - Main interface for creating sid datasets\n\n       Method: create_legacy_compatible_dataset()\n       - Copy from EnhancedRnxv3Obs.create_legacy_compatible_dataset()\n       - Provides backward compatibility\n\n    3. UPDATE THE __init__ METHOD:\n       Add: self.signal_mapper = SignalIDMapper()\n\n    4. MODIFY EXISTING METHODS:\n       - Keep existing create_rinex_netcdf_with_oft() for OFT compatibility\n       - Add sid option to your main interface methods\n       - Update data handlers to support sid dimension\n\n    5. UPDATE DATA_HANDLER/RNX_PARSER.PY:\n       - Modify concatenate_datasets() to handle sid dimension\n       - Add sid detection alongside OFT detection\n       - Update encoding to handle sid string coordinates\n\n    6. UPDATE PROCESSOR/PROCESSOR.PY:\n       - Add sid support to create_common_space_datatree()\n       - Handle both OFT and sid structures in alignment logic\n\n    BENEFITS OF THIS STRUCTURE:\n    ===========================\n\n    ✓ Solves Problem A: Bandwidth overlap handling\n      - Overlapping signals kept separate with metadata for filtering\n      - band properties include bandwidth information\n\n    ✓ Solves Problem B: code-specific performance differences\n      - Each sv|band|code combination gets unique sid\n      - No more priority-based LUT - all combinations preserved\n\n    ✓ Maintains compatibility:\n      - Legacy conversion available\n      - OFT structure still supported\n      - Existing code continues to work\n\n    ✓ Enhanced filtering capabilities:\n      - Filter by system, band, code independently\n      - Complex filtering with multiple criteria\n      - Overlap group filtering for analysis\n\n    MIGRATION PATH:\n    ===============\n\n    Phase 1: Add sid methods alongside existing OFT methods\n    Phase 2: Update data handlers to support both structures\n    Phase 3: Gradually migrate analysis code to use sid\n    Phase 4: Deprecate old frequency-mapping approach (optional)\n\n    EXAMPLE USAGE AFTER INTEGRATION:\n    =================================\n\n    # Create datasets with different structures\n    ds_oft = rnx.create_rinex_netcdf_with_oft()           # Current OFT structure\n    ds_signal = rnx.create_rinex_netcdf_with_signal_id()  # New sid structure\n    ds_legacy = rnx.create_rinex_netcdf(mapped_epochs)    # Legacy structure\n\n    # Advanced sid usage\n    ds_enhanced = rnx.to_ds(\n        keep_rnx_data_vars=[\"SNR\", \"Phase\"],\n        apply_overlap_filter=True,\n        overlap_preferences={'L1_E1_B1I': 'L1'}  # Prefer GPS L1 over Galileo E1\n    )\n    \"\"\"\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#base-reader","level":2,"title":"Base Reader","text":"<p>Abstract base class for GNSS data readers.</p> <p>Defines interface that all readers (RINEX v3, RINEX v2, future formats) must implement to ensure compatibility with downstream pipeline: - VOD calculation (canvod-vod) - Storage (canvod-store / MyIcechunkStore) - Grid operations (canvod-grids)</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator","level":2,"title":"<code>DatasetStructureValidator</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Validates xarray.Dataset structure for pipeline compatibility.</p> <p>All readers must produce Datasets that pass this validation to ensure compatibility with downstream VOD and storage operations.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic <code>BaseModel</code> with <code>arbitrary_types_allowed=True</code>.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>class DatasetStructureValidator(BaseModel):\n    \"\"\"Validates xarray.Dataset structure for pipeline compatibility.\n\n    All readers must produce Datasets that pass this validation\n    to ensure compatibility with downstream VOD and storage operations.\n\n    Notes\n    -----\n    This is a Pydantic `BaseModel` with `arbitrary_types_allowed=True`.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    dataset: xr.Dataset\n\n    def validate_dimensions(self) -&gt; None:\n        \"\"\"Validate required dimensions exist.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If required dimensions (epoch, sid) are missing.\n\n        \"\"\"\n        required_dims = {\"epoch\", \"sid\"}\n        missing_dims = required_dims - set(self.dataset.dims)\n        if missing_dims:\n            msg = f\"Missing required dimensions: {missing_dims}\"\n            raise ValueError(msg)\n\n    def validate_coordinates(self) -&gt; None:\n        \"\"\"Validate required coordinates exist and have correct types.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If required coordinates are missing or have incorrect dtypes.\n\n        \"\"\"\n        required_coords = {\n            \"epoch\": \"datetime64[ns]\",\n            \"sid\": \"object\",  # string\n            \"sv\": \"object\",\n            \"system\": \"object\",\n            \"band\": \"object\",\n            \"code\": \"object\",\n            \"freq_center\": \"float32\",\n            \"freq_min\": \"float32\",\n            \"freq_max\": \"float32\",\n        }\n\n        for coord, expected_dtype in required_coords.items():\n            if coord not in self.dataset.coords:\n                msg = f\"Missing required coordinate: {coord}\"\n                raise ValueError(msg)\n\n            actual_dtype = str(self.dataset[coord].dtype)\n            if expected_dtype == \"object\":\n                # String coordinates can be stored as object or Unicode string (&lt;U)\n                if actual_dtype not in [\"object\"] and not actual_dtype.startswith(\"&lt;U\"):\n                    msg = (\n                        f\"Coordinate {coord} has wrong dtype: \"\n                        f\"expected string, got {actual_dtype}\"\n                    )\n                    raise ValueError(msg)\n            elif expected_dtype not in actual_dtype:\n                msg = (\n                    f\"Coordinate {coord} has wrong dtype: \"\n                    f\"expected {expected_dtype}, got {actual_dtype}\"\n                )\n                raise ValueError(msg)\n\n    def validate_data_variables(self, required_vars: list[str] | None = None) -&gt; None:\n        \"\"\"Validate required data variables exist.\n\n        Parameters\n        ----------\n        required_vars : list of str, optional\n            List of required variables. If None, uses default minimum set.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If required variables are missing or have incorrect dimensions.\n\n        \"\"\"\n        if required_vars is None:\n            # Minimum required for VOD calculation\n            required_vars = [\"SNR\", \"Phase\"]\n\n        missing_vars = set(required_vars) - set(self.dataset.data_vars)\n        if missing_vars:\n            msg = f\"Missing required data variables: {missing_vars}\"\n            raise ValueError(msg)\n\n        # Validate all data vars have (epoch, sid) dimensions\n        for var in self.dataset.data_vars:\n            expected_dims = (\"epoch\", \"sid\")\n            actual_dims = self.dataset[var].dims\n            if actual_dims != expected_dims:\n                msg = (\n                    f\"Data variable {var} has wrong dimensions: \"\n                    f\"expected {expected_dims}, got {actual_dims}\"\n                )\n                raise ValueError(msg)\n\n    def validate_attributes(self) -&gt; None:\n        \"\"\"Validate required global attributes for storage.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If required attributes (Created, Software, Institution,\n            RINEX File Hash) are missing.\n\n        \"\"\"\n        required_attrs = {\n            \"Created\",\n            \"Software\",\n            \"Institution\",\n            \"RINEX File Hash\",  # Required for MyIcechunkStore deduplication\n        }\n\n        missing_attrs = required_attrs - set(self.dataset.attrs.keys())\n        if missing_attrs:\n            msg = f\"Missing required attributes: {missing_attrs}\"\n            raise ValueError(msg)\n\n    def validate_all(self, required_vars: list[str] | None = None) -&gt; None:\n        \"\"\"Run all validations.\n\n        Parameters\n        ----------\n        required_vars : list of str, optional\n            List of required data variables. If None, uses default minimum set.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If any validation fails.\n\n        \"\"\"\n        self.validate_dimensions()\n        self.validate_coordinates()\n        self.validate_data_variables(required_vars)\n        self.validate_attributes()\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_dimensions","level":3,"title":"<code>validate_dimensions()</code>","text":"<p>Validate required dimensions exist.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_dimensions--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_dimensions--raises","level":5,"title":"Raises","text":"<p>ValueError     If required dimensions (epoch, sid) are missing.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_dimensions(self) -&gt; None:\n    \"\"\"Validate required dimensions exist.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If required dimensions (epoch, sid) are missing.\n\n    \"\"\"\n    required_dims = {\"epoch\", \"sid\"}\n    missing_dims = required_dims - set(self.dataset.dims)\n    if missing_dims:\n        msg = f\"Missing required dimensions: {missing_dims}\"\n        raise ValueError(msg)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_coordinates","level":3,"title":"<code>validate_coordinates()</code>","text":"<p>Validate required coordinates exist and have correct types.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_coordinates--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_coordinates--raises","level":5,"title":"Raises","text":"<p>ValueError     If required coordinates are missing or have incorrect dtypes.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_coordinates(self) -&gt; None:\n    \"\"\"Validate required coordinates exist and have correct types.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If required coordinates are missing or have incorrect dtypes.\n\n    \"\"\"\n    required_coords = {\n        \"epoch\": \"datetime64[ns]\",\n        \"sid\": \"object\",  # string\n        \"sv\": \"object\",\n        \"system\": \"object\",\n        \"band\": \"object\",\n        \"code\": \"object\",\n        \"freq_center\": \"float32\",\n        \"freq_min\": \"float32\",\n        \"freq_max\": \"float32\",\n    }\n\n    for coord, expected_dtype in required_coords.items():\n        if coord not in self.dataset.coords:\n            msg = f\"Missing required coordinate: {coord}\"\n            raise ValueError(msg)\n\n        actual_dtype = str(self.dataset[coord].dtype)\n        if expected_dtype == \"object\":\n            # String coordinates can be stored as object or Unicode string (&lt;U)\n            if actual_dtype not in [\"object\"] and not actual_dtype.startswith(\"&lt;U\"):\n                msg = (\n                    f\"Coordinate {coord} has wrong dtype: \"\n                    f\"expected string, got {actual_dtype}\"\n                )\n                raise ValueError(msg)\n        elif expected_dtype not in actual_dtype:\n            msg = (\n                f\"Coordinate {coord} has wrong dtype: \"\n                f\"expected {expected_dtype}, got {actual_dtype}\"\n            )\n            raise ValueError(msg)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_data_variables","level":3,"title":"<code>validate_data_variables(required_vars=None)</code>","text":"<p>Validate required data variables exist.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_data_variables--parameters","level":5,"title":"Parameters","text":"<p>required_vars : list of str, optional     List of required variables. If None, uses default minimum set.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_data_variables--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_data_variables--raises","level":5,"title":"Raises","text":"<p>ValueError     If required variables are missing or have incorrect dimensions.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_data_variables(self, required_vars: list[str] | None = None) -&gt; None:\n    \"\"\"Validate required data variables exist.\n\n    Parameters\n    ----------\n    required_vars : list of str, optional\n        List of required variables. If None, uses default minimum set.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If required variables are missing or have incorrect dimensions.\n\n    \"\"\"\n    if required_vars is None:\n        # Minimum required for VOD calculation\n        required_vars = [\"SNR\", \"Phase\"]\n\n    missing_vars = set(required_vars) - set(self.dataset.data_vars)\n    if missing_vars:\n        msg = f\"Missing required data variables: {missing_vars}\"\n        raise ValueError(msg)\n\n    # Validate all data vars have (epoch, sid) dimensions\n    for var in self.dataset.data_vars:\n        expected_dims = (\"epoch\", \"sid\")\n        actual_dims = self.dataset[var].dims\n        if actual_dims != expected_dims:\n            msg = (\n                f\"Data variable {var} has wrong dimensions: \"\n                f\"expected {expected_dims}, got {actual_dims}\"\n            )\n            raise ValueError(msg)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_attributes","level":3,"title":"<code>validate_attributes()</code>","text":"<p>Validate required global attributes for storage.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_attributes--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_attributes--raises","level":5,"title":"Raises","text":"<p>ValueError     If required attributes (Created, Software, Institution,     RINEX File Hash) are missing.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_attributes(self) -&gt; None:\n    \"\"\"Validate required global attributes for storage.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If required attributes (Created, Software, Institution,\n        RINEX File Hash) are missing.\n\n    \"\"\"\n    required_attrs = {\n        \"Created\",\n        \"Software\",\n        \"Institution\",\n        \"RINEX File Hash\",  # Required for MyIcechunkStore deduplication\n    }\n\n    missing_attrs = required_attrs - set(self.dataset.attrs.keys())\n    if missing_attrs:\n        msg = f\"Missing required attributes: {missing_attrs}\"\n        raise ValueError(msg)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_all","level":3,"title":"<code>validate_all(required_vars=None)</code>","text":"<p>Run all validations.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_all--parameters","level":5,"title":"Parameters","text":"<p>required_vars : list of str, optional     List of required data variables. If None, uses default minimum set.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_all--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.DatasetStructureValidator.validate_all--raises","level":5,"title":"Raises","text":"<p>ValueError     If any validation fails.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_all(self, required_vars: list[str] | None = None) -&gt; None:\n    \"\"\"Run all validations.\n\n    Parameters\n    ----------\n    required_vars : list of str, optional\n        List of required data variables. If None, uses default minimum set.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If any validation fails.\n\n    \"\"\"\n    self.validate_dimensions()\n    self.validate_coordinates()\n    self.validate_data_variables(required_vars)\n    self.validate_attributes()\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader","level":2,"title":"<code>GNSSDataReader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all GNSS data format readers.</p> <p>All readers must: 1. Inherit from this class 2. Implement all abstract methods 3. Return xarray.Dataset that passes DatasetStructureValidator 4. Provide file hash for deduplication</p> <p>This ensures compatibility with: - canvod-vod: VOD calculation - canvod-store: MyIcechunkStore storage - canvod-grids: Grid projection operations</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader--examples","level":4,"title":"Examples","text":"<p>class Rnxv3Obs(GNSSDataReader): ...     def to_ds(self, **kwargs) -&gt; xr.Dataset: ...         # Implementation ...         return dataset ... reader = Rnxv3Obs(fpath=\"station.24o\") ds = reader.to_ds() reader.validate_output(ds)  # Automatic validation</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader--notes","level":4,"title":"Notes","text":"<p>This class uses <code>ABC</code> and defines abstract methods and properties for reader implementations.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>class GNSSDataReader(ABC):\n    \"\"\"Abstract base class for all GNSS data format readers.\n\n    All readers must:\n    1. Inherit from this class\n    2. Implement all abstract methods\n    3. Return xarray.Dataset that passes DatasetStructureValidator\n    4. Provide file hash for deduplication\n\n    This ensures compatibility with:\n    - canvod-vod: VOD calculation\n    - canvod-store: MyIcechunkStore storage\n    - canvod-grids: Grid projection operations\n\n    Examples\n    --------\n    &gt;&gt;&gt; class Rnxv3Obs(GNSSDataReader):\n    ...     def to_ds(self, **kwargs) -&gt; xr.Dataset:\n    ...         # Implementation\n    ...         return dataset\n    ...\n    &gt;&gt;&gt; reader = Rnxv3Obs(fpath=\"station.24o\")\n    &gt;&gt;&gt; ds = reader.to_ds()\n    &gt;&gt;&gt; reader.validate_output(ds)  # Automatic validation\n\n    Notes\n    -----\n    This class uses ``ABC`` and defines abstract methods and properties\n    for reader implementations.\n\n    \"\"\"\n\n    # Note: fpath is not @abstractmethod because Pydantic models define it as a field\n    # which provides the same interface\n    fpath: Path\n\n    @property\n    @abstractmethod\n    def file_hash(self) -&gt; str:\n        \"\"\"Return SHA256 hash of file for deduplication.\n\n        Used by MyIcechunkStore to avoid duplicate ingestion.\n        Must be deterministic and reproducible.\n\n        Returns\n        -------\n        str\n            Short hash (16 chars) or full hash of file content\n\n        \"\"\"\n\n    @abstractmethod\n    def to_ds(\n        self,\n        keep_rnx_data_vars: list[str] | None = None,\n        **kwargs: object,\n    ) -&gt; xr.Dataset:\n        \"\"\"Convert data to xarray.Dataset.\n\n        Must return Dataset with structure:\n        - Dims: (epoch, sid)\n        - Coords: epoch, sid, sv, system, band, code, freq_*\n        - Data vars: At minimum SNR, Phase\n        - Attrs: Must include \"RINEX File Hash\"\n\n        Parameters\n        ----------\n        keep_rnx_data_vars : list of str, optional\n            Data variables to include. If None, includes all available.\n        **kwargs\n            Implementation-specific parameters\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset that passes DatasetStructureValidator.\n\n        \"\"\"\n\n    @abstractmethod\n    def iter_epochs(self) -&gt; Iterator[object]:\n        \"\"\"Iterate over epochs in the file.\n\n        Returns\n        -------\n        Generator\n            Generator yielding Epoch objects.\n\n        Yields\n        ------\n        Epoch\n            Parsed epoch with satellites and observations.\n\n        \"\"\"\n\n    def validate_output(\n        self, dataset: xr.Dataset, required_vars: list[str] | None = None\n    ) -&gt; None:\n        \"\"\"Validate output Dataset structure.\n\n        Called automatically by to_ds() to ensure compatibility.\n        Can be called manually for testing.\n\n        Parameters\n        ----------\n        dataset : xr.Dataset\n            Dataset to validate\n        required_vars : list of str, optional\n            Required data variables. If None, uses minimum set.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        ValueError\n            If Dataset doesn't meet requirements.\n\n        \"\"\"\n        validator = DatasetStructureValidator(dataset=dataset)\n        validator.validate_all(required_vars=required_vars)\n\n    @property\n    @abstractmethod\n    def start_time(self) -&gt; datetime:\n        \"\"\"Return start time of observations.\n\n        Returns\n        -------\n        datetime\n            First observation timestamp in the file.\n\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def end_time(self) -&gt; datetime:\n        \"\"\"Return end time of observations.\n\n        Returns\n        -------\n        datetime\n            Last observation timestamp in the file.\n\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def systems(self) -&gt; list[str]:\n        \"\"\"Return list of GNSS systems in file.\n\n        Returns\n        -------\n        list of str\n            System identifiers: 'G', 'R', 'E', 'C', 'J', 'S', 'I'\n\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def num_epochs(self) -&gt; int:\n        \"\"\"Return number of epochs in file.\n\n        Returns\n        -------\n        int\n            Total number of observation epochs.\n\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def num_satellites(self) -&gt; int:\n        \"\"\"Return total number of unique satellites observed.\n\n        Returns\n        -------\n        int\n            Count of unique satellite vehicles across all systems.\n\n        \"\"\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation.\"\"\"\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"file='{self.fpath.name}', \"\n            f\"systems={self.systems}, \"\n            f\"epochs={self.num_epochs})\"\n        )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.file_hash","level":3,"title":"<code>file_hash</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return SHA256 hash of file for deduplication.</p> <p>Used by MyIcechunkStore to avoid duplicate ingestion. Must be deterministic and reproducible.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.file_hash--returns","level":5,"title":"Returns","text":"<p>str     Short hash (16 chars) or full hash of file content</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.start_time","level":3,"title":"<code>start_time</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return start time of observations.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.start_time--returns","level":5,"title":"Returns","text":"<p>datetime     First observation timestamp in the file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.end_time","level":3,"title":"<code>end_time</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return end time of observations.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.end_time--returns","level":5,"title":"Returns","text":"<p>datetime     Last observation timestamp in the file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.systems","level":3,"title":"<code>systems</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return list of GNSS systems in file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.systems--returns","level":5,"title":"Returns","text":"<p>list of str     System identifiers: 'G', 'R', 'E', 'C', 'J', 'S', 'I'</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.num_epochs","level":3,"title":"<code>num_epochs</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return number of epochs in file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.num_epochs--returns","level":5,"title":"Returns","text":"<p>int     Total number of observation epochs.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.num_satellites","level":3,"title":"<code>num_satellites</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return total number of unique satellites observed.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.num_satellites--returns","level":5,"title":"Returns","text":"<p>int     Count of unique satellite vehicles across all systems.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.to_ds","level":3,"title":"<code>to_ds(keep_rnx_data_vars=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Convert data to xarray.Dataset.</p> <p>Must return Dataset with structure: - Dims: (epoch, sid) - Coords: epoch, sid, sv, system, band, code, freq_* - Data vars: At minimum SNR, Phase - Attrs: Must include \"RINEX File Hash\"</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.to_ds--parameters","level":5,"title":"Parameters","text":"<p>keep_rnx_data_vars : list of str, optional     Data variables to include. If None, includes all available. **kwargs     Implementation-specific parameters</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.to_ds--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset that passes DatasetStructureValidator.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>@abstractmethod\ndef to_ds(\n    self,\n    keep_rnx_data_vars: list[str] | None = None,\n    **kwargs: object,\n) -&gt; xr.Dataset:\n    \"\"\"Convert data to xarray.Dataset.\n\n    Must return Dataset with structure:\n    - Dims: (epoch, sid)\n    - Coords: epoch, sid, sv, system, band, code, freq_*\n    - Data vars: At minimum SNR, Phase\n    - Attrs: Must include \"RINEX File Hash\"\n\n    Parameters\n    ----------\n    keep_rnx_data_vars : list of str, optional\n        Data variables to include. If None, includes all available.\n    **kwargs\n        Implementation-specific parameters\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset that passes DatasetStructureValidator.\n\n    \"\"\"\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.iter_epochs","level":3,"title":"<code>iter_epochs()</code>  <code>abstractmethod</code>","text":"<p>Iterate over epochs in the file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.iter_epochs--returns","level":5,"title":"Returns","text":"<p>Generator     Generator yielding Epoch objects.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.iter_epochs--yields","level":5,"title":"Yields","text":"<p>Epoch     Parsed epoch with satellites and observations.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>@abstractmethod\ndef iter_epochs(self) -&gt; Iterator[object]:\n    \"\"\"Iterate over epochs in the file.\n\n    Returns\n    -------\n    Generator\n        Generator yielding Epoch objects.\n\n    Yields\n    ------\n    Epoch\n        Parsed epoch with satellites and observations.\n\n    \"\"\"\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.validate_output","level":3,"title":"<code>validate_output(dataset, required_vars=None)</code>","text":"<p>Validate output Dataset structure.</p> <p>Called automatically by to_ds() to ensure compatibility. Can be called manually for testing.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.validate_output--parameters","level":5,"title":"Parameters","text":"<p>dataset : xr.Dataset     Dataset to validate required_vars : list of str, optional     Required data variables. If None, uses minimum set.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.validate_output--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.validate_output--raises","level":5,"title":"Raises","text":"<p>ValueError     If Dataset doesn't meet requirements.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def validate_output(\n    self, dataset: xr.Dataset, required_vars: list[str] | None = None\n) -&gt; None:\n    \"\"\"Validate output Dataset structure.\n\n    Called automatically by to_ds() to ensure compatibility.\n    Can be called manually for testing.\n\n    Parameters\n    ----------\n    dataset : xr.Dataset\n        Dataset to validate\n    required_vars : list of str, optional\n        Required data variables. If None, uses minimum set.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If Dataset doesn't meet requirements.\n\n    \"\"\"\n    validator = DatasetStructureValidator(dataset=dataset)\n    validator.validate_all(required_vars=required_vars)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.GNSSDataReader.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return the string representation.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation.\"\"\"\n    return (\n        f\"{self.__class__.__name__}(\"\n        f\"file='{self.fpath.name}', \"\n        f\"systems={self.systems}, \"\n        f\"epochs={self.num_epochs})\"\n    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory","level":2,"title":"<code>ReaderFactory</code>","text":"<p>Factory for creating appropriate reader based on file format.</p> <p>Automatically detects format and instantiates correct reader.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory--examples","level":4,"title":"Examples","text":"<p>reader = ReaderFactory.create(\"station.24o\") isinstance(reader, Rnxv3Obs) True</p> <p>reader = ReaderFactory.create(\"station.10o\") isinstance(reader, Rnxv2Obs) True</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>class ReaderFactory:\n    \"\"\"Factory for creating appropriate reader based on file format.\n\n    Automatically detects format and instantiates correct reader.\n\n    Examples\n    --------\n    &gt;&gt;&gt; reader = ReaderFactory.create(\"station.24o\")\n    &gt;&gt;&gt; isinstance(reader, Rnxv3Obs)\n    True\n\n    &gt;&gt;&gt; reader = ReaderFactory.create(\"station.10o\")\n    &gt;&gt;&gt; isinstance(reader, Rnxv2Obs)\n    True\n\n    \"\"\"\n\n    _readers: ClassVar[dict[str, type]] = {}\n\n    @classmethod\n    def register(cls, format_name: str, reader_class: type) -&gt; None:\n        \"\"\"Register a reader class for a format.\n\n        Parameters\n        ----------\n        format_name : str\n            Format identifier (e.g., 'rinex_v3', 'rinex_v2')\n        reader_class : type\n            Reader class (must inherit from GNSSDataReader)\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        TypeError\n            If reader_class does not inherit from GNSSDataReader.\n\n        \"\"\"\n        if not issubclass(reader_class, GNSSDataReader):\n            msg = f\"{reader_class} must inherit from GNSSDataReader\"\n            raise TypeError(msg)\n        cls._readers[format_name] = reader_class\n\n    @classmethod\n    def create(\n        cls,\n        fpath: Path | str,\n        **kwargs: object,\n    ) -&gt; GNSSDataReader:\n        \"\"\"Create appropriate reader for file.\n\n        Parameters\n        ----------\n        fpath : Path or str\n            Path to data file\n        **kwargs\n            Parameters to pass to reader constructor\n\n        Returns\n        -------\n        GNSSDataReader\n            Instantiated reader.\n\n        Raises\n        ------\n        ValueError\n            If file format cannot be determined.\n\n        \"\"\"\n        fpath = Path(fpath)\n\n        if not fpath.exists():\n            msg = f\"File not found: {fpath}\"\n            raise FileNotFoundError(msg)\n\n        # Detect format from file\n        format_name = cls._detect_format(fpath)\n\n        if format_name not in cls._readers:\n            msg = (\n                f\"No reader registered for format: {format_name}. \"\n                f\"Available: {list(cls._readers.keys())}\"\n            )\n            raise ValueError(msg)\n\n        reader_class = cls._readers[format_name]\n        return reader_class(fpath=fpath, **kwargs)\n\n    @staticmethod\n    def _detect_format(fpath: Path) -&gt; str:\n        \"\"\"Detect file format.\n\n        Parameters\n        ----------\n        fpath : Path\n            Path to file\n\n        Returns\n        -------\n        str\n            Format name.\n\n        \"\"\"\n        # Check RINEX version from first line\n        with fpath.open() as f:\n            first_line = f.readline()\n\n        # RINEX version is in columns 1-9\n        try:\n            version_str = first_line[:9].strip()\n            version = float(version_str)\n        except (ValueError, IndexError) as e:\n            msg = f\"Cannot determine file format: {e}\"\n            raise ValueError(msg) from e\n\n        rinex_v2_min = 2.0\n        rinex_v3_min = 3.0\n        rinex_v4_min = 4.0\n\n        if rinex_v3_min &lt;= version &lt; rinex_v4_min:\n            return \"rinex_v3\"\n        if rinex_v2_min &lt;= version &lt; rinex_v3_min:\n            return \"rinex_v2\"\n        msg = f\"Unsupported RINEX version: {version}\"\n        raise ValueError(msg)\n\n    @classmethod\n    def list_formats(cls) -&gt; list[str]:\n        \"\"\"List available formats.\n\n        Returns\n        -------\n        list of str\n            Registered format identifiers.\n\n        \"\"\"\n        return list(cls._readers.keys())\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory.register","level":3,"title":"<code>register(format_name, reader_class)</code>  <code>classmethod</code>","text":"<p>Register a reader class for a format.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory.register--parameters","level":5,"title":"Parameters","text":"<p>format_name : str     Format identifier (e.g., 'rinex_v3', 'rinex_v2') reader_class : type     Reader class (must inherit from GNSSDataReader)</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory.register--returns","level":5,"title":"Returns","text":"<p>None</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory.register--raises","level":5,"title":"Raises","text":"<p>TypeError     If reader_class does not inherit from GNSSDataReader.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>@classmethod\ndef register(cls, format_name: str, reader_class: type) -&gt; None:\n    \"\"\"Register a reader class for a format.\n\n    Parameters\n    ----------\n    format_name : str\n        Format identifier (e.g., 'rinex_v3', 'rinex_v2')\n    reader_class : type\n        Reader class (must inherit from GNSSDataReader)\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    TypeError\n        If reader_class does not inherit from GNSSDataReader.\n\n    \"\"\"\n    if not issubclass(reader_class, GNSSDataReader):\n        msg = f\"{reader_class} must inherit from GNSSDataReader\"\n        raise TypeError(msg)\n    cls._readers[format_name] = reader_class\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory.create","level":3,"title":"<code>create(fpath, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create appropriate reader for file.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory.create--parameters","level":5,"title":"Parameters","text":"<p>fpath : Path or str     Path to data file **kwargs     Parameters to pass to reader constructor</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory.create--returns","level":5,"title":"Returns","text":"<p>GNSSDataReader     Instantiated reader.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory.create--raises","level":5,"title":"Raises","text":"<p>ValueError     If file format cannot be determined.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    fpath: Path | str,\n    **kwargs: object,\n) -&gt; GNSSDataReader:\n    \"\"\"Create appropriate reader for file.\n\n    Parameters\n    ----------\n    fpath : Path or str\n        Path to data file\n    **kwargs\n        Parameters to pass to reader constructor\n\n    Returns\n    -------\n    GNSSDataReader\n        Instantiated reader.\n\n    Raises\n    ------\n    ValueError\n        If file format cannot be determined.\n\n    \"\"\"\n    fpath = Path(fpath)\n\n    if not fpath.exists():\n        msg = f\"File not found: {fpath}\"\n        raise FileNotFoundError(msg)\n\n    # Detect format from file\n    format_name = cls._detect_format(fpath)\n\n    if format_name not in cls._readers:\n        msg = (\n            f\"No reader registered for format: {format_name}. \"\n            f\"Available: {list(cls._readers.keys())}\"\n        )\n        raise ValueError(msg)\n\n    reader_class = cls._readers[format_name]\n    return reader_class(fpath=fpath, **kwargs)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory.list_formats","level":3,"title":"<code>list_formats()</code>  <code>classmethod</code>","text":"<p>List available formats.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.base.ReaderFactory.list_formats--returns","level":5,"title":"Returns","text":"<p>list of str     Registered format identifiers.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/base.py</code> <pre><code>@classmethod\ndef list_formats(cls) -&gt; list[str]:\n    \"\"\"List available formats.\n\n    Returns\n    -------\n    list of str\n        Registered format identifiers.\n\n    \"\"\"\n    return list(cls._readers.keys())\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#gnss-specifications","level":2,"title":"GNSS Specifications","text":"<p>GNSS specifications and core characteristics.</p> <p>This module contains fundamental GNSS definitions including: - Constants: Unit registry, physical constants, RINEX parameters - Exceptions: GNSS-specific error types - Metadata: CF-compliant metadata for coordinates and observables - Models: Pydantic validation models for RINEX data structures - Signals: Signal ID mapping and band properties - Utils: File hashing, version extraction, data type checks</p> <p>These components are used across all GNSS reader implementations.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#directory-matching","level":2,"title":"Directory Matching","text":"<p>Directory matching for RINEX data files.</p> <p>Identifies and matches RINEX data directories across dates and receivers.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.DataDirMatcher","level":2,"title":"<code>DataDirMatcher</code>","text":"<p>Match RINEX data directories for canopy and reference receivers.</p> <p>Scans a root directory structure to find dates with RINEX files present in both canopy and reference receiver directories.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.DataDirMatcher--parameters","level":4,"title":"Parameters","text":"<p>root : Path     Root directory containing receiver subdirectories reference_pattern : Path, optional     Relative path pattern for reference receiver     (default: \"01_reference/01_GNSS/01_raw\") canopy_pattern : Path, optional     Relative path pattern for canopy receiver     (default: \"02_canopy/01_GNSS/01_raw\")</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.DataDirMatcher--examples","level":4,"title":"Examples","text":"<p>from pathlib import Path matcher = DataDirMatcher( ...     root=Path(\"/data/01_Rosalia\"), ...     reference_pattern=Path(\"01_reference/01_GNSS/01_raw\"), ...     canopy_pattern=Path(\"02_canopy/01_GNSS/01_raw\") ... )</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>class DataDirMatcher:\n    \"\"\"Match RINEX data directories for canopy and reference receivers.\n\n    Scans a root directory structure to find dates with RINEX files\n    present in both canopy and reference receiver directories.\n\n    Parameters\n    ----------\n    root : Path\n        Root directory containing receiver subdirectories\n    reference_pattern : Path, optional\n        Relative path pattern for reference receiver\n        (default: \"01_reference/01_GNSS/01_raw\")\n    canopy_pattern : Path, optional\n        Relative path pattern for canopy receiver\n        (default: \"02_canopy/01_GNSS/01_raw\")\n\n    Examples\n    --------\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; matcher = DataDirMatcher(\n    ...     root=Path(\"/data/01_Rosalia\"),\n    ...     reference_pattern=Path(\"01_reference/01_GNSS/01_raw\"),\n    ...     canopy_pattern=Path(\"02_canopy/01_GNSS/01_raw\")\n    ... )\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Iterate over matched directories\n    &gt;&gt;&gt; for matched_dirs in matcher:\n    ...     print(matched_dirs.yyyydoy)\n    ...     rinex_files = list(matched_dirs.canopy_data_dir.glob(\"*.25o\"))\n    ...     print(f\"  Found {len(rinex_files)} RINEX files\")\n\n    &gt;&gt;&gt; # Get list of common dates\n    &gt;&gt;&gt; dates = matcher.get_common_dates()\n    &gt;&gt;&gt; print(f\"Found {len(dates)} dates with data\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        root: Path,\n        reference_pattern: Path = Path(\"01_reference/01_GNSS/01_raw\"),\n        canopy_pattern: Path = Path(\"02_canopy/01_GNSS/01_raw\"),\n    ) -&gt; None:\n        \"\"\"Initialize matcher with directory structure.\"\"\"\n        self.root = Path(root)\n        self.reference_dir = self.root / reference_pattern\n        self.canopy_dir = self.root / canopy_pattern\n\n        # Validate directories exist\n        self._validate_directory(self.root, \"Root\")\n        self._validate_directory(self.reference_dir, \"Reference\")\n        self._validate_directory(self.canopy_dir, \"Canopy\")\n\n    def __iter__(self) -&gt; Iterator[MatchedDirs]:\n        \"\"\"Iterate over matched directory pairs with RINEX files.\n\n        Yields\n        ------\n        MatchedDirs\n            Matched directories for each date.\n\n        \"\"\"\n        for date_str in self.get_common_dates():\n            yield MatchedDirs(\n                canopy_data_dir=self.canopy_dir / date_str,\n                reference_data_dir=self.reference_dir / date_str,\n                yyyydoy=YYYYDOY.from_yydoy_str(date_str),\n            )\n\n    def get_common_dates(self) -&gt; list[str]:\n        \"\"\"Get dates with RINEX files in both receivers.\n\n        Uses parallel processing to check directories efficiently.\n\n        Returns\n        -------\n        list[str]\n            Sorted list of date strings (YYDDD format, e.g., \"25001\")\n            that have RINEX files in both canopy and reference directories.\n\n        \"\"\"\n        # Find dates with RINEX in each receiver\n        ref_dates = self._get_dates_with_rinex(self.reference_dir)\n        can_dates = self._get_dates_with_rinex(self.canopy_dir)\n\n        # Find intersection\n        common = ref_dates &amp; can_dates\n        common.discard(\"00000\")  # Remove placeholder directories\n\n        # Sort naturally (numerical order)\n        return natsorted(common)\n\n    def _get_dates_with_rinex(self, base_dir: Path) -&gt; set[str]:\n        \"\"\"Find all date directories containing RINEX files.\n\n        Uses parallel processing to check multiple directories at once.\n\n        Parameters\n        ----------\n        base_dir : Path\n            Base directory to search (e.g., canopy or reference root).\n\n        Returns\n        -------\n        set[str]\n            Set of date directory names that contain RINEX files.\n\n        \"\"\"\n        # Get all subdirectories\n        date_dirs = (d for d in base_dir.iterdir() if d.is_dir())\n\n        # Check for RINEX files in parallel\n        dates_with_rinex = set()\n\n        with ThreadPoolExecutor() as executor:\n            future_to_dir = {\n                executor.submit(self._has_rinex_files, d): d for d in date_dirs\n            }\n\n            for future in as_completed(future_to_dir):\n                directory = future_to_dir[future]\n                if future.result():\n                    dates_with_rinex.add(directory.name)\n\n        return dates_with_rinex\n\n    @staticmethod\n    def _has_rinex_files(directory: Path) -&gt; bool:\n        \"\"\"Check if directory contains RINEX observation files.\n\n        Parameters\n        ----------\n        directory : Path\n            Directory to check.\n\n        Returns\n        -------\n        bool\n            True if RINEX files found.\n\n        \"\"\"\n        return _has_rinex_files(directory)\n\n    def _validate_directory(self, path: Path, name: str) -&gt; None:\n        \"\"\"Validate directory exists.\n\n        Parameters\n        ----------\n        path : Path\n            Directory to check.\n        name : str\n            Name for error message.\n\n        Raises\n        ------\n        FileNotFoundError\n            If directory doesn't exist.\n\n        \"\"\"\n        if not path.exists():\n            msg = f\"{name} directory not found: {path}\"\n            raise FileNotFoundError(msg)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.DataDirMatcher--iterate-over-matched-directories","level":3,"title":"Iterate over matched directories","text":"<p>for matched_dirs in matcher: ...     print(matched_dirs.yyyydoy) ...     rinex_files = list(matched_dirs.canopy_data_dir.glob(\"*.25o\")) ...     print(f\"  Found {len(rinex_files)} RINEX files\")</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.DataDirMatcher--get-list-of-common-dates","level":3,"title":"Get list of common dates","text":"<p>dates = matcher.get_common_dates() print(f\"Found {len(dates)} dates with data\")</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.DataDirMatcher.__init__","level":3,"title":"<code>__init__(root, reference_pattern=Path('01_reference/01_GNSS/01_raw'), canopy_pattern=Path('02_canopy/01_GNSS/01_raw'))</code>","text":"<p>Initialize matcher with directory structure.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>def __init__(\n    self,\n    root: Path,\n    reference_pattern: Path = Path(\"01_reference/01_GNSS/01_raw\"),\n    canopy_pattern: Path = Path(\"02_canopy/01_GNSS/01_raw\"),\n) -&gt; None:\n    \"\"\"Initialize matcher with directory structure.\"\"\"\n    self.root = Path(root)\n    self.reference_dir = self.root / reference_pattern\n    self.canopy_dir = self.root / canopy_pattern\n\n    # Validate directories exist\n    self._validate_directory(self.root, \"Root\")\n    self._validate_directory(self.reference_dir, \"Reference\")\n    self._validate_directory(self.canopy_dir, \"Canopy\")\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.DataDirMatcher.__iter__","level":3,"title":"<code>__iter__()</code>","text":"<p>Iterate over matched directory pairs with RINEX files.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.DataDirMatcher.__iter__--yields","level":5,"title":"Yields","text":"<p>MatchedDirs     Matched directories for each date.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>def __iter__(self) -&gt; Iterator[MatchedDirs]:\n    \"\"\"Iterate over matched directory pairs with RINEX files.\n\n    Yields\n    ------\n    MatchedDirs\n        Matched directories for each date.\n\n    \"\"\"\n    for date_str in self.get_common_dates():\n        yield MatchedDirs(\n            canopy_data_dir=self.canopy_dir / date_str,\n            reference_data_dir=self.reference_dir / date_str,\n            yyyydoy=YYYYDOY.from_yydoy_str(date_str),\n        )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.DataDirMatcher.get_common_dates","level":3,"title":"<code>get_common_dates()</code>","text":"<p>Get dates with RINEX files in both receivers.</p> <p>Uses parallel processing to check directories efficiently.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.DataDirMatcher.get_common_dates--returns","level":5,"title":"Returns","text":"<p>list[str]     Sorted list of date strings (YYDDD format, e.g., \"25001\")     that have RINEX files in both canopy and reference directories.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>def get_common_dates(self) -&gt; list[str]:\n    \"\"\"Get dates with RINEX files in both receivers.\n\n    Uses parallel processing to check directories efficiently.\n\n    Returns\n    -------\n    list[str]\n        Sorted list of date strings (YYDDD format, e.g., \"25001\")\n        that have RINEX files in both canopy and reference directories.\n\n    \"\"\"\n    # Find dates with RINEX in each receiver\n    ref_dates = self._get_dates_with_rinex(self.reference_dir)\n    can_dates = self._get_dates_with_rinex(self.canopy_dir)\n\n    # Find intersection\n    common = ref_dates &amp; can_dates\n    common.discard(\"00000\")  # Remove placeholder directories\n\n    # Sort naturally (numerical order)\n    return natsorted(common)\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.PairDataDirMatcher","level":2,"title":"<code>PairDataDirMatcher</code>","text":"<p>Match RINEX directories for receiver pairs across dates.</p> <p>Supports multi-receiver configurations where multiple canopy/reference pairs may exist at the same site. Requires a configuration dict specifying receiver locations and analysis pairs.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.PairDataDirMatcher--parameters","level":4,"title":"Parameters","text":"<p>base_dir : Path     Root directory containing all receiver data receivers : dict     Receiver configuration mapping receiver names to their directory paths.     The <code>directory</code> value is the full relative path from <code>base_dir</code> to the     raw RINEX data directory (before the <code>{YYDOY}</code> date folders).     Example: {\"canopy_01\": {\"directory\": \"02_canopy_01/01_GNSS/01_raw\"},               \"reference_01\": {\"directory\": \"01_reference_01/01_GNSS/01_raw\"}} analysis_pairs : dict     Analysis pair configuration specifying which receivers to match     Example: {\"pair_01\": {\"canopy_receiver\": \"canopy_01\",                            \"reference_receiver\": \"reference_01\"}}</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.PairDataDirMatcher--examples","level":4,"title":"Examples","text":"<p>receivers = { ...     \"canopy_01\": {\"directory\": \"02_canopy/01_GNSS/01_raw\"}, ...     \"reference_01\": {\"directory\": \"01_reference/01_GNSS/01_raw\"} ... } pairs = { ...     \"main_pair\": { ...         \"canopy_receiver\": \"canopy_01\", ...         \"reference_receiver\": \"reference_01\" ...     } ... }</p> <p>matcher = PairDataDirMatcher( ...     base_dir=Path(\"/data/01_Rosalia\"), ...     receivers=receivers, ...     analysis_pairs=pairs ... )</p> <p>for matched in matcher: ...     print(f\"{matched.yyyydoy}: {matched.pair_name}\") ...     print(f\"  Canopy: {matched.canopy_data_dir}\") ...     print(f\"  Reference: {matched.reference_data_dir}\")</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>class PairDataDirMatcher:\n    \"\"\"Match RINEX directories for receiver pairs across dates.\n\n    Supports multi-receiver configurations where multiple canopy/reference\n    pairs may exist at the same site. Requires a configuration dict\n    specifying receiver locations and analysis pairs.\n\n    Parameters\n    ----------\n    base_dir : Path\n        Root directory containing all receiver data\n    receivers : dict\n        Receiver configuration mapping receiver names to their directory paths.\n        The ``directory`` value is the full relative path from ``base_dir`` to the\n        raw RINEX data directory (before the ``{YYDOY}`` date folders).\n        Example: {\"canopy_01\": {\"directory\": \"02_canopy_01/01_GNSS/01_raw\"},\n                  \"reference_01\": {\"directory\": \"01_reference_01/01_GNSS/01_raw\"}}\n    analysis_pairs : dict\n        Analysis pair configuration specifying which receivers to match\n        Example: {\"pair_01\": {\"canopy_receiver\": \"canopy_01\",\n                               \"reference_receiver\": \"reference_01\"}}\n\n    Examples\n    --------\n    &gt;&gt;&gt; receivers = {\n    ...     \"canopy_01\": {\"directory\": \"02_canopy/01_GNSS/01_raw\"},\n    ...     \"reference_01\": {\"directory\": \"01_reference/01_GNSS/01_raw\"}\n    ... }\n    &gt;&gt;&gt; pairs = {\n    ...     \"main_pair\": {\n    ...         \"canopy_receiver\": \"canopy_01\",\n    ...         \"reference_receiver\": \"reference_01\"\n    ...     }\n    ... }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; matcher = PairDataDirMatcher(\n    ...     base_dir=Path(\"/data/01_Rosalia\"),\n    ...     receivers=receivers,\n    ...     analysis_pairs=pairs\n    ... )\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; for matched in matcher:\n    ...     print(f\"{matched.yyyydoy}: {matched.pair_name}\")\n    ...     print(f\"  Canopy: {matched.canopy_data_dir}\")\n    ...     print(f\"  Reference: {matched.reference_data_dir}\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        base_dir: Path,\n        receivers: dict[str, dict[str, str]],\n        analysis_pairs: dict[str, dict[str, str]],\n    ) -&gt; None:\n        \"\"\"Initialize pair matcher with receiver configuration.\"\"\"\n        self.base_dir = Path(base_dir)\n        self.receivers = receivers\n        self.analysis_pairs = analysis_pairs\n\n        # Validate receivers have directory config\n        self.receiver_dirs = self._build_receiver_dir_mapping()\n\n    def _build_receiver_dir_mapping(self) -&gt; dict[str, str]:\n        \"\"\"Map receiver names to their directory prefixes.\n\n        Returns\n        -------\n        dict[str, str]\n            Mapping of receiver name to directory path.\n\n        Raises\n        ------\n        ValueError\n            If receiver missing 'directory' in config.\n\n        \"\"\"\n        mapping = {}\n        for receiver_name, config in self.receivers.items():\n            if \"directory\" not in config:\n                msg = f\"Receiver '{receiver_name}' missing 'directory' in config\"\n                raise ValueError(msg)\n            mapping[receiver_name] = config[\"directory\"]\n        return mapping\n\n    def _get_receiver_path(self, receiver_name: str, yyyydoy: YYYYDOY) -&gt; Path:\n        \"\"\"Build full path to receiver data for a specific date.\n\n        Parameters\n        ----------\n        receiver_name : str\n            Receiver name (e.g., \"canopy_01\").\n        yyyydoy : YYYYDOY\n            Date object.\n\n        Returns\n        -------\n        Path\n            Full path to receiver's RINEX directory for the date.\n\n        \"\"\"\n        receiver_dir = self.receiver_dirs[receiver_name]\n\n        # Convert YYYYDDD to YYDDD format for directory name\n        yyddd_str = yyyydoy.yydoy\n\n        return self.base_dir / receiver_dir / yyddd_str\n\n    def _get_all_dates(self) -&gt; set[YYYYDOY]:\n        \"\"\"Find all dates that have data in any receiver directory.\n\n        Returns\n        -------\n        set[YYYYDOY]\n            Set of all dates with available data.\n\n        \"\"\"\n        all_dates = set()\n\n        for receiver_name in self.receivers:\n            receiver_dir = self.receiver_dirs[receiver_name]\n            receiver_base = self.base_dir / receiver_dir\n\n            if not receiver_base.exists():\n                continue\n\n            # Find all date directories (format: YYDDD - 5 digits)\n            for date_dir in receiver_base.iterdir():\n                if not date_dir.is_dir():\n                    continue\n\n                # Check if directory name is 5 digits\n                if len(date_dir.name) != DATE_DIR_LEN or not date_dir.name.isdigit():\n                    continue\n\n                # Skip placeholder directories\n                if date_dir.name == \"00000\":\n                    continue\n\n                try:\n                    yyyydoy = YYYYDOY.from_yydoy_str(date_dir.name)\n                    all_dates.add(yyyydoy)\n                except ValueError:\n                    continue\n\n        return all_dates\n\n    def __iter__(self) -&gt; Iterator[PairMatchedDirs]:\n        \"\"\"Iterate over all date/pair combinations with available data.\n\n        Yields\n        ------\n        PairMatchedDirs\n            Matched directories for a receiver pair on a specific date.\n\n        \"\"\"\n        all_dates = sorted(self._get_all_dates())\n\n        for yyyydoy in all_dates:\n            # For each configured analysis pair\n            for pair_name, pair_config in self.analysis_pairs.items():\n                canopy_rx = pair_config[\"canopy_receiver\"]\n                reference_rx = pair_config[\"reference_receiver\"]\n\n                # Build paths for this pair\n                canopy_path = self._get_receiver_path(canopy_rx, yyyydoy)\n                reference_path = self._get_receiver_path(reference_rx, yyyydoy)\n\n                # Check for RINEX files\n                canopy_has_files = _has_rinex_files(canopy_path)\n                reference_has_files = _has_rinex_files(reference_path)\n\n                # Only yield if both directories exist and have data\n                if canopy_has_files and reference_has_files:\n                    yield PairMatchedDirs(\n                        yyyydoy=yyyydoy,\n                        pair_name=pair_name,\n                        canopy_receiver=canopy_rx,\n                        reference_receiver=reference_rx,\n                        canopy_data_dir=canopy_path,\n                        reference_data_dir=reference_path,\n                    )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.PairDataDirMatcher.__init__","level":3,"title":"<code>__init__(base_dir, receivers, analysis_pairs)</code>","text":"<p>Initialize pair matcher with receiver configuration.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>def __init__(\n    self,\n    base_dir: Path,\n    receivers: dict[str, dict[str, str]],\n    analysis_pairs: dict[str, dict[str, str]],\n) -&gt; None:\n    \"\"\"Initialize pair matcher with receiver configuration.\"\"\"\n    self.base_dir = Path(base_dir)\n    self.receivers = receivers\n    self.analysis_pairs = analysis_pairs\n\n    # Validate receivers have directory config\n    self.receiver_dirs = self._build_receiver_dir_mapping()\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.PairDataDirMatcher.__iter__","level":3,"title":"<code>__iter__()</code>","text":"<p>Iterate over all date/pair combinations with available data.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.PairDataDirMatcher.__iter__--yields","level":5,"title":"Yields","text":"<p>PairMatchedDirs     Matched directories for a receiver pair on a specific date.</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/dir_matcher.py</code> <pre><code>def __iter__(self) -&gt; Iterator[PairMatchedDirs]:\n    \"\"\"Iterate over all date/pair combinations with available data.\n\n    Yields\n    ------\n    PairMatchedDirs\n        Matched directories for a receiver pair on a specific date.\n\n    \"\"\"\n    all_dates = sorted(self._get_all_dates())\n\n    for yyyydoy in all_dates:\n        # For each configured analysis pair\n        for pair_name, pair_config in self.analysis_pairs.items():\n            canopy_rx = pair_config[\"canopy_receiver\"]\n            reference_rx = pair_config[\"reference_receiver\"]\n\n            # Build paths for this pair\n            canopy_path = self._get_receiver_path(canopy_rx, yyyydoy)\n            reference_path = self._get_receiver_path(reference_rx, yyyydoy)\n\n            # Check for RINEX files\n            canopy_has_files = _has_rinex_files(canopy_path)\n            reference_has_files = _has_rinex_files(reference_path)\n\n            # Only yield if both directories exist and have data\n            if canopy_has_files and reference_has_files:\n                yield PairMatchedDirs(\n                    yyyydoy=yyyydoy,\n                    pair_name=pair_name,\n                    canopy_receiver=canopy_rx,\n                    reference_receiver=reference_rx,\n                    canopy_data_dir=canopy_path,\n                    reference_data_dir=reference_path,\n                )\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.MatchedDirs","level":2,"title":"<code>MatchedDirs</code>  <code>dataclass</code>","text":"<p>Matched directory paths for canopy and reference receivers.</p> <p>Immutable container representing a pair of directories containing RINEX data for the same date.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.MatchedDirs--parameters","level":4,"title":"Parameters","text":"<p>canopy_data_dir : Path     Path to canopy receiver RINEX directory. reference_data_dir : Path     Path to reference (open-sky) receiver RINEX directory. yyyydoy : YYYYDOY     Date object for this matched pair.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.MatchedDirs--examples","level":4,"title":"Examples","text":"<p>from pathlib import Path from canvod.utils.tools import YYYYDOY</p> <p>md = MatchedDirs( ...     canopy_data_dir=Path(\"/data/02_canopy/25001\"), ...     reference_data_dir=Path(\"/data/01_reference/25001\"), ...     yyyydoy=YYYYDOY.from_str(\"2025001\") ... ) md.yyyydoy.to_str() '2025001'</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/models.py</code> <pre><code>@dataclass(frozen=True)\nclass MatchedDirs:\n    \"\"\"Matched directory paths for canopy and reference receivers.\n\n    Immutable container representing a pair of directories containing\n    RINEX data for the same date.\n\n    Parameters\n    ----------\n    canopy_data_dir : Path\n        Path to canopy receiver RINEX directory.\n    reference_data_dir : Path\n        Path to reference (open-sky) receiver RINEX directory.\n    yyyydoy : YYYYDOY\n        Date object for this matched pair.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; from canvod.utils.tools import YYYYDOY\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; md = MatchedDirs(\n    ...     canopy_data_dir=Path(\"/data/02_canopy/25001\"),\n    ...     reference_data_dir=Path(\"/data/01_reference/25001\"),\n    ...     yyyydoy=YYYYDOY.from_str(\"2025001\")\n    ... )\n    &gt;&gt;&gt; md.yyyydoy.to_str()\n    '2025001'\n\n    \"\"\"\n\n    canopy_data_dir: Path\n    reference_data_dir: Path\n    yyyydoy: YYYYDOY\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.PairMatchedDirs","level":2,"title":"<code>PairMatchedDirs</code>  <code>dataclass</code>","text":"<p>Matched directories for a receiver pair on a specific date.</p> <p>Supports multi-receiver configurations where multiple canopy/reference pairs may exist at the same site.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.PairMatchedDirs--parameters","level":4,"title":"Parameters","text":"<p>yyyydoy : YYYYDOY     Date for this matched pair. pair_name : str     Identifier for this receiver pair (e.g., \"pair_01\"). canopy_receiver : str     Name of canopy receiver (e.g., \"canopy_01\"). reference_receiver : str     Name of reference receiver (e.g., \"reference_01\"). canopy_data_dir : Path     Path to canopy receiver RINEX directory. reference_data_dir : Path     Path to reference receiver RINEX directory.</p>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-readers/#canvod.readers.matching.PairMatchedDirs--examples","level":4,"title":"Examples","text":"<p>pmd = PairMatchedDirs( ...     yyyydoy=YYYYDOY.from_str(\"2025001\"), ...     pair_name=\"pair_01\", ...     canopy_receiver=\"canopy_01\", ...     reference_receiver=\"reference_01\", ...     canopy_data_dir=Path(\"/data/canopy_01/25001\"), ...     reference_data_dir=Path(\"/data/reference_01/25001\") ... ) pmd.pair_name 'pair_01'</p> Source code in <code>packages/canvod-readers/src/canvod/readers/matching/models.py</code> <pre><code>@dataclass\nclass PairMatchedDirs:\n    \"\"\"Matched directories for a receiver pair on a specific date.\n\n    Supports multi-receiver configurations where multiple canopy/reference\n    pairs may exist at the same site.\n\n    Parameters\n    ----------\n    yyyydoy : YYYYDOY\n        Date for this matched pair.\n    pair_name : str\n        Identifier for this receiver pair (e.g., \"pair_01\").\n    canopy_receiver : str\n        Name of canopy receiver (e.g., \"canopy_01\").\n    reference_receiver : str\n        Name of reference receiver (e.g., \"reference_01\").\n    canopy_data_dir : Path\n        Path to canopy receiver RINEX directory.\n    reference_data_dir : Path\n        Path to reference receiver RINEX directory.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pmd = PairMatchedDirs(\n    ...     yyyydoy=YYYYDOY.from_str(\"2025001\"),\n    ...     pair_name=\"pair_01\",\n    ...     canopy_receiver=\"canopy_01\",\n    ...     reference_receiver=\"reference_01\",\n    ...     canopy_data_dir=Path(\"/data/canopy_01/25001\"),\n    ...     reference_data_dir=Path(\"/data/reference_01/25001\")\n    ... )\n    &gt;&gt;&gt; pmd.pair_name\n    'pair_01'\n\n    \"\"\"\n\n    yyyydoy: YYYYDOY\n    pair_name: str\n    canopy_receiver: str\n    reference_receiver: str\n    canopy_data_dir: Path\n    reference_data_dir: Path\n</code></pre>","path":["API Reference","canvod.readers API Reference"],"tags":[]},{"location":"api/canvod-store/","level":1,"title":"canvod.store API Reference","text":"<p>Versioned data storage using Icechunk.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#package","level":2,"title":"Package","text":"<p>canvod-store: Icechunk storage for GNSS VOD data.</p> <p>This package provides versioned storage for GNSS VOD data using Icechunk. Manages both RINEX observation storage and VOD analysis results.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore","level":2,"title":"<code>MyIcechunkStore</code>","text":"<p>Core Icechunk store manager for GNSS data.</p> <p>This class encapsulates all operations on a single Icechunk repository, providing a clean interface for GNSS data storage and retrieval with integrated logging and proper resource management.</p> <p>Features: - Automatic repository creation/connection - Group management with validation - Session management with context managers - Integrated logging with file contexts - Configurable compression and chunking</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore--parameters","level":4,"title":"Parameters","text":"<p>store_path : Path     Path to the Icechunk store directory. store_type : str, default \"rinex_store\"     Type of store (\"rinex_store\" or \"vod_store\"). compression_level : int | None, optional     Override default compression level. compression_algorithm : str | None, optional     Override default compression algorithm.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore--attributes","level":4,"title":"Attributes","text":"<p>store_path : Path     Path to the Icechunk store directory. store_type : str     Type of store (\"rinex_store\" or \"vod_store\"). compression_level : int     Compression level (1-9). compression_algorithm : icechunk.CompressionAlgorithm     Compression algorithm enum. repo : icechunk.Repository     The Icechunk repository instance.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>@add_rich_display_to_store\nclass MyIcechunkStore:\n    \"\"\"\n    Core Icechunk store manager for GNSS data.\n\n    This class encapsulates all operations on a single Icechunk repository,\n    providing a clean interface for GNSS data storage and retrieval with\n    integrated logging and proper resource management.\n\n    Features:\n    - Automatic repository creation/connection\n    - Group management with validation\n    - Session management with context managers\n    - Integrated logging with file contexts\n    - Configurable compression and chunking\n\n    Parameters\n    ----------\n    store_path : Path\n        Path to the Icechunk store directory.\n    store_type : str, default \"rinex_store\"\n        Type of store (\"rinex_store\" or \"vod_store\").\n    compression_level : int | None, optional\n        Override default compression level.\n    compression_algorithm : str | None, optional\n        Override default compression algorithm.\n\n    Attributes\n    ----------\n    store_path : Path\n        Path to the Icechunk store directory.\n    store_type : str\n        Type of store (\"rinex_store\" or \"vod_store\").\n    compression_level : int\n        Compression level (1-9).\n    compression_algorithm : icechunk.CompressionAlgorithm\n        Compression algorithm enum.\n    repo : icechunk.Repository\n        The Icechunk repository instance.\n    \"\"\"\n\n    def __init__(\n        self,\n        store_path: Path,\n        store_type: str = \"rinex_store\",\n        compression_level: int | None = None,\n        compression_algorithm: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Icechunk store manager.\n\n        Parameters\n        ----------\n        store_path : Path\n            Path to the Icechunk store directory.\n        store_type : str, default \"rinex_store\"\n            Type of store (\"rinex_store\" or \"vod_store\").\n        compression_level : int | None, optional\n            Override default compression level.\n        compression_algorithm : str | None, optional\n            Override default compression algorithm.\n        \"\"\"\n        from canvod.utils.config import load_config\n\n        cfg = load_config()\n        ic_cfg = cfg.processing.icechunk\n        st_cfg = cfg.processing.storage\n\n        self.store_path = Path(store_path)\n        self.store_type = store_type\n        # Site name is parent directory name\n        self.site_name = self.store_path.parent.name\n\n        # Compression\n        self.compression_level = compression_level or ic_cfg.compression_level\n        compression_alg = compression_algorithm or ic_cfg.compression_algorithm\n        self.compression_algorithm = getattr(\n            icechunk.CompressionAlgorithm, compression_alg.capitalize()\n        )\n\n        # Chunk strategy\n        chunk_strategies = {\n            k: {\"epoch\": v.epoch, \"sid\": v.sid}\n            for k, v in ic_cfg.chunk_strategies.items()\n        }\n        self.chunk_strategy = chunk_strategies.get(store_type, {})\n\n        # Storage config cached for metadata rows\n        self._rinex_store_strategy = st_cfg.rinex_store_strategy\n        self._rinex_store_expire_days = st_cfg.rinex_store_expire_days\n        self._vod_store_strategy = st_cfg.vod_store_strategy\n\n        # Configure repository\n        self.config = icechunk.RepositoryConfig.default()\n        self.config.compression = icechunk.CompressionConfig(\n            level=self.compression_level, algorithm=self.compression_algorithm\n        )\n        self.config.inline_chunk_threshold_bytes = ic_cfg.inline_threshold\n        self.config.get_partial_values_concurrency = ic_cfg.get_concurrency\n\n        if ic_cfg.manifest_preload_enabled:\n            self.config.manifest = icechunk.ManifestConfig(\n                preload=icechunk.ManifestPreloadConfig(\n                    max_total_refs=ic_cfg.manifest_preload_max_refs,\n                    preload_if=icechunk.ManifestPreloadCondition.name_matches(\n                        ic_cfg.manifest_preload_pattern\n                    ),\n                )\n            )\n            self._logger.info(\n                f\"Manifest preload enabled: {ic_cfg.manifest_preload_pattern}\"\n            )\n\n        self._repo = None\n        self._logger = get_logger(__name__)\n\n        # Remove .DS_Store files that corrupt icechunk ref listing on macOS\n        self._clean_ds_store()\n        self._ensure_store_exists()\n\n    def _clean_ds_store(self) -&gt; None:\n        \"\"\"Remove .DS_Store files from the store directory tree.\n\n        macOS creates these files automatically and they corrupt icechunk's\n        ref listing, causing 'invalid ref type `.DS_Store`' errors.\n        \"\"\"\n        if not self.store_path.exists():\n            return\n        for ds_store in self.store_path.rglob(\".DS_Store\"):\n            ds_store.unlink()\n            self._logger.debug(f\"Removed {ds_store}\")\n\n    def _normalize_encodings(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"Normalize dataset encodings for Icechunk.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset to normalize.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with normalized encodings.\n        \"\"\"\n        for v in ds.data_vars:\n            if \"dtype\" in ds[v].encoding:\n                ds[v].encoding[\"dtype\"] = np.dtype(ds[v].dtype)\n        return ds\n\n    def _ensure_store_exists(self) -&gt; None:\n        \"\"\"Ensure the store exists, creating if necessary.\"\"\"\n        storage = icechunk.local_filesystem_storage(str(self.store_path))\n\n        if self.store_path.exists() and any(self.store_path.iterdir()):\n            self._logger.info(f\"Opening existing Icechunk store at {self.store_path}\")\n            self._repo = icechunk.Repository.open(storage=storage, config=self.config)\n        else:\n            self._logger.info(f\"Creating new Icechunk store at {self.store_path}\")\n            self.store_path.mkdir(parents=True, exist_ok=True)\n            self._repo = icechunk.Repository.create(storage=storage, config=self.config)\n\n    @property\n    def repo(self) -&gt; icechunk.Repository:\n        \"\"\"Get the repository instance.\"\"\"\n        if self._repo is None:\n            self._ensure_store_exists()\n        return self._repo\n\n    @contextlib.contextmanager\n    def readonly_session(\n        self,\n        branch: str = \"main\",\n    ) -&gt; Generator[\"icechunk.ReadonlySession\"]:\n        \"\"\"Context manager for readonly sessions.\n\n        Parameters\n        ----------\n        branch : str, default \"main\"\n            Branch name.\n\n        Returns\n        -------\n        Generator[icechunk.ReadonlySession, None, None]\n            Readonly session context manager.\n        \"\"\"\n        session = self.repo.readonly_session(branch)\n        try:\n            self._logger.debug(f\"Opened readonly session for branch '{branch}'\")\n            yield session\n        finally:\n            self._logger.debug(f\"Closed readonly session for branch '{branch}'\")\n\n    @contextlib.contextmanager\n    def writable_session(\n        self,\n        branch: str = \"main\",\n    ) -&gt; Generator[\"icechunk.WritableSession\"]:\n        \"\"\"Context manager for writable sessions.\n\n        Parameters\n        ----------\n        branch : str, default \"main\"\n            Branch name.\n\n        Returns\n        -------\n        Generator[icechunk.WritableSession, None, None]\n            Writable session context manager.\n        \"\"\"\n        session = self.repo.writable_session(branch)\n        try:\n            self._logger.debug(f\"Opened writable session for branch '{branch}'\")\n            yield session\n        finally:\n            self._logger.debug(f\"Closed writable session for branch '{branch}'\")\n\n    def get_branch_names(self) -&gt; list[str]:\n        \"\"\"\n        List all branches in the store.\n\n        Returns\n        -------\n        list[str]\n            List of branch names.\n        \"\"\"\n        try:\n            storage_config = icechunk.local_filesystem_storage(self.store_path)\n            repo = icechunk.Repository.open(\n                storage=storage_config,\n            )\n\n            return list(repo.list_branches())\n        except Exception as e:\n            self._logger.warning(f\"Failed to list branches in {repr(self)}: {e}\")\n            warnings.warn(f\"Failed to list branches in {repr(self)}: {e}\")\n            return []\n\n    def get_group_names(self, branch: str | None = None) -&gt; dict[str, list[str]]:\n        \"\"\"\n        List all groups in the store.\n\n        Parameters\n        ----------\n        branch: Optional[str]\n            Repository branch to examine. Defaults to listing groups from all branches.\n\n        Returns\n        -------\n        dict[str, list[str]]\n            Dictionary mapping branch names to lists of group names.\n\n        \"\"\"\n        try:\n            if not branch:\n                branches = self.get_branch_names()\n            else:\n                branches = [branch]\n\n            storage_config = icechunk.local_filesystem_storage(self.store_path)\n            repo = icechunk.Repository.open(\n                storage=storage_config,\n            )\n\n            group_dict = {}\n            for br in branches:\n                with self.readonly_session(br) as session:\n                    session = repo.readonly_session(br)\n                    root = zarr.open(session.store, mode=\"r\")\n                    group_dict[br] = list(root.group_keys())\n\n            return group_dict\n\n        except Exception as e:\n            self._logger.warning(f\"Failed to list groups in {repr(self)}: {e}\")\n            return {}\n\n    def list_groups(self, branch: str = \"main\") -&gt; list[str]:\n        \"\"\"\n        List all groups in a branch.\n\n        Parameters\n        ----------\n        branch : str\n            Branch name (default: \"main\")\n\n        Returns\n        -------\n        list[str]\n            List of group names in the branch\n        \"\"\"\n        group_dict = self.get_group_names(branch=branch)\n        if branch in group_dict:\n            return group_dict[branch]\n        return []\n\n    @property\n    def tree(self) -&gt; None:\n        \"\"\"\n        Display hierarchical tree of all branches, groups, and subgroups.\n        \"\"\"\n        self.print_tree(max_depth=None)\n\n    def print_tree(self, max_depth: int | None = None) -&gt; None:\n        \"\"\"\n        Display hierarchical tree of all branches, groups, and subgroups.\n\n        Parameters\n        ----------\n        max_depth : int | None\n            Maximum depth to display. None for unlimited depth.\n            - 0: Only show branches\n            - 1: Show branches and top-level groups\n            - 2: Show branches, groups, and first level of subgroups/arrays\n            - etc.\n        \"\"\"\n        try:\n            branches = self.get_branch_names()\n\n            for i, branch in enumerate(branches):\n                is_last_branch = i == len(branches) - 1\n                branch_prefix = \"└── \" if is_last_branch else \"├── \"\n\n                if max_depth is not None and max_depth &lt; 1:\n                    continue\n\n                session = self.repo.readonly_session(branch)\n                root = zarr.open(session.store, mode=\"r\")\n\n                if i == 0:\n                    sys.stdout.write(f\"{self.store_path}\\n\")\n\n                sys.stdout.write(f\"{branch_prefix}{branch}\\n\")\n                # Build tree recursively\n                branch_indent = \"    \" if is_last_branch else \"│   \"\n                self._build_tree(root, branch_indent, max_depth, current_depth=1)\n\n        except Exception as e:\n            self._logger.warning(f\"Failed to generate tree for {repr(self)}: {e}\")\n            sys.stdout.write(f\"Error generating tree: {e}\\n\")\n\n    def _build_tree(\n        self,\n        group: zarr.Group,\n        prefix: str,\n        max_depth: int | None,\n        current_depth: int = 0,\n    ) -&gt; None:\n        \"\"\"Recursively build a tree structure.\n\n        Parameters\n        ----------\n        group : zarr.Group\n            Root group to traverse.\n        prefix : str\n            Prefix string for tree formatting.\n        max_depth : int | None\n            Maximum depth to display. None for unlimited.\n        current_depth : int, default 0\n            Current recursion depth.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if max_depth is not None and current_depth &gt;= max_depth:\n            return\n\n        # Get all groups and arrays\n        groups = list(group.group_keys())\n        arrays = list(group.array_keys())\n        items = groups + arrays\n\n        for i, item_name in enumerate(items):\n            is_last = i == len(items) - 1\n            connector = \"└── \" if is_last else \"├── \"\n\n            if item_name in groups:\n                # It's a group\n                sys.stdout.write(f\"{prefix}{connector}{item_name}\\n\")\n\n                # Recurse into subgroup\n                subgroup = group[item_name]\n                new_prefix = prefix + (\"    \" if is_last else \"│   \")\n                self._build_tree(subgroup, new_prefix, max_depth, current_depth + 1)\n            else:\n                # It's an array\n                arr = group[item_name]\n                shape_str = str(arr.shape)\n                dtype_str = str(arr.dtype)\n                sys.stdout.write(\n                    f\"{prefix}{connector}{item_name} {shape_str} {dtype_str}\\n\"\n                )\n\n    def group_exists(self, group_name: str, branch: str = \"main\") -&gt; bool:\n        \"\"\"\n        Check if a group exists.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group to check.\n        branch : str, default \"main\"\n            Repository branch to examine.\n\n        Returns\n        -------\n        bool\n            True if the group exists, False otherwise.\n        \"\"\"\n        group_dict = self.get_group_names(branch)\n\n        # get_group_names returns dict like {'main': ['canopy_01', ...]}\n        if branch in group_dict:\n            exists = group_name in group_dict[branch]\n        else:\n            exists = False\n\n        self._logger.debug(\n            f\"Group '{group_name}' exists on branch '{branch}': {exists}\"\n        )\n        return exists\n\n    def read_group(\n        self,\n        group_name: str,\n        branch: str = \"main\",\n        time_slice: slice | None = None,\n        chunks: dict[str, Any] | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Read data from a group.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group to read.\n        branch : str, default \"main\"\n            Repository branch.\n        time_slice : slice | None, optional\n            Optional time slice for filtering.\n        chunks : dict[str, Any] | None, optional\n            Chunking specification (uses config defaults if None).\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset from the group.\n        \"\"\"\n        self._logger.info(f\"Reading group '{group_name}' from branch '{branch}'\")\n\n        with self.readonly_session(branch) as session:\n            # Use default chunking strategy if none provided\n            if chunks is None:\n                chunks = self.chunk_strategy or {\"epoch\": 34560, \"sid\": -1}\n\n            ds = xr.open_zarr(\n                session.store,\n                group=group_name,\n                chunks=chunks,\n                consolidated=False,\n            )\n\n            if time_slice is not None:\n                ds = ds.isel(epoch=time_slice)\n                self._logger.debug(f\"Applied time slice: {time_slice}\")\n\n            self._logger.info(\n                f\"Successfully read group '{group_name}' - shape: {dict(ds.sizes)}\"\n            )\n            return ds\n\n    def read_group_deduplicated(\n        self,\n        group_name: str,\n        branch: str = \"main\",\n        keep: str = \"last\",\n        time_slice: slice | None = None,\n        chunks: dict[str, Any] | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Read data from a group with automatic deduplication.\n\n        This method calls read_group() then removes duplicates using metadata table\n        intelligence when available, falling back to simple epoch deduplication.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group to read.\n        branch : str, default \"main\"\n            Repository branch.\n        keep : str, default \"last\"\n            Deduplication strategy for duplicate epochs.\n        time_slice : slice | None, optional\n            Optional time slice for filtering.\n        chunks : dict[str, Any] | None, optional\n            Chunking specification (uses config defaults if None).\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with duplicates removed (latest data only).\n        \"\"\"\n\n        if keep not in [\"last\"]:\n            raise ValueError(\"Currently only 'last' is supported for keep parameter.\")\n\n        self._logger.info(f\"Reading group '{group_name}' with deduplication\")\n\n        # First, read the raw data\n        ds = self.read_group(group_name, branch, time_slice, chunks)\n\n        # Then deduplicate using metadata table intelligence\n        with self.readonly_session(branch) as session:\n            try:\n                zmeta = zarr.open_group(session.store, mode=\"r\")[\n                    f\"{group_name}/metadata/table\"\n                ]\n\n                # Load metadata and get latest entries for each time range\n                data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n                df = pl.DataFrame(data)\n\n                # Ensure datetime dtypes\n                df = df.with_columns(\n                    [\n                        pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n                        pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n                    ]\n                )\n\n                # Get latest entry for each unique (start, end) combination\n                latest_entries = df.sort(\"written_at\").unique(\n                    subset=[\"start\", \"end\"], keep=keep\n                )\n\n                if latest_entries.height &gt; 0:\n                    # Create time masks for latest data only\n                    time_masks = []\n                    for row in latest_entries.iter_rows(named=True):\n                        start_time = row[\"start\"]\n                        end_time = row[\"end\"]\n                        mask = (ds.epoch &gt;= start_time) &amp; (ds.epoch &lt;= end_time)\n                        time_masks.append(mask)\n\n                    # Combine all masks with OR logic\n                    if time_masks:\n                        combined_mask = time_masks[0]\n                        for mask in time_masks[1:]:\n                            combined_mask = combined_mask | mask\n                        ds = ds.isel(epoch=combined_mask)\n\n                        self._logger.info(\n                            \"Deduplicated using metadata table: kept \"\n                            f\"{len(latest_entries)} time ranges\"\n                        )\n\n            except Exception as e:\n                # Fall back to simple deduplication\n                self._logger.warning(\n                    f\"Metadata-based deduplication failed, using simple approach: {e}\"\n                )\n                ds = ds.drop_duplicates(\"epoch\", keep=\"last\")\n                self._logger.info(\"Applied simple epoch deduplication (keep='last')\")\n\n        return ds\n\n    def _cleanse_dataset_attrs(self, dataset: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"Remove any attributes that might interfere with Icechunk storage.\"\"\"\n\n        attrs_to_remove = [\n            \"Created\",\n            \"File Path\",\n            \"File Type\",\n            \"Date\",\n            \"institution\",\n            \"Time of First Observation\",\n            \"GLONASS COD\",\n            \"GLONASS PHS\",\n            \"GLONASS BIS\",\n            \"Leap Seconds\",\n            # \"RINEX File Hash\",\n        ]\n        for attr in attrs_to_remove:\n            if attr in dataset.attrs:\n                del dataset.attrs[attr]\n        return dataset\n\n    def write_dataset(\n        self,\n        dataset: xr.Dataset,\n        group_name: str,\n        session: Any,\n        mode: str = \"a\",\n        chunks: dict[str, int] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Write a dataset to Icechunk with proper chunking.\n\n        Parameters\n        ----------\n        dataset : xr.Dataset\n            Dataset to write\n        group_name : str\n            Group path in store\n        session : Any\n            Active writable session or store handle.\n        mode : str\n            Write mode: 'w' (overwrite) or 'a' (append)\n        chunks : dict[str, int] | None\n            Chunking spec. If None, uses store's chunk_strategy.\n            Example: {'epoch': 34560, 'sid': -1}\n        \"\"\"\n        # Use explicit chunks, or fall back to store's chunk strategy\n        if chunks is None:\n            chunks = self.chunk_strategy\n\n        # Apply chunking if strategy defined\n        if chunks:\n            dataset = dataset.chunk(chunks)\n            self._logger.info(f\"Rechunked to {dict(dataset.chunks)} before write\")\n\n        # Normalize encodings\n        dataset = self._normalize_encodings(dataset)\n\n        # Calculate dataset metrics for tracing\n        dataset_size_mb = dataset.nbytes / 1024 / 1024\n        num_variables = len(dataset.data_vars)\n\n        # Write to Icechunk with OpenTelemetry tracing\n        try:\n            from canvodpy.utils.telemetry import trace_icechunk_write\n\n            with trace_icechunk_write(\n                group_name=group_name,\n                dataset_size_mb=dataset_size_mb,\n                num_variables=num_variables,\n            ):\n                to_icechunk(dataset, session, group=group_name, mode=mode)\n        except ImportError:\n            # Fallback if telemetry not available\n            to_icechunk(dataset, session, group=group_name, mode=mode)\n\n        self._logger.info(f\"Wrote dataset to group '{group_name}' (mode={mode})\")\n\n    def write_initial_group(\n        self,\n        dataset: xr.Dataset,\n        group_name: str,\n        branch: str = \"main\",\n        commit_message: str | None = None,\n    ) -&gt; None:\n        \"\"\"Write initial data to a new group.\"\"\"\n        if self.group_exists(group_name, branch):\n            raise ValueError(\n                f\"Group '{group_name}' already exists. Use append_to_group() instead.\"\n            )\n\n        with self.writable_session(branch) as session:\n            dataset = self._normalize_encodings(dataset)\n\n            rinex_hash = dataset.attrs.get(\"RINEX File Hash\")\n            if rinex_hash is None:\n                raise ValueError(\"Dataset missing 'RINEX File Hash' attribute\")\n            start = dataset.epoch.min().values\n            end = dataset.epoch.max().values\n\n            to_icechunk(dataset, session, group=group_name, mode=\"w\")\n\n            if commit_message is None:\n                version = get_version_from_pyproject()\n                commit_message = f\"[v{version}] Initial commit to group '{group_name}'\"\n\n            snapshot_id = session.commit(commit_message)\n\n            self.append_metadata(\n                group_name=group_name,\n                rinex_hash=rinex_hash,\n                start=start,\n                end=end,\n                snapshot_id=snapshot_id,\n                action=\"write\",  # Correct action for initial data\n                commit_msg=commit_message,\n                dataset_attrs=dataset.attrs,\n            )\n\n        self._logger.info(\n            f\"Created group '{group_name}' with {len(dataset.epoch)} epochs, \"\n            f\"hash={rinex_hash}\"\n        )\n\n    def backup_metadata_table(\n        self,\n        group_name: str,\n        session: Any,\n    ) -&gt; pl.DataFrame | None:\n        \"\"\"Backup the metadata table to a Polars DataFrame.\n\n        Parameters\n        ----------\n        group_name : str\n            Group name.\n        session : Any\n            Active session for reading.\n\n        Returns\n        -------\n        pl.DataFrame | None\n            DataFrame with metadata rows, or None if missing.\n        \"\"\"\n        try:\n            zroot = zarr.open_group(session.store, mode=\"r\")\n            meta_group_path = f\"{group_name}/metadata/table\"\n\n            if (\n                \"metadata\" not in zroot[group_name]\n                or \"table\" not in zroot[group_name][\"metadata\"]\n            ):\n                self._logger.info(\n                    \"No metadata table found for group \"\n                    f\"'{group_name}' - nothing to backup\"\n                )\n                return None\n\n            zmeta = zroot[meta_group_path]\n\n            # Load all columns into a dictionary\n            data = {}\n            for col_name in zmeta.array_keys():\n                data[col_name] = zmeta[col_name][:]\n\n            # Convert to Polars DataFrame\n            df = pl.DataFrame(data)\n\n            self._logger.info(\n                \"Backed up metadata table with \"\n                f\"{df.height} rows for group '{group_name}'\"\n            )\n            return df\n\n        except Exception as e:\n            self._logger.warning(\n                f\"Failed to backup metadata table for group '{group_name}': {e}\"\n            )\n            return None\n\n    def restore_metadata_table(\n        self,\n        group_name: str,\n        df: pl.DataFrame,\n        session: Any,\n    ) -&gt; None:\n        \"\"\"Restore the metadata table from a Polars DataFrame.\n\n        This recreates the full Zarr structure for the metadata table.\n\n        Parameters\n        ----------\n        group_name : str\n            Group name.\n        df : pl.DataFrame\n            Metadata table to restore.\n        session : Any\n            Active session for writing.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if df is None or df.height == 0:\n            self._logger.info(f\"No metadata to restore for group '{group_name}'\")\n            return\n\n        try:\n            zroot = zarr.open_group(session.store, mode=\"a\")\n            meta_group_path = f\"{group_name}/metadata/table\"\n\n            # Create the metadata subgroup\n            zmeta = zroot.require_group(meta_group_path)\n\n            # Create all arrays from the DataFrame\n            for col_name in df.columns:\n                col_data = df[col_name]\n\n                if col_name == \"index\":\n                    # Index column as int64\n                    arr = col_data.to_numpy().astype(\"i8\")\n                    dtype = \"i8\"\n                elif col_name in (\"start\", \"end\"):\n                    # Datetime columns\n                    arr = col_data.to_numpy().astype(\"datetime64[ns]\")\n                    dtype = \"M8[ns]\"\n                else:\n                    # String columns - use VariableLengthUTF8\n                    arr = col_data.to_list()  # Convert to list for VariableLengthUTF8\n                    dtype = VariableLengthUTF8()\n\n                # Create the array\n                zmeta.create_array(\n                    name=col_name,\n                    shape=(len(arr),),\n                    dtype=dtype,\n                    chunks=(1024,),\n                    overwrite=True,\n                )\n\n                # Write the data\n                zmeta[col_name][:] = arr\n\n            self._logger.info(\n                \"Restored metadata table with \"\n                f\"{df.height} rows for group '{group_name}'\"\n            )\n\n        except Exception as e:\n            self._logger.error(\n                f\"Failed to restore metadata table for group '{group_name}': {e}\"\n            )\n            raise RuntimeError(f\"Critical error: could not restore metadata table: {e}\")\n\n    def overwrite_file_in_group(\n        self,\n        dataset: xr.Dataset,\n        group_name: str,\n        rinex_hash: str,\n        start: np.datetime64,\n        end: np.datetime64,\n        branch: str = \"main\",\n        commit_message: str | None = None,\n    ) -&gt; None:\n        \"\"\"Overwrite a file's contribution to the group (same hash, new epoch range).\"\"\"\n\n        dataset = self._normalize_encodings(dataset)\n\n        # --- Step 3: rewrite store ---\n        with self.writable_session(branch) as session:\n            ds_from_store = xr.open_zarr(\n                session.store, group=group_name, consolidated=False\n            )\n\n            # Backup the existing metadata table\n            metadata_backup = self.backup_metadata_table(group_name, session)\n\n            mask = (ds_from_store.epoch.values &lt; start) | (\n                ds_from_store.epoch.values &gt; end\n            )\n            ds_from_store_cleansed = ds_from_store.isel(epoch=mask)\n            ds_from_store_cleansed = self._normalize_encodings(ds_from_store_cleansed)\n\n            # Check if any epochs remain after cleansing, then write leftovers.\n            if ds_from_store_cleansed.sizes.get(\"epoch\", 0) &gt; 0:\n                to_icechunk(ds_from_store_cleansed, session, group=group_name, mode=\"w\")\n            # no epochs left, reset group to empty\n            else:\n                to_icechunk(dataset.isel(epoch=[]), session, group=group_name, mode=\"w\")\n\n            # write back the backed up metadata table\n            self.restore_metadata_table(group_name, metadata_backup, session)\n\n            # Append the new dataset\n            to_icechunk(dataset, session, group=group_name, append_dim=\"epoch\")\n\n            if commit_message is None:\n                version = get_version_from_pyproject()\n                commit_message = (\n                    f\"[v{version}] Overwrote file {rinex_hash} in group '{group_name}'\"\n                )\n\n            snapshot_id = session.commit(commit_message)\n\n            self.append_metadata(\n                group_name=group_name,\n                rinex_hash=rinex_hash,\n                start=start,\n                end=end,\n                snapshot_id=snapshot_id,\n                action=\"overwrite\",\n                commit_msg=commit_message,\n                dataset_attrs=dataset.attrs,\n            )\n\n    def get_group_info(self, group_name: str, branch: str = \"main\") -&gt; dict[str, Any]:\n        \"\"\"\n        Get metadata about a group.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group.\n        branch : str, default \"main\"\n            Repository branch to examine.\n\n        Returns\n        -------\n        dict[str, Any]\n            Group metadata.\n\n        Raises\n        ------\n        ValueError\n            If the group does not exist.\n        \"\"\"\n        if not self.group_exists(group_name, branch):\n            raise ValueError(f\"Group '{group_name}' does not exist\")\n\n        ds = self.read_group(group_name, branch)\n\n        info = {\n            \"group_name\": group_name,\n            \"store_type\": self.store_type,\n            \"dimensions\": dict(ds.sizes),\n            \"variables\": list(ds.data_vars.keys()),\n            \"coordinates\": list(ds.coords.keys()),\n            \"attributes\": dict(ds.attrs),\n        }\n\n        # Add temporal information if epoch dimension exists\n        if \"epoch\" in ds.sizes:\n            info[\"temporal_info\"] = {\n                \"start\": str(ds.epoch.min().values),\n                \"end\": str(ds.epoch.max().values),\n                \"count\": ds.sizes[\"epoch\"],\n                \"resolution\": str(ds.epoch.diff(\"epoch\").median().values),\n            }\n\n        return info\n\n    def rel_path_for_commit(self, file_path: Path) -&gt; str:\n        \"\"\"\n        Generate relative path for commit messages.\n\n        Parameters\n        ----------\n        file_path : Path\n            Full file path.\n\n        Returns\n        -------\n        str\n            Relative path string with log_path_depth parts.\n        \"\"\"\n        depth = load_config().processing.logging.log_path_depth\n        return str(Path(*file_path.parts[-depth:]))\n\n    def get_store_stats(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get statistics about the store.\n\n        Returns\n        -------\n        dict[str, Any]\n            Store statistics.\n        \"\"\"\n        groups = self.get_group_names()\n        stats = {\n            \"store_path\": str(self.store_path),\n            \"store_type\": self.store_type,\n            \"compression_level\": self.compression_level,\n            \"compression_algorithm\": self.compression_algorithm.name,\n            \"total_groups\": len(groups),\n            \"groups\": groups,\n        }\n\n        # Add group-specific stats\n        for group_name in groups:\n            try:\n                info = self.get_group_info(group_name)\n                stats[f\"group_{group_name}\"] = {\n                    \"dimensions\": info[\"dimensions\"],\n                    \"variables_count\": len(info[\"variables\"]),\n                    \"has_temporal_data\": \"temporal_info\" in info,\n                }\n            except Exception as e:\n                self._logger.warning(\n                    f\"Failed to get stats for group '{group_name}': {e}\"\n                )\n\n        return stats\n\n    def append_to_group(\n        self,\n        dataset: xr.Dataset,\n        group_name: str,\n        append_dim: str = \"epoch\",\n        branch: str = \"main\",\n        action: str = \"write\",\n        commit_message: str | None = None,\n    ) -&gt; None:\n        \"\"\"Append data to an existing group.\"\"\"\n        if not self.group_exists(group_name, branch):\n            raise ValueError(\n                f\"Group '{group_name}' does not exist. Use write_initial_group() first.\"\n            )\n\n        dataset = self._normalize_encodings(dataset)\n\n        rinex_hash = dataset.attrs.get(\"RINEX File Hash\")\n        if rinex_hash is None:\n            raise ValueError(\"Dataset missing 'RINEX File Hash' attribute\")\n        start = dataset.epoch.min().values\n        end = dataset.epoch.max().values\n\n        with self.writable_session(branch) as session:\n            to_icechunk(dataset, session, group=group_name, append_dim=append_dim)\n\n            if commit_message is None and action == \"write\":\n                version = get_version_from_pyproject()\n                commit_message = f\"[v{version}] Wrote to group '{group_name}'\"\n            elif commit_message is None and action != \"append\":\n                version = get_version_from_pyproject()\n                commit_message = f\"[v{version}] Appended to group '{group_name}'\"\n\n            snapshot_id = session.commit(commit_message)\n\n            self.append_metadata(\n                group_name=group_name,\n                rinex_hash=rinex_hash,\n                start=start,\n                end=end,\n                snapshot_id=snapshot_id,\n                action=action,\n                commit_msg=commit_message,\n                dataset_attrs=dataset.attrs,\n            )\n\n        if action == \"append\":\n            self._logger.info(\n                f\"Appended {len(dataset.epoch)} epochs to group '{group_name}', \"\n                f\"hash={rinex_hash}\"\n            )\n        elif action == \"write\":\n            self._logger.info(\n                f\"Wrote {len(dataset.epoch)} epochs to group '{group_name}', \"\n                f\"hash={rinex_hash}\"\n            )\n        else:\n            self._logger.info(\n                f\"Action '{action}' completed for group '{group_name}', \"\n                f\"hash={rinex_hash}\"\n            )\n\n    def append_metadata(\n        self,\n        group_name: str,\n        rinex_hash: str,\n        start: np.datetime64,\n        end: np.datetime64,\n        snapshot_id: str,\n        action: str,\n        commit_msg: str,\n        dataset_attrs: dict,\n        branch: str = \"main\",\n    ) -&gt; None:\n        \"\"\"\n        Append a metadata row into the group_name/metadata/table.\n\n        Schema:\n            index           int64 (continuous row id)\n            rinex_hash      str   (UTF-8, VariableLengthUTF8)\n            start           datetime64[ns]\n            end             datetime64[ns]\n            snapshot_id     str   (UTF-8)\n            action          str   (UTF-8, e.g. \"insert\"|\"append\"|\"overwrite\"|\"skip\")\n            commit_msg      str   (UTF-8)\n            written_at      str   (UTF-8, ISO8601 with timezone)\n            write_strategy  str   (UTF-8, RINEX_STORE_STRATEGY or VOD_STORE_STRATEGY)\n            attrs           str   (UTF-8, JSON dump of dataset attrs)\n        \"\"\"\n        written_at = datetime.now().astimezone().isoformat()\n\n        row = {\n            \"rinex_hash\": str(rinex_hash),\n            \"start\": np.datetime64(start, \"ns\"),\n            \"end\": np.datetime64(end, \"ns\"),\n            \"snapshot_id\": str(snapshot_id),\n            \"action\": str(action),\n            \"commit_msg\": str(commit_msg),\n            \"written_at\": written_at,\n            \"write_strategy\": str(self._rinex_store_strategy)\n            if self.store_type == \"rinex_store\"\n            else str(self._vod_store_strategy),\n            \"attrs\": json.dumps(dataset_attrs, default=str),\n        }\n        df_row = pl.DataFrame([row])\n\n        with self.writable_session(branch) as session:\n            zroot = zarr.open_group(session.store, mode=\"a\")\n            meta_group_path = f\"{group_name}/metadata/table\"\n\n            if (\n                \"metadata\" not in zroot[group_name]\n                or \"table\" not in zroot[group_name][\"metadata\"]\n            ):\n                # --- First time: create arrays with correct dtypes ---\n                zmeta = zroot.require_group(meta_group_path)\n\n                # index counter\n                zmeta.create_array(\n                    name=\"index\", shape=(0,), dtype=\"i8\", chunks=(1024,), overwrite=True\n                )\n                zmeta[\"index\"].append([0])\n\n                for col in df_row.columns:\n                    if col in (\"start\", \"end\"):\n                        dtype = \"M8[ns]\"\n                        arr = np.array(df_row[col].to_numpy(), dtype=dtype)\n                    else:\n                        dtype = VariableLengthUTF8()\n                        arr = df_row[col].to_list()\n\n                    zmeta.create_array(\n                        name=col,\n                        shape=(0,),\n                        dtype=dtype,\n                        chunks=(1024,),\n                        overwrite=True,\n                    )\n                    zmeta[col].append(arr)\n\n            else:\n                # --- Append to existing ---\n                zmeta = zroot[meta_group_path]\n\n                # index increment\n                current_len = zmeta[\"index\"].shape[0]\n                next_idx = current_len\n                zmeta[\"index\"].append([next_idx])\n\n                for col in df_row.columns:\n                    if col in (\"start\", \"end\"):\n                        arr = np.array(df_row[col].to_numpy(), dtype=\"M8[ns]\")\n                    else:\n                        arr = df_row[col].to_list()\n                    zmeta[col].append(arr)\n\n            session.commit(f\"Appended metadata row for {group_name}, hash={rinex_hash}\")\n\n        self._logger.info(\n            f\"Metadata appended for group '{group_name}': \"\n            f\"hash={rinex_hash}, snapshot={snapshot_id}, action={action}\"\n        )\n\n    def append_metadata_bulk(\n        self,\n        group_name: str,\n        rows: list[dict[str, Any]],\n        session: Optional[\"icechunk.WritableSession\"] = None,\n    ) -&gt; None:\n        \"\"\"\n        Append multiple metadata rows in one commit.\n\n        Parameters\n        ----------\n        group_name : str\n            Group name (e.g. \"canopy\", \"reference\")\n        rows : list[dict[str, Any]]\n            List of metadata records matching the schema used in\n            append_metadata().\n        session : icechunk.WritableSession, optional\n            If provided, rows are written into this session (caller commits later).\n            If None, this method opens its own writable session and commits once.\n        \"\"\"\n        if not rows:\n            self._logger.info(f\"No metadata rows to append for group '{group_name}'\")\n            return\n\n        # Ensure datetime conversions for consistency\n        for row in rows:\n            if isinstance(row.get(\"start\"), str):\n                row[\"start\"] = np.datetime64(row[\"start\"])\n            if isinstance(row.get(\"end\"), str):\n                row[\"end\"] = np.datetime64(row[\"end\"])\n            if \"written_at\" not in row:\n                row[\"written_at\"] = datetime.now(UTC).isoformat()\n\n        # Prepare the Polars DataFrame\n        df = pl.DataFrame(rows)\n\n        def _do_append(session_obj: \"icechunk.WritableSession\") -&gt; None:\n            \"\"\"Append metadata rows to a writable session.\n\n            Parameters\n            ----------\n            session_obj : icechunk.WritableSession\n                Writable session to update.\n\n            Returns\n            -------\n            None\n            \"\"\"\n            zroot = zarr.open_group(session_obj.store, mode=\"a\")\n            meta_group_path = f\"{group_name}/metadata/table\"\n            zmeta = zroot.require_group(meta_group_path)\n\n            start_index = 0\n            if \"index\" in zmeta:\n                existing_len = zmeta[\"index\"].shape[0]\n                start_index = (\n                    int(zmeta[\"index\"][-1].item()) + 1 if existing_len &gt; 0 else 0\n                )\n\n            # Assign sequential indices\n            df_with_index = df.with_columns(\n                (pl.arange(start_index, start_index + df.height)).alias(\"index\")\n            )\n\n            # Write each column\n            for col_name in df_with_index.columns:\n                col_data = df_with_index[col_name]\n\n                if col_name == \"index\":\n                    dtype = \"i8\"\n                    arr = col_data.to_numpy().astype(dtype)\n                elif col_name in (\"start\", \"end\"):\n                    dtype = \"M8[ns]\"\n                    arr = col_data.to_numpy().astype(dtype)\n                else:\n                    # strings / jsons / ids\n                    dtype = VariableLengthUTF8()\n                    arr = col_data.to_list()\n\n                if col_name not in zmeta:\n                    # Create array if it doesn't exist\n                    zmeta.create_array(\n                        name=col_name,\n                        shape=(0,),\n                        dtype=dtype,\n                        chunks=(1024,),\n                        overwrite=True,\n                    )\n\n                # Resize and append\n                old_len = zmeta[col_name].shape[0]\n                new_len = old_len + len(arr)\n                zmeta[col_name].resize(new_len)\n                zmeta[col_name][old_len:new_len] = arr\n\n            self._logger.info(\n                f\"Appended {df_with_index.height} metadata rows to group '{group_name}'\"\n            )\n\n        if session is not None:\n            _do_append(session)\n        else:\n            with self.writable_session() as sess:\n                _do_append(sess)\n                sess.commit(f\"Bulk metadata append for {group_name}\")\n\n    def load_metadata(self, store: Any, group_name: str) -&gt; pl.DataFrame:\n        \"\"\"Load metadata directly from Zarr into a Polars DataFrame.\n\n        Parameters\n        ----------\n        store : Any\n            Zarr store or session store handle.\n        group_name : str\n            Group name.\n\n        Returns\n        -------\n        pl.DataFrame\n            Metadata table.\n        \"\"\"\n        zroot = zarr.open_group(store, mode=\"r\")\n        zmeta = zroot[f\"{group_name}/metadata/table\"]\n\n        # Read all columns into a dict of numpy arrays\n        data = {col: zmeta[col][...] for col in zmeta.array_keys()}\n\n        # Build Polars DataFrame\n        df = pl.DataFrame(data)\n\n        # Convert numeric datetime64 columns back to proper Polars datetimes\n        if df[\"start\"].dtype in (pl.Int64, pl.Float64):\n            df = df.with_columns(pl.col(\"start\").cast(pl.Datetime(\"ns\")))\n        if df[\"end\"].dtype in (pl.Int64, pl.Float64):\n            df = df.with_columns(pl.col(\"end\").cast(pl.Datetime(\"ns\")))\n        if df[\"written_at\"].dtype == pl.Utf8:\n            df = df.with_columns(pl.col(\"written_at\").str.to_datetime(\"%+\"))\n        return df\n\n    def read_metadata_table(self, session: Any, group_name: str) -&gt; pl.DataFrame:\n        \"\"\"Read the metadata table from a session.\n\n        Parameters\n        ----------\n        session : Any\n            Active session for reading.\n        group_name : str\n            Group name.\n\n        Returns\n        -------\n        pl.DataFrame\n            Metadata table.\n        \"\"\"\n        zmeta = zarr.open_group(\n            session.store,\n            mode=\"r\",\n        )[f\"{group_name}/metadata/table\"]\n\n        data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n        df = pl.DataFrame(data)\n\n        # Ensure start/end are proper datetime\n        df = df.with_columns(\n            [\n                pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n                pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n            ]\n        )\n        return df\n\n    def metadata_row_exists(\n        self,\n        group_name: str,\n        rinex_hash: str,\n        start: np.datetime64,\n        end: np.datetime64,\n        branch: str = \"main\",\n    ) -&gt; tuple[bool, pl.DataFrame]:\n        \"\"\"\n        Check whether a (start, end) interval exists in group metadata.\n\n        Parameters\n        ----------\n        group_name : str\n            Icechunk group name.\n        rinex_hash : str\n            Hash of the current RINEX dataset.\n        start : np.datetime64\n            Start time for the interval.\n        end : np.datetime64\n            End time for the interval.\n        branch : str, default \"main\"\n            Branch name in the Icechunk repository.\n\n        Returns\n        -------\n        tuple[bool, pl.DataFrame]\n            Existence flag and the matching metadata rows.\n\n        Raises\n        ------\n        ValueError\n            If a conflicting hash is found for the same interval.\n\n        Notes\n        -----\n        The metadata table is cast to `Datetime(\"ns\")` for `start` and `end`\n        before filtering.\n        \"\"\"\n        with self.readonly_session(branch) as session:\n            try:\n                zmeta = zarr.open_group(session.store, mode=\"r\")[\n                    f\"{group_name}/metadata/table\"\n                ]\n            except Exception:\n                return False, pl.DataFrame()\n\n            # Load all arrays into a dict\n            data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n            df = pl.DataFrame(data)\n\n            # Ensure datetime dtypes\n            df = df.with_columns(\n                [\n                    pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n                    pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n                ]\n            )\n\n            # Step 1: filter by start+end\n            matches = df.filter(\n                (pl.col(\"start\") == np.datetime64(start, \"ns\"))\n                &amp; (pl.col(\"end\") == np.datetime64(end, \"ns\"))\n            )\n\n            if matches.is_empty():\n                return False, matches\n\n            # Step 2: check hash consistency\n            unique_hashes = matches.select(\"rinex_hash\").unique()\n\n            if (\n                unique_hashes.height &gt; 1\n                or unique_hashes.item(0, \"rinex_hash\") != rinex_hash\n            ):\n                existing_hashes = unique_hashes.to_series().to_list()\n                raise ValueError(\n                    \"Metadata conflict: rows with start=\"\n                    f\"{start}, end={end} exist but hash differs \"\n                    f\"(existing={existing_hashes}, new={rinex_hash})\"\n                )\n\n            return True, matches\n\n    def batch_check_existing(self, group_name: str, file_hashes: list[str]) -&gt; set[str]:\n        \"\"\"Check which file hashes already exist in metadata.\"\"\"\n\n        try:\n            with self.readonly_session(\"main\") as session:\n                df = self.load_metadata(session.store, group_name)\n\n                # Filter to matching hashes\n                existing = df.filter(pl.col(\"rinex_hash\").is_in(file_hashes))\n                return set(existing[\"rinex_hash\"].to_list())\n\n        except (KeyError, zarr.errors.GroupNotFoundError, Exception):\n            # Branch/group/metadata doesn't exist yet (fresh store)\n            return set()\n\n    def append_metadata_bulk_store(\n        self,\n        group_name: str,\n        rows: list[dict[str, Any]],\n        store: Any,\n    ) -&gt; None:\n        \"\"\"\n        Append metadata rows into an open transaction store.\n\n        Parameters\n        ----------\n        group_name : str\n            Group name (e.g. \"canopy\", \"reference\").\n        rows : list[dict[str, Any]]\n            Metadata rows to append.\n        store : Any\n            Open Icechunk transaction store.\n        \"\"\"\n        if not rows:\n            return\n\n        zroot = zarr.open_group(store, mode=\"a\")\n        zmeta = zroot.require_group(f\"{group_name}/metadata/table\")\n\n        # Find next index\n        start_index = 0\n        if \"index\" in zmeta:\n            start_index = (\n                int(zmeta[\"index\"][-1]) + 1 if zmeta[\"index\"].shape[0] &gt; 0 else 0\n            )\n\n        for i, row in enumerate(rows, start=start_index):\n            row[\"index\"] = i\n\n        import polars as pl\n\n        df = pl.DataFrame(rows)\n\n        for col in df.columns:\n            list_only_cols = {\n                \"attrs\",\n                \"commit_msg\",\n                \"action\",\n                \"write_strategy\",\n                \"rinex_hash\",\n                \"snapshot_id\",\n            }\n            if col in list_only_cols:\n                values = df[col].to_list()\n            else:\n                values = df[col].to_numpy()\n\n            if col == \"index\":\n                dtype = \"i8\"\n            elif col in (\"start\", \"end\"):\n                dtype = \"M8[ns]\"\n            else:\n                dtype = VariableLengthUTF8()\n\n            if col not in zmeta:\n                zmeta.create_array(\n                    name=col, shape=(0,), dtype=dtype, chunks=(1024,), overwrite=True\n                )\n\n            arr = zmeta[col]\n            old_len = arr.shape[0]\n            new_len = old_len + len(values)\n            arr.resize(new_len)\n            arr[old_len:new_len] = values\n\n        self._logger.info(f\"Appended {df.height} metadata rows to group '{group_name}'\")\n\n    def expire_old_snapshots(\n        self,\n        days: int | None = None,\n        branch: str = \"main\",\n        delete_expired_branches: bool = True,\n        delete_expired_tags: bool = True,\n    ) -&gt; set[str]:\n        \"\"\"\n        Expire and garbage-collect snapshots older than the given retention period.\n\n        Parameters\n        ----------\n        days : int | None, optional\n            Number of days to retain snapshots. Defaults to config value.\n        branch : str, default \"main\"\n            Branch to apply expiration on.\n        delete_expired_branches : bool, default True\n            Whether to delete branches pointing to expired snapshots.\n        delete_expired_tags : bool, default True\n            Whether to delete tags pointing to expired snapshots.\n\n        Returns\n        -------\n        set[str]\n            Expired snapshot IDs.\n        \"\"\"\n        if days is None:\n            days = self._rinex_store_expire_days\n        cutoff = datetime.now(UTC) - timedelta(days=days)\n\n        # cutoff = datetime(2025, 10, 3, 16, 44, 1, tzinfo=timezone.utc)\n        self._logger.info(\n            f\"Running expiration on store '{self.store_type}' \"\n            f\"(branch '{branch}') with cutoff {cutoff.isoformat()}\"\n        )\n\n        # Expire snapshots older than cutoff\n        expired_ids = self.repo.expire_snapshots(\n            older_than=cutoff,\n            delete_expired_branches=delete_expired_branches,\n            delete_expired_tags=delete_expired_tags,\n        )\n\n        if expired_ids:\n            self._logger.info(\n                f\"Expired {len(expired_ids)} snapshots: {sorted(expired_ids)}\"\n            )\n        else:\n            self._logger.info(\"No snapshots to expire.\")\n\n        # Garbage-collect expired objects to reclaim storage\n        summary = self.repo.garbage_collect(delete_object_older_than=cutoff)\n        self._logger.info(\n            f\"Garbage collection summary: \"\n            f\"deleted_bytes={summary.bytes_deleted}, \"\n            f\"deleted_chunks={summary.chunks_deleted}, \"\n            f\"deleted_manifests={summary.manifests_deleted}, \"\n            f\"deleted_snapshots={summary.snapshots_deleted}, \"\n            f\"deleted_attributes={summary.attributes_deleted}, \"\n            f\"deleted_transaction_logs={summary.transaction_logs_deleted}\"\n        )\n\n        return expired_ids\n\n    def get_history(self, branch: str = \"main\", limit: int | None = None) -&gt; list[dict]:\n        \"\"\"\n        Return commit ancestry (history) for a branch.\n\n        Parameters\n        ----------\n        branch : str, default \"main\"\n            Branch name.\n        limit : int | None, optional\n            Maximum number of commits to return.\n\n        Returns\n        -------\n        list[dict]\n            Commit info dictionaries (id, message, written_at, parent_ids).\n        \"\"\"\n        self._logger.info(f\"Fetching ancestry for branch '{branch}'\")\n\n        history = []\n        for i, ancestor in enumerate(self.repo.ancestry(branch=branch)):\n            history.append(\n                {\n                    \"snapshot_id\": ancestor.id,\n                    \"commit_msg\": ancestor.message,\n                    \"written_at\": ancestor.written_at,\n                    \"parent_ids\": ancestor.parent_id,\n                }\n            )\n            if limit is not None and i + 1 &gt;= limit:\n                break\n\n        return history\n\n    def print_history(self, branch: str = \"main\", limit: int | None = 100) -&gt; None:\n        \"\"\"\n        Pretty-print the ancestry for quick inspection.\n        \"\"\"\n        for entry in self.get_history(branch=branch, limit=limit):\n            ts = entry[\"written_at\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n            print(f\"{ts} {entry['snapshot_id'][:8]} {entry['commit_msg']}\")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the developer-facing representation.\n\n        Returns\n        -------\n        str\n            Representation string.\n        \"\"\"\n        return (\n            \"MyIcechunkStore(\"\n            f\"store_path={self.store_path}, store_type={self.store_type})\"\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable summary.\n\n        Returns\n        -------\n        str\n            Summary string.\n        \"\"\"\n\n        # Capture tree output\n        old_stdout = sys.stdout\n        sys.stdout = buffer = io.StringIO()\n\n        try:\n            self.print_tree()\n            tree_output = buffer.getvalue()\n        finally:\n            sys.stdout = old_stdout\n\n        branches = self.get_branch_names()\n        group_dict = self.get_group_names()\n        total_groups = sum(len(groups) for groups in group_dict.values())\n\n        return (\n            f\"MyIcechunkStore: {self.store_path}\\n\"\n            f\"Branches: {len(branches)} | Total Groups: {total_groups}\\n\\n\"\n            f\"{tree_output}\"\n        )\n\n    def rechunk_group(\n        self,\n        group_name: str,\n        chunks: dict[str, int],\n        source_branch: str = \"main\",\n        temp_branch: str | None = None,\n        promote_to_main: bool = True,\n        delete_temp_branch: bool = True,\n    ) -&gt; str:\n        \"\"\"\n        Rechunk a group with optimal chunk sizes.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group to rechunk\n        chunks : dict[str, int]\n            Chunking specification, e.g. {'epoch': 34560, 'sid': -1}\n        source_branch : str\n            Branch to read original data from (default: \"main\")\n        temp_branch : str | None\n            Temporary branch name for rechunked data. If None, uses\n            \"{group_name}_rechunked\".\n        promote_to_main : bool\n            If True, reset main branch to rechunked snapshot after writing\n        delete_temp_branch : bool\n            If True, delete temporary branch after promotion (only if\n            promote_to_main=True).\n\n        Returns\n        -------\n        str\n            Snapshot ID of the rechunked data\n        \"\"\"\n        if temp_branch is None:\n            temp_branch = f\"{group_name}_rechunked_temp\"\n\n        self._logger.info(\n            f\"Starting rechunk of group '{group_name}' with chunks={chunks}\"\n        )\n\n        # Get CURRENT snapshot from source branch to preserve all other groups\n        current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n\n        # Create temp branch from current snapshot (preserves all existing groups)\n        try:\n            self.repo.create_branch(temp_branch, current_snapshot)\n            self._logger.info(\n                f\"Created temporary branch '{temp_branch}' from current {source_branch}\"\n            )\n        except Exception as e:\n            self._logger.warning(f\"Branch '{temp_branch}' may already exist: {e}\")\n\n        # Read original data\n        ds_original = self.read_group(group_name, branch=source_branch)\n        self._logger.info(f\"Original chunks: {ds_original.chunks}\")\n\n        # Rechunk\n        ds_rechunked = ds_original.chunk(chunks)\n        self._logger.info(f\"New chunks: {ds_rechunked.chunks}\")\n\n        # Clear encoding to avoid conflicts\n        for var in ds_rechunked.data_vars:\n            ds_rechunked[var].encoding = {}\n\n        # Write rechunked data (overwrites only this group)\n        with self.writable_session(temp_branch) as session:\n            to_icechunk(ds_rechunked, session, group=group_name, mode=\"w\")\n            snapshot_id = session.commit(f\"Rechunked {group_name} with chunks={chunks}\")\n\n        self._logger.info(\n            f\"Rechunked data written to branch '{temp_branch}', snapshot={snapshot_id}\"\n        )\n\n        # Promote to main if requested\n        if promote_to_main:\n            rechunked_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n            self.repo.reset_branch(source_branch, rechunked_snapshot)\n            self._logger.info(\n                f\"Reset branch '{source_branch}' to rechunked snapshot \"\n                f\"{rechunked_snapshot}\"\n            )\n\n            # Delete temp branch if requested\n            if delete_temp_branch:\n                self.repo.delete_branch(temp_branch)\n                self._logger.info(f\"Deleted temporary branch '{temp_branch}'\")\n\n        return snapshot_id\n\n    def rechunk_group_verbose(\n        self,\n        group_name: str,\n        chunks: dict[str, int] | None = None,\n        source_branch: str = \"main\",\n        temp_branch: str | None = None,\n        promote_to_main: bool = True,\n        delete_temp_branch: bool = True,\n    ) -&gt; str:\n        \"\"\"\n        Rechunk a group with optimal chunk sizes.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group to rechunk\n        chunks : dict[str, int] | None\n            Chunking specification, e.g. {'epoch': 34560, 'sid': -1}. Defaults\n            to `gnnsvodpy.globals.ICECHUNK_CHUNK_STRATEGIES`.\n        source_branch : str\n            Branch to read original data from (default: \"main\")\n        temp_branch : str | None\n            Temporary branch name for rechunked data. If None, uses\n            \"{group_name}_rechunked\".\n        promote_to_main : bool\n            If True, reset main branch to rechunked snapshot after writing\n        delete_temp_branch : bool\n            If True, delete temporary branch after promotion (only if\n            promote_to_main=True).\n\n        Returns\n        -------\n        str\n            Snapshot ID of the rechunked data\n        \"\"\"\n        if temp_branch is None:\n            temp_branch = f\"{group_name}_rechunked_temp\"\n\n        if chunks is None:\n            chunks = self.chunk_strategy or {\"epoch\": 34560, \"sid\": -1}\n\n        print(f\"\\n{'=' * 60}\")\n        print(f\"Starting rechunk of group '{group_name}'\")\n        print(f\"Target chunks: {chunks}\")\n        print(f\"{'=' * 60}\\n\")\n\n        self._logger.info(\n            f\"Starting rechunk of group '{group_name}' with chunks={chunks}\"\n        )\n\n        # Get CURRENT snapshot from source branch to preserve all other groups\n        print(f\"[1/7] Getting current snapshot from branch '{source_branch}'...\")\n        current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n        print(f\"      ✓ Current snapshot: {current_snapshot[:12]}\")\n\n        # Create temp branch from current snapshot (preserves all existing groups)\n        print(f\"\\n[2/7] Creating temporary branch '{temp_branch}'...\")\n        try:\n            self.repo.create_branch(temp_branch, current_snapshot)\n            print(f\"      ✓ Branch '{temp_branch}' created\")\n            self._logger.info(\n                f\"Created temporary branch '{temp_branch}' from current {source_branch}\"\n            )\n        except Exception as e:\n            print(\n                f\"      ⚠ Branch '{temp_branch}' already exists, using existing branch\"\n            )\n            self._logger.warning(f\"Branch '{temp_branch}' may already exist: {e}\")\n\n        # Read original data\n        print(f\"\\n[3/7] Reading original data from '{group_name}'...\")\n        ds_original = self.read_group(group_name, branch=source_branch)\n\n        # Unify chunks if inconsistent\n        try:\n            ds_original = ds_original.unify_chunks()\n            print(\"      ✓ Unified inconsistent chunks\")\n        except (TypeError, ValueError):\n            pass  # Chunks are already consistent\n\n        print(f\"      ✓ Data shape: {dict(ds_original.sizes)}\")\n        print(f\"      ✓ Original chunks: {ds_original.chunks}\")\n        self._logger.info(f\"Original chunks: {ds_original.chunks}\")\n\n        # Rechunk\n        print(\"\\n[4/7] Rechunking data...\")\n        ds_rechunked = ds_original.chunk(chunks)\n        ds_rechunked = ds_rechunked.unify_chunks()\n        print(f\"      ✓ New chunks: {ds_rechunked.chunks}\")\n        self._logger.info(f\"New chunks: {ds_rechunked.chunks}\")\n\n        # Clear encoding to avoid conflicts\n        for var in ds_rechunked.data_vars:\n            ds_rechunked[var].encoding = {}\n        for coord in ds_rechunked.coords:\n            if \"chunks\" in ds_rechunked[coord].encoding:\n                del ds_rechunked[coord].encoding[\"chunks\"]\n\n        # Write rechunked data first (overwrites entire group)\n        print(f\"\\n[5/7] Writing rechunked data to branch '{temp_branch}'...\")\n        print(\"      This may take several minutes for large datasets...\")\n        with self.writable_session(temp_branch) as session:\n            to_icechunk(ds_rechunked, session, group=group_name, mode=\"w\")\n            session.commit(f\"Wrote rechunked data for {group_name}\")\n        print(\"      ✓ Data written successfully\")\n\n        # Copy subgroups after writing rechunked data\n        print(f\"\\n[6/7] Copying subgroups from '{group_name}'...\")\n        with self.writable_session(temp_branch) as session:\n            with self.readonly_session(source_branch) as icsession:\n                source_group = zarr.open_group(icsession.store, mode=\"r\")[group_name]\n            target_group = zarr.open_group(session.store, mode=\"a\")[group_name]\n\n            subgroup_count = 0\n            for subgroup_name in source_group.group_keys():\n                print(f\"      ✓ Copying subgroup '{subgroup_name}'...\")\n                source_subgroup = source_group[subgroup_name]\n                target_subgroup = target_group.create_group(\n                    subgroup_name, overwrite=True\n                )\n\n                # Copy arrays from subgroup\n                for array_name in source_subgroup.array_keys():\n                    source_array = source_subgroup[array_name]\n                    target_array = target_subgroup.create_array(\n                        array_name,\n                        shape=source_array.shape,\n                        dtype=source_array.dtype,\n                        chunks=source_array.chunks,\n                        overwrite=True,\n                    )\n                    target_array[:] = source_array[:]\n\n                # Copy subgroup attributes\n                target_subgroup.attrs.update(source_subgroup.attrs)\n                subgroup_count += 1\n\n            if subgroup_count &gt; 0:\n                snapshot_id = session.commit(\n                    f\"Rechunked {group_name} with chunks={chunks}\"\n                )\n                print(f\"      ✓ {subgroup_count} subgroups copied\")\n            else:\n                snapshot_id = next(self.repo.ancestry(branch=temp_branch)).id\n                print(\"      ✓ No subgroups to copy\")\n\n        print(f\"      ✓ Snapshot ID: {snapshot_id[:12]}\")\n        self._logger.info(\n            f\"Rechunked data written to branch '{temp_branch}', snapshot={snapshot_id}\"\n        )\n\n        # Promote to main if requested\n        if promote_to_main:\n            print(f\"\\n[7/7] Promoting to '{source_branch}' branch...\")\n            rechunked_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n            self.repo.reset_branch(source_branch, rechunked_snapshot)\n            print(\n                f\"      ✓ Branch '{source_branch}' reset to {rechunked_snapshot[:12]}\"\n            )\n            self._logger.info(\n                f\"Reset branch '{source_branch}' to rechunked snapshot \"\n                f\"{rechunked_snapshot}\"\n            )\n\n            # Delete temp branch if requested\n            if delete_temp_branch:\n                print(f\"      ✓ Deleting temporary branch '{temp_branch}'...\")\n                self.delete_branch(temp_branch)\n                print(\"      ✓ Temporary branch deleted\")\n                self._logger.info(f\"Deleted temporary branch '{temp_branch}'\")\n        else:\n            print(\"\\n[7/7] Skipping promotion (promote_to_main=False)\")\n            print(f\"      Rechunked data available on branch '{temp_branch}'\")\n\n        print(f\"\\n{'=' * 60}\")\n        print(f\"✓ Rechunking complete for '{group_name}'\")\n        print(f\"{'=' * 60}\\n\")\n\n        return snapshot_id\n\n    def create_release_tag(self, tag_name: str, snapshot_id: str | None = None) -&gt; None:\n        \"\"\"\n        Create an immutable tag for an important version.\n\n        Parameters\n        ----------\n        tag_name : str\n            Name for the tag (e.g., \"v2024_complete\", \"before_reprocess\")\n        snapshot_id : str | None\n            Snapshot to tag. If None, uses current tip of main branch.\n        \"\"\"\n        if snapshot_id is None:\n            # Tag current main branch tip\n            snapshot_id = next(self.repo.ancestry(branch=\"main\")).id\n\n        self.repo.create_tag(tag_name, snapshot_id)\n        self._logger.info(f\"Created tag '{tag_name}' at snapshot {snapshot_id[:8]}\")\n\n    def list_tags(self) -&gt; list[str]:\n        \"\"\"List all tags in the repository.\"\"\"\n        return list(self.repo.list_tags())\n\n    def delete_tag(self, tag_name: str) -&gt; None:\n        \"\"\"Delete a tag (use with caution - tags are meant to be permanent).\"\"\"\n        self.repo.delete_tag(tag_name)\n        self._logger.warning(f\"Deleted tag '{tag_name}'\")\n\n    def plot_commit_graph(self, max_commits: int = 100) -&gt; \"Figure\":\n        \"\"\"\n        Visualize commit history as an interactive git-like graph.\n\n        Creates an interactive visualization showing:\n        - Branches with different colors\n        - Chronological commit ordering\n        - Branch divergence points\n        - Commit messages on hover\n        - Click to see commit details\n\n        Parameters\n        ----------\n        max_commits : int\n            Maximum number of commits to display (default: 100).\n\n        Returns\n        -------\n        Figure\n            Interactive plotly figure (works in marimo and Jupyter).\n        \"\"\"\n        from collections import defaultdict\n        from datetime import datetime\n\n        import plotly.graph_objects as go\n\n        # Collect all commits with full metadata\n        commit_map = {}  # id -&gt; commit data\n        branch_tips = {}  # branch -&gt; latest commit id\n\n        for branch in self.repo.list_branches():\n            ancestors = list(self.repo.ancestry(branch=branch))\n            if ancestors:\n                branch_tips[branch] = ancestors[0].id\n\n            for ancestor in ancestors:\n                if ancestor.id not in commit_map:\n                    commit_map[ancestor.id] = {\n                        \"id\": ancestor.id,\n                        \"parent_id\": ancestor.parent_id,\n                        \"message\": ancestor.message,\n                        \"written_at\": ancestor.written_at,\n                        \"branches\": [branch],\n                    }\n                else:\n                    # Multiple branches point to same commit\n                    commit_map[ancestor.id][\"branches\"].append(branch)\n\n                if len(commit_map) &gt;= max_commits:\n                    break\n            if len(commit_map) &gt;= max_commits:\n                break\n\n        # Build parent-child relationships\n        commits_list = list(commit_map.values())\n        commits_list.sort(key=lambda c: c[\"written_at\"])  # Oldest first\n\n        # Assign horizontal positions (chronological)\n        commit_x_positions = {}\n        for idx, commit in enumerate(commits_list):\n            commit[\"x\"] = idx\n            commit_x_positions[commit[\"id\"]] = idx\n\n        # Assign vertical positions: commits shared by branches stay on same Y\n        # Only diverge when branches have different commits\n        branch_names = sorted(\n            self.repo.list_branches(), key=lambda b: (b != \"main\", b)\n        )  # main first\n\n        # Build a set of all commit IDs for each branch\n        branch_commits = {}\n        for branch in branch_names:\n            history = list(self.repo.ancestry(branch=branch))\n            branch_commits[branch] = {h.id for h in history if h.id in commit_map}\n\n        # Find where branches diverge\n        def branches_share_commit(\n            commit_id: str,\n            branches: list[str],\n        ) -&gt; list[str]:\n            \"\"\"Return branches that contain a commit.\n\n            Parameters\n            ----------\n            commit_id : str\n                Commit identifier to check.\n            branches : list[str]\n                Branch names to search.\n\n            Returns\n            -------\n            list[str]\n                Branches that contain the commit.\n            \"\"\"\n            return [b for b in branches if commit_id in branch_commits[b]]\n\n        # Assign Y position: all commits on a single horizontal line initially\n        # We'll use vertical offset for parallel branch indicators\n        for commit in commits_list:\n            commit[\"y\"] = 0  # All on same timeline\n            commit[\"branch_set\"] = frozenset(commit[\"branches\"])\n\n        # Color palette for branches\n        colors = [\n            \"#4a9a4a\",  # green (main)\n            \"#5580c8\",  # blue\n            \"#d97643\",  # orange\n            \"#9b59b6\",  # purple\n            \"#e74c3c\",  # red\n            \"#1abc9c\",  # turquoise\n            \"#f39c12\",  # yellow\n            \"#34495e\",  # dark gray\n        ]\n        branch_colors = {b: colors[i % len(colors)] for i, b in enumerate(branch_names)}\n\n        # Build edges: draw parallel lines for shared commits (metro-style)\n        edges_by_branch = defaultdict(list)  # branch -&gt; list of edge dicts\n\n        for commit in commits_list:\n            if commit[\"parent_id\"] and commit[\"parent_id\"] in commit_map:\n                parent = commit_map[commit[\"parent_id\"]]\n\n                # Find which branches share both this commit and its parent\n                shared_branches = [\n                    b for b in commit[\"branches\"] if b in parent[\"branches\"]\n                ]\n\n                for branch in shared_branches:\n                    edges_by_branch[branch].append(\n                        {\n                            \"x0\": parent[\"x\"],\n                            \"y0\": parent[\"y\"],\n                            \"x1\": commit[\"x\"],\n                            \"y1\": commit[\"y\"],\n                        }\n                    )\n\n        # Create plotly figure\n        fig = go.Figure()\n\n        # Draw edges grouped by branch (parallel lines for shared paths)\n        for branch_idx, branch in enumerate(branch_names):\n            if branch not in edges_by_branch:\n                continue\n\n            color = branch_colors[branch]\n\n            # Draw each edge as a separate line\n            for edge in edges_by_branch[branch]:\n                # Vertical offset for parallel lines (metro-style)\n                offset = (branch_idx - (len(branch_names) - 1) / 2) * 0.15\n\n                fig.add_trace(\n                    go.Scatter(\n                        x=[edge[\"x0\"], edge[\"x1\"]],\n                        y=[edge[\"y0\"] + offset, edge[\"y1\"] + offset],\n                        mode=\"lines\",\n                        line=dict(color=color, width=3),\n                        hoverinfo=\"skip\",\n                        showlegend=False,\n                        opacity=0.7,\n                    )\n                )\n\n        # Draw commits (nodes) - one trace per unique commit\n        # Color by which branches include it\n        x_vals = [c[\"x\"] for c in commits_list]\n        y_vals = [c[\"y\"] for c in commits_list]\n\n        # Format hover text\n        hover_texts = []\n        marker_colors = []\n        marker_symbols = []\n\n        for c in commits_list:\n            # Handle both string and datetime objects\n            if isinstance(c[\"written_at\"], str):\n                time_str = datetime.fromisoformat(c[\"written_at\"]).strftime(\n                    \"%Y-%m-%d %H:%M\"\n                )\n            else:\n                time_str = c[\"written_at\"].strftime(\"%Y-%m-%d %H:%M\")\n\n            branches_str = \", \".join(c[\"branches\"])\n            hover_texts.append(\n                f\"&lt;b&gt;{c['message'] or 'No message'}&lt;/b&gt;&lt;br&gt;\"\n                f\"Commit: {c['id'][:12]}&lt;br&gt;\"\n                f\"Branches: {branches_str}&lt;br&gt;\"\n                f\"Time: {time_str}\"\n            )\n\n            # Color by first branch (priority: main)\n            if \"main\" in c[\"branches\"]:\n                marker_colors.append(branch_colors[\"main\"])\n            else:\n                marker_colors.append(branch_colors[c[\"branches\"][0]])\n\n            # Star for branch tips\n            if c[\"id\"] in branch_tips.values():\n                marker_symbols.append(\"star\")\n            else:\n                marker_symbols.append(\"circle\")\n\n        fig.add_trace(\n            go.Scatter(\n                x=x_vals,\n                y=y_vals,\n                mode=\"markers\",\n                name=\"Commits\",\n                marker=dict(\n                    size=14,\n                    color=marker_colors,\n                    symbol=marker_symbols,\n                    line=dict(color=\"white\", width=2),\n                ),\n                hovertext=hover_texts,\n                hoverinfo=\"text\",\n                showlegend=False,\n            )\n        )\n\n        # Add legend traces (invisible points just for legend)\n        for branch_idx, branch in enumerate(branch_names):\n            fig.add_trace(\n                go.Scatter(\n                    x=[None],\n                    y=[None],\n                    mode=\"markers\",\n                    name=branch,\n                    marker=dict(\n                        size=10,\n                        color=branch_colors[branch],\n                        line=dict(color=\"white\", width=2),\n                    ),\n                    showlegend=True,\n                )\n            )\n\n        # Layout styling\n        title_text = (\n            f\"Commit Graph: {self.site_name} ({len(commits_list)} commits, \"\n            f\"{len(branch_names)} branches)\"\n        )\n        fig.update_layout(\n            title=dict(\n                text=title_text,\n                font=dict(size=16, color=\"#e5e5e5\"),\n            ),\n            xaxis=dict(\n                title=\"Time (oldest ← → newest)\",\n                showticklabels=False,\n                showgrid=True,\n                gridcolor=\"rgba(255,255,255,0.1)\",\n                zeroline=False,\n            ),\n            yaxis=dict(\n                title=\"\",\n                showticklabels=False,\n                showgrid=False,\n                zeroline=False,\n                range=[-1, 1],  # Fixed range for single timeline\n            ),\n            plot_bgcolor=\"#1a1a1a\",\n            paper_bgcolor=\"#1a1a1a\",\n            font=dict(color=\"#e5e5e5\"),\n            hovermode=\"closest\",\n            height=400,\n            width=max(800, len(commits_list) * 50),\n            legend=dict(\n                title=\"Branches\",\n                orientation=\"h\",\n                x=0,\n                y=-0.15,\n                bgcolor=\"rgba(30,30,30,0.8)\",\n                bordercolor=\"rgba(255,255,255,0.2)\",\n                borderwidth=1,\n            ),\n        )\n\n        return fig\n\n    def cleanup_stale_branches(\n        self, keep_patterns: list[str] | None = None\n    ) -&gt; list[str]:\n        \"\"\"\n        Delete stale temporary branches (e.g., from failed rechunking).\n\n        Parameters\n        ----------\n        keep_patterns : list[str] | None\n            Patterns to preserve. Default: [\"main\", \"dev\"]\n\n        Returns\n        -------\n        list[str]\n            Names of deleted branches\n        \"\"\"\n        if keep_patterns is None:\n            keep_patterns = [\"main\", \"dev\"]\n\n        deleted = []\n\n        for branch in self.repo.list_branches():\n            # Keep if matches any pattern\n            should_keep = any(pattern in branch for pattern in keep_patterns)\n\n            if not should_keep:\n                # Check if it's a temp branch from rechunking\n                if \"_rechunked_temp\" in branch or \"_temp\" in branch:\n                    try:\n                        self.repo.delete_branch(branch)\n                        deleted.append(branch)\n                        self._logger.info(f\"Deleted stale branch: {branch}\")\n                    except Exception as e:\n                        self._logger.warning(f\"Failed to delete branch {branch}: {e}\")\n\n        return deleted\n\n    def delete_branch(self, branch_name: str) -&gt; None:\n        \"\"\"Delete a branch.\"\"\"\n        if branch_name == \"main\":\n            raise ValueError(\"Cannot delete 'main' branch\")\n\n        self.repo.delete_branch(branch_name)\n        self._logger.info(f\"Deleted branch '{branch_name}'\")\n\n    def get_snapshot_info(self, snapshot_id: str) -&gt; dict:\n        \"\"\"\n        Get detailed information about a specific snapshot.\n\n        Parameters\n        ----------\n        snapshot_id : str\n            Snapshot ID to inspect\n\n        Returns\n        -------\n        dict\n            Snapshot metadata and statistics\n        \"\"\"\n        # Find the snapshot in ancestry\n        for ancestor in self.repo.ancestry(branch=\"main\"):\n            if ancestor.id == snapshot_id or ancestor.id.startswith(snapshot_id):\n                info = {\n                    \"snapshot_id\": ancestor.id,\n                    \"message\": ancestor.message,\n                    \"written_at\": ancestor.written_at,\n                    \"parent_id\": ancestor.parent_id,\n                }\n\n                # Try to get groups at this snapshot\n                try:\n                    session = self.repo.readonly_session(snapshot_id=ancestor.id)\n                    root = zarr.open(session.store, mode=\"r\")\n                    info[\"groups\"] = list(root.group_keys())\n                    info[\"arrays\"] = list(root.array_keys())\n                except Exception as e:\n                    self._logger.warning(f\"Could not inspect snapshot contents: {e}\")\n\n                return info\n\n        raise ValueError(f\"Snapshot {snapshot_id} not found in history\")\n\n    def compare_snapshots(self, snapshot_id_1: str, snapshot_id_2: str) -&gt; dict:\n        \"\"\"\n        Compare two snapshots to see what changed.\n\n        Parameters\n        ----------\n        snapshot_id_1 : str\n            First snapshot (older)\n        snapshot_id_2 : str\n            Second snapshot (newer)\n\n        Returns\n        -------\n        dict\n            Comparison results showing added/removed/modified groups\n        \"\"\"\n        info_1 = self.get_snapshot_info(snapshot_id_1)\n        info_2 = self.get_snapshot_info(snapshot_id_2)\n\n        groups_1 = set(info_1.get(\"groups\", []))\n        groups_2 = set(info_2.get(\"groups\", []))\n\n        return {\n            \"snapshot_1\": snapshot_id_1[:8],\n            \"snapshot_2\": snapshot_id_2[:8],\n            \"added_groups\": list(groups_2 - groups_1),\n            \"removed_groups\": list(groups_1 - groups_2),\n            \"common_groups\": list(groups_1 &amp; groups_2),\n            \"time_diff\": (info_2[\"written_at\"] - info_1[\"written_at\"]).total_seconds(),\n        }\n\n    def maintenance(\n        self, expire_days: int = 7, cleanup_branches: bool = True, run_gc: bool = True\n    ) -&gt; dict:\n        \"\"\"\n        Run full maintenance on the store.\n\n        Parameters\n        ----------\n        expire_days : int\n            Days of snapshot history to keep\n        cleanup_branches : bool\n            Remove stale temporary branches\n        run_gc : bool\n            Run garbage collection after expiration\n\n        Returns\n        -------\n        dict\n            Summary of maintenance actions\n        \"\"\"\n        self._logger.info(f\"Starting maintenance on {self.store_type}\")\n\n        results = {\"expired_snapshots\": 0, \"deleted_branches\": [], \"gc_summary\": None}\n\n        # Expire old snapshots\n        expired_ids = self.expire_old_snapshots(days=expire_days)\n        results[\"expired_snapshots\"] = len(expired_ids)\n\n        # Cleanup stale branches\n        if cleanup_branches:\n            deleted_branches = self.cleanup_stale_branches()\n            results[\"deleted_branches\"] = deleted_branches\n\n        # Garbage collection\n        if run_gc:\n            from datetime import datetime, timedelta\n\n            cutoff = datetime.now(UTC) - timedelta(days=expire_days)\n            gc_summary = self.repo.garbage_collect(delete_object_older_than=cutoff)\n            results[\"gc_summary\"] = {\n                \"bytes_deleted\": gc_summary.bytes_deleted,\n                \"chunks_deleted\": gc_summary.chunks_deleted,\n                \"manifests_deleted\": gc_summary.manifests_deleted,\n            }\n\n        self._logger.info(f\"Maintenance complete: {results}\")\n        return results\n\n    def sanitize_store(\n        self,\n        source_branch: str = \"main\",\n        temp_branch: str = \"sanitize_temp\",\n        promote_to_main: bool = True,\n        delete_temp_branch: bool = True,\n    ) -&gt; str:\n        \"\"\"\n        Sanitize all groups by removing NaN-only SIDs and cleaning coordinates.\n\n        Creates a temporary branch, applies sanitization to all groups, then\n        optionally promotes to main and cleans up.\n\n        Parameters\n        ----------\n        source_branch : str, default \"main\"\n            Branch to read original data from.\n        temp_branch : str, default \"sanitize_temp\"\n            Temporary branch name for sanitized data.\n        promote_to_main : bool, default True\n            If True, reset main branch to sanitized snapshot after writing.\n        delete_temp_branch : bool, default True\n            If True, delete temporary branch after promotion.\n\n        Returns\n        -------\n        str\n            Snapshot ID of the sanitized data.\n        \"\"\"\n        import time\n\n        from icechunk.xarray import to_icechunk\n\n        print(f\"\\n{'=' * 60}\")\n        print(\"Starting store sanitization\")\n        print(f\"{'=' * 60}\\n\")\n\n        # Step 1: Get current snapshot\n        print(f\"[1/6] Getting current snapshot from '{source_branch}'...\")\n        current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n        print(f\"      ✓ Current snapshot: {current_snapshot[:12]}\")\n\n        # Step 2: Create temp branch\n        print(f\"\\n[2/6] Creating temporary branch '{temp_branch}'...\")\n        try:\n            self.repo.create_branch(temp_branch, current_snapshot)\n            print(f\"      ✓ Branch '{temp_branch}' created\")\n        except Exception:\n            print(\"      ⚠ Branch exists, deleting and recreating...\")\n            self.delete_branch(temp_branch)\n            self.repo.create_branch(temp_branch, current_snapshot)\n            print(f\"      ✓ Branch '{temp_branch}' created\")\n\n        # Step 3: Get all groups\n        print(\"\\n[3/6] Discovering groups...\")\n        groups = self.list_groups()\n        print(f\"      ✓ Found {len(groups)} groups: {groups}\")\n\n        # Step 4: Sanitize each group\n        print(\"\\n[4/6] Sanitizing groups...\")\n        sanitized_count = 0\n\n        for group_name in groups:\n            print(f\"\\n      Processing '{group_name}'...\")\n            t_start = time.time()\n\n            try:\n                # Read original data\n                ds_original = self.read_group(group_name, branch=source_branch)\n                original_sids = len(ds_original.sid)\n                print(f\"        • Original: {original_sids} SIDs\")\n\n                # Sanitize: remove SIDs with all-NaN data\n                ds_sanitized = self._sanitize_dataset(ds_original)\n                sanitized_sids = len(ds_sanitized.sid)\n                removed_sids = original_sids - sanitized_sids\n\n                print(\n                    f\"        • Sanitized: {sanitized_sids} SIDs \"\n                    f\"(removed {removed_sids})\"\n                )\n\n                # Write sanitized data\n                with self.writable_session(temp_branch) as session:\n                    to_icechunk(ds_sanitized, session, group=group_name, mode=\"w\")\n\n                    # Copy metadata subgroups if they exist\n                    try:\n                        with self.readonly_session(source_branch) as read_session:\n                            source_group = zarr.open_group(\n                                read_session.store, mode=\"r\"\n                            )[group_name]\n                            if \"metadata\" in source_group.group_keys():\n                                # Copy entire metadata subgroup\n                                dest_group = zarr.open_group(session.store)[group_name]\n                                zarr.copy(\n                                    source_group[\"metadata\"],\n                                    dest_group,\n                                    name=\"metadata\",\n                                )\n                                print(\"        • Copied metadata subgroup\")\n                    except Exception as e:\n                        print(f\"        ⚠ Could not copy metadata: {e}\")\n\n                    session.commit(\n                        f\"Sanitized {group_name}: removed {removed_sids} empty SIDs\"\n                    )\n\n                t_elapsed = time.time() - t_start\n                print(f\"        ✓ Completed in {t_elapsed:.2f}s\")\n                sanitized_count += 1\n\n            except Exception as e:\n                print(f\"        ✗ Failed: {e}\")\n                continue\n\n        print(f\"\\n      ✓ Sanitized {sanitized_count}/{len(groups)} groups\")\n\n        # Step 5: Get final snapshot\n        print(\"\\n[5/6] Getting sanitized snapshot...\")\n        sanitized_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n        print(f\"      ✓ Snapshot: {sanitized_snapshot[:12]}\")\n\n        # Step 6: Promote to main\n        if promote_to_main:\n            print(f\"\\n[6/6] Promoting to '{source_branch}' branch...\")\n            self.repo.reset_branch(source_branch, sanitized_snapshot)\n            print(\n                f\"      ✓ Branch '{source_branch}' reset to {sanitized_snapshot[:12]}\"\n            )\n\n            if delete_temp_branch:\n                print(f\"      ✓ Deleting temporary branch '{temp_branch}'...\")\n                self.delete_branch(temp_branch)\n                print(\"      ✓ Temporary branch deleted\")\n        else:\n            print(\"\\n[6/6] Skipping promotion (promote_to_main=False)\")\n            print(f\"      Sanitized data available on branch '{temp_branch}'\")\n\n        print(f\"\\n{'=' * 60}\")\n        print(\"✓ Sanitization complete\")\n        print(f\"{'=' * 60}\\n\")\n\n        return sanitized_snapshot\n\n    def _sanitize_dataset(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"\n        Remove SIDs that have all-NaN data and clean coordinate metadata.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset to sanitize\n\n        Returns\n        -------\n        xr.Dataset\n            Sanitized dataset with NaN-only SIDs removed\n        \"\"\"\n        # Find SIDs that have at least some non-NaN data across all variables\n        has_data = ds.to_array().notnull().any(dim=[\"variable\", \"epoch\"])\n\n        # Keep only SIDs with data\n        sids_with_data = ds.sid.values[has_data.values]\n        ds_clean = ds.sel(sid=sids_with_data)\n\n        # Clean coordinate metadata - remove NaN values from string coordinates\n        for coord in [\"band\", \"system\", \"code\", \"sv\"]:\n            if coord in ds_clean.coords:\n                coord_values = ds_clean[coord].values\n                # Convert object arrays, handling NaN\n                if coord_values.dtype == object:\n                    clean_values = []\n                    for val in coord_values:\n                        if isinstance(val, float) and np.isnan(val):\n                            clean_values.append(\"\")\n                        elif val is None or (isinstance(val, str) and val == \"nan\"):\n                            clean_values.append(\"\")\n                        else:\n                            clean_values.append(str(val))\n                    ds_clean = ds_clean.assign_coords({coord: (\"sid\", clean_values)})\n\n        # Numeric coordinates can keep NaN if needed\n        for coord in [\"freq_center\", \"freq_min\", \"freq_max\"]:\n            if coord in ds_clean.coords:\n                # These are fine as-is since they're numeric\n                pass\n\n        return ds_clean\n\n    def safe_temporal_aggregate(\n        self,\n        group: str,\n        freq: str = \"1D\",\n        vars_to_aggregate: Sequence[str] = (\"VOD\",),\n        geometry_vars: Sequence[str] = (\"phi\", \"theta\"),\n        drop_empty: bool = True,\n        branch: str = \"main\",\n    ) -&gt; xr.Dataset:\n        \"\"\"Safely aggregate temporally irregular VOD data.\n\n        Parameters\n        ----------\n        group : str\n            Group name to aggregate.\n        freq : str, default \"1D\"\n            Resample frequency string.\n        vars_to_aggregate : Sequence[str], optional\n            Variables to aggregate using mean.\n        geometry_vars : Sequence[str], optional\n            Geometry variables to preserve via first() per bin.\n        drop_empty : bool, default True\n            Drop empty epochs after aggregation.\n        branch : str, default \"main\"\n            Branch name to read from.\n\n        Returns\n        -------\n        xr.Dataset\n            Aggregated dataset.\n        \"\"\"\n\n        with self.readonly_session(branch=branch) as session:\n            ds = xr.open_zarr(session.store, group=group, consolidated=False)\n\n            print(\n                f\"📦 Aggregating group '{group}' from branch '{branch}' → freq={freq}\"\n            )\n\n            # 1️⃣ Aggregate numeric variables\n            merged_vars = []\n            for var in vars_to_aggregate:\n                if var in ds:\n                    merged_vars.append(ds[var].resample(epoch=freq).mean())\n                else:\n                    print(f\"⚠️ Skipping missing variable: {var}\")\n            ds_agg = xr.merge(merged_vars)\n\n            # 2️⃣ Preserve geometry variables (use first() per bin)\n            for var in geometry_vars:\n                if var in ds:\n                    ds_agg[var] = ds[var].resample(epoch=freq).first()\n\n            # 3️⃣ Add remaining coordinates\n            for coord in ds.coords:\n                if coord not in ds_agg.coords and coord != \"epoch\":\n                    ds_agg[coord] = ds[coord]\n\n            # 4️⃣ Drop all-NaN epochs if requested\n            if drop_empty and \"VOD\" in ds_agg:\n                valid_mask = ds_agg[\"VOD\"].notnull().any(dim=\"sid\").compute()\n                ds_agg = ds_agg.isel(epoch=valid_mask)\n\n            print(f\"✅ Aggregation done: {dict(ds_agg.sizes)}\")\n            return ds_agg\n\n    def safe_temporal_aggregate_to_branch(\n        self,\n        source_group: str,\n        target_group: str,\n        target_branch: str,\n        freq: str = \"1D\",\n        overwrite: bool = False,\n        **kwargs: Any,\n    ) -&gt; xr.Dataset:\n        \"\"\"Aggregate a group and save to a new Icechunk branch/group.\n\n        Parameters\n        ----------\n        source_group : str\n            Source group name.\n        target_group : str\n            Target group name.\n        target_branch : str\n            Target branch name.\n        freq : str, default \"1D\"\n            Resample frequency string.\n        overwrite : bool, default False\n            Whether to overwrite an existing branch.\n        **kwargs : Any\n            Additional keyword args passed to safe_temporal_aggregate().\n\n        Returns\n        -------\n        xr.Dataset\n            Aggregated dataset written to the target branch.\n        \"\"\"\n\n        print(\n            f\"🚀 Creating new aggregated branch '{target_branch}' at '{target_group}'\"\n        )\n\n        # Compute safe aggregation\n        ds_agg = self.safe_temporal_aggregate(\n            group=source_group,\n            freq=freq,\n            **kwargs,\n        )\n\n        # Write to new branch\n        current_snapshot = next(self.repo.ancestry(branch=\"main\")).id\n        self.delete_branch(target_branch)\n        self.repo.create_branch(target_branch, current_snapshot)\n        with self.writable_session(target_branch) as session:\n            to_icechunk(\n                obj=ds_agg,\n                session=session,\n                group=target_group,\n                mode=\"w\",\n            )\n            session.commit(f\"Saved aggregated data to {target_group} at freq={freq}\")\n\n        print(\n            f\"✅ Saved aggregated dataset to branch '{target_branch}' \"\n            f\"(group '{target_group}')\"\n        )\n        return ds_agg\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.repo","level":3,"title":"<code>repo</code>  <code>property</code>","text":"<p>Get the repository instance.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.tree","level":3,"title":"<code>tree</code>  <code>property</code>","text":"<p>Display hierarchical tree of all branches, groups, and subgroups.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.__init__","level":3,"title":"<code>__init__(store_path, store_type='rinex_store', compression_level=None, compression_algorithm=None)</code>","text":"<p>Initialize the Icechunk store manager.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.__init__--parameters","level":5,"title":"Parameters","text":"<p>store_path : Path     Path to the Icechunk store directory. store_type : str, default \"rinex_store\"     Type of store (\"rinex_store\" or \"vod_store\"). compression_level : int | None, optional     Override default compression level. compression_algorithm : str | None, optional     Override default compression algorithm.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def __init__(\n    self,\n    store_path: Path,\n    store_type: str = \"rinex_store\",\n    compression_level: int | None = None,\n    compression_algorithm: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Icechunk store manager.\n\n    Parameters\n    ----------\n    store_path : Path\n        Path to the Icechunk store directory.\n    store_type : str, default \"rinex_store\"\n        Type of store (\"rinex_store\" or \"vod_store\").\n    compression_level : int | None, optional\n        Override default compression level.\n    compression_algorithm : str | None, optional\n        Override default compression algorithm.\n    \"\"\"\n    from canvod.utils.config import load_config\n\n    cfg = load_config()\n    ic_cfg = cfg.processing.icechunk\n    st_cfg = cfg.processing.storage\n\n    self.store_path = Path(store_path)\n    self.store_type = store_type\n    # Site name is parent directory name\n    self.site_name = self.store_path.parent.name\n\n    # Compression\n    self.compression_level = compression_level or ic_cfg.compression_level\n    compression_alg = compression_algorithm or ic_cfg.compression_algorithm\n    self.compression_algorithm = getattr(\n        icechunk.CompressionAlgorithm, compression_alg.capitalize()\n    )\n\n    # Chunk strategy\n    chunk_strategies = {\n        k: {\"epoch\": v.epoch, \"sid\": v.sid}\n        for k, v in ic_cfg.chunk_strategies.items()\n    }\n    self.chunk_strategy = chunk_strategies.get(store_type, {})\n\n    # Storage config cached for metadata rows\n    self._rinex_store_strategy = st_cfg.rinex_store_strategy\n    self._rinex_store_expire_days = st_cfg.rinex_store_expire_days\n    self._vod_store_strategy = st_cfg.vod_store_strategy\n\n    # Configure repository\n    self.config = icechunk.RepositoryConfig.default()\n    self.config.compression = icechunk.CompressionConfig(\n        level=self.compression_level, algorithm=self.compression_algorithm\n    )\n    self.config.inline_chunk_threshold_bytes = ic_cfg.inline_threshold\n    self.config.get_partial_values_concurrency = ic_cfg.get_concurrency\n\n    if ic_cfg.manifest_preload_enabled:\n        self.config.manifest = icechunk.ManifestConfig(\n            preload=icechunk.ManifestPreloadConfig(\n                max_total_refs=ic_cfg.manifest_preload_max_refs,\n                preload_if=icechunk.ManifestPreloadCondition.name_matches(\n                    ic_cfg.manifest_preload_pattern\n                ),\n            )\n        )\n        self._logger.info(\n            f\"Manifest preload enabled: {ic_cfg.manifest_preload_pattern}\"\n        )\n\n    self._repo = None\n    self._logger = get_logger(__name__)\n\n    # Remove .DS_Store files that corrupt icechunk ref listing on macOS\n    self._clean_ds_store()\n    self._ensure_store_exists()\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.readonly_session","level":3,"title":"<code>readonly_session(branch='main')</code>","text":"<p>Context manager for readonly sessions.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.readonly_session--parameters","level":5,"title":"Parameters","text":"<p>branch : str, default \"main\"     Branch name.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.readonly_session--returns","level":5,"title":"Returns","text":"<p>Generator[icechunk.ReadonlySession, None, None]     Readonly session context manager.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>@contextlib.contextmanager\ndef readonly_session(\n    self,\n    branch: str = \"main\",\n) -&gt; Generator[\"icechunk.ReadonlySession\"]:\n    \"\"\"Context manager for readonly sessions.\n\n    Parameters\n    ----------\n    branch : str, default \"main\"\n        Branch name.\n\n    Returns\n    -------\n    Generator[icechunk.ReadonlySession, None, None]\n        Readonly session context manager.\n    \"\"\"\n    session = self.repo.readonly_session(branch)\n    try:\n        self._logger.debug(f\"Opened readonly session for branch '{branch}'\")\n        yield session\n    finally:\n        self._logger.debug(f\"Closed readonly session for branch '{branch}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.writable_session","level":3,"title":"<code>writable_session(branch='main')</code>","text":"<p>Context manager for writable sessions.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.writable_session--parameters","level":5,"title":"Parameters","text":"<p>branch : str, default \"main\"     Branch name.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.writable_session--returns","level":5,"title":"Returns","text":"<p>Generator[icechunk.WritableSession, None, None]     Writable session context manager.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>@contextlib.contextmanager\ndef writable_session(\n    self,\n    branch: str = \"main\",\n) -&gt; Generator[\"icechunk.WritableSession\"]:\n    \"\"\"Context manager for writable sessions.\n\n    Parameters\n    ----------\n    branch : str, default \"main\"\n        Branch name.\n\n    Returns\n    -------\n    Generator[icechunk.WritableSession, None, None]\n        Writable session context manager.\n    \"\"\"\n    session = self.repo.writable_session(branch)\n    try:\n        self._logger.debug(f\"Opened writable session for branch '{branch}'\")\n        yield session\n    finally:\n        self._logger.debug(f\"Closed writable session for branch '{branch}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_branch_names","level":3,"title":"<code>get_branch_names()</code>","text":"<p>List all branches in the store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_branch_names--returns","level":5,"title":"Returns","text":"<p>list[str]     List of branch names.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_branch_names(self) -&gt; list[str]:\n    \"\"\"\n    List all branches in the store.\n\n    Returns\n    -------\n    list[str]\n        List of branch names.\n    \"\"\"\n    try:\n        storage_config = icechunk.local_filesystem_storage(self.store_path)\n        repo = icechunk.Repository.open(\n            storage=storage_config,\n        )\n\n        return list(repo.list_branches())\n    except Exception as e:\n        self._logger.warning(f\"Failed to list branches in {repr(self)}: {e}\")\n        warnings.warn(f\"Failed to list branches in {repr(self)}: {e}\")\n        return []\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_group_names","level":3,"title":"<code>get_group_names(branch=None)</code>","text":"<p>List all groups in the store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_group_names--parameters","level":5,"title":"Parameters","text":"<p>branch: Optional[str]     Repository branch to examine. Defaults to listing groups from all branches.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_group_names--returns","level":5,"title":"Returns","text":"<p>dict[str, list[str]]     Dictionary mapping branch names to lists of group names.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_group_names(self, branch: str | None = None) -&gt; dict[str, list[str]]:\n    \"\"\"\n    List all groups in the store.\n\n    Parameters\n    ----------\n    branch: Optional[str]\n        Repository branch to examine. Defaults to listing groups from all branches.\n\n    Returns\n    -------\n    dict[str, list[str]]\n        Dictionary mapping branch names to lists of group names.\n\n    \"\"\"\n    try:\n        if not branch:\n            branches = self.get_branch_names()\n        else:\n            branches = [branch]\n\n        storage_config = icechunk.local_filesystem_storage(self.store_path)\n        repo = icechunk.Repository.open(\n            storage=storage_config,\n        )\n\n        group_dict = {}\n        for br in branches:\n            with self.readonly_session(br) as session:\n                session = repo.readonly_session(br)\n                root = zarr.open(session.store, mode=\"r\")\n                group_dict[br] = list(root.group_keys())\n\n        return group_dict\n\n    except Exception as e:\n        self._logger.warning(f\"Failed to list groups in {repr(self)}: {e}\")\n        return {}\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.list_groups","level":3,"title":"<code>list_groups(branch='main')</code>","text":"<p>List all groups in a branch.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.list_groups--parameters","level":5,"title":"Parameters","text":"<p>branch : str     Branch name (default: \"main\")</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.list_groups--returns","level":5,"title":"Returns","text":"<p>list[str]     List of group names in the branch</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def list_groups(self, branch: str = \"main\") -&gt; list[str]:\n    \"\"\"\n    List all groups in a branch.\n\n    Parameters\n    ----------\n    branch : str\n        Branch name (default: \"main\")\n\n    Returns\n    -------\n    list[str]\n        List of group names in the branch\n    \"\"\"\n    group_dict = self.get_group_names(branch=branch)\n    if branch in group_dict:\n        return group_dict[branch]\n    return []\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.print_tree","level":3,"title":"<code>print_tree(max_depth=None)</code>","text":"<p>Display hierarchical tree of all branches, groups, and subgroups.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.print_tree--parameters","level":5,"title":"Parameters","text":"<p>max_depth : int | None     Maximum depth to display. None for unlimited depth.     - 0: Only show branches     - 1: Show branches and top-level groups     - 2: Show branches, groups, and first level of subgroups/arrays     - etc.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def print_tree(self, max_depth: int | None = None) -&gt; None:\n    \"\"\"\n    Display hierarchical tree of all branches, groups, and subgroups.\n\n    Parameters\n    ----------\n    max_depth : int | None\n        Maximum depth to display. None for unlimited depth.\n        - 0: Only show branches\n        - 1: Show branches and top-level groups\n        - 2: Show branches, groups, and first level of subgroups/arrays\n        - etc.\n    \"\"\"\n    try:\n        branches = self.get_branch_names()\n\n        for i, branch in enumerate(branches):\n            is_last_branch = i == len(branches) - 1\n            branch_prefix = \"└── \" if is_last_branch else \"├── \"\n\n            if max_depth is not None and max_depth &lt; 1:\n                continue\n\n            session = self.repo.readonly_session(branch)\n            root = zarr.open(session.store, mode=\"r\")\n\n            if i == 0:\n                sys.stdout.write(f\"{self.store_path}\\n\")\n\n            sys.stdout.write(f\"{branch_prefix}{branch}\\n\")\n            # Build tree recursively\n            branch_indent = \"    \" if is_last_branch else \"│   \"\n            self._build_tree(root, branch_indent, max_depth, current_depth=1)\n\n    except Exception as e:\n        self._logger.warning(f\"Failed to generate tree for {repr(self)}: {e}\")\n        sys.stdout.write(f\"Error generating tree: {e}\\n\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.group_exists","level":3,"title":"<code>group_exists(group_name, branch='main')</code>","text":"<p>Check if a group exists.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.group_exists--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group to check. branch : str, default \"main\"     Repository branch to examine.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.group_exists--returns","level":5,"title":"Returns","text":"<p>bool     True if the group exists, False otherwise.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def group_exists(self, group_name: str, branch: str = \"main\") -&gt; bool:\n    \"\"\"\n    Check if a group exists.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group to check.\n    branch : str, default \"main\"\n        Repository branch to examine.\n\n    Returns\n    -------\n    bool\n        True if the group exists, False otherwise.\n    \"\"\"\n    group_dict = self.get_group_names(branch)\n\n    # get_group_names returns dict like {'main': ['canopy_01', ...]}\n    if branch in group_dict:\n        exists = group_name in group_dict[branch]\n    else:\n        exists = False\n\n    self._logger.debug(\n        f\"Group '{group_name}' exists on branch '{branch}': {exists}\"\n    )\n    return exists\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.read_group","level":3,"title":"<code>read_group(group_name, branch='main', time_slice=None, chunks=None)</code>","text":"<p>Read data from a group.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.read_group--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group to read. branch : str, default \"main\"     Repository branch. time_slice : slice | None, optional     Optional time slice for filtering. chunks : dict[str, Any] | None, optional     Chunking specification (uses config defaults if None).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.read_group--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset from the group.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def read_group(\n    self,\n    group_name: str,\n    branch: str = \"main\",\n    time_slice: slice | None = None,\n    chunks: dict[str, Any] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read data from a group.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group to read.\n    branch : str, default \"main\"\n        Repository branch.\n    time_slice : slice | None, optional\n        Optional time slice for filtering.\n    chunks : dict[str, Any] | None, optional\n        Chunking specification (uses config defaults if None).\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset from the group.\n    \"\"\"\n    self._logger.info(f\"Reading group '{group_name}' from branch '{branch}'\")\n\n    with self.readonly_session(branch) as session:\n        # Use default chunking strategy if none provided\n        if chunks is None:\n            chunks = self.chunk_strategy or {\"epoch\": 34560, \"sid\": -1}\n\n        ds = xr.open_zarr(\n            session.store,\n            group=group_name,\n            chunks=chunks,\n            consolidated=False,\n        )\n\n        if time_slice is not None:\n            ds = ds.isel(epoch=time_slice)\n            self._logger.debug(f\"Applied time slice: {time_slice}\")\n\n        self._logger.info(\n            f\"Successfully read group '{group_name}' - shape: {dict(ds.sizes)}\"\n        )\n        return ds\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.read_group_deduplicated","level":3,"title":"<code>read_group_deduplicated(group_name, branch='main', keep='last', time_slice=None, chunks=None)</code>","text":"<p>Read data from a group with automatic deduplication.</p> <p>This method calls read_group() then removes duplicates using metadata table intelligence when available, falling back to simple epoch deduplication.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.read_group_deduplicated--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group to read. branch : str, default \"main\"     Repository branch. keep : str, default \"last\"     Deduplication strategy for duplicate epochs. time_slice : slice | None, optional     Optional time slice for filtering. chunks : dict[str, Any] | None, optional     Chunking specification (uses config defaults if None).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.read_group_deduplicated--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with duplicates removed (latest data only).</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def read_group_deduplicated(\n    self,\n    group_name: str,\n    branch: str = \"main\",\n    keep: str = \"last\",\n    time_slice: slice | None = None,\n    chunks: dict[str, Any] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read data from a group with automatic deduplication.\n\n    This method calls read_group() then removes duplicates using metadata table\n    intelligence when available, falling back to simple epoch deduplication.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group to read.\n    branch : str, default \"main\"\n        Repository branch.\n    keep : str, default \"last\"\n        Deduplication strategy for duplicate epochs.\n    time_slice : slice | None, optional\n        Optional time slice for filtering.\n    chunks : dict[str, Any] | None, optional\n        Chunking specification (uses config defaults if None).\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with duplicates removed (latest data only).\n    \"\"\"\n\n    if keep not in [\"last\"]:\n        raise ValueError(\"Currently only 'last' is supported for keep parameter.\")\n\n    self._logger.info(f\"Reading group '{group_name}' with deduplication\")\n\n    # First, read the raw data\n    ds = self.read_group(group_name, branch, time_slice, chunks)\n\n    # Then deduplicate using metadata table intelligence\n    with self.readonly_session(branch) as session:\n        try:\n            zmeta = zarr.open_group(session.store, mode=\"r\")[\n                f\"{group_name}/metadata/table\"\n            ]\n\n            # Load metadata and get latest entries for each time range\n            data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n            df = pl.DataFrame(data)\n\n            # Ensure datetime dtypes\n            df = df.with_columns(\n                [\n                    pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n                    pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n                ]\n            )\n\n            # Get latest entry for each unique (start, end) combination\n            latest_entries = df.sort(\"written_at\").unique(\n                subset=[\"start\", \"end\"], keep=keep\n            )\n\n            if latest_entries.height &gt; 0:\n                # Create time masks for latest data only\n                time_masks = []\n                for row in latest_entries.iter_rows(named=True):\n                    start_time = row[\"start\"]\n                    end_time = row[\"end\"]\n                    mask = (ds.epoch &gt;= start_time) &amp; (ds.epoch &lt;= end_time)\n                    time_masks.append(mask)\n\n                # Combine all masks with OR logic\n                if time_masks:\n                    combined_mask = time_masks[0]\n                    for mask in time_masks[1:]:\n                        combined_mask = combined_mask | mask\n                    ds = ds.isel(epoch=combined_mask)\n\n                    self._logger.info(\n                        \"Deduplicated using metadata table: kept \"\n                        f\"{len(latest_entries)} time ranges\"\n                    )\n\n        except Exception as e:\n            # Fall back to simple deduplication\n            self._logger.warning(\n                f\"Metadata-based deduplication failed, using simple approach: {e}\"\n            )\n            ds = ds.drop_duplicates(\"epoch\", keep=\"last\")\n            self._logger.info(\"Applied simple epoch deduplication (keep='last')\")\n\n    return ds\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.write_dataset","level":3,"title":"<code>write_dataset(dataset, group_name, session, mode='a', chunks=None)</code>","text":"<p>Write a dataset to Icechunk with proper chunking.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.write_dataset--parameters","level":5,"title":"Parameters","text":"<p>dataset : xr.Dataset     Dataset to write group_name : str     Group path in store session : Any     Active writable session or store handle. mode : str     Write mode: 'w' (overwrite) or 'a' (append) chunks : dict[str, int] | None     Chunking spec. If None, uses store's chunk_strategy.     Example: {'epoch': 34560, 'sid': -1}</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def write_dataset(\n    self,\n    dataset: xr.Dataset,\n    group_name: str,\n    session: Any,\n    mode: str = \"a\",\n    chunks: dict[str, int] | None = None,\n) -&gt; None:\n    \"\"\"\n    Write a dataset to Icechunk with proper chunking.\n\n    Parameters\n    ----------\n    dataset : xr.Dataset\n        Dataset to write\n    group_name : str\n        Group path in store\n    session : Any\n        Active writable session or store handle.\n    mode : str\n        Write mode: 'w' (overwrite) or 'a' (append)\n    chunks : dict[str, int] | None\n        Chunking spec. If None, uses store's chunk_strategy.\n        Example: {'epoch': 34560, 'sid': -1}\n    \"\"\"\n    # Use explicit chunks, or fall back to store's chunk strategy\n    if chunks is None:\n        chunks = self.chunk_strategy\n\n    # Apply chunking if strategy defined\n    if chunks:\n        dataset = dataset.chunk(chunks)\n        self._logger.info(f\"Rechunked to {dict(dataset.chunks)} before write\")\n\n    # Normalize encodings\n    dataset = self._normalize_encodings(dataset)\n\n    # Calculate dataset metrics for tracing\n    dataset_size_mb = dataset.nbytes / 1024 / 1024\n    num_variables = len(dataset.data_vars)\n\n    # Write to Icechunk with OpenTelemetry tracing\n    try:\n        from canvodpy.utils.telemetry import trace_icechunk_write\n\n        with trace_icechunk_write(\n            group_name=group_name,\n            dataset_size_mb=dataset_size_mb,\n            num_variables=num_variables,\n        ):\n            to_icechunk(dataset, session, group=group_name, mode=mode)\n    except ImportError:\n        # Fallback if telemetry not available\n        to_icechunk(dataset, session, group=group_name, mode=mode)\n\n    self._logger.info(f\"Wrote dataset to group '{group_name}' (mode={mode})\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.write_initial_group","level":3,"title":"<code>write_initial_group(dataset, group_name, branch='main', commit_message=None)</code>","text":"<p>Write initial data to a new group.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def write_initial_group(\n    self,\n    dataset: xr.Dataset,\n    group_name: str,\n    branch: str = \"main\",\n    commit_message: str | None = None,\n) -&gt; None:\n    \"\"\"Write initial data to a new group.\"\"\"\n    if self.group_exists(group_name, branch):\n        raise ValueError(\n            f\"Group '{group_name}' already exists. Use append_to_group() instead.\"\n        )\n\n    with self.writable_session(branch) as session:\n        dataset = self._normalize_encodings(dataset)\n\n        rinex_hash = dataset.attrs.get(\"RINEX File Hash\")\n        if rinex_hash is None:\n            raise ValueError(\"Dataset missing 'RINEX File Hash' attribute\")\n        start = dataset.epoch.min().values\n        end = dataset.epoch.max().values\n\n        to_icechunk(dataset, session, group=group_name, mode=\"w\")\n\n        if commit_message is None:\n            version = get_version_from_pyproject()\n            commit_message = f\"[v{version}] Initial commit to group '{group_name}'\"\n\n        snapshot_id = session.commit(commit_message)\n\n        self.append_metadata(\n            group_name=group_name,\n            rinex_hash=rinex_hash,\n            start=start,\n            end=end,\n            snapshot_id=snapshot_id,\n            action=\"write\",  # Correct action for initial data\n            commit_msg=commit_message,\n            dataset_attrs=dataset.attrs,\n        )\n\n    self._logger.info(\n        f\"Created group '{group_name}' with {len(dataset.epoch)} epochs, \"\n        f\"hash={rinex_hash}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.backup_metadata_table","level":3,"title":"<code>backup_metadata_table(group_name, session)</code>","text":"<p>Backup the metadata table to a Polars DataFrame.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.backup_metadata_table--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Group name. session : Any     Active session for reading.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.backup_metadata_table--returns","level":5,"title":"Returns","text":"<p>pl.DataFrame | None     DataFrame with metadata rows, or None if missing.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def backup_metadata_table(\n    self,\n    group_name: str,\n    session: Any,\n) -&gt; pl.DataFrame | None:\n    \"\"\"Backup the metadata table to a Polars DataFrame.\n\n    Parameters\n    ----------\n    group_name : str\n        Group name.\n    session : Any\n        Active session for reading.\n\n    Returns\n    -------\n    pl.DataFrame | None\n        DataFrame with metadata rows, or None if missing.\n    \"\"\"\n    try:\n        zroot = zarr.open_group(session.store, mode=\"r\")\n        meta_group_path = f\"{group_name}/metadata/table\"\n\n        if (\n            \"metadata\" not in zroot[group_name]\n            or \"table\" not in zroot[group_name][\"metadata\"]\n        ):\n            self._logger.info(\n                \"No metadata table found for group \"\n                f\"'{group_name}' - nothing to backup\"\n            )\n            return None\n\n        zmeta = zroot[meta_group_path]\n\n        # Load all columns into a dictionary\n        data = {}\n        for col_name in zmeta.array_keys():\n            data[col_name] = zmeta[col_name][:]\n\n        # Convert to Polars DataFrame\n        df = pl.DataFrame(data)\n\n        self._logger.info(\n            \"Backed up metadata table with \"\n            f\"{df.height} rows for group '{group_name}'\"\n        )\n        return df\n\n    except Exception as e:\n        self._logger.warning(\n            f\"Failed to backup metadata table for group '{group_name}': {e}\"\n        )\n        return None\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.restore_metadata_table","level":3,"title":"<code>restore_metadata_table(group_name, df, session)</code>","text":"<p>Restore the metadata table from a Polars DataFrame.</p> <p>This recreates the full Zarr structure for the metadata table.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.restore_metadata_table--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Group name. df : pl.DataFrame     Metadata table to restore. session : Any     Active session for writing.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.restore_metadata_table--returns","level":5,"title":"Returns","text":"<p>None</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def restore_metadata_table(\n    self,\n    group_name: str,\n    df: pl.DataFrame,\n    session: Any,\n) -&gt; None:\n    \"\"\"Restore the metadata table from a Polars DataFrame.\n\n    This recreates the full Zarr structure for the metadata table.\n\n    Parameters\n    ----------\n    group_name : str\n        Group name.\n    df : pl.DataFrame\n        Metadata table to restore.\n    session : Any\n        Active session for writing.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if df is None or df.height == 0:\n        self._logger.info(f\"No metadata to restore for group '{group_name}'\")\n        return\n\n    try:\n        zroot = zarr.open_group(session.store, mode=\"a\")\n        meta_group_path = f\"{group_name}/metadata/table\"\n\n        # Create the metadata subgroup\n        zmeta = zroot.require_group(meta_group_path)\n\n        # Create all arrays from the DataFrame\n        for col_name in df.columns:\n            col_data = df[col_name]\n\n            if col_name == \"index\":\n                # Index column as int64\n                arr = col_data.to_numpy().astype(\"i8\")\n                dtype = \"i8\"\n            elif col_name in (\"start\", \"end\"):\n                # Datetime columns\n                arr = col_data.to_numpy().astype(\"datetime64[ns]\")\n                dtype = \"M8[ns]\"\n            else:\n                # String columns - use VariableLengthUTF8\n                arr = col_data.to_list()  # Convert to list for VariableLengthUTF8\n                dtype = VariableLengthUTF8()\n\n            # Create the array\n            zmeta.create_array(\n                name=col_name,\n                shape=(len(arr),),\n                dtype=dtype,\n                chunks=(1024,),\n                overwrite=True,\n            )\n\n            # Write the data\n            zmeta[col_name][:] = arr\n\n        self._logger.info(\n            \"Restored metadata table with \"\n            f\"{df.height} rows for group '{group_name}'\"\n        )\n\n    except Exception as e:\n        self._logger.error(\n            f\"Failed to restore metadata table for group '{group_name}': {e}\"\n        )\n        raise RuntimeError(f\"Critical error: could not restore metadata table: {e}\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.overwrite_file_in_group","level":3,"title":"<code>overwrite_file_in_group(dataset, group_name, rinex_hash, start, end, branch='main', commit_message=None)</code>","text":"<p>Overwrite a file's contribution to the group (same hash, new epoch range).</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def overwrite_file_in_group(\n    self,\n    dataset: xr.Dataset,\n    group_name: str,\n    rinex_hash: str,\n    start: np.datetime64,\n    end: np.datetime64,\n    branch: str = \"main\",\n    commit_message: str | None = None,\n) -&gt; None:\n    \"\"\"Overwrite a file's contribution to the group (same hash, new epoch range).\"\"\"\n\n    dataset = self._normalize_encodings(dataset)\n\n    # --- Step 3: rewrite store ---\n    with self.writable_session(branch) as session:\n        ds_from_store = xr.open_zarr(\n            session.store, group=group_name, consolidated=False\n        )\n\n        # Backup the existing metadata table\n        metadata_backup = self.backup_metadata_table(group_name, session)\n\n        mask = (ds_from_store.epoch.values &lt; start) | (\n            ds_from_store.epoch.values &gt; end\n        )\n        ds_from_store_cleansed = ds_from_store.isel(epoch=mask)\n        ds_from_store_cleansed = self._normalize_encodings(ds_from_store_cleansed)\n\n        # Check if any epochs remain after cleansing, then write leftovers.\n        if ds_from_store_cleansed.sizes.get(\"epoch\", 0) &gt; 0:\n            to_icechunk(ds_from_store_cleansed, session, group=group_name, mode=\"w\")\n        # no epochs left, reset group to empty\n        else:\n            to_icechunk(dataset.isel(epoch=[]), session, group=group_name, mode=\"w\")\n\n        # write back the backed up metadata table\n        self.restore_metadata_table(group_name, metadata_backup, session)\n\n        # Append the new dataset\n        to_icechunk(dataset, session, group=group_name, append_dim=\"epoch\")\n\n        if commit_message is None:\n            version = get_version_from_pyproject()\n            commit_message = (\n                f\"[v{version}] Overwrote file {rinex_hash} in group '{group_name}'\"\n            )\n\n        snapshot_id = session.commit(commit_message)\n\n        self.append_metadata(\n            group_name=group_name,\n            rinex_hash=rinex_hash,\n            start=start,\n            end=end,\n            snapshot_id=snapshot_id,\n            action=\"overwrite\",\n            commit_msg=commit_message,\n            dataset_attrs=dataset.attrs,\n        )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_group_info","level":3,"title":"<code>get_group_info(group_name, branch='main')</code>","text":"<p>Get metadata about a group.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_group_info--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group. branch : str, default \"main\"     Repository branch to examine.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_group_info--returns","level":5,"title":"Returns","text":"<p>dict[str, Any]     Group metadata.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_group_info--raises","level":5,"title":"Raises","text":"<p>ValueError     If the group does not exist.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_group_info(self, group_name: str, branch: str = \"main\") -&gt; dict[str, Any]:\n    \"\"\"\n    Get metadata about a group.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group.\n    branch : str, default \"main\"\n        Repository branch to examine.\n\n    Returns\n    -------\n    dict[str, Any]\n        Group metadata.\n\n    Raises\n    ------\n    ValueError\n        If the group does not exist.\n    \"\"\"\n    if not self.group_exists(group_name, branch):\n        raise ValueError(f\"Group '{group_name}' does not exist\")\n\n    ds = self.read_group(group_name, branch)\n\n    info = {\n        \"group_name\": group_name,\n        \"store_type\": self.store_type,\n        \"dimensions\": dict(ds.sizes),\n        \"variables\": list(ds.data_vars.keys()),\n        \"coordinates\": list(ds.coords.keys()),\n        \"attributes\": dict(ds.attrs),\n    }\n\n    # Add temporal information if epoch dimension exists\n    if \"epoch\" in ds.sizes:\n        info[\"temporal_info\"] = {\n            \"start\": str(ds.epoch.min().values),\n            \"end\": str(ds.epoch.max().values),\n            \"count\": ds.sizes[\"epoch\"],\n            \"resolution\": str(ds.epoch.diff(\"epoch\").median().values),\n        }\n\n    return info\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.rel_path_for_commit","level":3,"title":"<code>rel_path_for_commit(file_path)</code>","text":"<p>Generate relative path for commit messages.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.rel_path_for_commit--parameters","level":5,"title":"Parameters","text":"<p>file_path : Path     Full file path.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.rel_path_for_commit--returns","level":5,"title":"Returns","text":"<p>str     Relative path string with log_path_depth parts.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def rel_path_for_commit(self, file_path: Path) -&gt; str:\n    \"\"\"\n    Generate relative path for commit messages.\n\n    Parameters\n    ----------\n    file_path : Path\n        Full file path.\n\n    Returns\n    -------\n    str\n        Relative path string with log_path_depth parts.\n    \"\"\"\n    depth = load_config().processing.logging.log_path_depth\n    return str(Path(*file_path.parts[-depth:]))\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_store_stats","level":3,"title":"<code>get_store_stats()</code>","text":"<p>Get statistics about the store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_store_stats--returns","level":5,"title":"Returns","text":"<p>dict[str, Any]     Store statistics.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_store_stats(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get statistics about the store.\n\n    Returns\n    -------\n    dict[str, Any]\n        Store statistics.\n    \"\"\"\n    groups = self.get_group_names()\n    stats = {\n        \"store_path\": str(self.store_path),\n        \"store_type\": self.store_type,\n        \"compression_level\": self.compression_level,\n        \"compression_algorithm\": self.compression_algorithm.name,\n        \"total_groups\": len(groups),\n        \"groups\": groups,\n    }\n\n    # Add group-specific stats\n    for group_name in groups:\n        try:\n            info = self.get_group_info(group_name)\n            stats[f\"group_{group_name}\"] = {\n                \"dimensions\": info[\"dimensions\"],\n                \"variables_count\": len(info[\"variables\"]),\n                \"has_temporal_data\": \"temporal_info\" in info,\n            }\n        except Exception as e:\n            self._logger.warning(\n                f\"Failed to get stats for group '{group_name}': {e}\"\n            )\n\n    return stats\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.append_to_group","level":3,"title":"<code>append_to_group(dataset, group_name, append_dim='epoch', branch='main', action='write', commit_message=None)</code>","text":"<p>Append data to an existing group.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def append_to_group(\n    self,\n    dataset: xr.Dataset,\n    group_name: str,\n    append_dim: str = \"epoch\",\n    branch: str = \"main\",\n    action: str = \"write\",\n    commit_message: str | None = None,\n) -&gt; None:\n    \"\"\"Append data to an existing group.\"\"\"\n    if not self.group_exists(group_name, branch):\n        raise ValueError(\n            f\"Group '{group_name}' does not exist. Use write_initial_group() first.\"\n        )\n\n    dataset = self._normalize_encodings(dataset)\n\n    rinex_hash = dataset.attrs.get(\"RINEX File Hash\")\n    if rinex_hash is None:\n        raise ValueError(\"Dataset missing 'RINEX File Hash' attribute\")\n    start = dataset.epoch.min().values\n    end = dataset.epoch.max().values\n\n    with self.writable_session(branch) as session:\n        to_icechunk(dataset, session, group=group_name, append_dim=append_dim)\n\n        if commit_message is None and action == \"write\":\n            version = get_version_from_pyproject()\n            commit_message = f\"[v{version}] Wrote to group '{group_name}'\"\n        elif commit_message is None and action != \"append\":\n            version = get_version_from_pyproject()\n            commit_message = f\"[v{version}] Appended to group '{group_name}'\"\n\n        snapshot_id = session.commit(commit_message)\n\n        self.append_metadata(\n            group_name=group_name,\n            rinex_hash=rinex_hash,\n            start=start,\n            end=end,\n            snapshot_id=snapshot_id,\n            action=action,\n            commit_msg=commit_message,\n            dataset_attrs=dataset.attrs,\n        )\n\n    if action == \"append\":\n        self._logger.info(\n            f\"Appended {len(dataset.epoch)} epochs to group '{group_name}', \"\n            f\"hash={rinex_hash}\"\n        )\n    elif action == \"write\":\n        self._logger.info(\n            f\"Wrote {len(dataset.epoch)} epochs to group '{group_name}', \"\n            f\"hash={rinex_hash}\"\n        )\n    else:\n        self._logger.info(\n            f\"Action '{action}' completed for group '{group_name}', \"\n            f\"hash={rinex_hash}\"\n        )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.append_metadata","level":3,"title":"<code>append_metadata(group_name, rinex_hash, start, end, snapshot_id, action, commit_msg, dataset_attrs, branch='main')</code>","text":"<p>Append a metadata row into the group_name/metadata/table.</p> Schema <p>index           int64 (continuous row id) rinex_hash      str   (UTF-8, VariableLengthUTF8) start           datetime64[ns] end             datetime64[ns] snapshot_id     str   (UTF-8) action          str   (UTF-8, e.g. \"insert\"|\"append\"|\"overwrite\"|\"skip\") commit_msg      str   (UTF-8) written_at      str   (UTF-8, ISO8601 with timezone) write_strategy  str   (UTF-8, RINEX_STORE_STRATEGY or VOD_STORE_STRATEGY) attrs           str   (UTF-8, JSON dump of dataset attrs)</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def append_metadata(\n    self,\n    group_name: str,\n    rinex_hash: str,\n    start: np.datetime64,\n    end: np.datetime64,\n    snapshot_id: str,\n    action: str,\n    commit_msg: str,\n    dataset_attrs: dict,\n    branch: str = \"main\",\n) -&gt; None:\n    \"\"\"\n    Append a metadata row into the group_name/metadata/table.\n\n    Schema:\n        index           int64 (continuous row id)\n        rinex_hash      str   (UTF-8, VariableLengthUTF8)\n        start           datetime64[ns]\n        end             datetime64[ns]\n        snapshot_id     str   (UTF-8)\n        action          str   (UTF-8, e.g. \"insert\"|\"append\"|\"overwrite\"|\"skip\")\n        commit_msg      str   (UTF-8)\n        written_at      str   (UTF-8, ISO8601 with timezone)\n        write_strategy  str   (UTF-8, RINEX_STORE_STRATEGY or VOD_STORE_STRATEGY)\n        attrs           str   (UTF-8, JSON dump of dataset attrs)\n    \"\"\"\n    written_at = datetime.now().astimezone().isoformat()\n\n    row = {\n        \"rinex_hash\": str(rinex_hash),\n        \"start\": np.datetime64(start, \"ns\"),\n        \"end\": np.datetime64(end, \"ns\"),\n        \"snapshot_id\": str(snapshot_id),\n        \"action\": str(action),\n        \"commit_msg\": str(commit_msg),\n        \"written_at\": written_at,\n        \"write_strategy\": str(self._rinex_store_strategy)\n        if self.store_type == \"rinex_store\"\n        else str(self._vod_store_strategy),\n        \"attrs\": json.dumps(dataset_attrs, default=str),\n    }\n    df_row = pl.DataFrame([row])\n\n    with self.writable_session(branch) as session:\n        zroot = zarr.open_group(session.store, mode=\"a\")\n        meta_group_path = f\"{group_name}/metadata/table\"\n\n        if (\n            \"metadata\" not in zroot[group_name]\n            or \"table\" not in zroot[group_name][\"metadata\"]\n        ):\n            # --- First time: create arrays with correct dtypes ---\n            zmeta = zroot.require_group(meta_group_path)\n\n            # index counter\n            zmeta.create_array(\n                name=\"index\", shape=(0,), dtype=\"i8\", chunks=(1024,), overwrite=True\n            )\n            zmeta[\"index\"].append([0])\n\n            for col in df_row.columns:\n                if col in (\"start\", \"end\"):\n                    dtype = \"M8[ns]\"\n                    arr = np.array(df_row[col].to_numpy(), dtype=dtype)\n                else:\n                    dtype = VariableLengthUTF8()\n                    arr = df_row[col].to_list()\n\n                zmeta.create_array(\n                    name=col,\n                    shape=(0,),\n                    dtype=dtype,\n                    chunks=(1024,),\n                    overwrite=True,\n                )\n                zmeta[col].append(arr)\n\n        else:\n            # --- Append to existing ---\n            zmeta = zroot[meta_group_path]\n\n            # index increment\n            current_len = zmeta[\"index\"].shape[0]\n            next_idx = current_len\n            zmeta[\"index\"].append([next_idx])\n\n            for col in df_row.columns:\n                if col in (\"start\", \"end\"):\n                    arr = np.array(df_row[col].to_numpy(), dtype=\"M8[ns]\")\n                else:\n                    arr = df_row[col].to_list()\n                zmeta[col].append(arr)\n\n        session.commit(f\"Appended metadata row for {group_name}, hash={rinex_hash}\")\n\n    self._logger.info(\n        f\"Metadata appended for group '{group_name}': \"\n        f\"hash={rinex_hash}, snapshot={snapshot_id}, action={action}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.append_metadata_bulk","level":3,"title":"<code>append_metadata_bulk(group_name, rows, session=None)</code>","text":"<p>Append multiple metadata rows in one commit.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.append_metadata_bulk--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Group name (e.g. \"canopy\", \"reference\") rows : list[dict[str, Any]]     List of metadata records matching the schema used in     append_metadata(). session : icechunk.WritableSession, optional     If provided, rows are written into this session (caller commits later).     If None, this method opens its own writable session and commits once.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def append_metadata_bulk(\n    self,\n    group_name: str,\n    rows: list[dict[str, Any]],\n    session: Optional[\"icechunk.WritableSession\"] = None,\n) -&gt; None:\n    \"\"\"\n    Append multiple metadata rows in one commit.\n\n    Parameters\n    ----------\n    group_name : str\n        Group name (e.g. \"canopy\", \"reference\")\n    rows : list[dict[str, Any]]\n        List of metadata records matching the schema used in\n        append_metadata().\n    session : icechunk.WritableSession, optional\n        If provided, rows are written into this session (caller commits later).\n        If None, this method opens its own writable session and commits once.\n    \"\"\"\n    if not rows:\n        self._logger.info(f\"No metadata rows to append for group '{group_name}'\")\n        return\n\n    # Ensure datetime conversions for consistency\n    for row in rows:\n        if isinstance(row.get(\"start\"), str):\n            row[\"start\"] = np.datetime64(row[\"start\"])\n        if isinstance(row.get(\"end\"), str):\n            row[\"end\"] = np.datetime64(row[\"end\"])\n        if \"written_at\" not in row:\n            row[\"written_at\"] = datetime.now(UTC).isoformat()\n\n    # Prepare the Polars DataFrame\n    df = pl.DataFrame(rows)\n\n    def _do_append(session_obj: \"icechunk.WritableSession\") -&gt; None:\n        \"\"\"Append metadata rows to a writable session.\n\n        Parameters\n        ----------\n        session_obj : icechunk.WritableSession\n            Writable session to update.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        zroot = zarr.open_group(session_obj.store, mode=\"a\")\n        meta_group_path = f\"{group_name}/metadata/table\"\n        zmeta = zroot.require_group(meta_group_path)\n\n        start_index = 0\n        if \"index\" in zmeta:\n            existing_len = zmeta[\"index\"].shape[0]\n            start_index = (\n                int(zmeta[\"index\"][-1].item()) + 1 if existing_len &gt; 0 else 0\n            )\n\n        # Assign sequential indices\n        df_with_index = df.with_columns(\n            (pl.arange(start_index, start_index + df.height)).alias(\"index\")\n        )\n\n        # Write each column\n        for col_name in df_with_index.columns:\n            col_data = df_with_index[col_name]\n\n            if col_name == \"index\":\n                dtype = \"i8\"\n                arr = col_data.to_numpy().astype(dtype)\n            elif col_name in (\"start\", \"end\"):\n                dtype = \"M8[ns]\"\n                arr = col_data.to_numpy().astype(dtype)\n            else:\n                # strings / jsons / ids\n                dtype = VariableLengthUTF8()\n                arr = col_data.to_list()\n\n            if col_name not in zmeta:\n                # Create array if it doesn't exist\n                zmeta.create_array(\n                    name=col_name,\n                    shape=(0,),\n                    dtype=dtype,\n                    chunks=(1024,),\n                    overwrite=True,\n                )\n\n            # Resize and append\n            old_len = zmeta[col_name].shape[0]\n            new_len = old_len + len(arr)\n            zmeta[col_name].resize(new_len)\n            zmeta[col_name][old_len:new_len] = arr\n\n        self._logger.info(\n            f\"Appended {df_with_index.height} metadata rows to group '{group_name}'\"\n        )\n\n    if session is not None:\n        _do_append(session)\n    else:\n        with self.writable_session() as sess:\n            _do_append(sess)\n            sess.commit(f\"Bulk metadata append for {group_name}\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.load_metadata","level":3,"title":"<code>load_metadata(store, group_name)</code>","text":"<p>Load metadata directly from Zarr into a Polars DataFrame.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.load_metadata--parameters","level":5,"title":"Parameters","text":"<p>store : Any     Zarr store or session store handle. group_name : str     Group name.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.load_metadata--returns","level":5,"title":"Returns","text":"<p>pl.DataFrame     Metadata table.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def load_metadata(self, store: Any, group_name: str) -&gt; pl.DataFrame:\n    \"\"\"Load metadata directly from Zarr into a Polars DataFrame.\n\n    Parameters\n    ----------\n    store : Any\n        Zarr store or session store handle.\n    group_name : str\n        Group name.\n\n    Returns\n    -------\n    pl.DataFrame\n        Metadata table.\n    \"\"\"\n    zroot = zarr.open_group(store, mode=\"r\")\n    zmeta = zroot[f\"{group_name}/metadata/table\"]\n\n    # Read all columns into a dict of numpy arrays\n    data = {col: zmeta[col][...] for col in zmeta.array_keys()}\n\n    # Build Polars DataFrame\n    df = pl.DataFrame(data)\n\n    # Convert numeric datetime64 columns back to proper Polars datetimes\n    if df[\"start\"].dtype in (pl.Int64, pl.Float64):\n        df = df.with_columns(pl.col(\"start\").cast(pl.Datetime(\"ns\")))\n    if df[\"end\"].dtype in (pl.Int64, pl.Float64):\n        df = df.with_columns(pl.col(\"end\").cast(pl.Datetime(\"ns\")))\n    if df[\"written_at\"].dtype == pl.Utf8:\n        df = df.with_columns(pl.col(\"written_at\").str.to_datetime(\"%+\"))\n    return df\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.read_metadata_table","level":3,"title":"<code>read_metadata_table(session, group_name)</code>","text":"<p>Read the metadata table from a session.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.read_metadata_table--parameters","level":5,"title":"Parameters","text":"<p>session : Any     Active session for reading. group_name : str     Group name.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.read_metadata_table--returns","level":5,"title":"Returns","text":"<p>pl.DataFrame     Metadata table.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def read_metadata_table(self, session: Any, group_name: str) -&gt; pl.DataFrame:\n    \"\"\"Read the metadata table from a session.\n\n    Parameters\n    ----------\n    session : Any\n        Active session for reading.\n    group_name : str\n        Group name.\n\n    Returns\n    -------\n    pl.DataFrame\n        Metadata table.\n    \"\"\"\n    zmeta = zarr.open_group(\n        session.store,\n        mode=\"r\",\n    )[f\"{group_name}/metadata/table\"]\n\n    data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n    df = pl.DataFrame(data)\n\n    # Ensure start/end are proper datetime\n    df = df.with_columns(\n        [\n            pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n            pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n        ]\n    )\n    return df\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.metadata_row_exists","level":3,"title":"<code>metadata_row_exists(group_name, rinex_hash, start, end, branch='main')</code>","text":"<p>Check whether a (start, end) interval exists in group metadata.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.metadata_row_exists--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Icechunk group name. rinex_hash : str     Hash of the current RINEX dataset. start : np.datetime64     Start time for the interval. end : np.datetime64     End time for the interval. branch : str, default \"main\"     Branch name in the Icechunk repository.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.metadata_row_exists--returns","level":5,"title":"Returns","text":"<p>tuple[bool, pl.DataFrame]     Existence flag and the matching metadata rows.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.metadata_row_exists--raises","level":5,"title":"Raises","text":"<p>ValueError     If a conflicting hash is found for the same interval.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.metadata_row_exists--notes","level":5,"title":"Notes","text":"<p>The metadata table is cast to <code>Datetime(\"ns\")</code> for <code>start</code> and <code>end</code> before filtering.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def metadata_row_exists(\n    self,\n    group_name: str,\n    rinex_hash: str,\n    start: np.datetime64,\n    end: np.datetime64,\n    branch: str = \"main\",\n) -&gt; tuple[bool, pl.DataFrame]:\n    \"\"\"\n    Check whether a (start, end) interval exists in group metadata.\n\n    Parameters\n    ----------\n    group_name : str\n        Icechunk group name.\n    rinex_hash : str\n        Hash of the current RINEX dataset.\n    start : np.datetime64\n        Start time for the interval.\n    end : np.datetime64\n        End time for the interval.\n    branch : str, default \"main\"\n        Branch name in the Icechunk repository.\n\n    Returns\n    -------\n    tuple[bool, pl.DataFrame]\n        Existence flag and the matching metadata rows.\n\n    Raises\n    ------\n    ValueError\n        If a conflicting hash is found for the same interval.\n\n    Notes\n    -----\n    The metadata table is cast to `Datetime(\"ns\")` for `start` and `end`\n    before filtering.\n    \"\"\"\n    with self.readonly_session(branch) as session:\n        try:\n            zmeta = zarr.open_group(session.store, mode=\"r\")[\n                f\"{group_name}/metadata/table\"\n            ]\n        except Exception:\n            return False, pl.DataFrame()\n\n        # Load all arrays into a dict\n        data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n        df = pl.DataFrame(data)\n\n        # Ensure datetime dtypes\n        df = df.with_columns(\n            [\n                pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n                pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n            ]\n        )\n\n        # Step 1: filter by start+end\n        matches = df.filter(\n            (pl.col(\"start\") == np.datetime64(start, \"ns\"))\n            &amp; (pl.col(\"end\") == np.datetime64(end, \"ns\"))\n        )\n\n        if matches.is_empty():\n            return False, matches\n\n        # Step 2: check hash consistency\n        unique_hashes = matches.select(\"rinex_hash\").unique()\n\n        if (\n            unique_hashes.height &gt; 1\n            or unique_hashes.item(0, \"rinex_hash\") != rinex_hash\n        ):\n            existing_hashes = unique_hashes.to_series().to_list()\n            raise ValueError(\n                \"Metadata conflict: rows with start=\"\n                f\"{start}, end={end} exist but hash differs \"\n                f\"(existing={existing_hashes}, new={rinex_hash})\"\n            )\n\n        return True, matches\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.batch_check_existing","level":3,"title":"<code>batch_check_existing(group_name, file_hashes)</code>","text":"<p>Check which file hashes already exist in metadata.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def batch_check_existing(self, group_name: str, file_hashes: list[str]) -&gt; set[str]:\n    \"\"\"Check which file hashes already exist in metadata.\"\"\"\n\n    try:\n        with self.readonly_session(\"main\") as session:\n            df = self.load_metadata(session.store, group_name)\n\n            # Filter to matching hashes\n            existing = df.filter(pl.col(\"rinex_hash\").is_in(file_hashes))\n            return set(existing[\"rinex_hash\"].to_list())\n\n    except (KeyError, zarr.errors.GroupNotFoundError, Exception):\n        # Branch/group/metadata doesn't exist yet (fresh store)\n        return set()\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.append_metadata_bulk_store","level":3,"title":"<code>append_metadata_bulk_store(group_name, rows, store)</code>","text":"<p>Append metadata rows into an open transaction store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.append_metadata_bulk_store--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Group name (e.g. \"canopy\", \"reference\"). rows : list[dict[str, Any]]     Metadata rows to append. store : Any     Open Icechunk transaction store.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def append_metadata_bulk_store(\n    self,\n    group_name: str,\n    rows: list[dict[str, Any]],\n    store: Any,\n) -&gt; None:\n    \"\"\"\n    Append metadata rows into an open transaction store.\n\n    Parameters\n    ----------\n    group_name : str\n        Group name (e.g. \"canopy\", \"reference\").\n    rows : list[dict[str, Any]]\n        Metadata rows to append.\n    store : Any\n        Open Icechunk transaction store.\n    \"\"\"\n    if not rows:\n        return\n\n    zroot = zarr.open_group(store, mode=\"a\")\n    zmeta = zroot.require_group(f\"{group_name}/metadata/table\")\n\n    # Find next index\n    start_index = 0\n    if \"index\" in zmeta:\n        start_index = (\n            int(zmeta[\"index\"][-1]) + 1 if zmeta[\"index\"].shape[0] &gt; 0 else 0\n        )\n\n    for i, row in enumerate(rows, start=start_index):\n        row[\"index\"] = i\n\n    import polars as pl\n\n    df = pl.DataFrame(rows)\n\n    for col in df.columns:\n        list_only_cols = {\n            \"attrs\",\n            \"commit_msg\",\n            \"action\",\n            \"write_strategy\",\n            \"rinex_hash\",\n            \"snapshot_id\",\n        }\n        if col in list_only_cols:\n            values = df[col].to_list()\n        else:\n            values = df[col].to_numpy()\n\n        if col == \"index\":\n            dtype = \"i8\"\n        elif col in (\"start\", \"end\"):\n            dtype = \"M8[ns]\"\n        else:\n            dtype = VariableLengthUTF8()\n\n        if col not in zmeta:\n            zmeta.create_array(\n                name=col, shape=(0,), dtype=dtype, chunks=(1024,), overwrite=True\n            )\n\n        arr = zmeta[col]\n        old_len = arr.shape[0]\n        new_len = old_len + len(values)\n        arr.resize(new_len)\n        arr[old_len:new_len] = values\n\n    self._logger.info(f\"Appended {df.height} metadata rows to group '{group_name}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.expire_old_snapshots","level":3,"title":"<code>expire_old_snapshots(days=None, branch='main', delete_expired_branches=True, delete_expired_tags=True)</code>","text":"<p>Expire and garbage-collect snapshots older than the given retention period.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.expire_old_snapshots--parameters","level":5,"title":"Parameters","text":"<p>days : int | None, optional     Number of days to retain snapshots. Defaults to config value. branch : str, default \"main\"     Branch to apply expiration on. delete_expired_branches : bool, default True     Whether to delete branches pointing to expired snapshots. delete_expired_tags : bool, default True     Whether to delete tags pointing to expired snapshots.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.expire_old_snapshots--returns","level":5,"title":"Returns","text":"<p>set[str]     Expired snapshot IDs.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def expire_old_snapshots(\n    self,\n    days: int | None = None,\n    branch: str = \"main\",\n    delete_expired_branches: bool = True,\n    delete_expired_tags: bool = True,\n) -&gt; set[str]:\n    \"\"\"\n    Expire and garbage-collect snapshots older than the given retention period.\n\n    Parameters\n    ----------\n    days : int | None, optional\n        Number of days to retain snapshots. Defaults to config value.\n    branch : str, default \"main\"\n        Branch to apply expiration on.\n    delete_expired_branches : bool, default True\n        Whether to delete branches pointing to expired snapshots.\n    delete_expired_tags : bool, default True\n        Whether to delete tags pointing to expired snapshots.\n\n    Returns\n    -------\n    set[str]\n        Expired snapshot IDs.\n    \"\"\"\n    if days is None:\n        days = self._rinex_store_expire_days\n    cutoff = datetime.now(UTC) - timedelta(days=days)\n\n    # cutoff = datetime(2025, 10, 3, 16, 44, 1, tzinfo=timezone.utc)\n    self._logger.info(\n        f\"Running expiration on store '{self.store_type}' \"\n        f\"(branch '{branch}') with cutoff {cutoff.isoformat()}\"\n    )\n\n    # Expire snapshots older than cutoff\n    expired_ids = self.repo.expire_snapshots(\n        older_than=cutoff,\n        delete_expired_branches=delete_expired_branches,\n        delete_expired_tags=delete_expired_tags,\n    )\n\n    if expired_ids:\n        self._logger.info(\n            f\"Expired {len(expired_ids)} snapshots: {sorted(expired_ids)}\"\n        )\n    else:\n        self._logger.info(\"No snapshots to expire.\")\n\n    # Garbage-collect expired objects to reclaim storage\n    summary = self.repo.garbage_collect(delete_object_older_than=cutoff)\n    self._logger.info(\n        f\"Garbage collection summary: \"\n        f\"deleted_bytes={summary.bytes_deleted}, \"\n        f\"deleted_chunks={summary.chunks_deleted}, \"\n        f\"deleted_manifests={summary.manifests_deleted}, \"\n        f\"deleted_snapshots={summary.snapshots_deleted}, \"\n        f\"deleted_attributes={summary.attributes_deleted}, \"\n        f\"deleted_transaction_logs={summary.transaction_logs_deleted}\"\n    )\n\n    return expired_ids\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_history","level":3,"title":"<code>get_history(branch='main', limit=None)</code>","text":"<p>Return commit ancestry (history) for a branch.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_history--parameters","level":5,"title":"Parameters","text":"<p>branch : str, default \"main\"     Branch name. limit : int | None, optional     Maximum number of commits to return.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_history--returns","level":5,"title":"Returns","text":"<p>list[dict]     Commit info dictionaries (id, message, written_at, parent_ids).</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_history(self, branch: str = \"main\", limit: int | None = None) -&gt; list[dict]:\n    \"\"\"\n    Return commit ancestry (history) for a branch.\n\n    Parameters\n    ----------\n    branch : str, default \"main\"\n        Branch name.\n    limit : int | None, optional\n        Maximum number of commits to return.\n\n    Returns\n    -------\n    list[dict]\n        Commit info dictionaries (id, message, written_at, parent_ids).\n    \"\"\"\n    self._logger.info(f\"Fetching ancestry for branch '{branch}'\")\n\n    history = []\n    for i, ancestor in enumerate(self.repo.ancestry(branch=branch)):\n        history.append(\n            {\n                \"snapshot_id\": ancestor.id,\n                \"commit_msg\": ancestor.message,\n                \"written_at\": ancestor.written_at,\n                \"parent_ids\": ancestor.parent_id,\n            }\n        )\n        if limit is not None and i + 1 &gt;= limit:\n            break\n\n    return history\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.print_history","level":3,"title":"<code>print_history(branch='main', limit=100)</code>","text":"<p>Pretty-print the ancestry for quick inspection.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def print_history(self, branch: str = \"main\", limit: int | None = 100) -&gt; None:\n    \"\"\"\n    Pretty-print the ancestry for quick inspection.\n    \"\"\"\n    for entry in self.get_history(branch=branch, limit=limit):\n        ts = entry[\"written_at\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n        print(f\"{ts} {entry['snapshot_id'][:8]} {entry['commit_msg']}\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return the developer-facing representation.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.__repr__--returns","level":5,"title":"Returns","text":"<p>str     Representation string.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the developer-facing representation.\n\n    Returns\n    -------\n    str\n        Representation string.\n    \"\"\"\n    return (\n        \"MyIcechunkStore(\"\n        f\"store_path={self.store_path}, store_type={self.store_type})\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.__str__","level":3,"title":"<code>__str__()</code>","text":"<p>Return a human-readable summary.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.__str__--returns","level":5,"title":"Returns","text":"<p>str     Summary string.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable summary.\n\n    Returns\n    -------\n    str\n        Summary string.\n    \"\"\"\n\n    # Capture tree output\n    old_stdout = sys.stdout\n    sys.stdout = buffer = io.StringIO()\n\n    try:\n        self.print_tree()\n        tree_output = buffer.getvalue()\n    finally:\n        sys.stdout = old_stdout\n\n    branches = self.get_branch_names()\n    group_dict = self.get_group_names()\n    total_groups = sum(len(groups) for groups in group_dict.values())\n\n    return (\n        f\"MyIcechunkStore: {self.store_path}\\n\"\n        f\"Branches: {len(branches)} | Total Groups: {total_groups}\\n\\n\"\n        f\"{tree_output}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.rechunk_group","level":3,"title":"<code>rechunk_group(group_name, chunks, source_branch='main', temp_branch=None, promote_to_main=True, delete_temp_branch=True)</code>","text":"<p>Rechunk a group with optimal chunk sizes.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.rechunk_group--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group to rechunk chunks : dict[str, int]     Chunking specification, e.g. {'epoch': 34560, 'sid': -1} source_branch : str     Branch to read original data from (default: \"main\") temp_branch : str | None     Temporary branch name for rechunked data. If None, uses     \"{group_name}_rechunked\". promote_to_main : bool     If True, reset main branch to rechunked snapshot after writing delete_temp_branch : bool     If True, delete temporary branch after promotion (only if     promote_to_main=True).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.rechunk_group--returns","level":5,"title":"Returns","text":"<p>str     Snapshot ID of the rechunked data</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def rechunk_group(\n    self,\n    group_name: str,\n    chunks: dict[str, int],\n    source_branch: str = \"main\",\n    temp_branch: str | None = None,\n    promote_to_main: bool = True,\n    delete_temp_branch: bool = True,\n) -&gt; str:\n    \"\"\"\n    Rechunk a group with optimal chunk sizes.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group to rechunk\n    chunks : dict[str, int]\n        Chunking specification, e.g. {'epoch': 34560, 'sid': -1}\n    source_branch : str\n        Branch to read original data from (default: \"main\")\n    temp_branch : str | None\n        Temporary branch name for rechunked data. If None, uses\n        \"{group_name}_rechunked\".\n    promote_to_main : bool\n        If True, reset main branch to rechunked snapshot after writing\n    delete_temp_branch : bool\n        If True, delete temporary branch after promotion (only if\n        promote_to_main=True).\n\n    Returns\n    -------\n    str\n        Snapshot ID of the rechunked data\n    \"\"\"\n    if temp_branch is None:\n        temp_branch = f\"{group_name}_rechunked_temp\"\n\n    self._logger.info(\n        f\"Starting rechunk of group '{group_name}' with chunks={chunks}\"\n    )\n\n    # Get CURRENT snapshot from source branch to preserve all other groups\n    current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n\n    # Create temp branch from current snapshot (preserves all existing groups)\n    try:\n        self.repo.create_branch(temp_branch, current_snapshot)\n        self._logger.info(\n            f\"Created temporary branch '{temp_branch}' from current {source_branch}\"\n        )\n    except Exception as e:\n        self._logger.warning(f\"Branch '{temp_branch}' may already exist: {e}\")\n\n    # Read original data\n    ds_original = self.read_group(group_name, branch=source_branch)\n    self._logger.info(f\"Original chunks: {ds_original.chunks}\")\n\n    # Rechunk\n    ds_rechunked = ds_original.chunk(chunks)\n    self._logger.info(f\"New chunks: {ds_rechunked.chunks}\")\n\n    # Clear encoding to avoid conflicts\n    for var in ds_rechunked.data_vars:\n        ds_rechunked[var].encoding = {}\n\n    # Write rechunked data (overwrites only this group)\n    with self.writable_session(temp_branch) as session:\n        to_icechunk(ds_rechunked, session, group=group_name, mode=\"w\")\n        snapshot_id = session.commit(f\"Rechunked {group_name} with chunks={chunks}\")\n\n    self._logger.info(\n        f\"Rechunked data written to branch '{temp_branch}', snapshot={snapshot_id}\"\n    )\n\n    # Promote to main if requested\n    if promote_to_main:\n        rechunked_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n        self.repo.reset_branch(source_branch, rechunked_snapshot)\n        self._logger.info(\n            f\"Reset branch '{source_branch}' to rechunked snapshot \"\n            f\"{rechunked_snapshot}\"\n        )\n\n        # Delete temp branch if requested\n        if delete_temp_branch:\n            self.repo.delete_branch(temp_branch)\n            self._logger.info(f\"Deleted temporary branch '{temp_branch}'\")\n\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.rechunk_group_verbose","level":3,"title":"<code>rechunk_group_verbose(group_name, chunks=None, source_branch='main', temp_branch=None, promote_to_main=True, delete_temp_branch=True)</code>","text":"<p>Rechunk a group with optimal chunk sizes.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.rechunk_group_verbose--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group to rechunk chunks : dict[str, int] | None     Chunking specification, e.g. {'epoch': 34560, 'sid': -1}. Defaults     to <code>gnnsvodpy.globals.ICECHUNK_CHUNK_STRATEGIES</code>. source_branch : str     Branch to read original data from (default: \"main\") temp_branch : str | None     Temporary branch name for rechunked data. If None, uses     \"{group_name}_rechunked\". promote_to_main : bool     If True, reset main branch to rechunked snapshot after writing delete_temp_branch : bool     If True, delete temporary branch after promotion (only if     promote_to_main=True).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.rechunk_group_verbose--returns","level":5,"title":"Returns","text":"<p>str     Snapshot ID of the rechunked data</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def rechunk_group_verbose(\n    self,\n    group_name: str,\n    chunks: dict[str, int] | None = None,\n    source_branch: str = \"main\",\n    temp_branch: str | None = None,\n    promote_to_main: bool = True,\n    delete_temp_branch: bool = True,\n) -&gt; str:\n    \"\"\"\n    Rechunk a group with optimal chunk sizes.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group to rechunk\n    chunks : dict[str, int] | None\n        Chunking specification, e.g. {'epoch': 34560, 'sid': -1}. Defaults\n        to `gnnsvodpy.globals.ICECHUNK_CHUNK_STRATEGIES`.\n    source_branch : str\n        Branch to read original data from (default: \"main\")\n    temp_branch : str | None\n        Temporary branch name for rechunked data. If None, uses\n        \"{group_name}_rechunked\".\n    promote_to_main : bool\n        If True, reset main branch to rechunked snapshot after writing\n    delete_temp_branch : bool\n        If True, delete temporary branch after promotion (only if\n        promote_to_main=True).\n\n    Returns\n    -------\n    str\n        Snapshot ID of the rechunked data\n    \"\"\"\n    if temp_branch is None:\n        temp_branch = f\"{group_name}_rechunked_temp\"\n\n    if chunks is None:\n        chunks = self.chunk_strategy or {\"epoch\": 34560, \"sid\": -1}\n\n    print(f\"\\n{'=' * 60}\")\n    print(f\"Starting rechunk of group '{group_name}'\")\n    print(f\"Target chunks: {chunks}\")\n    print(f\"{'=' * 60}\\n\")\n\n    self._logger.info(\n        f\"Starting rechunk of group '{group_name}' with chunks={chunks}\"\n    )\n\n    # Get CURRENT snapshot from source branch to preserve all other groups\n    print(f\"[1/7] Getting current snapshot from branch '{source_branch}'...\")\n    current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n    print(f\"      ✓ Current snapshot: {current_snapshot[:12]}\")\n\n    # Create temp branch from current snapshot (preserves all existing groups)\n    print(f\"\\n[2/7] Creating temporary branch '{temp_branch}'...\")\n    try:\n        self.repo.create_branch(temp_branch, current_snapshot)\n        print(f\"      ✓ Branch '{temp_branch}' created\")\n        self._logger.info(\n            f\"Created temporary branch '{temp_branch}' from current {source_branch}\"\n        )\n    except Exception as e:\n        print(\n            f\"      ⚠ Branch '{temp_branch}' already exists, using existing branch\"\n        )\n        self._logger.warning(f\"Branch '{temp_branch}' may already exist: {e}\")\n\n    # Read original data\n    print(f\"\\n[3/7] Reading original data from '{group_name}'...\")\n    ds_original = self.read_group(group_name, branch=source_branch)\n\n    # Unify chunks if inconsistent\n    try:\n        ds_original = ds_original.unify_chunks()\n        print(\"      ✓ Unified inconsistent chunks\")\n    except (TypeError, ValueError):\n        pass  # Chunks are already consistent\n\n    print(f\"      ✓ Data shape: {dict(ds_original.sizes)}\")\n    print(f\"      ✓ Original chunks: {ds_original.chunks}\")\n    self._logger.info(f\"Original chunks: {ds_original.chunks}\")\n\n    # Rechunk\n    print(\"\\n[4/7] Rechunking data...\")\n    ds_rechunked = ds_original.chunk(chunks)\n    ds_rechunked = ds_rechunked.unify_chunks()\n    print(f\"      ✓ New chunks: {ds_rechunked.chunks}\")\n    self._logger.info(f\"New chunks: {ds_rechunked.chunks}\")\n\n    # Clear encoding to avoid conflicts\n    for var in ds_rechunked.data_vars:\n        ds_rechunked[var].encoding = {}\n    for coord in ds_rechunked.coords:\n        if \"chunks\" in ds_rechunked[coord].encoding:\n            del ds_rechunked[coord].encoding[\"chunks\"]\n\n    # Write rechunked data first (overwrites entire group)\n    print(f\"\\n[5/7] Writing rechunked data to branch '{temp_branch}'...\")\n    print(\"      This may take several minutes for large datasets...\")\n    with self.writable_session(temp_branch) as session:\n        to_icechunk(ds_rechunked, session, group=group_name, mode=\"w\")\n        session.commit(f\"Wrote rechunked data for {group_name}\")\n    print(\"      ✓ Data written successfully\")\n\n    # Copy subgroups after writing rechunked data\n    print(f\"\\n[6/7] Copying subgroups from '{group_name}'...\")\n    with self.writable_session(temp_branch) as session:\n        with self.readonly_session(source_branch) as icsession:\n            source_group = zarr.open_group(icsession.store, mode=\"r\")[group_name]\n        target_group = zarr.open_group(session.store, mode=\"a\")[group_name]\n\n        subgroup_count = 0\n        for subgroup_name in source_group.group_keys():\n            print(f\"      ✓ Copying subgroup '{subgroup_name}'...\")\n            source_subgroup = source_group[subgroup_name]\n            target_subgroup = target_group.create_group(\n                subgroup_name, overwrite=True\n            )\n\n            # Copy arrays from subgroup\n            for array_name in source_subgroup.array_keys():\n                source_array = source_subgroup[array_name]\n                target_array = target_subgroup.create_array(\n                    array_name,\n                    shape=source_array.shape,\n                    dtype=source_array.dtype,\n                    chunks=source_array.chunks,\n                    overwrite=True,\n                )\n                target_array[:] = source_array[:]\n\n            # Copy subgroup attributes\n            target_subgroup.attrs.update(source_subgroup.attrs)\n            subgroup_count += 1\n\n        if subgroup_count &gt; 0:\n            snapshot_id = session.commit(\n                f\"Rechunked {group_name} with chunks={chunks}\"\n            )\n            print(f\"      ✓ {subgroup_count} subgroups copied\")\n        else:\n            snapshot_id = next(self.repo.ancestry(branch=temp_branch)).id\n            print(\"      ✓ No subgroups to copy\")\n\n    print(f\"      ✓ Snapshot ID: {snapshot_id[:12]}\")\n    self._logger.info(\n        f\"Rechunked data written to branch '{temp_branch}', snapshot={snapshot_id}\"\n    )\n\n    # Promote to main if requested\n    if promote_to_main:\n        print(f\"\\n[7/7] Promoting to '{source_branch}' branch...\")\n        rechunked_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n        self.repo.reset_branch(source_branch, rechunked_snapshot)\n        print(\n            f\"      ✓ Branch '{source_branch}' reset to {rechunked_snapshot[:12]}\"\n        )\n        self._logger.info(\n            f\"Reset branch '{source_branch}' to rechunked snapshot \"\n            f\"{rechunked_snapshot}\"\n        )\n\n        # Delete temp branch if requested\n        if delete_temp_branch:\n            print(f\"      ✓ Deleting temporary branch '{temp_branch}'...\")\n            self.delete_branch(temp_branch)\n            print(\"      ✓ Temporary branch deleted\")\n            self._logger.info(f\"Deleted temporary branch '{temp_branch}'\")\n    else:\n        print(\"\\n[7/7] Skipping promotion (promote_to_main=False)\")\n        print(f\"      Rechunked data available on branch '{temp_branch}'\")\n\n    print(f\"\\n{'=' * 60}\")\n    print(f\"✓ Rechunking complete for '{group_name}'\")\n    print(f\"{'=' * 60}\\n\")\n\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.create_release_tag","level":3,"title":"<code>create_release_tag(tag_name, snapshot_id=None)</code>","text":"<p>Create an immutable tag for an important version.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.create_release_tag--parameters","level":5,"title":"Parameters","text":"<p>tag_name : str     Name for the tag (e.g., \"v2024_complete\", \"before_reprocess\") snapshot_id : str | None     Snapshot to tag. If None, uses current tip of main branch.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def create_release_tag(self, tag_name: str, snapshot_id: str | None = None) -&gt; None:\n    \"\"\"\n    Create an immutable tag for an important version.\n\n    Parameters\n    ----------\n    tag_name : str\n        Name for the tag (e.g., \"v2024_complete\", \"before_reprocess\")\n    snapshot_id : str | None\n        Snapshot to tag. If None, uses current tip of main branch.\n    \"\"\"\n    if snapshot_id is None:\n        # Tag current main branch tip\n        snapshot_id = next(self.repo.ancestry(branch=\"main\")).id\n\n    self.repo.create_tag(tag_name, snapshot_id)\n    self._logger.info(f\"Created tag '{tag_name}' at snapshot {snapshot_id[:8]}\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.list_tags","level":3,"title":"<code>list_tags()</code>","text":"<p>List all tags in the repository.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def list_tags(self) -&gt; list[str]:\n    \"\"\"List all tags in the repository.\"\"\"\n    return list(self.repo.list_tags())\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.delete_tag","level":3,"title":"<code>delete_tag(tag_name)</code>","text":"<p>Delete a tag (use with caution - tags are meant to be permanent).</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def delete_tag(self, tag_name: str) -&gt; None:\n    \"\"\"Delete a tag (use with caution - tags are meant to be permanent).\"\"\"\n    self.repo.delete_tag(tag_name)\n    self._logger.warning(f\"Deleted tag '{tag_name}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.plot_commit_graph","level":3,"title":"<code>plot_commit_graph(max_commits=100)</code>","text":"<p>Visualize commit history as an interactive git-like graph.</p> <p>Creates an interactive visualization showing: - Branches with different colors - Chronological commit ordering - Branch divergence points - Commit messages on hover - Click to see commit details</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.plot_commit_graph--parameters","level":5,"title":"Parameters","text":"<p>max_commits : int     Maximum number of commits to display (default: 100).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.plot_commit_graph--returns","level":5,"title":"Returns","text":"<p>Figure     Interactive plotly figure (works in marimo and Jupyter).</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def plot_commit_graph(self, max_commits: int = 100) -&gt; \"Figure\":\n    \"\"\"\n    Visualize commit history as an interactive git-like graph.\n\n    Creates an interactive visualization showing:\n    - Branches with different colors\n    - Chronological commit ordering\n    - Branch divergence points\n    - Commit messages on hover\n    - Click to see commit details\n\n    Parameters\n    ----------\n    max_commits : int\n        Maximum number of commits to display (default: 100).\n\n    Returns\n    -------\n    Figure\n        Interactive plotly figure (works in marimo and Jupyter).\n    \"\"\"\n    from collections import defaultdict\n    from datetime import datetime\n\n    import plotly.graph_objects as go\n\n    # Collect all commits with full metadata\n    commit_map = {}  # id -&gt; commit data\n    branch_tips = {}  # branch -&gt; latest commit id\n\n    for branch in self.repo.list_branches():\n        ancestors = list(self.repo.ancestry(branch=branch))\n        if ancestors:\n            branch_tips[branch] = ancestors[0].id\n\n        for ancestor in ancestors:\n            if ancestor.id not in commit_map:\n                commit_map[ancestor.id] = {\n                    \"id\": ancestor.id,\n                    \"parent_id\": ancestor.parent_id,\n                    \"message\": ancestor.message,\n                    \"written_at\": ancestor.written_at,\n                    \"branches\": [branch],\n                }\n            else:\n                # Multiple branches point to same commit\n                commit_map[ancestor.id][\"branches\"].append(branch)\n\n            if len(commit_map) &gt;= max_commits:\n                break\n        if len(commit_map) &gt;= max_commits:\n            break\n\n    # Build parent-child relationships\n    commits_list = list(commit_map.values())\n    commits_list.sort(key=lambda c: c[\"written_at\"])  # Oldest first\n\n    # Assign horizontal positions (chronological)\n    commit_x_positions = {}\n    for idx, commit in enumerate(commits_list):\n        commit[\"x\"] = idx\n        commit_x_positions[commit[\"id\"]] = idx\n\n    # Assign vertical positions: commits shared by branches stay on same Y\n    # Only diverge when branches have different commits\n    branch_names = sorted(\n        self.repo.list_branches(), key=lambda b: (b != \"main\", b)\n    )  # main first\n\n    # Build a set of all commit IDs for each branch\n    branch_commits = {}\n    for branch in branch_names:\n        history = list(self.repo.ancestry(branch=branch))\n        branch_commits[branch] = {h.id for h in history if h.id in commit_map}\n\n    # Find where branches diverge\n    def branches_share_commit(\n        commit_id: str,\n        branches: list[str],\n    ) -&gt; list[str]:\n        \"\"\"Return branches that contain a commit.\n\n        Parameters\n        ----------\n        commit_id : str\n            Commit identifier to check.\n        branches : list[str]\n            Branch names to search.\n\n        Returns\n        -------\n        list[str]\n            Branches that contain the commit.\n        \"\"\"\n        return [b for b in branches if commit_id in branch_commits[b]]\n\n    # Assign Y position: all commits on a single horizontal line initially\n    # We'll use vertical offset for parallel branch indicators\n    for commit in commits_list:\n        commit[\"y\"] = 0  # All on same timeline\n        commit[\"branch_set\"] = frozenset(commit[\"branches\"])\n\n    # Color palette for branches\n    colors = [\n        \"#4a9a4a\",  # green (main)\n        \"#5580c8\",  # blue\n        \"#d97643\",  # orange\n        \"#9b59b6\",  # purple\n        \"#e74c3c\",  # red\n        \"#1abc9c\",  # turquoise\n        \"#f39c12\",  # yellow\n        \"#34495e\",  # dark gray\n    ]\n    branch_colors = {b: colors[i % len(colors)] for i, b in enumerate(branch_names)}\n\n    # Build edges: draw parallel lines for shared commits (metro-style)\n    edges_by_branch = defaultdict(list)  # branch -&gt; list of edge dicts\n\n    for commit in commits_list:\n        if commit[\"parent_id\"] and commit[\"parent_id\"] in commit_map:\n            parent = commit_map[commit[\"parent_id\"]]\n\n            # Find which branches share both this commit and its parent\n            shared_branches = [\n                b for b in commit[\"branches\"] if b in parent[\"branches\"]\n            ]\n\n            for branch in shared_branches:\n                edges_by_branch[branch].append(\n                    {\n                        \"x0\": parent[\"x\"],\n                        \"y0\": parent[\"y\"],\n                        \"x1\": commit[\"x\"],\n                        \"y1\": commit[\"y\"],\n                    }\n                )\n\n    # Create plotly figure\n    fig = go.Figure()\n\n    # Draw edges grouped by branch (parallel lines for shared paths)\n    for branch_idx, branch in enumerate(branch_names):\n        if branch not in edges_by_branch:\n            continue\n\n        color = branch_colors[branch]\n\n        # Draw each edge as a separate line\n        for edge in edges_by_branch[branch]:\n            # Vertical offset for parallel lines (metro-style)\n            offset = (branch_idx - (len(branch_names) - 1) / 2) * 0.15\n\n            fig.add_trace(\n                go.Scatter(\n                    x=[edge[\"x0\"], edge[\"x1\"]],\n                    y=[edge[\"y0\"] + offset, edge[\"y1\"] + offset],\n                    mode=\"lines\",\n                    line=dict(color=color, width=3),\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                    opacity=0.7,\n                )\n            )\n\n    # Draw commits (nodes) - one trace per unique commit\n    # Color by which branches include it\n    x_vals = [c[\"x\"] for c in commits_list]\n    y_vals = [c[\"y\"] for c in commits_list]\n\n    # Format hover text\n    hover_texts = []\n    marker_colors = []\n    marker_symbols = []\n\n    for c in commits_list:\n        # Handle both string and datetime objects\n        if isinstance(c[\"written_at\"], str):\n            time_str = datetime.fromisoformat(c[\"written_at\"]).strftime(\n                \"%Y-%m-%d %H:%M\"\n            )\n        else:\n            time_str = c[\"written_at\"].strftime(\"%Y-%m-%d %H:%M\")\n\n        branches_str = \", \".join(c[\"branches\"])\n        hover_texts.append(\n            f\"&lt;b&gt;{c['message'] or 'No message'}&lt;/b&gt;&lt;br&gt;\"\n            f\"Commit: {c['id'][:12]}&lt;br&gt;\"\n            f\"Branches: {branches_str}&lt;br&gt;\"\n            f\"Time: {time_str}\"\n        )\n\n        # Color by first branch (priority: main)\n        if \"main\" in c[\"branches\"]:\n            marker_colors.append(branch_colors[\"main\"])\n        else:\n            marker_colors.append(branch_colors[c[\"branches\"][0]])\n\n        # Star for branch tips\n        if c[\"id\"] in branch_tips.values():\n            marker_symbols.append(\"star\")\n        else:\n            marker_symbols.append(\"circle\")\n\n    fig.add_trace(\n        go.Scatter(\n            x=x_vals,\n            y=y_vals,\n            mode=\"markers\",\n            name=\"Commits\",\n            marker=dict(\n                size=14,\n                color=marker_colors,\n                symbol=marker_symbols,\n                line=dict(color=\"white\", width=2),\n            ),\n            hovertext=hover_texts,\n            hoverinfo=\"text\",\n            showlegend=False,\n        )\n    )\n\n    # Add legend traces (invisible points just for legend)\n    for branch_idx, branch in enumerate(branch_names):\n        fig.add_trace(\n            go.Scatter(\n                x=[None],\n                y=[None],\n                mode=\"markers\",\n                name=branch,\n                marker=dict(\n                    size=10,\n                    color=branch_colors[branch],\n                    line=dict(color=\"white\", width=2),\n                ),\n                showlegend=True,\n            )\n        )\n\n    # Layout styling\n    title_text = (\n        f\"Commit Graph: {self.site_name} ({len(commits_list)} commits, \"\n        f\"{len(branch_names)} branches)\"\n    )\n    fig.update_layout(\n        title=dict(\n            text=title_text,\n            font=dict(size=16, color=\"#e5e5e5\"),\n        ),\n        xaxis=dict(\n            title=\"Time (oldest ← → newest)\",\n            showticklabels=False,\n            showgrid=True,\n            gridcolor=\"rgba(255,255,255,0.1)\",\n            zeroline=False,\n        ),\n        yaxis=dict(\n            title=\"\",\n            showticklabels=False,\n            showgrid=False,\n            zeroline=False,\n            range=[-1, 1],  # Fixed range for single timeline\n        ),\n        plot_bgcolor=\"#1a1a1a\",\n        paper_bgcolor=\"#1a1a1a\",\n        font=dict(color=\"#e5e5e5\"),\n        hovermode=\"closest\",\n        height=400,\n        width=max(800, len(commits_list) * 50),\n        legend=dict(\n            title=\"Branches\",\n            orientation=\"h\",\n            x=0,\n            y=-0.15,\n            bgcolor=\"rgba(30,30,30,0.8)\",\n            bordercolor=\"rgba(255,255,255,0.2)\",\n            borderwidth=1,\n        ),\n    )\n\n    return fig\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.cleanup_stale_branches","level":3,"title":"<code>cleanup_stale_branches(keep_patterns=None)</code>","text":"<p>Delete stale temporary branches (e.g., from failed rechunking).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.cleanup_stale_branches--parameters","level":5,"title":"Parameters","text":"<p>keep_patterns : list[str] | None     Patterns to preserve. Default: [\"main\", \"dev\"]</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.cleanup_stale_branches--returns","level":5,"title":"Returns","text":"<p>list[str]     Names of deleted branches</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def cleanup_stale_branches(\n    self, keep_patterns: list[str] | None = None\n) -&gt; list[str]:\n    \"\"\"\n    Delete stale temporary branches (e.g., from failed rechunking).\n\n    Parameters\n    ----------\n    keep_patterns : list[str] | None\n        Patterns to preserve. Default: [\"main\", \"dev\"]\n\n    Returns\n    -------\n    list[str]\n        Names of deleted branches\n    \"\"\"\n    if keep_patterns is None:\n        keep_patterns = [\"main\", \"dev\"]\n\n    deleted = []\n\n    for branch in self.repo.list_branches():\n        # Keep if matches any pattern\n        should_keep = any(pattern in branch for pattern in keep_patterns)\n\n        if not should_keep:\n            # Check if it's a temp branch from rechunking\n            if \"_rechunked_temp\" in branch or \"_temp\" in branch:\n                try:\n                    self.repo.delete_branch(branch)\n                    deleted.append(branch)\n                    self._logger.info(f\"Deleted stale branch: {branch}\")\n                except Exception as e:\n                    self._logger.warning(f\"Failed to delete branch {branch}: {e}\")\n\n    return deleted\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.delete_branch","level":3,"title":"<code>delete_branch(branch_name)</code>","text":"<p>Delete a branch.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def delete_branch(self, branch_name: str) -&gt; None:\n    \"\"\"Delete a branch.\"\"\"\n    if branch_name == \"main\":\n        raise ValueError(\"Cannot delete 'main' branch\")\n\n    self.repo.delete_branch(branch_name)\n    self._logger.info(f\"Deleted branch '{branch_name}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_snapshot_info","level":3,"title":"<code>get_snapshot_info(snapshot_id)</code>","text":"<p>Get detailed information about a specific snapshot.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_snapshot_info--parameters","level":5,"title":"Parameters","text":"<p>snapshot_id : str     Snapshot ID to inspect</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.get_snapshot_info--returns","level":5,"title":"Returns","text":"<p>dict     Snapshot metadata and statistics</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_snapshot_info(self, snapshot_id: str) -&gt; dict:\n    \"\"\"\n    Get detailed information about a specific snapshot.\n\n    Parameters\n    ----------\n    snapshot_id : str\n        Snapshot ID to inspect\n\n    Returns\n    -------\n    dict\n        Snapshot metadata and statistics\n    \"\"\"\n    # Find the snapshot in ancestry\n    for ancestor in self.repo.ancestry(branch=\"main\"):\n        if ancestor.id == snapshot_id or ancestor.id.startswith(snapshot_id):\n            info = {\n                \"snapshot_id\": ancestor.id,\n                \"message\": ancestor.message,\n                \"written_at\": ancestor.written_at,\n                \"parent_id\": ancestor.parent_id,\n            }\n\n            # Try to get groups at this snapshot\n            try:\n                session = self.repo.readonly_session(snapshot_id=ancestor.id)\n                root = zarr.open(session.store, mode=\"r\")\n                info[\"groups\"] = list(root.group_keys())\n                info[\"arrays\"] = list(root.array_keys())\n            except Exception as e:\n                self._logger.warning(f\"Could not inspect snapshot contents: {e}\")\n\n            return info\n\n    raise ValueError(f\"Snapshot {snapshot_id} not found in history\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.compare_snapshots","level":3,"title":"<code>compare_snapshots(snapshot_id_1, snapshot_id_2)</code>","text":"<p>Compare two snapshots to see what changed.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.compare_snapshots--parameters","level":5,"title":"Parameters","text":"<p>snapshot_id_1 : str     First snapshot (older) snapshot_id_2 : str     Second snapshot (newer)</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.compare_snapshots--returns","level":5,"title":"Returns","text":"<p>dict     Comparison results showing added/removed/modified groups</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def compare_snapshots(self, snapshot_id_1: str, snapshot_id_2: str) -&gt; dict:\n    \"\"\"\n    Compare two snapshots to see what changed.\n\n    Parameters\n    ----------\n    snapshot_id_1 : str\n        First snapshot (older)\n    snapshot_id_2 : str\n        Second snapshot (newer)\n\n    Returns\n    -------\n    dict\n        Comparison results showing added/removed/modified groups\n    \"\"\"\n    info_1 = self.get_snapshot_info(snapshot_id_1)\n    info_2 = self.get_snapshot_info(snapshot_id_2)\n\n    groups_1 = set(info_1.get(\"groups\", []))\n    groups_2 = set(info_2.get(\"groups\", []))\n\n    return {\n        \"snapshot_1\": snapshot_id_1[:8],\n        \"snapshot_2\": snapshot_id_2[:8],\n        \"added_groups\": list(groups_2 - groups_1),\n        \"removed_groups\": list(groups_1 - groups_2),\n        \"common_groups\": list(groups_1 &amp; groups_2),\n        \"time_diff\": (info_2[\"written_at\"] - info_1[\"written_at\"]).total_seconds(),\n    }\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.maintenance","level":3,"title":"<code>maintenance(expire_days=7, cleanup_branches=True, run_gc=True)</code>","text":"<p>Run full maintenance on the store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.maintenance--parameters","level":5,"title":"Parameters","text":"<p>expire_days : int     Days of snapshot history to keep cleanup_branches : bool     Remove stale temporary branches run_gc : bool     Run garbage collection after expiration</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.maintenance--returns","level":5,"title":"Returns","text":"<p>dict     Summary of maintenance actions</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def maintenance(\n    self, expire_days: int = 7, cleanup_branches: bool = True, run_gc: bool = True\n) -&gt; dict:\n    \"\"\"\n    Run full maintenance on the store.\n\n    Parameters\n    ----------\n    expire_days : int\n        Days of snapshot history to keep\n    cleanup_branches : bool\n        Remove stale temporary branches\n    run_gc : bool\n        Run garbage collection after expiration\n\n    Returns\n    -------\n    dict\n        Summary of maintenance actions\n    \"\"\"\n    self._logger.info(f\"Starting maintenance on {self.store_type}\")\n\n    results = {\"expired_snapshots\": 0, \"deleted_branches\": [], \"gc_summary\": None}\n\n    # Expire old snapshots\n    expired_ids = self.expire_old_snapshots(days=expire_days)\n    results[\"expired_snapshots\"] = len(expired_ids)\n\n    # Cleanup stale branches\n    if cleanup_branches:\n        deleted_branches = self.cleanup_stale_branches()\n        results[\"deleted_branches\"] = deleted_branches\n\n    # Garbage collection\n    if run_gc:\n        from datetime import datetime, timedelta\n\n        cutoff = datetime.now(UTC) - timedelta(days=expire_days)\n        gc_summary = self.repo.garbage_collect(delete_object_older_than=cutoff)\n        results[\"gc_summary\"] = {\n            \"bytes_deleted\": gc_summary.bytes_deleted,\n            \"chunks_deleted\": gc_summary.chunks_deleted,\n            \"manifests_deleted\": gc_summary.manifests_deleted,\n        }\n\n    self._logger.info(f\"Maintenance complete: {results}\")\n    return results\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.sanitize_store","level":3,"title":"<code>sanitize_store(source_branch='main', temp_branch='sanitize_temp', promote_to_main=True, delete_temp_branch=True)</code>","text":"<p>Sanitize all groups by removing NaN-only SIDs and cleaning coordinates.</p> <p>Creates a temporary branch, applies sanitization to all groups, then optionally promotes to main and cleans up.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.sanitize_store--parameters","level":5,"title":"Parameters","text":"<p>source_branch : str, default \"main\"     Branch to read original data from. temp_branch : str, default \"sanitize_temp\"     Temporary branch name for sanitized data. promote_to_main : bool, default True     If True, reset main branch to sanitized snapshot after writing. delete_temp_branch : bool, default True     If True, delete temporary branch after promotion.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.sanitize_store--returns","level":5,"title":"Returns","text":"<p>str     Snapshot ID of the sanitized data.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def sanitize_store(\n    self,\n    source_branch: str = \"main\",\n    temp_branch: str = \"sanitize_temp\",\n    promote_to_main: bool = True,\n    delete_temp_branch: bool = True,\n) -&gt; str:\n    \"\"\"\n    Sanitize all groups by removing NaN-only SIDs and cleaning coordinates.\n\n    Creates a temporary branch, applies sanitization to all groups, then\n    optionally promotes to main and cleans up.\n\n    Parameters\n    ----------\n    source_branch : str, default \"main\"\n        Branch to read original data from.\n    temp_branch : str, default \"sanitize_temp\"\n        Temporary branch name for sanitized data.\n    promote_to_main : bool, default True\n        If True, reset main branch to sanitized snapshot after writing.\n    delete_temp_branch : bool, default True\n        If True, delete temporary branch after promotion.\n\n    Returns\n    -------\n    str\n        Snapshot ID of the sanitized data.\n    \"\"\"\n    import time\n\n    from icechunk.xarray import to_icechunk\n\n    print(f\"\\n{'=' * 60}\")\n    print(\"Starting store sanitization\")\n    print(f\"{'=' * 60}\\n\")\n\n    # Step 1: Get current snapshot\n    print(f\"[1/6] Getting current snapshot from '{source_branch}'...\")\n    current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n    print(f\"      ✓ Current snapshot: {current_snapshot[:12]}\")\n\n    # Step 2: Create temp branch\n    print(f\"\\n[2/6] Creating temporary branch '{temp_branch}'...\")\n    try:\n        self.repo.create_branch(temp_branch, current_snapshot)\n        print(f\"      ✓ Branch '{temp_branch}' created\")\n    except Exception:\n        print(\"      ⚠ Branch exists, deleting and recreating...\")\n        self.delete_branch(temp_branch)\n        self.repo.create_branch(temp_branch, current_snapshot)\n        print(f\"      ✓ Branch '{temp_branch}' created\")\n\n    # Step 3: Get all groups\n    print(\"\\n[3/6] Discovering groups...\")\n    groups = self.list_groups()\n    print(f\"      ✓ Found {len(groups)} groups: {groups}\")\n\n    # Step 4: Sanitize each group\n    print(\"\\n[4/6] Sanitizing groups...\")\n    sanitized_count = 0\n\n    for group_name in groups:\n        print(f\"\\n      Processing '{group_name}'...\")\n        t_start = time.time()\n\n        try:\n            # Read original data\n            ds_original = self.read_group(group_name, branch=source_branch)\n            original_sids = len(ds_original.sid)\n            print(f\"        • Original: {original_sids} SIDs\")\n\n            # Sanitize: remove SIDs with all-NaN data\n            ds_sanitized = self._sanitize_dataset(ds_original)\n            sanitized_sids = len(ds_sanitized.sid)\n            removed_sids = original_sids - sanitized_sids\n\n            print(\n                f\"        • Sanitized: {sanitized_sids} SIDs \"\n                f\"(removed {removed_sids})\"\n            )\n\n            # Write sanitized data\n            with self.writable_session(temp_branch) as session:\n                to_icechunk(ds_sanitized, session, group=group_name, mode=\"w\")\n\n                # Copy metadata subgroups if they exist\n                try:\n                    with self.readonly_session(source_branch) as read_session:\n                        source_group = zarr.open_group(\n                            read_session.store, mode=\"r\"\n                        )[group_name]\n                        if \"metadata\" in source_group.group_keys():\n                            # Copy entire metadata subgroup\n                            dest_group = zarr.open_group(session.store)[group_name]\n                            zarr.copy(\n                                source_group[\"metadata\"],\n                                dest_group,\n                                name=\"metadata\",\n                            )\n                            print(\"        • Copied metadata subgroup\")\n                except Exception as e:\n                    print(f\"        ⚠ Could not copy metadata: {e}\")\n\n                session.commit(\n                    f\"Sanitized {group_name}: removed {removed_sids} empty SIDs\"\n                )\n\n            t_elapsed = time.time() - t_start\n            print(f\"        ✓ Completed in {t_elapsed:.2f}s\")\n            sanitized_count += 1\n\n        except Exception as e:\n            print(f\"        ✗ Failed: {e}\")\n            continue\n\n    print(f\"\\n      ✓ Sanitized {sanitized_count}/{len(groups)} groups\")\n\n    # Step 5: Get final snapshot\n    print(\"\\n[5/6] Getting sanitized snapshot...\")\n    sanitized_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n    print(f\"      ✓ Snapshot: {sanitized_snapshot[:12]}\")\n\n    # Step 6: Promote to main\n    if promote_to_main:\n        print(f\"\\n[6/6] Promoting to '{source_branch}' branch...\")\n        self.repo.reset_branch(source_branch, sanitized_snapshot)\n        print(\n            f\"      ✓ Branch '{source_branch}' reset to {sanitized_snapshot[:12]}\"\n        )\n\n        if delete_temp_branch:\n            print(f\"      ✓ Deleting temporary branch '{temp_branch}'...\")\n            self.delete_branch(temp_branch)\n            print(\"      ✓ Temporary branch deleted\")\n    else:\n        print(\"\\n[6/6] Skipping promotion (promote_to_main=False)\")\n        print(f\"      Sanitized data available on branch '{temp_branch}'\")\n\n    print(f\"\\n{'=' * 60}\")\n    print(\"✓ Sanitization complete\")\n    print(f\"{'=' * 60}\\n\")\n\n    return sanitized_snapshot\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.safe_temporal_aggregate","level":3,"title":"<code>safe_temporal_aggregate(group, freq='1D', vars_to_aggregate=('VOD',), geometry_vars=('phi', 'theta'), drop_empty=True, branch='main')</code>","text":"<p>Safely aggregate temporally irregular VOD data.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.safe_temporal_aggregate--parameters","level":5,"title":"Parameters","text":"<p>group : str     Group name to aggregate. freq : str, default \"1D\"     Resample frequency string. vars_to_aggregate : Sequence[str], optional     Variables to aggregate using mean. geometry_vars : Sequence[str], optional     Geometry variables to preserve via first() per bin. drop_empty : bool, default True     Drop empty epochs after aggregation. branch : str, default \"main\"     Branch name to read from.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.safe_temporal_aggregate--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Aggregated dataset.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def safe_temporal_aggregate(\n    self,\n    group: str,\n    freq: str = \"1D\",\n    vars_to_aggregate: Sequence[str] = (\"VOD\",),\n    geometry_vars: Sequence[str] = (\"phi\", \"theta\"),\n    drop_empty: bool = True,\n    branch: str = \"main\",\n) -&gt; xr.Dataset:\n    \"\"\"Safely aggregate temporally irregular VOD data.\n\n    Parameters\n    ----------\n    group : str\n        Group name to aggregate.\n    freq : str, default \"1D\"\n        Resample frequency string.\n    vars_to_aggregate : Sequence[str], optional\n        Variables to aggregate using mean.\n    geometry_vars : Sequence[str], optional\n        Geometry variables to preserve via first() per bin.\n    drop_empty : bool, default True\n        Drop empty epochs after aggregation.\n    branch : str, default \"main\"\n        Branch name to read from.\n\n    Returns\n    -------\n    xr.Dataset\n        Aggregated dataset.\n    \"\"\"\n\n    with self.readonly_session(branch=branch) as session:\n        ds = xr.open_zarr(session.store, group=group, consolidated=False)\n\n        print(\n            f\"📦 Aggregating group '{group}' from branch '{branch}' → freq={freq}\"\n        )\n\n        # 1️⃣ Aggregate numeric variables\n        merged_vars = []\n        for var in vars_to_aggregate:\n            if var in ds:\n                merged_vars.append(ds[var].resample(epoch=freq).mean())\n            else:\n                print(f\"⚠️ Skipping missing variable: {var}\")\n        ds_agg = xr.merge(merged_vars)\n\n        # 2️⃣ Preserve geometry variables (use first() per bin)\n        for var in geometry_vars:\n            if var in ds:\n                ds_agg[var] = ds[var].resample(epoch=freq).first()\n\n        # 3️⃣ Add remaining coordinates\n        for coord in ds.coords:\n            if coord not in ds_agg.coords and coord != \"epoch\":\n                ds_agg[coord] = ds[coord]\n\n        # 4️⃣ Drop all-NaN epochs if requested\n        if drop_empty and \"VOD\" in ds_agg:\n            valid_mask = ds_agg[\"VOD\"].notnull().any(dim=\"sid\").compute()\n            ds_agg = ds_agg.isel(epoch=valid_mask)\n\n        print(f\"✅ Aggregation done: {dict(ds_agg.sizes)}\")\n        return ds_agg\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.safe_temporal_aggregate_to_branch","level":3,"title":"<code>safe_temporal_aggregate_to_branch(source_group, target_group, target_branch, freq='1D', overwrite=False, **kwargs)</code>","text":"<p>Aggregate a group and save to a new Icechunk branch/group.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.safe_temporal_aggregate_to_branch--parameters","level":5,"title":"Parameters","text":"<p>source_group : str     Source group name. target_group : str     Target group name. target_branch : str     Target branch name. freq : str, default \"1D\"     Resample frequency string. overwrite : bool, default False     Whether to overwrite an existing branch. **kwargs : Any     Additional keyword args passed to safe_temporal_aggregate().</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.MyIcechunkStore.safe_temporal_aggregate_to_branch--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Aggregated dataset written to the target branch.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def safe_temporal_aggregate_to_branch(\n    self,\n    source_group: str,\n    target_group: str,\n    target_branch: str,\n    freq: str = \"1D\",\n    overwrite: bool = False,\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"Aggregate a group and save to a new Icechunk branch/group.\n\n    Parameters\n    ----------\n    source_group : str\n        Source group name.\n    target_group : str\n        Target group name.\n    target_branch : str\n        Target branch name.\n    freq : str, default \"1D\"\n        Resample frequency string.\n    overwrite : bool, default False\n        Whether to overwrite an existing branch.\n    **kwargs : Any\n        Additional keyword args passed to safe_temporal_aggregate().\n\n    Returns\n    -------\n    xr.Dataset\n        Aggregated dataset written to the target branch.\n    \"\"\"\n\n    print(\n        f\"🚀 Creating new aggregated branch '{target_branch}' at '{target_group}'\"\n    )\n\n    # Compute safe aggregation\n    ds_agg = self.safe_temporal_aggregate(\n        group=source_group,\n        freq=freq,\n        **kwargs,\n    )\n\n    # Write to new branch\n    current_snapshot = next(self.repo.ancestry(branch=\"main\")).id\n    self.delete_branch(target_branch)\n    self.repo.create_branch(target_branch, current_snapshot)\n    with self.writable_session(target_branch) as session:\n        to_icechunk(\n            obj=ds_agg,\n            session=session,\n            group=target_group,\n            mode=\"w\",\n        )\n        session.commit(f\"Saved aggregated data to {target_group} at freq={freq}\")\n\n    print(\n        f\"✅ Saved aggregated dataset to branch '{target_branch}' \"\n        f\"(group '{target_group}')\"\n    )\n    return ds_agg\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite","level":2,"title":"<code>GnssResearchSite</code>","text":"<p>High-level manager for a GNSS research site with dual Icechunk stores.</p> <p>This class coordinates between RINEX data storage (Level 1) and VOD analysis storage (Level 2), providing a unified interface for site-wide operations.</p> <p>Architecture: - RINEX Store: Raw/standardized observations per receiver - VOD Store: Analysis products comparing receiver pairs</p> <p>Features: - Automatic store initialization from config - Receiver management and validation - Analysis workflow coordination - Unified logging and error handling</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite--parameters","level":4,"title":"Parameters","text":"<p>site_name : str     Name of the research site (must exist in config).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite--raises","level":4,"title":"Raises","text":"<p>KeyError     If <code>site_name</code> is not found in the RESEARCH_SITES config.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>class GnssResearchSite:\n    \"\"\"\n    High-level manager for a GNSS research site with dual Icechunk stores.\n\n    This class coordinates between RINEX data storage (Level 1) and\n    VOD analysis storage (Level 2), providing a unified interface\n    for site-wide operations.\n\n    Architecture:\n    - RINEX Store: Raw/standardized observations per receiver\n    - VOD Store: Analysis products comparing receiver pairs\n\n    Features:\n    - Automatic store initialization from config\n    - Receiver management and validation\n    - Analysis workflow coordination\n    - Unified logging and error handling\n\n    Parameters\n    ----------\n    site_name : str\n        Name of the research site (must exist in config).\n\n    Raises\n    ------\n    KeyError\n        If ``site_name`` is not found in the RESEARCH_SITES config.\n    \"\"\"\n\n    def __init__(self, site_name: str) -&gt; None:\n        \"\"\"Initialize the site manager.\n\n        Parameters\n        ----------\n        site_name : str\n            Name of the research site.\n        \"\"\"\n        from canvod.utils.config import load_config\n\n        config = load_config()\n        sites = config.sites.sites\n\n        if site_name not in sites:\n            available_sites = list(sites.keys())\n            raise KeyError(\n                f\"Site '{site_name}' not found in config. \"\n                f\"Available sites: {available_sites}\"\n            )\n\n        self.site_name = site_name\n        self._site_config = sites[site_name]\n        self._logger = get_logger(__name__).bind(site=site_name)\n\n        rinex_store_path = config.processing.storage.get_rinex_store_path(site_name)\n        vod_store_path = config.processing.storage.get_vod_store_path(site_name)\n\n        # Initialize stores using paths from processing.yaml\n        self.rinex_store = create_rinex_store(rinex_store_path)\n        self.vod_store = create_vod_store(vod_store_path)\n\n        self._logger.info(\n            f\"Initialized GNSS research site: {site_name}\",\n            rinex_store=str(rinex_store_path),\n            vod_store=str(vod_store_path),\n        )\n\n    @property\n    def site_config(self) -&gt; dict[str, Any]:\n        \"\"\"Get the site configuration as a dictionary.\"\"\"\n        return self._site_config.model_dump()\n\n    @property\n    def receivers(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Get all configured receivers for this site.\"\"\"\n        return {\n            name: cfg.model_dump() for name, cfg in self._site_config.receivers.items()\n        }\n\n    @property\n    def active_receivers(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Get only active receivers for this site.\"\"\"\n        return {\n            name: config\n            for name, config in self.receivers.items()\n            if config.get(\"active\", True)\n        }\n\n    @property\n    def vod_analyses(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Get all configured VOD analyses for this site.\n\n        Returns auto-derived analyses from scs_from if no explicit\n        vod_analyses are configured.\n        \"\"\"\n        if self._site_config.vod_analyses is not None:\n            return {\n                name: cfg.model_dump()\n                for name, cfg in self._site_config.vod_analyses.items()\n            }\n        return self.get_auto_vod_analyses()\n\n    @property\n    def active_vod_analyses(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Get only active VOD analyses for this site.\"\"\"\n        return {\n            name: config\n            for name, config in self.vod_analyses.items()\n            if config.get(\"active\", True)\n        }\n\n    @classmethod\n    def from_rinex_store_path(\n        cls,\n        rinex_store_path: Path,\n    ) -&gt; GnssResearchSite:\n        \"\"\"\n        Create a GnssResearchSite instance from a RINEX store path.\n\n        Parameters\n        ----------\n        rinex_store_path : Path\n            Path to the RINEX Icechunk store.\n\n        Returns\n        -------\n        GnssResearchSite\n            Initialized research site manager.\n\n        Raises\n        ------\n        ValueError\n            If no matching site is found for the given path.\n        \"\"\"\n        # Load config to get store paths\n        from canvod.utils.config import load_config\n\n        config = load_config()\n\n        # Try to match against each site's expected rinex store path\n        for site_name in config.sites.sites.keys():\n            expected_path = config.processing.storage.get_rinex_store_path(site_name)\n            if expected_path == rinex_store_path:\n                return cls(site_name)\n\n        raise ValueError(\n            f\"No research site found for RINEX store path: {rinex_store_path}\"\n        )\n\n    def get_reference_canopy_pairs(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Expand scs_from into (reference_name, canopy_name) pairs.\n\n        Returns\n        -------\n        list[tuple[str, str]]\n            List of (reference_name, canopy_name) pairs.\n        \"\"\"\n        return self._site_config.get_reference_canopy_pairs()\n\n    def get_auto_vod_analyses(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Derive VOD analysis pairs from scs_from configuration.\n\n        Creates one VOD pair per (canopy, reference_for_that_canopy) combination.\n\n        Returns\n        -------\n        dict[str, dict[str, Any]]\n            Auto-derived VOD analyses keyed by analysis name.\n        \"\"\"\n        analyses: dict[str, dict[str, Any]] = {}\n        for ref_name, canopy_name in self.get_reference_canopy_pairs():\n            analysis_name = f\"{canopy_name}_vs_{ref_name}\"\n            analyses[analysis_name] = {\n                \"canopy_receiver\": canopy_name,\n                \"reference_receiver\": f\"{ref_name}_{canopy_name}\",\n                \"description\": f\"VOD analysis {canopy_name} vs {ref_name}\",\n            }\n        return analyses\n\n    def validate_site_config(self) -&gt; bool:\n        \"\"\"\n        Validate that the site configuration is consistent.\n\n        Returns\n        -------\n        bool\n            True if configuration is valid.\n\n        Raises\n        ------\n        ValueError\n            If configuration is invalid.\n        \"\"\"\n        # Check that all VOD analyses reference valid receivers\n        # Build set of valid reference store groups (e.g. reference_01_canopy_01)\n        valid_ref_groups = {\n            f\"{ref}_{canopy}\" for ref, canopy in self.get_reference_canopy_pairs()\n        }\n\n        for analysis_name, analysis_config in self.vod_analyses.items():\n            canopy_rx = analysis_config[\"canopy_receiver\"]\n            ref_rx = analysis_config[\"reference_receiver\"]\n\n            if canopy_rx not in self.receivers:\n                raise ValueError(\n                    f\"VOD analysis '{analysis_name}' references \"\n                    f\"unknown canopy receiver: {canopy_rx}\"\n                )\n            # ref_rx can be either a raw receiver name or a store group name\n            if ref_rx not in self.receivers and ref_rx not in valid_ref_groups:\n                raise ValueError(\n                    f\"VOD analysis '{analysis_name}' references \"\n                    f\"unknown reference receiver/group: {ref_rx}\"\n                )\n\n            # Check canopy type\n            canopy_type = self.receivers[canopy_rx][\"type\"]\n            if canopy_type != \"canopy\":\n                raise ValueError(\n                    f\"Receiver '{canopy_rx}' used as canopy but type is '{canopy_type}'\"\n                )\n            # Check reference type (only if it's a raw receiver name)\n            if ref_rx in self.receivers:\n                ref_type = self.receivers[ref_rx][\"type\"]\n                if ref_type != \"reference\":\n                    raise ValueError(\n                        f\"Receiver '{ref_rx}' used as reference but type is '{ref_type}'\"\n                    )\n\n        self._logger.debug(\"Site configuration validation passed\")\n        return True\n\n    def get_receiver_groups(self) -&gt; list[str]:\n        \"\"\"\n        Get list of receiver groups that exist in the RINEX store.\n\n        Returns\n        -------\n        list[str]\n            Existing receiver group names.\n        \"\"\"\n        return self.rinex_store.list_groups()\n\n    def get_vod_analysis_groups(self) -&gt; list[str]:\n        \"\"\"\n        Get list of VOD analysis groups that exist in the VOD store.\n\n        Returns\n        -------\n        list[str]\n            Existing VOD analysis group names.\n        \"\"\"\n        return self.vod_store.list_groups()\n\n    def ingest_rinex_data(\n        self, dataset: xr.Dataset, receiver_name: str, commit_message: str | None = None\n    ) -&gt; None:\n        \"\"\"\n        Ingest RINEX data for a specific receiver.\n\n        Parameters\n        ----------\n        dataset : xr.Dataset\n            Processed RINEX dataset to store.\n        receiver_name : str\n            Name of the receiver (must be configured).\n        commit_message : str, optional\n            Commit message to store with the data.\n\n        Raises\n        ------\n        ValueError\n            If ``receiver_name`` is not configured.\n        \"\"\"\n        if receiver_name not in self.receivers:\n            available_receivers = list(self.receivers.keys())\n            raise ValueError(\n                f\"Receiver '{receiver_name}' not configured. \"\n                f\"Available: {available_receivers}\"\n            )\n\n        self._logger.info(f\"Ingesting RINEX data for receiver '{receiver_name}'\")\n\n        self.rinex_store.write_or_append_group(\n            dataset=dataset, group_name=receiver_name, commit_message=commit_message\n        )\n\n        self._logger.info(f\"Successfully ingested data for receiver '{receiver_name}'\")\n\n    def read_receiver_data(\n        self, receiver_name: str, time_range: tuple[datetime, datetime] | None = None\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Read data from a specific receiver.\n\n        Parameters\n        ----------\n        receiver_name : str\n            Name of the receiver.\n        time_range : tuple of datetime, optional\n            (start_time, end_time) for filtering the data.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing receiver observations.\n\n        Raises\n        ------\n        ValueError\n            If the receiver group does not exist.\n        \"\"\"\n        if not self.rinex_store.group_exists(receiver_name):\n            available_groups = self.get_receiver_groups()\n            raise ValueError(\n                f\"No data found for receiver '{receiver_name}'. \"\n                f\"Available: {available_groups}\"\n            )\n\n        self._logger.info(f\"Reading data for receiver '{receiver_name}'\")\n\n        if self.rinex_store._rinex_store_strategy == \"append\":\n            ds = self.rinex_store.read_group_deduplicated(receiver_name, keep=\"last\")\n        else:\n            ds = self.rinex_store.read_group(receiver_name)\n\n        # Apply time filtering if specified\n        if time_range is not None:\n            start_time, end_time = time_range\n            ds = ds.where(\n                (ds.epoch &gt;= np.datetime64(start_time, \"ns\"))\n                &amp; (ds.epoch &lt;= np.datetime64(end_time, \"ns\")),\n                drop=True,\n            )\n\n            self._logger.debug(f\"Applied time filter: {start_time} to {end_time}\")\n\n        return ds\n\n    def store_vod_analysis(\n        self,\n        vod_dataset: xr.Dataset,\n        analysis_name: str,\n        commit_message: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Store VOD analysis results.\n\n        Parameters\n        ----------\n        vod_dataset : xr.Dataset\n            Dataset containing VOD analysis results.\n        analysis_name : str\n            Name of the analysis (must be configured).\n        commit_message : str, optional\n            Commit message to store with the results.\n\n        Raises\n        ------\n        ValueError\n            If ``analysis_name`` is not configured.\n        \"\"\"\n        if analysis_name not in self.vod_analyses:\n            available_analyses = list(self.vod_analyses.keys())\n            raise ValueError(\n                f\"VOD analysis '{analysis_name}' not configured. \"\n                f\"Available: {available_analyses}\"\n            )\n\n        self._logger.info(f\"Storing VOD analysis results: '{analysis_name}'\")\n\n        self.vod_store.write_or_append_group(\n            dataset=vod_dataset, group_name=analysis_name, commit_message=commit_message\n        )\n\n        self._logger.info(f\"Successfully stored VOD analysis: '{analysis_name}'\")\n\n    def read_vod_analysis(\n        self, analysis_name: str, time_range: tuple[datetime, datetime] | None = None\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Read VOD analysis results.\n\n        Parameters\n        ----------\n        analysis_name : str\n            Name of the analysis.\n        time_range : tuple of datetime, optional\n            (start_time, end_time) for filtering the results.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing VOD analysis results.\n\n        Raises\n        ------\n        ValueError\n            If the analysis group does not exist.\n        \"\"\"\n        if not self.vod_store.group_exists(analysis_name):\n            available_groups = self.get_vod_analysis_groups()\n            raise ValueError(\n                f\"No VOD results found for analysis '{analysis_name}'. \"\n                f\"Available: {available_groups}\"\n            )\n\n        self._logger.info(f\"Reading VOD analysis: '{analysis_name}'\")\n\n        ds = self.vod_store.read_group(analysis_name)\n\n        # Apply time filtering if specified\n        if time_range is not None:\n            start_time, end_time = time_range\n            ds = ds.sel(epoch=slice(start_time, end_time))\n            self._logger.debug(f\"Applied time filter: {start_time} to {end_time}\")\n\n        return ds\n\n    def prepare_vod_input_data(\n        self, analysis_name: str, time_range: tuple[datetime, datetime] | None = None\n    ) -&gt; tuple[xr.Dataset, xr.Dataset]:\n        \"\"\"\n        Prepare aligned input data for VOD analysis.\n\n        Reads data from both receivers specified in the analysis configuration\n        and returns them aligned for VOD processing.\n\n        Parameters\n        ----------\n        analysis_name : str\n            Name of the VOD analysis configuration.\n        time_range : tuple of datetime, optional\n            (start_time, end_time) for filtering the data.\n\n        Returns\n        -------\n        tuple of (xr.Dataset, xr.Dataset)\n            Tuple of (canopy_dataset, reference_dataset).\n\n        Raises\n        ------\n        ValueError\n            If the analysis is not configured or data is missing.\n        \"\"\"\n        if analysis_name not in self.vod_analyses:\n            available_analyses = list(self.vod_analyses.keys())\n            raise ValueError(\n                f\"VOD analysis '{analysis_name}' not configured. \"\n                f\"Available: {available_analyses}\"\n            )\n\n        analysis_config = self.vod_analyses[analysis_name]\n        canopy_receiver = analysis_config[\"canopy_receiver\"]\n        reference_receiver = analysis_config[\"reference_receiver\"]\n\n        self._logger.info(\n            f\"Preparing VOD input data: {canopy_receiver} vs {reference_receiver}\"\n        )\n\n        # Read data from both receivers\n        canopy_data = self.read_receiver_data(canopy_receiver, time_range)\n        reference_data = self.read_receiver_data(reference_receiver, time_range)\n\n        self._logger.info(\n            f\"Loaded data - Canopy: {dict(canopy_data.dims)}, \"\n            f\"Reference: {dict(reference_data.dims)}\"\n        )\n\n        return canopy_data, reference_data\n\n    def calculate_vod(\n        self,\n        analysis_name: str,\n        calculator_class: type[VODCalculator] | None = None,\n        time_range: tuple[datetime, datetime] | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Calculate VOD for a configured analysis pair.\n\n        Parameters\n        ----------\n        analysis_name : str\n            Analysis name from config (e.g., 'canopy_01_vs_reference_01')\n        calculator_class : type[VODCalculator], optional\n            VOD calculator class to use. If None, uses TauOmegaZerothOrder.\n        time_range : tuple of datetime, optional\n            (start_time, end_time) for filtering the data\n\n        Returns\n        -------\n        xr.Dataset\n            VOD dataset\n\n        Note\n        ----\n        Requires canvod-vod to be installed.\n        \"\"\"\n        if calculator_class is None:\n            try:\n                from canvod.vod import TauOmegaZerothOrder\n\n                calculator_class = TauOmegaZerothOrder\n            except ImportError as e:\n                raise ImportError(\n                    \"canvod-vod package required for VOD calculation. \"\n                    \"Install with: pip install canvod-vod\"\n                ) from e\n\n        canopy_ds, reference_ds = self.prepare_vod_input_data(analysis_name, time_range)\n\n        # Use the calculator's class method for calculation\n        vod_ds = calculator_class.from_datasets(canopy_ds, reference_ds, align=True)\n\n        # Add metadata\n        analysis_config = self.vod_analyses[analysis_name]\n        vod_ds.attrs[\"analysis_name\"] = analysis_name\n        vod_ds.attrs[\"canopy_receiver\"] = analysis_config[\"canopy_receiver\"]\n        vod_ds.attrs[\"reference_receiver\"] = analysis_config[\"reference_receiver\"]\n        vod_ds.attrs[\"calculator\"] = calculator_class.__name__\n        vod_ds.attrs[\"canopy_hash\"] = canopy_ds.attrs.get(\"RINEX File Hash\", \"unknown\")\n        vod_ds.attrs[\"reference_hash\"] = reference_ds.attrs.get(\n            \"RINEX File Hash\", \"unknown\"\n        )\n\n        self._logger.info(\n            f\"VOD calculated for {analysis_name} using {calculator_class.__name__}\"\n        )\n        return vod_ds\n\n    def store_vod(\n        self,\n        vod_ds: xr.Dataset,\n        analysis_name: str,\n    ) -&gt; str:\n        \"\"\"\n        Store VOD dataset in VOD store.\n\n        Parameters\n        ----------\n        vod_ds : xr.Dataset\n            VOD dataset to store\n        analysis_name : str\n            Analysis name (group name in store)\n\n        Returns\n        -------\n        str\n            Snapshot ID\n        \"\"\"\n        from gnssvodpy.utils.tools import get_version_from_pyproject\n        from icechunk.xarray import to_icechunk\n\n        canopy_hash = vod_ds.attrs.get(\"canopy_hash\", \"unknown\")\n        reference_hash = vod_ds.attrs.get(\"reference_hash\", \"unknown\")\n        combined_hash = f\"{canopy_hash}_{reference_hash}\"\n\n        with self.vod_store.writable_session() as session:\n            groups = self.vod_store.list_groups() or []\n\n            if analysis_name not in groups:\n                to_icechunk(vod_ds, session, group=analysis_name, mode=\"w\")\n                action = \"write\"\n            else:\n                to_icechunk(vod_ds, session, group=analysis_name, append_dim=\"epoch\")\n                action = \"append\"\n\n            version = get_version_from_pyproject()\n            commit_msg = f\"[v{version}] VOD for {analysis_name}\"\n            snapshot_id = session.commit(commit_msg)\n\n        self.vod_store.append_metadata(\n            group_name=analysis_name,\n            rinex_hash=combined_hash,\n            start=vod_ds[\"epoch\"].values[0],\n            end=vod_ds[\"epoch\"].values[-1],\n            snapshot_id=snapshot_id,\n            action=action,\n            commit_msg=commit_msg,\n            dataset_attrs=dict(vod_ds.attrs),\n        )\n\n        self._logger.info(\n            f\"VOD stored for {analysis_name}, snapshot={snapshot_id[:8]}...\"\n        )\n        return snapshot_id\n\n    def get_site_summary(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get a comprehensive summary of the research site.\n\n        Returns\n        -------\n        dict\n            Dictionary with site statistics, data availability, and store paths.\n        \"\"\"\n        rinex_groups = self.get_receiver_groups()\n        vod_groups = self.get_vod_analysis_groups()\n\n        summary = {\n            \"site_name\": self.site_name,\n            \"site_config\": {\n                \"total_receivers\": len(self.receivers),\n                \"active_receivers\": len(self.active_receivers),\n                \"total_vod_analyses\": len(self.vod_analyses),\n                \"active_vod_analyses\": len(self.active_vod_analyses),\n            },\n            \"data_status\": {\n                \"rinex_groups_exist\": len(rinex_groups),\n                \"rinex_groups\": rinex_groups,\n                \"vod_groups_exist\": len(vod_groups),\n                \"vod_groups\": vod_groups,\n            },\n            \"stores\": {\n                \"rinex_store_path\": str(self.rinex_store.store_path),\n                \"vod_store_path\": str(self.vod_store.store_path),\n            },\n        }\n\n        # Add receiver details\n        summary[\"receivers\"] = {}\n        for receiver_name, receiver_config in self.active_receivers.items():\n            has_data = receiver_name in rinex_groups\n            summary[\"receivers\"][receiver_name] = {\n                \"type\": receiver_config[\"type\"],\n                \"description\": receiver_config[\"description\"],\n                \"has_data\": has_data,\n            }\n\n            if has_data:\n                try:\n                    info = self.rinex_store.get_group_info(receiver_name)\n                    summary[\"receivers\"][receiver_name][\"data_info\"] = {\n                        \"dimensions\": info[\"dimensions\"],\n                        \"variables\": len(info[\"variables\"]),\n                        \"temporal_info\": info.get(\"temporal_info\", {}),\n                    }\n                except Exception as e:\n                    self._logger.warning(f\"Failed to get info for {receiver_name}: {e}\")\n\n        # Add VOD analysis details\n        summary[\"vod_analyses\"] = {}\n        for analysis_name, analysis_config in self.active_vod_analyses.items():\n            has_results = analysis_name in vod_groups\n            summary[\"vod_analyses\"][analysis_name] = {\n                \"canopy_receiver\": analysis_config[\"canopy_receiver\"],\n                \"reference_receiver\": analysis_config[\"reference_receiver\"],\n                \"description\": analysis_config[\"description\"],\n                \"has_results\": has_results,\n            }\n\n            if has_results:\n                try:\n                    info = self.vod_store.get_group_info(analysis_name)\n                    summary[\"vod_analyses\"][analysis_name][\"results_info\"] = {\n                        \"dimensions\": info[\"dimensions\"],\n                        \"variables\": len(info[\"variables\"]),\n                        \"temporal_info\": info.get(\"temporal_info\", {}),\n                    }\n                except Exception as e:\n                    self._logger.warning(\n                        f\"Failed to get VOD info for {analysis_name}: {e}\"\n                    )\n\n        return summary\n\n    def is_day_complete(\n        self,\n        yyyydoy: str,\n        receiver_types: list[str] | None = None,\n        completeness_threshold: float = 0.95,\n    ) -&gt; bool:\n        \"\"\"\n        Check if a day has complete data coverage for all receiver types.\n\n        Parameters\n        ----------\n        yyyydoy : str\n            Date in YYYYDOY format (e.g., \"2024256\")\n        receiver_types : List[str], optional\n            Receiver types to check. Defaults to ['canopy', 'reference']\n        completeness_threshold : float\n            Fraction of expected epochs that must exist (default 0.95 = 95%)\n            Allows for small gaps due to receiver issues\n\n        Returns\n        -------\n        bool\n            True if all receiver types have complete data for this day\n        \"\"\"\n        if receiver_types is None:\n            receiver_types = [\"canopy\", \"reference\"]\n\n        from gnssvodpy.utils.date_time import YYYYDOY\n\n        yyyydoy_obj = YYYYDOY.from_str(yyyydoy)\n\n        # Expected epochs for 24h at 30s sampling\n        expected_epochs = int(24 * 3600 / 30)  # 2880 epochs\n        required_epochs = int(expected_epochs * completeness_threshold)\n\n        for receiver_type in receiver_types:\n            # Get receiver name for this type\n            receiver_name = None\n            for name, config in self.active_receivers.items():\n                if config.get(\"type\") == receiver_type:\n                    receiver_name = name\n                    break\n\n            if not receiver_name:\n                self._logger.warning(f\"No receiver configured for type {receiver_type}\")\n                return False\n\n            try:\n                # Try to read data for this day\n                time_range = (yyyydoy_obj.start_datetime(), yyyydoy_obj.end_datetime())\n\n                ds = self.read_receiver_data(\n                    receiver_name=receiver_name, time_range=time_range\n                )\n\n                # Check epoch count\n                n_epochs = ds.sizes.get(\"epoch\", 0)\n\n                if n_epochs &lt; required_epochs:\n                    self._logger.info(\n                        f\"{receiver_name} {yyyydoy}: Only \"\n                        f\"{n_epochs}/{expected_epochs} epochs \"\n                        f\"({n_epochs / expected_epochs * 100:.1f}%) - incomplete\"\n                    )\n                    return False\n\n                self._logger.debug(\n                    f\"{receiver_name} {yyyydoy}: \"\n                    f\"{n_epochs}/{expected_epochs} epochs - complete\"\n                )\n\n            except (ValueError, KeyError, Exception) as e:\n                # No data exists or error reading\n                self._logger.debug(f\"{receiver_name} {yyyydoy}: No data found - {e}\")\n                return False\n\n        # All receiver types have complete data\n        return True\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the developer-facing representation.\n\n        Returns\n        -------\n        str\n            Representation string.\n        \"\"\"\n        return f\"GnssResearchSite(site_name='{self.site_name}')\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable summary.\n\n        Returns\n        -------\n        str\n            Summary string.\n        \"\"\"\n        rinex_groups = len(self.get_receiver_groups())\n        vod_groups = len(self.get_vod_analysis_groups())\n        return (\n            f\"GNSS Research Site: {self.site_name}\\n\"\n            f\"  Receivers: {len(self.active_receivers)} configured, \"\n            f\"{rinex_groups} with data\\n\"\n            f\"  VOD Analyses: {len(self.active_vod_analyses)} configured, \"\n            f\"{vod_groups} with results\"\n        )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.site_config","level":3,"title":"<code>site_config</code>  <code>property</code>","text":"<p>Get the site configuration as a dictionary.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.receivers","level":3,"title":"<code>receivers</code>  <code>property</code>","text":"<p>Get all configured receivers for this site.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.active_receivers","level":3,"title":"<code>active_receivers</code>  <code>property</code>","text":"<p>Get only active receivers for this site.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.vod_analyses","level":3,"title":"<code>vod_analyses</code>  <code>property</code>","text":"<p>Get all configured VOD analyses for this site.</p> <p>Returns auto-derived analyses from scs_from if no explicit vod_analyses are configured.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.active_vod_analyses","level":3,"title":"<code>active_vod_analyses</code>  <code>property</code>","text":"<p>Get only active VOD analyses for this site.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.__init__","level":3,"title":"<code>__init__(site_name)</code>","text":"<p>Initialize the site manager.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.__init__--parameters","level":5,"title":"Parameters","text":"<p>site_name : str     Name of the research site.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def __init__(self, site_name: str) -&gt; None:\n    \"\"\"Initialize the site manager.\n\n    Parameters\n    ----------\n    site_name : str\n        Name of the research site.\n    \"\"\"\n    from canvod.utils.config import load_config\n\n    config = load_config()\n    sites = config.sites.sites\n\n    if site_name not in sites:\n        available_sites = list(sites.keys())\n        raise KeyError(\n            f\"Site '{site_name}' not found in config. \"\n            f\"Available sites: {available_sites}\"\n        )\n\n    self.site_name = site_name\n    self._site_config = sites[site_name]\n    self._logger = get_logger(__name__).bind(site=site_name)\n\n    rinex_store_path = config.processing.storage.get_rinex_store_path(site_name)\n    vod_store_path = config.processing.storage.get_vod_store_path(site_name)\n\n    # Initialize stores using paths from processing.yaml\n    self.rinex_store = create_rinex_store(rinex_store_path)\n    self.vod_store = create_vod_store(vod_store_path)\n\n    self._logger.info(\n        f\"Initialized GNSS research site: {site_name}\",\n        rinex_store=str(rinex_store_path),\n        vod_store=str(vod_store_path),\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.from_rinex_store_path","level":3,"title":"<code>from_rinex_store_path(rinex_store_path)</code>  <code>classmethod</code>","text":"<p>Create a GnssResearchSite instance from a RINEX store path.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.from_rinex_store_path--parameters","level":5,"title":"Parameters","text":"<p>rinex_store_path : Path     Path to the RINEX Icechunk store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.from_rinex_store_path--returns","level":5,"title":"Returns","text":"<p>GnssResearchSite     Initialized research site manager.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.from_rinex_store_path--raises","level":5,"title":"Raises","text":"<p>ValueError     If no matching site is found for the given path.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>@classmethod\ndef from_rinex_store_path(\n    cls,\n    rinex_store_path: Path,\n) -&gt; GnssResearchSite:\n    \"\"\"\n    Create a GnssResearchSite instance from a RINEX store path.\n\n    Parameters\n    ----------\n    rinex_store_path : Path\n        Path to the RINEX Icechunk store.\n\n    Returns\n    -------\n    GnssResearchSite\n        Initialized research site manager.\n\n    Raises\n    ------\n    ValueError\n        If no matching site is found for the given path.\n    \"\"\"\n    # Load config to get store paths\n    from canvod.utils.config import load_config\n\n    config = load_config()\n\n    # Try to match against each site's expected rinex store path\n    for site_name in config.sites.sites.keys():\n        expected_path = config.processing.storage.get_rinex_store_path(site_name)\n        if expected_path == rinex_store_path:\n            return cls(site_name)\n\n    raise ValueError(\n        f\"No research site found for RINEX store path: {rinex_store_path}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.get_reference_canopy_pairs","level":3,"title":"<code>get_reference_canopy_pairs()</code>","text":"<p>Expand scs_from into (reference_name, canopy_name) pairs.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.get_reference_canopy_pairs--returns","level":5,"title":"Returns","text":"<p>list[tuple[str, str]]     List of (reference_name, canopy_name) pairs.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def get_reference_canopy_pairs(self) -&gt; list[tuple[str, str]]:\n    \"\"\"Expand scs_from into (reference_name, canopy_name) pairs.\n\n    Returns\n    -------\n    list[tuple[str, str]]\n        List of (reference_name, canopy_name) pairs.\n    \"\"\"\n    return self._site_config.get_reference_canopy_pairs()\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.get_auto_vod_analyses","level":3,"title":"<code>get_auto_vod_analyses()</code>","text":"<p>Derive VOD analysis pairs from scs_from configuration.</p> <p>Creates one VOD pair per (canopy, reference_for_that_canopy) combination.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.get_auto_vod_analyses--returns","level":5,"title":"Returns","text":"<p>dict[str, dict[str, Any]]     Auto-derived VOD analyses keyed by analysis name.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def get_auto_vod_analyses(self) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Derive VOD analysis pairs from scs_from configuration.\n\n    Creates one VOD pair per (canopy, reference_for_that_canopy) combination.\n\n    Returns\n    -------\n    dict[str, dict[str, Any]]\n        Auto-derived VOD analyses keyed by analysis name.\n    \"\"\"\n    analyses: dict[str, dict[str, Any]] = {}\n    for ref_name, canopy_name in self.get_reference_canopy_pairs():\n        analysis_name = f\"{canopy_name}_vs_{ref_name}\"\n        analyses[analysis_name] = {\n            \"canopy_receiver\": canopy_name,\n            \"reference_receiver\": f\"{ref_name}_{canopy_name}\",\n            \"description\": f\"VOD analysis {canopy_name} vs {ref_name}\",\n        }\n    return analyses\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.validate_site_config","level":3,"title":"<code>validate_site_config()</code>","text":"<p>Validate that the site configuration is consistent.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.validate_site_config--returns","level":5,"title":"Returns","text":"<p>bool     True if configuration is valid.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.validate_site_config--raises","level":5,"title":"Raises","text":"<p>ValueError     If configuration is invalid.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def validate_site_config(self) -&gt; bool:\n    \"\"\"\n    Validate that the site configuration is consistent.\n\n    Returns\n    -------\n    bool\n        True if configuration is valid.\n\n    Raises\n    ------\n    ValueError\n        If configuration is invalid.\n    \"\"\"\n    # Check that all VOD analyses reference valid receivers\n    # Build set of valid reference store groups (e.g. reference_01_canopy_01)\n    valid_ref_groups = {\n        f\"{ref}_{canopy}\" for ref, canopy in self.get_reference_canopy_pairs()\n    }\n\n    for analysis_name, analysis_config in self.vod_analyses.items():\n        canopy_rx = analysis_config[\"canopy_receiver\"]\n        ref_rx = analysis_config[\"reference_receiver\"]\n\n        if canopy_rx not in self.receivers:\n            raise ValueError(\n                f\"VOD analysis '{analysis_name}' references \"\n                f\"unknown canopy receiver: {canopy_rx}\"\n            )\n        # ref_rx can be either a raw receiver name or a store group name\n        if ref_rx not in self.receivers and ref_rx not in valid_ref_groups:\n            raise ValueError(\n                f\"VOD analysis '{analysis_name}' references \"\n                f\"unknown reference receiver/group: {ref_rx}\"\n            )\n\n        # Check canopy type\n        canopy_type = self.receivers[canopy_rx][\"type\"]\n        if canopy_type != \"canopy\":\n            raise ValueError(\n                f\"Receiver '{canopy_rx}' used as canopy but type is '{canopy_type}'\"\n            )\n        # Check reference type (only if it's a raw receiver name)\n        if ref_rx in self.receivers:\n            ref_type = self.receivers[ref_rx][\"type\"]\n            if ref_type != \"reference\":\n                raise ValueError(\n                    f\"Receiver '{ref_rx}' used as reference but type is '{ref_type}'\"\n                )\n\n    self._logger.debug(\"Site configuration validation passed\")\n    return True\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.get_receiver_groups","level":3,"title":"<code>get_receiver_groups()</code>","text":"<p>Get list of receiver groups that exist in the RINEX store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.get_receiver_groups--returns","level":5,"title":"Returns","text":"<p>list[str]     Existing receiver group names.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def get_receiver_groups(self) -&gt; list[str]:\n    \"\"\"\n    Get list of receiver groups that exist in the RINEX store.\n\n    Returns\n    -------\n    list[str]\n        Existing receiver group names.\n    \"\"\"\n    return self.rinex_store.list_groups()\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.get_vod_analysis_groups","level":3,"title":"<code>get_vod_analysis_groups()</code>","text":"<p>Get list of VOD analysis groups that exist in the VOD store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.get_vod_analysis_groups--returns","level":5,"title":"Returns","text":"<p>list[str]     Existing VOD analysis group names.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def get_vod_analysis_groups(self) -&gt; list[str]:\n    \"\"\"\n    Get list of VOD analysis groups that exist in the VOD store.\n\n    Returns\n    -------\n    list[str]\n        Existing VOD analysis group names.\n    \"\"\"\n    return self.vod_store.list_groups()\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.ingest_rinex_data","level":3,"title":"<code>ingest_rinex_data(dataset, receiver_name, commit_message=None)</code>","text":"<p>Ingest RINEX data for a specific receiver.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.ingest_rinex_data--parameters","level":5,"title":"Parameters","text":"<p>dataset : xr.Dataset     Processed RINEX dataset to store. receiver_name : str     Name of the receiver (must be configured). commit_message : str, optional     Commit message to store with the data.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.ingest_rinex_data--raises","level":5,"title":"Raises","text":"<p>ValueError     If <code>receiver_name</code> is not configured.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def ingest_rinex_data(\n    self, dataset: xr.Dataset, receiver_name: str, commit_message: str | None = None\n) -&gt; None:\n    \"\"\"\n    Ingest RINEX data for a specific receiver.\n\n    Parameters\n    ----------\n    dataset : xr.Dataset\n        Processed RINEX dataset to store.\n    receiver_name : str\n        Name of the receiver (must be configured).\n    commit_message : str, optional\n        Commit message to store with the data.\n\n    Raises\n    ------\n    ValueError\n        If ``receiver_name`` is not configured.\n    \"\"\"\n    if receiver_name not in self.receivers:\n        available_receivers = list(self.receivers.keys())\n        raise ValueError(\n            f\"Receiver '{receiver_name}' not configured. \"\n            f\"Available: {available_receivers}\"\n        )\n\n    self._logger.info(f\"Ingesting RINEX data for receiver '{receiver_name}'\")\n\n    self.rinex_store.write_or_append_group(\n        dataset=dataset, group_name=receiver_name, commit_message=commit_message\n    )\n\n    self._logger.info(f\"Successfully ingested data for receiver '{receiver_name}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.read_receiver_data","level":3,"title":"<code>read_receiver_data(receiver_name, time_range=None)</code>","text":"<p>Read data from a specific receiver.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.read_receiver_data--parameters","level":5,"title":"Parameters","text":"<p>receiver_name : str     Name of the receiver. time_range : tuple of datetime, optional     (start_time, end_time) for filtering the data.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.read_receiver_data--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset containing receiver observations.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.read_receiver_data--raises","level":5,"title":"Raises","text":"<p>ValueError     If the receiver group does not exist.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def read_receiver_data(\n    self, receiver_name: str, time_range: tuple[datetime, datetime] | None = None\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read data from a specific receiver.\n\n    Parameters\n    ----------\n    receiver_name : str\n        Name of the receiver.\n    time_range : tuple of datetime, optional\n        (start_time, end_time) for filtering the data.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing receiver observations.\n\n    Raises\n    ------\n    ValueError\n        If the receiver group does not exist.\n    \"\"\"\n    if not self.rinex_store.group_exists(receiver_name):\n        available_groups = self.get_receiver_groups()\n        raise ValueError(\n            f\"No data found for receiver '{receiver_name}'. \"\n            f\"Available: {available_groups}\"\n        )\n\n    self._logger.info(f\"Reading data for receiver '{receiver_name}'\")\n\n    if self.rinex_store._rinex_store_strategy == \"append\":\n        ds = self.rinex_store.read_group_deduplicated(receiver_name, keep=\"last\")\n    else:\n        ds = self.rinex_store.read_group(receiver_name)\n\n    # Apply time filtering if specified\n    if time_range is not None:\n        start_time, end_time = time_range\n        ds = ds.where(\n            (ds.epoch &gt;= np.datetime64(start_time, \"ns\"))\n            &amp; (ds.epoch &lt;= np.datetime64(end_time, \"ns\")),\n            drop=True,\n        )\n\n        self._logger.debug(f\"Applied time filter: {start_time} to {end_time}\")\n\n    return ds\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.store_vod_analysis","level":3,"title":"<code>store_vod_analysis(vod_dataset, analysis_name, commit_message=None)</code>","text":"<p>Store VOD analysis results.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.store_vod_analysis--parameters","level":5,"title":"Parameters","text":"<p>vod_dataset : xr.Dataset     Dataset containing VOD analysis results. analysis_name : str     Name of the analysis (must be configured). commit_message : str, optional     Commit message to store with the results.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.store_vod_analysis--raises","level":5,"title":"Raises","text":"<p>ValueError     If <code>analysis_name</code> is not configured.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def store_vod_analysis(\n    self,\n    vod_dataset: xr.Dataset,\n    analysis_name: str,\n    commit_message: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Store VOD analysis results.\n\n    Parameters\n    ----------\n    vod_dataset : xr.Dataset\n        Dataset containing VOD analysis results.\n    analysis_name : str\n        Name of the analysis (must be configured).\n    commit_message : str, optional\n        Commit message to store with the results.\n\n    Raises\n    ------\n    ValueError\n        If ``analysis_name`` is not configured.\n    \"\"\"\n    if analysis_name not in self.vod_analyses:\n        available_analyses = list(self.vod_analyses.keys())\n        raise ValueError(\n            f\"VOD analysis '{analysis_name}' not configured. \"\n            f\"Available: {available_analyses}\"\n        )\n\n    self._logger.info(f\"Storing VOD analysis results: '{analysis_name}'\")\n\n    self.vod_store.write_or_append_group(\n        dataset=vod_dataset, group_name=analysis_name, commit_message=commit_message\n    )\n\n    self._logger.info(f\"Successfully stored VOD analysis: '{analysis_name}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.read_vod_analysis","level":3,"title":"<code>read_vod_analysis(analysis_name, time_range=None)</code>","text":"<p>Read VOD analysis results.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.read_vod_analysis--parameters","level":5,"title":"Parameters","text":"<p>analysis_name : str     Name of the analysis. time_range : tuple of datetime, optional     (start_time, end_time) for filtering the results.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.read_vod_analysis--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset containing VOD analysis results.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.read_vod_analysis--raises","level":5,"title":"Raises","text":"<p>ValueError     If the analysis group does not exist.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def read_vod_analysis(\n    self, analysis_name: str, time_range: tuple[datetime, datetime] | None = None\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read VOD analysis results.\n\n    Parameters\n    ----------\n    analysis_name : str\n        Name of the analysis.\n    time_range : tuple of datetime, optional\n        (start_time, end_time) for filtering the results.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing VOD analysis results.\n\n    Raises\n    ------\n    ValueError\n        If the analysis group does not exist.\n    \"\"\"\n    if not self.vod_store.group_exists(analysis_name):\n        available_groups = self.get_vod_analysis_groups()\n        raise ValueError(\n            f\"No VOD results found for analysis '{analysis_name}'. \"\n            f\"Available: {available_groups}\"\n        )\n\n    self._logger.info(f\"Reading VOD analysis: '{analysis_name}'\")\n\n    ds = self.vod_store.read_group(analysis_name)\n\n    # Apply time filtering if specified\n    if time_range is not None:\n        start_time, end_time = time_range\n        ds = ds.sel(epoch=slice(start_time, end_time))\n        self._logger.debug(f\"Applied time filter: {start_time} to {end_time}\")\n\n    return ds\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.prepare_vod_input_data","level":3,"title":"<code>prepare_vod_input_data(analysis_name, time_range=None)</code>","text":"<p>Prepare aligned input data for VOD analysis.</p> <p>Reads data from both receivers specified in the analysis configuration and returns them aligned for VOD processing.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.prepare_vod_input_data--parameters","level":5,"title":"Parameters","text":"<p>analysis_name : str     Name of the VOD analysis configuration. time_range : tuple of datetime, optional     (start_time, end_time) for filtering the data.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.prepare_vod_input_data--returns","level":5,"title":"Returns","text":"<p>tuple of (xr.Dataset, xr.Dataset)     Tuple of (canopy_dataset, reference_dataset).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.prepare_vod_input_data--raises","level":5,"title":"Raises","text":"<p>ValueError     If the analysis is not configured or data is missing.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def prepare_vod_input_data(\n    self, analysis_name: str, time_range: tuple[datetime, datetime] | None = None\n) -&gt; tuple[xr.Dataset, xr.Dataset]:\n    \"\"\"\n    Prepare aligned input data for VOD analysis.\n\n    Reads data from both receivers specified in the analysis configuration\n    and returns them aligned for VOD processing.\n\n    Parameters\n    ----------\n    analysis_name : str\n        Name of the VOD analysis configuration.\n    time_range : tuple of datetime, optional\n        (start_time, end_time) for filtering the data.\n\n    Returns\n    -------\n    tuple of (xr.Dataset, xr.Dataset)\n        Tuple of (canopy_dataset, reference_dataset).\n\n    Raises\n    ------\n    ValueError\n        If the analysis is not configured or data is missing.\n    \"\"\"\n    if analysis_name not in self.vod_analyses:\n        available_analyses = list(self.vod_analyses.keys())\n        raise ValueError(\n            f\"VOD analysis '{analysis_name}' not configured. \"\n            f\"Available: {available_analyses}\"\n        )\n\n    analysis_config = self.vod_analyses[analysis_name]\n    canopy_receiver = analysis_config[\"canopy_receiver\"]\n    reference_receiver = analysis_config[\"reference_receiver\"]\n\n    self._logger.info(\n        f\"Preparing VOD input data: {canopy_receiver} vs {reference_receiver}\"\n    )\n\n    # Read data from both receivers\n    canopy_data = self.read_receiver_data(canopy_receiver, time_range)\n    reference_data = self.read_receiver_data(reference_receiver, time_range)\n\n    self._logger.info(\n        f\"Loaded data - Canopy: {dict(canopy_data.dims)}, \"\n        f\"Reference: {dict(reference_data.dims)}\"\n    )\n\n    return canopy_data, reference_data\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.calculate_vod","level":3,"title":"<code>calculate_vod(analysis_name, calculator_class=None, time_range=None)</code>","text":"<p>Calculate VOD for a configured analysis pair.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.calculate_vod--parameters","level":5,"title":"Parameters","text":"<p>analysis_name : str     Analysis name from config (e.g., 'canopy_01_vs_reference_01') calculator_class : type[VODCalculator], optional     VOD calculator class to use. If None, uses TauOmegaZerothOrder. time_range : tuple of datetime, optional     (start_time, end_time) for filtering the data</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.calculate_vod--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     VOD dataset</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.calculate_vod--note","level":5,"title":"Note","text":"<p>Requires canvod-vod to be installed.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def calculate_vod(\n    self,\n    analysis_name: str,\n    calculator_class: type[VODCalculator] | None = None,\n    time_range: tuple[datetime, datetime] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Calculate VOD for a configured analysis pair.\n\n    Parameters\n    ----------\n    analysis_name : str\n        Analysis name from config (e.g., 'canopy_01_vs_reference_01')\n    calculator_class : type[VODCalculator], optional\n        VOD calculator class to use. If None, uses TauOmegaZerothOrder.\n    time_range : tuple of datetime, optional\n        (start_time, end_time) for filtering the data\n\n    Returns\n    -------\n    xr.Dataset\n        VOD dataset\n\n    Note\n    ----\n    Requires canvod-vod to be installed.\n    \"\"\"\n    if calculator_class is None:\n        try:\n            from canvod.vod import TauOmegaZerothOrder\n\n            calculator_class = TauOmegaZerothOrder\n        except ImportError as e:\n            raise ImportError(\n                \"canvod-vod package required for VOD calculation. \"\n                \"Install with: pip install canvod-vod\"\n            ) from e\n\n    canopy_ds, reference_ds = self.prepare_vod_input_data(analysis_name, time_range)\n\n    # Use the calculator's class method for calculation\n    vod_ds = calculator_class.from_datasets(canopy_ds, reference_ds, align=True)\n\n    # Add metadata\n    analysis_config = self.vod_analyses[analysis_name]\n    vod_ds.attrs[\"analysis_name\"] = analysis_name\n    vod_ds.attrs[\"canopy_receiver\"] = analysis_config[\"canopy_receiver\"]\n    vod_ds.attrs[\"reference_receiver\"] = analysis_config[\"reference_receiver\"]\n    vod_ds.attrs[\"calculator\"] = calculator_class.__name__\n    vod_ds.attrs[\"canopy_hash\"] = canopy_ds.attrs.get(\"RINEX File Hash\", \"unknown\")\n    vod_ds.attrs[\"reference_hash\"] = reference_ds.attrs.get(\n        \"RINEX File Hash\", \"unknown\"\n    )\n\n    self._logger.info(\n        f\"VOD calculated for {analysis_name} using {calculator_class.__name__}\"\n    )\n    return vod_ds\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.store_vod","level":3,"title":"<code>store_vod(vod_ds, analysis_name)</code>","text":"<p>Store VOD dataset in VOD store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.store_vod--parameters","level":5,"title":"Parameters","text":"<p>vod_ds : xr.Dataset     VOD dataset to store analysis_name : str     Analysis name (group name in store)</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.store_vod--returns","level":5,"title":"Returns","text":"<p>str     Snapshot ID</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def store_vod(\n    self,\n    vod_ds: xr.Dataset,\n    analysis_name: str,\n) -&gt; str:\n    \"\"\"\n    Store VOD dataset in VOD store.\n\n    Parameters\n    ----------\n    vod_ds : xr.Dataset\n        VOD dataset to store\n    analysis_name : str\n        Analysis name (group name in store)\n\n    Returns\n    -------\n    str\n        Snapshot ID\n    \"\"\"\n    from gnssvodpy.utils.tools import get_version_from_pyproject\n    from icechunk.xarray import to_icechunk\n\n    canopy_hash = vod_ds.attrs.get(\"canopy_hash\", \"unknown\")\n    reference_hash = vod_ds.attrs.get(\"reference_hash\", \"unknown\")\n    combined_hash = f\"{canopy_hash}_{reference_hash}\"\n\n    with self.vod_store.writable_session() as session:\n        groups = self.vod_store.list_groups() or []\n\n        if analysis_name not in groups:\n            to_icechunk(vod_ds, session, group=analysis_name, mode=\"w\")\n            action = \"write\"\n        else:\n            to_icechunk(vod_ds, session, group=analysis_name, append_dim=\"epoch\")\n            action = \"append\"\n\n        version = get_version_from_pyproject()\n        commit_msg = f\"[v{version}] VOD for {analysis_name}\"\n        snapshot_id = session.commit(commit_msg)\n\n    self.vod_store.append_metadata(\n        group_name=analysis_name,\n        rinex_hash=combined_hash,\n        start=vod_ds[\"epoch\"].values[0],\n        end=vod_ds[\"epoch\"].values[-1],\n        snapshot_id=snapshot_id,\n        action=action,\n        commit_msg=commit_msg,\n        dataset_attrs=dict(vod_ds.attrs),\n    )\n\n    self._logger.info(\n        f\"VOD stored for {analysis_name}, snapshot={snapshot_id[:8]}...\"\n    )\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.get_site_summary","level":3,"title":"<code>get_site_summary()</code>","text":"<p>Get a comprehensive summary of the research site.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.get_site_summary--returns","level":5,"title":"Returns","text":"<p>dict     Dictionary with site statistics, data availability, and store paths.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def get_site_summary(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get a comprehensive summary of the research site.\n\n    Returns\n    -------\n    dict\n        Dictionary with site statistics, data availability, and store paths.\n    \"\"\"\n    rinex_groups = self.get_receiver_groups()\n    vod_groups = self.get_vod_analysis_groups()\n\n    summary = {\n        \"site_name\": self.site_name,\n        \"site_config\": {\n            \"total_receivers\": len(self.receivers),\n            \"active_receivers\": len(self.active_receivers),\n            \"total_vod_analyses\": len(self.vod_analyses),\n            \"active_vod_analyses\": len(self.active_vod_analyses),\n        },\n        \"data_status\": {\n            \"rinex_groups_exist\": len(rinex_groups),\n            \"rinex_groups\": rinex_groups,\n            \"vod_groups_exist\": len(vod_groups),\n            \"vod_groups\": vod_groups,\n        },\n        \"stores\": {\n            \"rinex_store_path\": str(self.rinex_store.store_path),\n            \"vod_store_path\": str(self.vod_store.store_path),\n        },\n    }\n\n    # Add receiver details\n    summary[\"receivers\"] = {}\n    for receiver_name, receiver_config in self.active_receivers.items():\n        has_data = receiver_name in rinex_groups\n        summary[\"receivers\"][receiver_name] = {\n            \"type\": receiver_config[\"type\"],\n            \"description\": receiver_config[\"description\"],\n            \"has_data\": has_data,\n        }\n\n        if has_data:\n            try:\n                info = self.rinex_store.get_group_info(receiver_name)\n                summary[\"receivers\"][receiver_name][\"data_info\"] = {\n                    \"dimensions\": info[\"dimensions\"],\n                    \"variables\": len(info[\"variables\"]),\n                    \"temporal_info\": info.get(\"temporal_info\", {}),\n                }\n            except Exception as e:\n                self._logger.warning(f\"Failed to get info for {receiver_name}: {e}\")\n\n    # Add VOD analysis details\n    summary[\"vod_analyses\"] = {}\n    for analysis_name, analysis_config in self.active_vod_analyses.items():\n        has_results = analysis_name in vod_groups\n        summary[\"vod_analyses\"][analysis_name] = {\n            \"canopy_receiver\": analysis_config[\"canopy_receiver\"],\n            \"reference_receiver\": analysis_config[\"reference_receiver\"],\n            \"description\": analysis_config[\"description\"],\n            \"has_results\": has_results,\n        }\n\n        if has_results:\n            try:\n                info = self.vod_store.get_group_info(analysis_name)\n                summary[\"vod_analyses\"][analysis_name][\"results_info\"] = {\n                    \"dimensions\": info[\"dimensions\"],\n                    \"variables\": len(info[\"variables\"]),\n                    \"temporal_info\": info.get(\"temporal_info\", {}),\n                }\n            except Exception as e:\n                self._logger.warning(\n                    f\"Failed to get VOD info for {analysis_name}: {e}\"\n                )\n\n    return summary\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.is_day_complete","level":3,"title":"<code>is_day_complete(yyyydoy, receiver_types=None, completeness_threshold=0.95)</code>","text":"<p>Check if a day has complete data coverage for all receiver types.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.is_day_complete--parameters","level":5,"title":"Parameters","text":"<p>yyyydoy : str     Date in YYYYDOY format (e.g., \"2024256\") receiver_types : List[str], optional     Receiver types to check. Defaults to ['canopy', 'reference'] completeness_threshold : float     Fraction of expected epochs that must exist (default 0.95 = 95%)     Allows for small gaps due to receiver issues</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.is_day_complete--returns","level":5,"title":"Returns","text":"<p>bool     True if all receiver types have complete data for this day</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def is_day_complete(\n    self,\n    yyyydoy: str,\n    receiver_types: list[str] | None = None,\n    completeness_threshold: float = 0.95,\n) -&gt; bool:\n    \"\"\"\n    Check if a day has complete data coverage for all receiver types.\n\n    Parameters\n    ----------\n    yyyydoy : str\n        Date in YYYYDOY format (e.g., \"2024256\")\n    receiver_types : List[str], optional\n        Receiver types to check. Defaults to ['canopy', 'reference']\n    completeness_threshold : float\n        Fraction of expected epochs that must exist (default 0.95 = 95%)\n        Allows for small gaps due to receiver issues\n\n    Returns\n    -------\n    bool\n        True if all receiver types have complete data for this day\n    \"\"\"\n    if receiver_types is None:\n        receiver_types = [\"canopy\", \"reference\"]\n\n    from gnssvodpy.utils.date_time import YYYYDOY\n\n    yyyydoy_obj = YYYYDOY.from_str(yyyydoy)\n\n    # Expected epochs for 24h at 30s sampling\n    expected_epochs = int(24 * 3600 / 30)  # 2880 epochs\n    required_epochs = int(expected_epochs * completeness_threshold)\n\n    for receiver_type in receiver_types:\n        # Get receiver name for this type\n        receiver_name = None\n        for name, config in self.active_receivers.items():\n            if config.get(\"type\") == receiver_type:\n                receiver_name = name\n                break\n\n        if not receiver_name:\n            self._logger.warning(f\"No receiver configured for type {receiver_type}\")\n            return False\n\n        try:\n            # Try to read data for this day\n            time_range = (yyyydoy_obj.start_datetime(), yyyydoy_obj.end_datetime())\n\n            ds = self.read_receiver_data(\n                receiver_name=receiver_name, time_range=time_range\n            )\n\n            # Check epoch count\n            n_epochs = ds.sizes.get(\"epoch\", 0)\n\n            if n_epochs &lt; required_epochs:\n                self._logger.info(\n                    f\"{receiver_name} {yyyydoy}: Only \"\n                    f\"{n_epochs}/{expected_epochs} epochs \"\n                    f\"({n_epochs / expected_epochs * 100:.1f}%) - incomplete\"\n                )\n                return False\n\n            self._logger.debug(\n                f\"{receiver_name} {yyyydoy}: \"\n                f\"{n_epochs}/{expected_epochs} epochs - complete\"\n            )\n\n        except (ValueError, KeyError, Exception) as e:\n            # No data exists or error reading\n            self._logger.debug(f\"{receiver_name} {yyyydoy}: No data found - {e}\")\n            return False\n\n    # All receiver types have complete data\n    return True\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return the developer-facing representation.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.__repr__--returns","level":5,"title":"Returns","text":"<p>str     Representation string.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the developer-facing representation.\n\n    Returns\n    -------\n    str\n        Representation string.\n    \"\"\"\n    return f\"GnssResearchSite(site_name='{self.site_name}')\"\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.__str__","level":3,"title":"<code>__str__()</code>","text":"<p>Return a human-readable summary.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.GnssResearchSite.__str__--returns","level":5,"title":"Returns","text":"<p>str     Summary string.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable summary.\n\n    Returns\n    -------\n    str\n        Summary string.\n    \"\"\"\n    rinex_groups = len(self.get_receiver_groups())\n    vod_groups = len(self.get_vod_analysis_groups())\n    return (\n        f\"GNSS Research Site: {self.site_name}\\n\"\n        f\"  Receivers: {len(self.active_receivers)} configured, \"\n        f\"{rinex_groups} with data\\n\"\n        f\"  VOD Analyses: {len(self.active_vod_analyses)} configured, \"\n        f\"{vod_groups} with results\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader","level":2,"title":"<code>IcechunkDataReader</code>","text":"<p>Replacement for RinexFilesParser that reads from Icechunk stores.</p> <p>This class provides a similar interface to the old parser but reads pre-processed data from Icechunk stores instead of processing RINEX files.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader--parameters","level":4,"title":"Parameters","text":"<p>matched_dirs : MatchedDirs     Directory information for date/location matching site_name : str, optional     Name of research site (defaults to DEFAULT_RESEARCH_SITE) n_max_workers : int, optional     Maximum number of workers for parallel operations (defaults to N_MAX_THREADS) enable_gc : bool, optional     Whether to enable garbage collection between operations (default True) gc_delay : float, optional     Delay in seconds after garbage collection (default 1.0)</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>class IcechunkDataReader:\n    \"\"\"\n    Replacement for RinexFilesParser that reads from Icechunk stores.\n\n    This class provides a similar interface to the old parser but reads\n    pre-processed data from Icechunk stores instead of processing RINEX files.\n\n    Parameters\n    ----------\n    matched_dirs : MatchedDirs\n        Directory information for date/location matching\n    site_name : str, optional\n        Name of research site (defaults to DEFAULT_RESEARCH_SITE)\n    n_max_workers : int, optional\n        Maximum number of workers for parallel operations (defaults to N_MAX_THREADS)\n    enable_gc : bool, optional\n        Whether to enable garbage collection between operations (default True)\n    gc_delay : float, optional\n        Delay in seconds after garbage collection (default 1.0)\n    \"\"\"\n\n    def __init__(\n        self,\n        matched_dirs: MatchedDirs,\n        site_name: str | None = None,\n        n_max_workers: int | None = None,\n        enable_gc: bool = True,\n        gc_delay: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the Icechunk data reader.\n\n        Parameters\n        ----------\n        matched_dirs : MatchedDirs\n            Directory information for date/location matching.\n        site_name : str | None, optional\n            Name of research site. If ``None``, uses the first site from config.\n        n_max_workers : int | None, optional\n            Maximum number of workers for parallel operations. Defaults to config.\n        enable_gc : bool, optional\n            Whether to enable garbage collection between operations.\n        gc_delay : float, optional\n            Delay in seconds after garbage collection.\n        \"\"\"\n        config = load_config()\n        if site_name is None:\n            site_name = next(iter(config.sites.sites))\n        if n_max_workers is None:\n            n_max_workers = config.processing.processing.n_max_threads\n\n        self.matched_dirs = matched_dirs\n        self.site_name = site_name\n        self.n_max_workers = n_max_workers\n        self.enable_gc = enable_gc\n        self.gc_delay = gc_delay\n\n        self._logger = get_logger(__name__).bind(\n            site=site_name,\n            date=matched_dirs.yyyydoy.to_str(),\n        )\n        self._site = GnssResearchSite(site_name)\n\n        date_obj = self.matched_dirs.yyyydoy.date\n        self._start_time = datetime.combine(date_obj, datetime.min.time())\n        self._end_time = datetime.combine(date_obj, datetime.max.time())\n        self._time_range = (self._start_time, self._end_time)\n\n        # ✅ single persistent pool\n        self._pool = ProcessPoolExecutor(\n            max_workers=min(self.n_max_workers, 16),\n        )\n\n        self._logger.info(\n            f\"Initialized Icechunk data reader for {matched_dirs.yyyydoy.to_str()}\"\n        )\n\n    def __del__(self) -&gt; None:\n        \"\"\"Ensure the pool is shut down when the reader is deleted.\"\"\"\n        try:\n            self._pool.shutdown(wait=True)\n        except Exception:\n            pass\n\n    def _memory_cleanup(self) -&gt; None:\n        \"\"\"Perform memory cleanup if enabled.\"\"\"\n        if self.enable_gc:\n            gc.collect()\n            if self.gc_delay &gt; 0:\n                time.sleep(self.gc_delay)\n\n    def get_receiver_by_type(self, receiver_type: str) -&gt; list[str]:\n        \"\"\"\n        Get list of receiver names by type.\n\n        Parameters\n        ----------\n        receiver_type : str\n            Type of receiver ('canopy', 'reference')\n\n        Returns\n        -------\n        list[str]\n            List of receiver names of the specified type\n        \"\"\"\n        return [\n            name\n            for name, config in self._site.active_receivers.items()\n            if config[\"type\"] == receiver_type\n        ]\n\n    def _get_receiver_name_for_type(self, receiver_type: str) -&gt; str | None:\n        \"\"\"Get the first configured receiver name for a given type.\"\"\"\n        for name, config in self._site.active_receivers.items():\n            if config[\"type\"] == receiver_type:\n                return name\n        return None\n\n    def _memory_cleanup(self) -&gt; None:\n        \"\"\"Perform memory cleanup if enabled.\"\"\"\n        if self.enable_gc:\n            gc.collect()\n            if self.gc_delay &gt; 0:\n                time.sleep(self.gc_delay)\n\n    def parsed_rinex_data_gen_v2(\n        self,\n        keep_vars: list[str] | None = None,\n        receiver_types: list[str] | None = None,\n    ) -&gt; Generator[xr.Dataset]:\n        \"\"\"\n        Generator that processes RINEX files, augments them (φ, θ, r),\n        and appends to Icechunk stores on-the-fly.\n\n        Yields enriched daily datasets (already augmented).\n        \"\"\"\n\n        if receiver_types is None:\n            receiver_types = [\"canopy\", \"reference\"]\n\n        self._logger.info(\n            f\"Starting RINEX processing and ingestion for types: {receiver_types}\"\n        )\n\n        # --- 1) Cache auxiliaries once per day ---\n        from canvodpy.orchestrator import RinexDataProcessor\n\n        processor = RinexDataProcessor(\n            matched_data_dirs=self.matched_dirs, icechunk_reader=self\n        )\n\n        ephem_ds = prep_aux_ds(processor.get_ephemeride_ds())\n        clk_ds = prep_aux_ds(processor.get_clk_ds())\n\n        aux_ds_dict = {\"ephem\": ephem_ds, \"clk\": clk_ds}\n        approx_pos = None  # computed on first canopy dataset\n\n        for receiver_type in receiver_types:\n            # --- resolve dirs and names ---\n            if receiver_type == \"canopy\":\n                rinex_dir = self.matched_dirs.canopy_data_dir\n                receiver_name = self._get_receiver_name_for_type(\"canopy\")\n                store_group = \"canopy\"\n            elif receiver_type == \"reference\":\n                rinex_dir = self.matched_dirs.reference_data_dir\n                receiver_name = self._get_receiver_name_for_type(\"reference\")\n                store_group = \"reference\"\n            else:\n                self._logger.warning(f\"Unknown receiver type: {receiver_type}\")\n                continue\n\n            if not receiver_name:\n                self._logger.warning(f\"No configured receiver for type {receiver_type}\")\n                continue\n\n            rinex_files = self._get_rinex_files(rinex_dir)\n            if not rinex_files:\n                self._logger.warning(f\"No RINEX files found in {rinex_dir}\")\n                continue\n\n            self._logger.info(\n                f\"Processing {len(rinex_files)} RINEX files for {receiver_type}\"\n            )\n\n            # --- parallel preprocessing ---\n            futures = {\n                self._pool.submit(preprocess_rnx, f, keep_vars): f for f in rinex_files\n            }\n            results: list[tuple[Path, xr.Dataset]] = []\n\n            for fut in tqdm(\n                as_completed(futures),\n                total=len(futures),\n                desc=f\"Processing {receiver_type}\",\n            ):\n                try:\n                    fname, ds = fut.result()\n                    results.append((fname, ds))\n                except Exception as e:\n                    self._logger.error(f\"Failed preprocessing: {e}\")\n\n            results.sort(key=lambda x: x[0].name)  # chronological order\n\n            # --- per-file commit ---\n            for idx, (fname, ds) in enumerate(results):\n                log = self._logger.bind(file=str(fname))\n                try:\n                    rel_path = self._site.rinex_store.rel_path_for_commit(fname)\n                    version = get_version_from_pyproject()\n\n                    rinex_hash = ds.attrs.get(\"RINEX File Hash\")\n                    if not rinex_hash:\n                        log.warning(\"Dataset missing hash → skipping\")\n                        continue\n\n                    start_epoch = np.datetime64(ds.epoch.min().values)\n                    end_epoch = np.datetime64(ds.epoch.max().values)\n\n                    exists, matches = self._site.rinex_store.metadata_row_exists(\n                        store_group, rinex_hash, start_epoch, end_epoch\n                    )\n\n                    ds = self._site.rinex_store._cleanse_dataset_attrs(ds)\n\n                    # --- 2) Compute approx_pos once from first canopy file ---\n                    if receiver_type == \"canopy\" and approx_pos is None:\n                        approx_pos = processor.get_approx_position(ds)\n\n                    # --- 3) Augment with φ, θ, r ---\n                    matched = processor.match_datasets(ds, **aux_ds_dict)\n                    ephem_matched = matched[\"ephem\"]\n                    ds = processor.add_azi_ele(\n                        rnx_obs_ds=ds,\n                        ephem_ds=ephem_matched,\n                        rx_x=approx_pos.x,\n                        rx_y=approx_pos.y,\n                        rx_z=approx_pos.z,\n                    )\n\n                    # --- 4) Store to Icechunk ---\n                    existing_groups = self._site.rinex_store.list_groups()\n                    if not exists and store_group not in existing_groups and idx == 0:\n                        msg = (\n                            f\"[v{version}] Initial commit {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch})\"\n                        )\n                        self._site.rinex_store.write_initial_group(\n                            dataset=ds, group_name=store_group, commit_message=msg\n                        )\n                        log.info(msg)\n                        continue\n\n                    match (exists, self._site.rinex_store._rinex_store_strategy):\n                        case (True, \"skip\"):\n                            log.info(f\"[v{version}] Skipped {rel_path}\")\n                            # just metadata row\n                            self._site.rinex_store.append_metadata(\n                                group_name=store_group,\n                                rinex_hash=rinex_hash,\n                                start=start_epoch,\n                                end=end_epoch,\n                                snapshot_id=\"none\",\n                                action=\"skip\",\n                                commit_msg=\"skip\",\n                                dataset_attrs=ds.attrs,\n                            )\n\n                        case (True, \"overwrite\"):\n                            msg = f\"[v{version}] Overwrote {rel_path}\"\n                            self._site.rinex_store.overwrite_file_in_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                rinex_hash=rinex_hash,\n                                start=start_epoch,\n                                end=end_epoch,\n                                commit_message=msg,\n                            )\n\n                        case (True, \"append\"):\n                            msg = f\"[v{version}] Appended {rel_path}\"\n                            self._site.rinex_store.append_to_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                append_dim=\"epoch\",\n                                action=\"append\",\n                                commit_message=msg,\n                            )\n\n                        case (False, _):\n                            msg = f\"[v{version}] Wrote {rel_path}\"\n                            self._site.rinex_store.append_to_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                append_dim=\"epoch\",\n                                action=\"write\",\n                                commit_message=msg,\n                            )\n                except Exception as e:\n                    log.exception(\"file_commit_failed\", error=str(e))\n                    raise\n\n                self._memory_cleanup()\n\n            # --- 5) Yield full daily dataset (already enriched) ---\n            final_ds = self._site.read_receiver_data(store_group, self._time_range)\n            self._logger.info(\n                f\"Yielding {receiver_type} dataset: {dict(final_ds.sizes)}\"\n            )\n            yield final_ds\n            self._memory_cleanup()\n\n        self._logger.info(\"RINEX processing and ingestion completed\")\n\n    def parsed_rinex_data_gen(\n        self,\n        keep_vars: list[str] | None = None,\n        receiver_types: list[str] | None = None,\n    ) -&gt; Generator[xr.Dataset]:\n        \"\"\"\n        Generator that processes RINEX files and appends to Icechunk stores on-the-fly.\n\n        Parameters\n        ----------\n        keep_vars : list[str], optional\n            List of variables to keep in datasets\n        receiver_types : list[str], optional\n            List of receiver types to process ('canopy', 'reference').\n            If None, defaults to ['canopy', 'reference']\n\n        Yields\n        ------\n        xr.Dataset\n            Processed and ingested datasets for each receiver type, in order\n        \"\"\"\n\n        if receiver_types is None:\n            receiver_types = [\"canopy\", \"reference\"]\n\n        self._logger.info(\n            f\"Starting RINEX processing and ingestion for types: {receiver_types}\"\n        )\n\n        for receiver_type in receiver_types:\n            # --- resolve dirs and names ---\n            if receiver_type == \"canopy\":\n                rinex_dir = self.matched_dirs.canopy_data_dir\n                receiver_name = self._get_receiver_name_for_type(\"canopy\")\n                store_group = \"canopy\"\n            elif receiver_type == \"reference\":\n                rinex_dir = self.matched_dirs.reference_data_dir\n                receiver_name = self._get_receiver_name_for_type(\"reference\")\n                store_group = \"reference\"\n            else:\n                self._logger.warning(f\"Unknown receiver type: {receiver_type}\")\n                continue\n\n            if not receiver_name:\n                self._logger.warning(f\"No configured receiver for type {receiver_type}\")\n                continue\n\n            rinex_files = self._get_rinex_files(rinex_dir)\n            if not rinex_files:\n                self._logger.warning(f\"No RINEX files found in {rinex_dir}\")\n                continue\n\n            self._logger.info(\n                f\"Processing {len(rinex_files)} RINEX files for {receiver_type}\"\n            )\n\n            groups = self._site.rinex_store.list_groups() or []\n\n            # --- one pool per receiver type ---\n            futures = {\n                self._pool.submit(preprocess_rnx, f, keep_vars): f for f in rinex_files\n            }\n            results: list[tuple[Path, xr.Dataset]] = []\n\n            # ✅ progressbar over all files, not per batch\n            for fut in tqdm(\n                as_completed(futures),\n                total=len(futures),\n                desc=f\"Processing {receiver_type}\",\n            ):\n                try:\n                    fname, ds = fut.result()\n                    results.append((fname, ds))\n                except Exception as e:\n                    self._logger.error(f\"Failed preprocessing: {e}\")\n\n            # --- sort all results once (chronological order) ---\n            results.sort(key=lambda x: x[0].name)\n\n            # --- sequential append to Icechunk ---\n            for idx, (fname, ds) in enumerate(results):\n                log = self._logger.bind(file=str(fname))\n                try:\n                    rel_path = self._site.rinex_store.rel_path_for_commit(fname)\n                    version = get_version_from_pyproject()\n\n                    rinex_hash = ds.attrs.get(\"RINEX File Hash\")\n                    if not rinex_hash:\n                        log.warning(\n                            f\"No RINEX hash found in dataset from {fname}. \"\n                            \"Skipping duplicate detection for this file.\"\n                        )\n                        continue\n\n                    start_epoch = np.datetime64(ds.epoch.min().values)\n                    end_epoch = np.datetime64(ds.epoch.max().values)\n\n                    exists, matches = self._site.rinex_store.metadata_row_exists(\n                        store_group, rinex_hash, start_epoch, end_epoch\n                    )\n\n                    ds = self._site.rinex_store._cleanse_dataset_attrs(ds)\n\n                    # --- Initial commit ---\n                    if not exists and store_group not in groups and idx == 0:\n                        msg = (\n                            f\"[v{version}] Initial commit with {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                            f\"to group '{store_group}'\"\n                        )\n                        self._site.rinex_store.write_initial_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            commit_message=msg,\n                        )\n                        groups.append(store_group)\n                        log.info(msg)\n                        continue\n\n                    # --- Handle strategies with match ---\n                    match (exists, self._site.rinex_store._rinex_store_strategy):\n                        case (True, \"skip\"):\n                            msg = (\n                                f\"[v{version}] Skipped {rel_path} \"\n                                f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                                f\"in group '{store_group}'\"\n                            )\n                            log.info(msg)\n                            self._site.rinex_store.append_metadata(\n                                group_name=store_group,\n                                rinex_hash=rinex_hash,\n                                start=start_epoch,\n                                end=end_epoch,\n                                snapshot_id=\"none\",\n                                action=\"skip\",\n                                commit_msg=msg,\n                                dataset_attrs=ds.attrs,\n                            )\n\n                        case (True, \"overwrite\"):\n                            msg = (\n                                f\"[v{version}] Overwrote {rel_path} \"\n                                f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                                f\"in group '{store_group}'\"\n                            )\n                            log.info(msg)\n                            self._site.rinex_store.overwrite_file_in_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                rinex_hash=rinex_hash,\n                                start=start_epoch,\n                                end=end_epoch,\n                                commit_message=msg,\n                            )\n\n                        case (True, \"append\"):\n                            msg = (\n                                f\"[v{version}] Appended {rel_path} \"\n                                f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                                f\"to group '{store_group}'\"\n                            )\n                            self._site.rinex_store.append_to_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                append_dim=\"epoch\",\n                                action=\"append\",\n                                commit_message=msg,\n                            )\n                            log.info(msg)\n\n                        case (False, _):\n                            msg = (\n                                f\"[v{version}] Wrote {rel_path} \"\n                                f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                                f\"to group '{store_group}'\"\n                            )\n                            self._site.rinex_store.append_to_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                append_dim=\"epoch\",\n                                action=\"write\",\n                                commit_message=msg,\n                            )\n                            log.info(msg)\n\n                except Exception as e:\n                    log.exception(\"file_commit_failed\", error=str(e))\n                    raise\n\n                self._memory_cleanup()\n\n            # --- read back final dataset ---\n            final_ds = self._site.read_receiver_data(store_group, self._time_range)\n            self._logger.info(\n                f\"Yielding {receiver_type} dataset: {dict(final_ds.sizes)}\"\n            )\n            yield final_ds\n\n            self._memory_cleanup()\n\n        self._logger.info(\"RINEX processing and ingestion completed\")\n\n    def _get_receiver_name_for_type(self, receiver_type: str) -&gt; str | None:\n        \"\"\"Get the first configured receiver name for a given type.\"\"\"\n        for name, config in self._site.active_receivers.items():\n            if config[\"type\"] == receiver_type:\n                return name\n        return None\n\n    def _get_rinex_files(self, rinex_dir: Path) -&gt; list[Path]:\n        \"\"\"Get sorted list of RINEX files from directory.\"\"\"\n\n        if not rinex_dir.exists():\n            self._logger.warning(f\"Directory does not exist: {rinex_dir}\")\n            return []\n\n        # Look for common RINEX file patterns\n        patterns = [\"*.??o\", \"*.??O\", \"*.rnx\", \"*.RNX\"]\n        rinex_files = []\n\n        for pattern in patterns:\n            files = list(rinex_dir.glob(pattern))\n            rinex_files.extend(files)\n\n        return natsorted(rinex_files)\n\n    def _append_to_icechunk_store(\n        self,\n        dataset: xr.Dataset,\n        receiver_name: str,\n        receiver_type: str,\n    ) -&gt; None:\n        \"\"\"Append dataset to the appropriate Icechunk store.\"\"\"\n        from gnssvodpy.utils.tools import get_version_from_pyproject\n\n        try:\n            version = get_version_from_pyproject()\n            date_str = self.matched_dirs.yyyydoy.to_str()\n\n            commit_message = (\n                f\"[v{version}] Processed and ingested {receiver_type} data \"\n                f\"for {date_str}\"\n            )\n\n            # Use site's ingestion method\n            self._site.ingest_rinex_data(dataset, receiver_name, commit_message)\n\n            self._logger.info(\n                f\"Successfully appended {receiver_type} data to store as \"\n                f\"'{receiver_name}'\"\n            )\n\n        except Exception as e:\n            self._logger.exception(\n                f\"Failed to append {receiver_type} data to store\", error=str(e)\n            )\n            raise\n\n    def get_available_receivers(self) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Get available receivers grouped by type.\n\n        Returns\n        -------\n        dict[str, list[str]]\n            Dictionary mapping receiver types to lists of receiver names.\n        \"\"\"\n        available = {}\n        for receiver_type in [\"canopy\", \"reference\"]:\n            receivers = self.get_receiver_by_type(receiver_type)\n            # Filter to only receivers that have data\n            with_data = [r for r in receivers if self._site.rinex_store.group_exists(r)]\n            available[receiver_type] = with_data\n\n        return available\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable summary.\n\n        Returns\n        -------\n        str\n            Summary string.\n        \"\"\"\n        available = self.get_available_receivers()\n        return (\n            f\"IcechunkDataReader for {self.matched_dirs.yyyydoy.to_str()}\\n\"\n            f\"  Site: {self.site_name}\\n\"\n            f\"  Available receivers: {dict(available)}\\n\"\n            f\"  Workers: {self.n_max_workers}, GC enabled: {self.enable_gc}\"\n        )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.__init__","level":3,"title":"<code>__init__(matched_dirs, site_name=None, n_max_workers=None, enable_gc=True, gc_delay=1.0)</code>","text":"<p>Initialize the Icechunk data reader.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.__init__--parameters","level":5,"title":"Parameters","text":"<p>matched_dirs : MatchedDirs     Directory information for date/location matching. site_name : str | None, optional     Name of research site. If <code>None</code>, uses the first site from config. n_max_workers : int | None, optional     Maximum number of workers for parallel operations. Defaults to config. enable_gc : bool, optional     Whether to enable garbage collection between operations. gc_delay : float, optional     Delay in seconds after garbage collection.</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def __init__(\n    self,\n    matched_dirs: MatchedDirs,\n    site_name: str | None = None,\n    n_max_workers: int | None = None,\n    enable_gc: bool = True,\n    gc_delay: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the Icechunk data reader.\n\n    Parameters\n    ----------\n    matched_dirs : MatchedDirs\n        Directory information for date/location matching.\n    site_name : str | None, optional\n        Name of research site. If ``None``, uses the first site from config.\n    n_max_workers : int | None, optional\n        Maximum number of workers for parallel operations. Defaults to config.\n    enable_gc : bool, optional\n        Whether to enable garbage collection between operations.\n    gc_delay : float, optional\n        Delay in seconds after garbage collection.\n    \"\"\"\n    config = load_config()\n    if site_name is None:\n        site_name = next(iter(config.sites.sites))\n    if n_max_workers is None:\n        n_max_workers = config.processing.processing.n_max_threads\n\n    self.matched_dirs = matched_dirs\n    self.site_name = site_name\n    self.n_max_workers = n_max_workers\n    self.enable_gc = enable_gc\n    self.gc_delay = gc_delay\n\n    self._logger = get_logger(__name__).bind(\n        site=site_name,\n        date=matched_dirs.yyyydoy.to_str(),\n    )\n    self._site = GnssResearchSite(site_name)\n\n    date_obj = self.matched_dirs.yyyydoy.date\n    self._start_time = datetime.combine(date_obj, datetime.min.time())\n    self._end_time = datetime.combine(date_obj, datetime.max.time())\n    self._time_range = (self._start_time, self._end_time)\n\n    # ✅ single persistent pool\n    self._pool = ProcessPoolExecutor(\n        max_workers=min(self.n_max_workers, 16),\n    )\n\n    self._logger.info(\n        f\"Initialized Icechunk data reader for {matched_dirs.yyyydoy.to_str()}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.__del__","level":3,"title":"<code>__del__()</code>","text":"<p>Ensure the pool is shut down when the reader is deleted.</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Ensure the pool is shut down when the reader is deleted.\"\"\"\n    try:\n        self._pool.shutdown(wait=True)\n    except Exception:\n        pass\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.get_receiver_by_type","level":3,"title":"<code>get_receiver_by_type(receiver_type)</code>","text":"<p>Get list of receiver names by type.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.get_receiver_by_type--parameters","level":5,"title":"Parameters","text":"<p>receiver_type : str     Type of receiver ('canopy', 'reference')</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.get_receiver_by_type--returns","level":5,"title":"Returns","text":"<p>list[str]     List of receiver names of the specified type</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def get_receiver_by_type(self, receiver_type: str) -&gt; list[str]:\n    \"\"\"\n    Get list of receiver names by type.\n\n    Parameters\n    ----------\n    receiver_type : str\n        Type of receiver ('canopy', 'reference')\n\n    Returns\n    -------\n    list[str]\n        List of receiver names of the specified type\n    \"\"\"\n    return [\n        name\n        for name, config in self._site.active_receivers.items()\n        if config[\"type\"] == receiver_type\n    ]\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.parsed_rinex_data_gen_v2","level":3,"title":"<code>parsed_rinex_data_gen_v2(keep_vars=None, receiver_types=None)</code>","text":"<p>Generator that processes RINEX files, augments them (φ, θ, r), and appends to Icechunk stores on-the-fly.</p> <p>Yields enriched daily datasets (already augmented).</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def parsed_rinex_data_gen_v2(\n    self,\n    keep_vars: list[str] | None = None,\n    receiver_types: list[str] | None = None,\n) -&gt; Generator[xr.Dataset]:\n    \"\"\"\n    Generator that processes RINEX files, augments them (φ, θ, r),\n    and appends to Icechunk stores on-the-fly.\n\n    Yields enriched daily datasets (already augmented).\n    \"\"\"\n\n    if receiver_types is None:\n        receiver_types = [\"canopy\", \"reference\"]\n\n    self._logger.info(\n        f\"Starting RINEX processing and ingestion for types: {receiver_types}\"\n    )\n\n    # --- 1) Cache auxiliaries once per day ---\n    from canvodpy.orchestrator import RinexDataProcessor\n\n    processor = RinexDataProcessor(\n        matched_data_dirs=self.matched_dirs, icechunk_reader=self\n    )\n\n    ephem_ds = prep_aux_ds(processor.get_ephemeride_ds())\n    clk_ds = prep_aux_ds(processor.get_clk_ds())\n\n    aux_ds_dict = {\"ephem\": ephem_ds, \"clk\": clk_ds}\n    approx_pos = None  # computed on first canopy dataset\n\n    for receiver_type in receiver_types:\n        # --- resolve dirs and names ---\n        if receiver_type == \"canopy\":\n            rinex_dir = self.matched_dirs.canopy_data_dir\n            receiver_name = self._get_receiver_name_for_type(\"canopy\")\n            store_group = \"canopy\"\n        elif receiver_type == \"reference\":\n            rinex_dir = self.matched_dirs.reference_data_dir\n            receiver_name = self._get_receiver_name_for_type(\"reference\")\n            store_group = \"reference\"\n        else:\n            self._logger.warning(f\"Unknown receiver type: {receiver_type}\")\n            continue\n\n        if not receiver_name:\n            self._logger.warning(f\"No configured receiver for type {receiver_type}\")\n            continue\n\n        rinex_files = self._get_rinex_files(rinex_dir)\n        if not rinex_files:\n            self._logger.warning(f\"No RINEX files found in {rinex_dir}\")\n            continue\n\n        self._logger.info(\n            f\"Processing {len(rinex_files)} RINEX files for {receiver_type}\"\n        )\n\n        # --- parallel preprocessing ---\n        futures = {\n            self._pool.submit(preprocess_rnx, f, keep_vars): f for f in rinex_files\n        }\n        results: list[tuple[Path, xr.Dataset]] = []\n\n        for fut in tqdm(\n            as_completed(futures),\n            total=len(futures),\n            desc=f\"Processing {receiver_type}\",\n        ):\n            try:\n                fname, ds = fut.result()\n                results.append((fname, ds))\n            except Exception as e:\n                self._logger.error(f\"Failed preprocessing: {e}\")\n\n        results.sort(key=lambda x: x[0].name)  # chronological order\n\n        # --- per-file commit ---\n        for idx, (fname, ds) in enumerate(results):\n            log = self._logger.bind(file=str(fname))\n            try:\n                rel_path = self._site.rinex_store.rel_path_for_commit(fname)\n                version = get_version_from_pyproject()\n\n                rinex_hash = ds.attrs.get(\"RINEX File Hash\")\n                if not rinex_hash:\n                    log.warning(\"Dataset missing hash → skipping\")\n                    continue\n\n                start_epoch = np.datetime64(ds.epoch.min().values)\n                end_epoch = np.datetime64(ds.epoch.max().values)\n\n                exists, matches = self._site.rinex_store.metadata_row_exists(\n                    store_group, rinex_hash, start_epoch, end_epoch\n                )\n\n                ds = self._site.rinex_store._cleanse_dataset_attrs(ds)\n\n                # --- 2) Compute approx_pos once from first canopy file ---\n                if receiver_type == \"canopy\" and approx_pos is None:\n                    approx_pos = processor.get_approx_position(ds)\n\n                # --- 3) Augment with φ, θ, r ---\n                matched = processor.match_datasets(ds, **aux_ds_dict)\n                ephem_matched = matched[\"ephem\"]\n                ds = processor.add_azi_ele(\n                    rnx_obs_ds=ds,\n                    ephem_ds=ephem_matched,\n                    rx_x=approx_pos.x,\n                    rx_y=approx_pos.y,\n                    rx_z=approx_pos.z,\n                )\n\n                # --- 4) Store to Icechunk ---\n                existing_groups = self._site.rinex_store.list_groups()\n                if not exists and store_group not in existing_groups and idx == 0:\n                    msg = (\n                        f\"[v{version}] Initial commit {rel_path} \"\n                        f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch})\"\n                    )\n                    self._site.rinex_store.write_initial_group(\n                        dataset=ds, group_name=store_group, commit_message=msg\n                    )\n                    log.info(msg)\n                    continue\n\n                match (exists, self._site.rinex_store._rinex_store_strategy):\n                    case (True, \"skip\"):\n                        log.info(f\"[v{version}] Skipped {rel_path}\")\n                        # just metadata row\n                        self._site.rinex_store.append_metadata(\n                            group_name=store_group,\n                            rinex_hash=rinex_hash,\n                            start=start_epoch,\n                            end=end_epoch,\n                            snapshot_id=\"none\",\n                            action=\"skip\",\n                            commit_msg=\"skip\",\n                            dataset_attrs=ds.attrs,\n                        )\n\n                    case (True, \"overwrite\"):\n                        msg = f\"[v{version}] Overwrote {rel_path}\"\n                        self._site.rinex_store.overwrite_file_in_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            rinex_hash=rinex_hash,\n                            start=start_epoch,\n                            end=end_epoch,\n                            commit_message=msg,\n                        )\n\n                    case (True, \"append\"):\n                        msg = f\"[v{version}] Appended {rel_path}\"\n                        self._site.rinex_store.append_to_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            append_dim=\"epoch\",\n                            action=\"append\",\n                            commit_message=msg,\n                        )\n\n                    case (False, _):\n                        msg = f\"[v{version}] Wrote {rel_path}\"\n                        self._site.rinex_store.append_to_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            append_dim=\"epoch\",\n                            action=\"write\",\n                            commit_message=msg,\n                        )\n            except Exception as e:\n                log.exception(\"file_commit_failed\", error=str(e))\n                raise\n\n            self._memory_cleanup()\n\n        # --- 5) Yield full daily dataset (already enriched) ---\n        final_ds = self._site.read_receiver_data(store_group, self._time_range)\n        self._logger.info(\n            f\"Yielding {receiver_type} dataset: {dict(final_ds.sizes)}\"\n        )\n        yield final_ds\n        self._memory_cleanup()\n\n    self._logger.info(\"RINEX processing and ingestion completed\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.parsed_rinex_data_gen","level":3,"title":"<code>parsed_rinex_data_gen(keep_vars=None, receiver_types=None)</code>","text":"<p>Generator that processes RINEX files and appends to Icechunk stores on-the-fly.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.parsed_rinex_data_gen--parameters","level":5,"title":"Parameters","text":"<p>keep_vars : list[str], optional     List of variables to keep in datasets receiver_types : list[str], optional     List of receiver types to process ('canopy', 'reference').     If None, defaults to ['canopy', 'reference']</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.parsed_rinex_data_gen--yields","level":5,"title":"Yields","text":"<p>xr.Dataset     Processed and ingested datasets for each receiver type, in order</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def parsed_rinex_data_gen(\n    self,\n    keep_vars: list[str] | None = None,\n    receiver_types: list[str] | None = None,\n) -&gt; Generator[xr.Dataset]:\n    \"\"\"\n    Generator that processes RINEX files and appends to Icechunk stores on-the-fly.\n\n    Parameters\n    ----------\n    keep_vars : list[str], optional\n        List of variables to keep in datasets\n    receiver_types : list[str], optional\n        List of receiver types to process ('canopy', 'reference').\n        If None, defaults to ['canopy', 'reference']\n\n    Yields\n    ------\n    xr.Dataset\n        Processed and ingested datasets for each receiver type, in order\n    \"\"\"\n\n    if receiver_types is None:\n        receiver_types = [\"canopy\", \"reference\"]\n\n    self._logger.info(\n        f\"Starting RINEX processing and ingestion for types: {receiver_types}\"\n    )\n\n    for receiver_type in receiver_types:\n        # --- resolve dirs and names ---\n        if receiver_type == \"canopy\":\n            rinex_dir = self.matched_dirs.canopy_data_dir\n            receiver_name = self._get_receiver_name_for_type(\"canopy\")\n            store_group = \"canopy\"\n        elif receiver_type == \"reference\":\n            rinex_dir = self.matched_dirs.reference_data_dir\n            receiver_name = self._get_receiver_name_for_type(\"reference\")\n            store_group = \"reference\"\n        else:\n            self._logger.warning(f\"Unknown receiver type: {receiver_type}\")\n            continue\n\n        if not receiver_name:\n            self._logger.warning(f\"No configured receiver for type {receiver_type}\")\n            continue\n\n        rinex_files = self._get_rinex_files(rinex_dir)\n        if not rinex_files:\n            self._logger.warning(f\"No RINEX files found in {rinex_dir}\")\n            continue\n\n        self._logger.info(\n            f\"Processing {len(rinex_files)} RINEX files for {receiver_type}\"\n        )\n\n        groups = self._site.rinex_store.list_groups() or []\n\n        # --- one pool per receiver type ---\n        futures = {\n            self._pool.submit(preprocess_rnx, f, keep_vars): f for f in rinex_files\n        }\n        results: list[tuple[Path, xr.Dataset]] = []\n\n        # ✅ progressbar over all files, not per batch\n        for fut in tqdm(\n            as_completed(futures),\n            total=len(futures),\n            desc=f\"Processing {receiver_type}\",\n        ):\n            try:\n                fname, ds = fut.result()\n                results.append((fname, ds))\n            except Exception as e:\n                self._logger.error(f\"Failed preprocessing: {e}\")\n\n        # --- sort all results once (chronological order) ---\n        results.sort(key=lambda x: x[0].name)\n\n        # --- sequential append to Icechunk ---\n        for idx, (fname, ds) in enumerate(results):\n            log = self._logger.bind(file=str(fname))\n            try:\n                rel_path = self._site.rinex_store.rel_path_for_commit(fname)\n                version = get_version_from_pyproject()\n\n                rinex_hash = ds.attrs.get(\"RINEX File Hash\")\n                if not rinex_hash:\n                    log.warning(\n                        f\"No RINEX hash found in dataset from {fname}. \"\n                        \"Skipping duplicate detection for this file.\"\n                    )\n                    continue\n\n                start_epoch = np.datetime64(ds.epoch.min().values)\n                end_epoch = np.datetime64(ds.epoch.max().values)\n\n                exists, matches = self._site.rinex_store.metadata_row_exists(\n                    store_group, rinex_hash, start_epoch, end_epoch\n                )\n\n                ds = self._site.rinex_store._cleanse_dataset_attrs(ds)\n\n                # --- Initial commit ---\n                if not exists and store_group not in groups and idx == 0:\n                    msg = (\n                        f\"[v{version}] Initial commit with {rel_path} \"\n                        f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                        f\"to group '{store_group}'\"\n                    )\n                    self._site.rinex_store.write_initial_group(\n                        dataset=ds,\n                        group_name=store_group,\n                        commit_message=msg,\n                    )\n                    groups.append(store_group)\n                    log.info(msg)\n                    continue\n\n                # --- Handle strategies with match ---\n                match (exists, self._site.rinex_store._rinex_store_strategy):\n                    case (True, \"skip\"):\n                        msg = (\n                            f\"[v{version}] Skipped {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                            f\"in group '{store_group}'\"\n                        )\n                        log.info(msg)\n                        self._site.rinex_store.append_metadata(\n                            group_name=store_group,\n                            rinex_hash=rinex_hash,\n                            start=start_epoch,\n                            end=end_epoch,\n                            snapshot_id=\"none\",\n                            action=\"skip\",\n                            commit_msg=msg,\n                            dataset_attrs=ds.attrs,\n                        )\n\n                    case (True, \"overwrite\"):\n                        msg = (\n                            f\"[v{version}] Overwrote {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                            f\"in group '{store_group}'\"\n                        )\n                        log.info(msg)\n                        self._site.rinex_store.overwrite_file_in_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            rinex_hash=rinex_hash,\n                            start=start_epoch,\n                            end=end_epoch,\n                            commit_message=msg,\n                        )\n\n                    case (True, \"append\"):\n                        msg = (\n                            f\"[v{version}] Appended {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                            f\"to group '{store_group}'\"\n                        )\n                        self._site.rinex_store.append_to_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            append_dim=\"epoch\",\n                            action=\"append\",\n                            commit_message=msg,\n                        )\n                        log.info(msg)\n\n                    case (False, _):\n                        msg = (\n                            f\"[v{version}] Wrote {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                            f\"to group '{store_group}'\"\n                        )\n                        self._site.rinex_store.append_to_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            append_dim=\"epoch\",\n                            action=\"write\",\n                            commit_message=msg,\n                        )\n                        log.info(msg)\n\n            except Exception as e:\n                log.exception(\"file_commit_failed\", error=str(e))\n                raise\n\n            self._memory_cleanup()\n\n        # --- read back final dataset ---\n        final_ds = self._site.read_receiver_data(store_group, self._time_range)\n        self._logger.info(\n            f\"Yielding {receiver_type} dataset: {dict(final_ds.sizes)}\"\n        )\n        yield final_ds\n\n        self._memory_cleanup()\n\n    self._logger.info(\"RINEX processing and ingestion completed\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.get_available_receivers","level":3,"title":"<code>get_available_receivers()</code>","text":"<p>Get available receivers grouped by type.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.get_available_receivers--returns","level":5,"title":"Returns","text":"<p>dict[str, list[str]]     Dictionary mapping receiver types to lists of receiver names.</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def get_available_receivers(self) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Get available receivers grouped by type.\n\n    Returns\n    -------\n    dict[str, list[str]]\n        Dictionary mapping receiver types to lists of receiver names.\n    \"\"\"\n    available = {}\n    for receiver_type in [\"canopy\", \"reference\"]:\n        receivers = self.get_receiver_by_type(receiver_type)\n        # Filter to only receivers that have data\n        with_data = [r for r in receivers if self._site.rinex_store.group_exists(r)]\n        available[receiver_type] = with_data\n\n    return available\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.__str__","level":3,"title":"<code>__str__()</code>","text":"<p>Return a human-readable summary.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.IcechunkDataReader.__str__--returns","level":5,"title":"Returns","text":"<p>str     Summary string.</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable summary.\n\n    Returns\n    -------\n    str\n        Summary string.\n    \"\"\"\n    available = self.get_available_receivers()\n    return (\n        f\"IcechunkDataReader for {self.matched_dirs.yyyydoy.to_str()}\\n\"\n        f\"  Site: {self.site_name}\\n\"\n        f\"  Available receivers: {dict(available)}\\n\"\n        f\"  Workers: {self.n_max_workers}, GC enabled: {self.enable_gc}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.create_rinex_store","level":2,"title":"<code>create_rinex_store(store_path)</code>","text":"<p>Create a RINEX Icechunk store with appropriate configuration.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.create_rinex_store--parameters","level":4,"title":"Parameters","text":"<p>store_path : Path     Path to the store directory.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.create_rinex_store--returns","level":4,"title":"Returns","text":"<p>MyIcechunkStore     Configured store for RINEX data.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def create_rinex_store(store_path: Path) -&gt; MyIcechunkStore:\n    \"\"\"\n    Create a RINEX Icechunk store with appropriate configuration.\n\n    Parameters\n    ----------\n    store_path : Path\n        Path to the store directory.\n\n    Returns\n    -------\n    MyIcechunkStore\n        Configured store for RINEX data.\n    \"\"\"\n    return MyIcechunkStore(store_path=store_path, store_type=\"rinex_store\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.create_vod_store","level":2,"title":"<code>create_vod_store(store_path)</code>","text":"<p>Create a VOD Icechunk store with appropriate configuration.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.create_vod_store--parameters","level":4,"title":"Parameters","text":"<p>store_path : Path     Path to the store directory.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.create_vod_store--returns","level":4,"title":"Returns","text":"<p>MyIcechunkStore     Configured store for VOD analysis data.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def create_vod_store(store_path: Path) -&gt; MyIcechunkStore:\n    \"\"\"\n    Create a VOD Icechunk store with appropriate configuration.\n\n    Parameters\n    ----------\n    store_path : Path\n        Path to the store directory.\n\n    Returns\n    -------\n    MyIcechunkStore\n        Configured store for VOD analysis data.\n    \"\"\"\n    return MyIcechunkStore(store_path=store_path, store_type=\"vod_store\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#store-manager","level":2,"title":"Store Manager","text":"<p>Research site manager that coordinates RINEX and VOD Icechunk stores.</p> <p>This module provides the GnssResearchSite class that manages both stores for a research site and provides high-level operations across them.</p> <p>Module: src/gnssvodpy/icechunk_manager/manager.py</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite","level":2,"title":"<code>GnssResearchSite</code>","text":"<p>High-level manager for a GNSS research site with dual Icechunk stores.</p> <p>This class coordinates between RINEX data storage (Level 1) and VOD analysis storage (Level 2), providing a unified interface for site-wide operations.</p> <p>Architecture: - RINEX Store: Raw/standardized observations per receiver - VOD Store: Analysis products comparing receiver pairs</p> <p>Features: - Automatic store initialization from config - Receiver management and validation - Analysis workflow coordination - Unified logging and error handling</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite--parameters","level":4,"title":"Parameters","text":"<p>site_name : str     Name of the research site (must exist in config).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite--raises","level":4,"title":"Raises","text":"<p>KeyError     If <code>site_name</code> is not found in the RESEARCH_SITES config.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>class GnssResearchSite:\n    \"\"\"\n    High-level manager for a GNSS research site with dual Icechunk stores.\n\n    This class coordinates between RINEX data storage (Level 1) and\n    VOD analysis storage (Level 2), providing a unified interface\n    for site-wide operations.\n\n    Architecture:\n    - RINEX Store: Raw/standardized observations per receiver\n    - VOD Store: Analysis products comparing receiver pairs\n\n    Features:\n    - Automatic store initialization from config\n    - Receiver management and validation\n    - Analysis workflow coordination\n    - Unified logging and error handling\n\n    Parameters\n    ----------\n    site_name : str\n        Name of the research site (must exist in config).\n\n    Raises\n    ------\n    KeyError\n        If ``site_name`` is not found in the RESEARCH_SITES config.\n    \"\"\"\n\n    def __init__(self, site_name: str) -&gt; None:\n        \"\"\"Initialize the site manager.\n\n        Parameters\n        ----------\n        site_name : str\n            Name of the research site.\n        \"\"\"\n        from canvod.utils.config import load_config\n\n        config = load_config()\n        sites = config.sites.sites\n\n        if site_name not in sites:\n            available_sites = list(sites.keys())\n            raise KeyError(\n                f\"Site '{site_name}' not found in config. \"\n                f\"Available sites: {available_sites}\"\n            )\n\n        self.site_name = site_name\n        self._site_config = sites[site_name]\n        self._logger = get_logger(__name__).bind(site=site_name)\n\n        rinex_store_path = config.processing.storage.get_rinex_store_path(site_name)\n        vod_store_path = config.processing.storage.get_vod_store_path(site_name)\n\n        # Initialize stores using paths from processing.yaml\n        self.rinex_store = create_rinex_store(rinex_store_path)\n        self.vod_store = create_vod_store(vod_store_path)\n\n        self._logger.info(\n            f\"Initialized GNSS research site: {site_name}\",\n            rinex_store=str(rinex_store_path),\n            vod_store=str(vod_store_path),\n        )\n\n    @property\n    def site_config(self) -&gt; dict[str, Any]:\n        \"\"\"Get the site configuration as a dictionary.\"\"\"\n        return self._site_config.model_dump()\n\n    @property\n    def receivers(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Get all configured receivers for this site.\"\"\"\n        return {\n            name: cfg.model_dump() for name, cfg in self._site_config.receivers.items()\n        }\n\n    @property\n    def active_receivers(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Get only active receivers for this site.\"\"\"\n        return {\n            name: config\n            for name, config in self.receivers.items()\n            if config.get(\"active\", True)\n        }\n\n    @property\n    def vod_analyses(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Get all configured VOD analyses for this site.\n\n        Returns auto-derived analyses from scs_from if no explicit\n        vod_analyses are configured.\n        \"\"\"\n        if self._site_config.vod_analyses is not None:\n            return {\n                name: cfg.model_dump()\n                for name, cfg in self._site_config.vod_analyses.items()\n            }\n        return self.get_auto_vod_analyses()\n\n    @property\n    def active_vod_analyses(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Get only active VOD analyses for this site.\"\"\"\n        return {\n            name: config\n            for name, config in self.vod_analyses.items()\n            if config.get(\"active\", True)\n        }\n\n    @classmethod\n    def from_rinex_store_path(\n        cls,\n        rinex_store_path: Path,\n    ) -&gt; GnssResearchSite:\n        \"\"\"\n        Create a GnssResearchSite instance from a RINEX store path.\n\n        Parameters\n        ----------\n        rinex_store_path : Path\n            Path to the RINEX Icechunk store.\n\n        Returns\n        -------\n        GnssResearchSite\n            Initialized research site manager.\n\n        Raises\n        ------\n        ValueError\n            If no matching site is found for the given path.\n        \"\"\"\n        # Load config to get store paths\n        from canvod.utils.config import load_config\n\n        config = load_config()\n\n        # Try to match against each site's expected rinex store path\n        for site_name in config.sites.sites.keys():\n            expected_path = config.processing.storage.get_rinex_store_path(site_name)\n            if expected_path == rinex_store_path:\n                return cls(site_name)\n\n        raise ValueError(\n            f\"No research site found for RINEX store path: {rinex_store_path}\"\n        )\n\n    def get_reference_canopy_pairs(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Expand scs_from into (reference_name, canopy_name) pairs.\n\n        Returns\n        -------\n        list[tuple[str, str]]\n            List of (reference_name, canopy_name) pairs.\n        \"\"\"\n        return self._site_config.get_reference_canopy_pairs()\n\n    def get_auto_vod_analyses(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Derive VOD analysis pairs from scs_from configuration.\n\n        Creates one VOD pair per (canopy, reference_for_that_canopy) combination.\n\n        Returns\n        -------\n        dict[str, dict[str, Any]]\n            Auto-derived VOD analyses keyed by analysis name.\n        \"\"\"\n        analyses: dict[str, dict[str, Any]] = {}\n        for ref_name, canopy_name in self.get_reference_canopy_pairs():\n            analysis_name = f\"{canopy_name}_vs_{ref_name}\"\n            analyses[analysis_name] = {\n                \"canopy_receiver\": canopy_name,\n                \"reference_receiver\": f\"{ref_name}_{canopy_name}\",\n                \"description\": f\"VOD analysis {canopy_name} vs {ref_name}\",\n            }\n        return analyses\n\n    def validate_site_config(self) -&gt; bool:\n        \"\"\"\n        Validate that the site configuration is consistent.\n\n        Returns\n        -------\n        bool\n            True if configuration is valid.\n\n        Raises\n        ------\n        ValueError\n            If configuration is invalid.\n        \"\"\"\n        # Check that all VOD analyses reference valid receivers\n        # Build set of valid reference store groups (e.g. reference_01_canopy_01)\n        valid_ref_groups = {\n            f\"{ref}_{canopy}\" for ref, canopy in self.get_reference_canopy_pairs()\n        }\n\n        for analysis_name, analysis_config in self.vod_analyses.items():\n            canopy_rx = analysis_config[\"canopy_receiver\"]\n            ref_rx = analysis_config[\"reference_receiver\"]\n\n            if canopy_rx not in self.receivers:\n                raise ValueError(\n                    f\"VOD analysis '{analysis_name}' references \"\n                    f\"unknown canopy receiver: {canopy_rx}\"\n                )\n            # ref_rx can be either a raw receiver name or a store group name\n            if ref_rx not in self.receivers and ref_rx not in valid_ref_groups:\n                raise ValueError(\n                    f\"VOD analysis '{analysis_name}' references \"\n                    f\"unknown reference receiver/group: {ref_rx}\"\n                )\n\n            # Check canopy type\n            canopy_type = self.receivers[canopy_rx][\"type\"]\n            if canopy_type != \"canopy\":\n                raise ValueError(\n                    f\"Receiver '{canopy_rx}' used as canopy but type is '{canopy_type}'\"\n                )\n            # Check reference type (only if it's a raw receiver name)\n            if ref_rx in self.receivers:\n                ref_type = self.receivers[ref_rx][\"type\"]\n                if ref_type != \"reference\":\n                    raise ValueError(\n                        f\"Receiver '{ref_rx}' used as reference but type is '{ref_type}'\"\n                    )\n\n        self._logger.debug(\"Site configuration validation passed\")\n        return True\n\n    def get_receiver_groups(self) -&gt; list[str]:\n        \"\"\"\n        Get list of receiver groups that exist in the RINEX store.\n\n        Returns\n        -------\n        list[str]\n            Existing receiver group names.\n        \"\"\"\n        return self.rinex_store.list_groups()\n\n    def get_vod_analysis_groups(self) -&gt; list[str]:\n        \"\"\"\n        Get list of VOD analysis groups that exist in the VOD store.\n\n        Returns\n        -------\n        list[str]\n            Existing VOD analysis group names.\n        \"\"\"\n        return self.vod_store.list_groups()\n\n    def ingest_rinex_data(\n        self, dataset: xr.Dataset, receiver_name: str, commit_message: str | None = None\n    ) -&gt; None:\n        \"\"\"\n        Ingest RINEX data for a specific receiver.\n\n        Parameters\n        ----------\n        dataset : xr.Dataset\n            Processed RINEX dataset to store.\n        receiver_name : str\n            Name of the receiver (must be configured).\n        commit_message : str, optional\n            Commit message to store with the data.\n\n        Raises\n        ------\n        ValueError\n            If ``receiver_name`` is not configured.\n        \"\"\"\n        if receiver_name not in self.receivers:\n            available_receivers = list(self.receivers.keys())\n            raise ValueError(\n                f\"Receiver '{receiver_name}' not configured. \"\n                f\"Available: {available_receivers}\"\n            )\n\n        self._logger.info(f\"Ingesting RINEX data for receiver '{receiver_name}'\")\n\n        self.rinex_store.write_or_append_group(\n            dataset=dataset, group_name=receiver_name, commit_message=commit_message\n        )\n\n        self._logger.info(f\"Successfully ingested data for receiver '{receiver_name}'\")\n\n    def read_receiver_data(\n        self, receiver_name: str, time_range: tuple[datetime, datetime] | None = None\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Read data from a specific receiver.\n\n        Parameters\n        ----------\n        receiver_name : str\n            Name of the receiver.\n        time_range : tuple of datetime, optional\n            (start_time, end_time) for filtering the data.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing receiver observations.\n\n        Raises\n        ------\n        ValueError\n            If the receiver group does not exist.\n        \"\"\"\n        if not self.rinex_store.group_exists(receiver_name):\n            available_groups = self.get_receiver_groups()\n            raise ValueError(\n                f\"No data found for receiver '{receiver_name}'. \"\n                f\"Available: {available_groups}\"\n            )\n\n        self._logger.info(f\"Reading data for receiver '{receiver_name}'\")\n\n        if self.rinex_store._rinex_store_strategy == \"append\":\n            ds = self.rinex_store.read_group_deduplicated(receiver_name, keep=\"last\")\n        else:\n            ds = self.rinex_store.read_group(receiver_name)\n\n        # Apply time filtering if specified\n        if time_range is not None:\n            start_time, end_time = time_range\n            ds = ds.where(\n                (ds.epoch &gt;= np.datetime64(start_time, \"ns\"))\n                &amp; (ds.epoch &lt;= np.datetime64(end_time, \"ns\")),\n                drop=True,\n            )\n\n            self._logger.debug(f\"Applied time filter: {start_time} to {end_time}\")\n\n        return ds\n\n    def store_vod_analysis(\n        self,\n        vod_dataset: xr.Dataset,\n        analysis_name: str,\n        commit_message: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Store VOD analysis results.\n\n        Parameters\n        ----------\n        vod_dataset : xr.Dataset\n            Dataset containing VOD analysis results.\n        analysis_name : str\n            Name of the analysis (must be configured).\n        commit_message : str, optional\n            Commit message to store with the results.\n\n        Raises\n        ------\n        ValueError\n            If ``analysis_name`` is not configured.\n        \"\"\"\n        if analysis_name not in self.vod_analyses:\n            available_analyses = list(self.vod_analyses.keys())\n            raise ValueError(\n                f\"VOD analysis '{analysis_name}' not configured. \"\n                f\"Available: {available_analyses}\"\n            )\n\n        self._logger.info(f\"Storing VOD analysis results: '{analysis_name}'\")\n\n        self.vod_store.write_or_append_group(\n            dataset=vod_dataset, group_name=analysis_name, commit_message=commit_message\n        )\n\n        self._logger.info(f\"Successfully stored VOD analysis: '{analysis_name}'\")\n\n    def read_vod_analysis(\n        self, analysis_name: str, time_range: tuple[datetime, datetime] | None = None\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Read VOD analysis results.\n\n        Parameters\n        ----------\n        analysis_name : str\n            Name of the analysis.\n        time_range : tuple of datetime, optional\n            (start_time, end_time) for filtering the results.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing VOD analysis results.\n\n        Raises\n        ------\n        ValueError\n            If the analysis group does not exist.\n        \"\"\"\n        if not self.vod_store.group_exists(analysis_name):\n            available_groups = self.get_vod_analysis_groups()\n            raise ValueError(\n                f\"No VOD results found for analysis '{analysis_name}'. \"\n                f\"Available: {available_groups}\"\n            )\n\n        self._logger.info(f\"Reading VOD analysis: '{analysis_name}'\")\n\n        ds = self.vod_store.read_group(analysis_name)\n\n        # Apply time filtering if specified\n        if time_range is not None:\n            start_time, end_time = time_range\n            ds = ds.sel(epoch=slice(start_time, end_time))\n            self._logger.debug(f\"Applied time filter: {start_time} to {end_time}\")\n\n        return ds\n\n    def prepare_vod_input_data(\n        self, analysis_name: str, time_range: tuple[datetime, datetime] | None = None\n    ) -&gt; tuple[xr.Dataset, xr.Dataset]:\n        \"\"\"\n        Prepare aligned input data for VOD analysis.\n\n        Reads data from both receivers specified in the analysis configuration\n        and returns them aligned for VOD processing.\n\n        Parameters\n        ----------\n        analysis_name : str\n            Name of the VOD analysis configuration.\n        time_range : tuple of datetime, optional\n            (start_time, end_time) for filtering the data.\n\n        Returns\n        -------\n        tuple of (xr.Dataset, xr.Dataset)\n            Tuple of (canopy_dataset, reference_dataset).\n\n        Raises\n        ------\n        ValueError\n            If the analysis is not configured or data is missing.\n        \"\"\"\n        if analysis_name not in self.vod_analyses:\n            available_analyses = list(self.vod_analyses.keys())\n            raise ValueError(\n                f\"VOD analysis '{analysis_name}' not configured. \"\n                f\"Available: {available_analyses}\"\n            )\n\n        analysis_config = self.vod_analyses[analysis_name]\n        canopy_receiver = analysis_config[\"canopy_receiver\"]\n        reference_receiver = analysis_config[\"reference_receiver\"]\n\n        self._logger.info(\n            f\"Preparing VOD input data: {canopy_receiver} vs {reference_receiver}\"\n        )\n\n        # Read data from both receivers\n        canopy_data = self.read_receiver_data(canopy_receiver, time_range)\n        reference_data = self.read_receiver_data(reference_receiver, time_range)\n\n        self._logger.info(\n            f\"Loaded data - Canopy: {dict(canopy_data.dims)}, \"\n            f\"Reference: {dict(reference_data.dims)}\"\n        )\n\n        return canopy_data, reference_data\n\n    def calculate_vod(\n        self,\n        analysis_name: str,\n        calculator_class: type[VODCalculator] | None = None,\n        time_range: tuple[datetime, datetime] | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Calculate VOD for a configured analysis pair.\n\n        Parameters\n        ----------\n        analysis_name : str\n            Analysis name from config (e.g., 'canopy_01_vs_reference_01')\n        calculator_class : type[VODCalculator], optional\n            VOD calculator class to use. If None, uses TauOmegaZerothOrder.\n        time_range : tuple of datetime, optional\n            (start_time, end_time) for filtering the data\n\n        Returns\n        -------\n        xr.Dataset\n            VOD dataset\n\n        Note\n        ----\n        Requires canvod-vod to be installed.\n        \"\"\"\n        if calculator_class is None:\n            try:\n                from canvod.vod import TauOmegaZerothOrder\n\n                calculator_class = TauOmegaZerothOrder\n            except ImportError as e:\n                raise ImportError(\n                    \"canvod-vod package required for VOD calculation. \"\n                    \"Install with: pip install canvod-vod\"\n                ) from e\n\n        canopy_ds, reference_ds = self.prepare_vod_input_data(analysis_name, time_range)\n\n        # Use the calculator's class method for calculation\n        vod_ds = calculator_class.from_datasets(canopy_ds, reference_ds, align=True)\n\n        # Add metadata\n        analysis_config = self.vod_analyses[analysis_name]\n        vod_ds.attrs[\"analysis_name\"] = analysis_name\n        vod_ds.attrs[\"canopy_receiver\"] = analysis_config[\"canopy_receiver\"]\n        vod_ds.attrs[\"reference_receiver\"] = analysis_config[\"reference_receiver\"]\n        vod_ds.attrs[\"calculator\"] = calculator_class.__name__\n        vod_ds.attrs[\"canopy_hash\"] = canopy_ds.attrs.get(\"RINEX File Hash\", \"unknown\")\n        vod_ds.attrs[\"reference_hash\"] = reference_ds.attrs.get(\n            \"RINEX File Hash\", \"unknown\"\n        )\n\n        self._logger.info(\n            f\"VOD calculated for {analysis_name} using {calculator_class.__name__}\"\n        )\n        return vod_ds\n\n    def store_vod(\n        self,\n        vod_ds: xr.Dataset,\n        analysis_name: str,\n    ) -&gt; str:\n        \"\"\"\n        Store VOD dataset in VOD store.\n\n        Parameters\n        ----------\n        vod_ds : xr.Dataset\n            VOD dataset to store\n        analysis_name : str\n            Analysis name (group name in store)\n\n        Returns\n        -------\n        str\n            Snapshot ID\n        \"\"\"\n        from gnssvodpy.utils.tools import get_version_from_pyproject\n        from icechunk.xarray import to_icechunk\n\n        canopy_hash = vod_ds.attrs.get(\"canopy_hash\", \"unknown\")\n        reference_hash = vod_ds.attrs.get(\"reference_hash\", \"unknown\")\n        combined_hash = f\"{canopy_hash}_{reference_hash}\"\n\n        with self.vod_store.writable_session() as session:\n            groups = self.vod_store.list_groups() or []\n\n            if analysis_name not in groups:\n                to_icechunk(vod_ds, session, group=analysis_name, mode=\"w\")\n                action = \"write\"\n            else:\n                to_icechunk(vod_ds, session, group=analysis_name, append_dim=\"epoch\")\n                action = \"append\"\n\n            version = get_version_from_pyproject()\n            commit_msg = f\"[v{version}] VOD for {analysis_name}\"\n            snapshot_id = session.commit(commit_msg)\n\n        self.vod_store.append_metadata(\n            group_name=analysis_name,\n            rinex_hash=combined_hash,\n            start=vod_ds[\"epoch\"].values[0],\n            end=vod_ds[\"epoch\"].values[-1],\n            snapshot_id=snapshot_id,\n            action=action,\n            commit_msg=commit_msg,\n            dataset_attrs=dict(vod_ds.attrs),\n        )\n\n        self._logger.info(\n            f\"VOD stored for {analysis_name}, snapshot={snapshot_id[:8]}...\"\n        )\n        return snapshot_id\n\n    def get_site_summary(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get a comprehensive summary of the research site.\n\n        Returns\n        -------\n        dict\n            Dictionary with site statistics, data availability, and store paths.\n        \"\"\"\n        rinex_groups = self.get_receiver_groups()\n        vod_groups = self.get_vod_analysis_groups()\n\n        summary = {\n            \"site_name\": self.site_name,\n            \"site_config\": {\n                \"total_receivers\": len(self.receivers),\n                \"active_receivers\": len(self.active_receivers),\n                \"total_vod_analyses\": len(self.vod_analyses),\n                \"active_vod_analyses\": len(self.active_vod_analyses),\n            },\n            \"data_status\": {\n                \"rinex_groups_exist\": len(rinex_groups),\n                \"rinex_groups\": rinex_groups,\n                \"vod_groups_exist\": len(vod_groups),\n                \"vod_groups\": vod_groups,\n            },\n            \"stores\": {\n                \"rinex_store_path\": str(self.rinex_store.store_path),\n                \"vod_store_path\": str(self.vod_store.store_path),\n            },\n        }\n\n        # Add receiver details\n        summary[\"receivers\"] = {}\n        for receiver_name, receiver_config in self.active_receivers.items():\n            has_data = receiver_name in rinex_groups\n            summary[\"receivers\"][receiver_name] = {\n                \"type\": receiver_config[\"type\"],\n                \"description\": receiver_config[\"description\"],\n                \"has_data\": has_data,\n            }\n\n            if has_data:\n                try:\n                    info = self.rinex_store.get_group_info(receiver_name)\n                    summary[\"receivers\"][receiver_name][\"data_info\"] = {\n                        \"dimensions\": info[\"dimensions\"],\n                        \"variables\": len(info[\"variables\"]),\n                        \"temporal_info\": info.get(\"temporal_info\", {}),\n                    }\n                except Exception as e:\n                    self._logger.warning(f\"Failed to get info for {receiver_name}: {e}\")\n\n        # Add VOD analysis details\n        summary[\"vod_analyses\"] = {}\n        for analysis_name, analysis_config in self.active_vod_analyses.items():\n            has_results = analysis_name in vod_groups\n            summary[\"vod_analyses\"][analysis_name] = {\n                \"canopy_receiver\": analysis_config[\"canopy_receiver\"],\n                \"reference_receiver\": analysis_config[\"reference_receiver\"],\n                \"description\": analysis_config[\"description\"],\n                \"has_results\": has_results,\n            }\n\n            if has_results:\n                try:\n                    info = self.vod_store.get_group_info(analysis_name)\n                    summary[\"vod_analyses\"][analysis_name][\"results_info\"] = {\n                        \"dimensions\": info[\"dimensions\"],\n                        \"variables\": len(info[\"variables\"]),\n                        \"temporal_info\": info.get(\"temporal_info\", {}),\n                    }\n                except Exception as e:\n                    self._logger.warning(\n                        f\"Failed to get VOD info for {analysis_name}: {e}\"\n                    )\n\n        return summary\n\n    def is_day_complete(\n        self,\n        yyyydoy: str,\n        receiver_types: list[str] | None = None,\n        completeness_threshold: float = 0.95,\n    ) -&gt; bool:\n        \"\"\"\n        Check if a day has complete data coverage for all receiver types.\n\n        Parameters\n        ----------\n        yyyydoy : str\n            Date in YYYYDOY format (e.g., \"2024256\")\n        receiver_types : List[str], optional\n            Receiver types to check. Defaults to ['canopy', 'reference']\n        completeness_threshold : float\n            Fraction of expected epochs that must exist (default 0.95 = 95%)\n            Allows for small gaps due to receiver issues\n\n        Returns\n        -------\n        bool\n            True if all receiver types have complete data for this day\n        \"\"\"\n        if receiver_types is None:\n            receiver_types = [\"canopy\", \"reference\"]\n\n        from gnssvodpy.utils.date_time import YYYYDOY\n\n        yyyydoy_obj = YYYYDOY.from_str(yyyydoy)\n\n        # Expected epochs for 24h at 30s sampling\n        expected_epochs = int(24 * 3600 / 30)  # 2880 epochs\n        required_epochs = int(expected_epochs * completeness_threshold)\n\n        for receiver_type in receiver_types:\n            # Get receiver name for this type\n            receiver_name = None\n            for name, config in self.active_receivers.items():\n                if config.get(\"type\") == receiver_type:\n                    receiver_name = name\n                    break\n\n            if not receiver_name:\n                self._logger.warning(f\"No receiver configured for type {receiver_type}\")\n                return False\n\n            try:\n                # Try to read data for this day\n                time_range = (yyyydoy_obj.start_datetime(), yyyydoy_obj.end_datetime())\n\n                ds = self.read_receiver_data(\n                    receiver_name=receiver_name, time_range=time_range\n                )\n\n                # Check epoch count\n                n_epochs = ds.sizes.get(\"epoch\", 0)\n\n                if n_epochs &lt; required_epochs:\n                    self._logger.info(\n                        f\"{receiver_name} {yyyydoy}: Only \"\n                        f\"{n_epochs}/{expected_epochs} epochs \"\n                        f\"({n_epochs / expected_epochs * 100:.1f}%) - incomplete\"\n                    )\n                    return False\n\n                self._logger.debug(\n                    f\"{receiver_name} {yyyydoy}: \"\n                    f\"{n_epochs}/{expected_epochs} epochs - complete\"\n                )\n\n            except (ValueError, KeyError, Exception) as e:\n                # No data exists or error reading\n                self._logger.debug(f\"{receiver_name} {yyyydoy}: No data found - {e}\")\n                return False\n\n        # All receiver types have complete data\n        return True\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the developer-facing representation.\n\n        Returns\n        -------\n        str\n            Representation string.\n        \"\"\"\n        return f\"GnssResearchSite(site_name='{self.site_name}')\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable summary.\n\n        Returns\n        -------\n        str\n            Summary string.\n        \"\"\"\n        rinex_groups = len(self.get_receiver_groups())\n        vod_groups = len(self.get_vod_analysis_groups())\n        return (\n            f\"GNSS Research Site: {self.site_name}\\n\"\n            f\"  Receivers: {len(self.active_receivers)} configured, \"\n            f\"{rinex_groups} with data\\n\"\n            f\"  VOD Analyses: {len(self.active_vod_analyses)} configured, \"\n            f\"{vod_groups} with results\"\n        )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.site_config","level":3,"title":"<code>site_config</code>  <code>property</code>","text":"<p>Get the site configuration as a dictionary.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.receivers","level":3,"title":"<code>receivers</code>  <code>property</code>","text":"<p>Get all configured receivers for this site.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.active_receivers","level":3,"title":"<code>active_receivers</code>  <code>property</code>","text":"<p>Get only active receivers for this site.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.vod_analyses","level":3,"title":"<code>vod_analyses</code>  <code>property</code>","text":"<p>Get all configured VOD analyses for this site.</p> <p>Returns auto-derived analyses from scs_from if no explicit vod_analyses are configured.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.active_vod_analyses","level":3,"title":"<code>active_vod_analyses</code>  <code>property</code>","text":"<p>Get only active VOD analyses for this site.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.__init__","level":3,"title":"<code>__init__(site_name)</code>","text":"<p>Initialize the site manager.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.__init__--parameters","level":5,"title":"Parameters","text":"<p>site_name : str     Name of the research site.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def __init__(self, site_name: str) -&gt; None:\n    \"\"\"Initialize the site manager.\n\n    Parameters\n    ----------\n    site_name : str\n        Name of the research site.\n    \"\"\"\n    from canvod.utils.config import load_config\n\n    config = load_config()\n    sites = config.sites.sites\n\n    if site_name not in sites:\n        available_sites = list(sites.keys())\n        raise KeyError(\n            f\"Site '{site_name}' not found in config. \"\n            f\"Available sites: {available_sites}\"\n        )\n\n    self.site_name = site_name\n    self._site_config = sites[site_name]\n    self._logger = get_logger(__name__).bind(site=site_name)\n\n    rinex_store_path = config.processing.storage.get_rinex_store_path(site_name)\n    vod_store_path = config.processing.storage.get_vod_store_path(site_name)\n\n    # Initialize stores using paths from processing.yaml\n    self.rinex_store = create_rinex_store(rinex_store_path)\n    self.vod_store = create_vod_store(vod_store_path)\n\n    self._logger.info(\n        f\"Initialized GNSS research site: {site_name}\",\n        rinex_store=str(rinex_store_path),\n        vod_store=str(vod_store_path),\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.from_rinex_store_path","level":3,"title":"<code>from_rinex_store_path(rinex_store_path)</code>  <code>classmethod</code>","text":"<p>Create a GnssResearchSite instance from a RINEX store path.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.from_rinex_store_path--parameters","level":5,"title":"Parameters","text":"<p>rinex_store_path : Path     Path to the RINEX Icechunk store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.from_rinex_store_path--returns","level":5,"title":"Returns","text":"<p>GnssResearchSite     Initialized research site manager.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.from_rinex_store_path--raises","level":5,"title":"Raises","text":"<p>ValueError     If no matching site is found for the given path.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>@classmethod\ndef from_rinex_store_path(\n    cls,\n    rinex_store_path: Path,\n) -&gt; GnssResearchSite:\n    \"\"\"\n    Create a GnssResearchSite instance from a RINEX store path.\n\n    Parameters\n    ----------\n    rinex_store_path : Path\n        Path to the RINEX Icechunk store.\n\n    Returns\n    -------\n    GnssResearchSite\n        Initialized research site manager.\n\n    Raises\n    ------\n    ValueError\n        If no matching site is found for the given path.\n    \"\"\"\n    # Load config to get store paths\n    from canvod.utils.config import load_config\n\n    config = load_config()\n\n    # Try to match against each site's expected rinex store path\n    for site_name in config.sites.sites.keys():\n        expected_path = config.processing.storage.get_rinex_store_path(site_name)\n        if expected_path == rinex_store_path:\n            return cls(site_name)\n\n    raise ValueError(\n        f\"No research site found for RINEX store path: {rinex_store_path}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.get_reference_canopy_pairs","level":3,"title":"<code>get_reference_canopy_pairs()</code>","text":"<p>Expand scs_from into (reference_name, canopy_name) pairs.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.get_reference_canopy_pairs--returns","level":5,"title":"Returns","text":"<p>list[tuple[str, str]]     List of (reference_name, canopy_name) pairs.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def get_reference_canopy_pairs(self) -&gt; list[tuple[str, str]]:\n    \"\"\"Expand scs_from into (reference_name, canopy_name) pairs.\n\n    Returns\n    -------\n    list[tuple[str, str]]\n        List of (reference_name, canopy_name) pairs.\n    \"\"\"\n    return self._site_config.get_reference_canopy_pairs()\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.get_auto_vod_analyses","level":3,"title":"<code>get_auto_vod_analyses()</code>","text":"<p>Derive VOD analysis pairs from scs_from configuration.</p> <p>Creates one VOD pair per (canopy, reference_for_that_canopy) combination.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.get_auto_vod_analyses--returns","level":5,"title":"Returns","text":"<p>dict[str, dict[str, Any]]     Auto-derived VOD analyses keyed by analysis name.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def get_auto_vod_analyses(self) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Derive VOD analysis pairs from scs_from configuration.\n\n    Creates one VOD pair per (canopy, reference_for_that_canopy) combination.\n\n    Returns\n    -------\n    dict[str, dict[str, Any]]\n        Auto-derived VOD analyses keyed by analysis name.\n    \"\"\"\n    analyses: dict[str, dict[str, Any]] = {}\n    for ref_name, canopy_name in self.get_reference_canopy_pairs():\n        analysis_name = f\"{canopy_name}_vs_{ref_name}\"\n        analyses[analysis_name] = {\n            \"canopy_receiver\": canopy_name,\n            \"reference_receiver\": f\"{ref_name}_{canopy_name}\",\n            \"description\": f\"VOD analysis {canopy_name} vs {ref_name}\",\n        }\n    return analyses\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.validate_site_config","level":3,"title":"<code>validate_site_config()</code>","text":"<p>Validate that the site configuration is consistent.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.validate_site_config--returns","level":5,"title":"Returns","text":"<p>bool     True if configuration is valid.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.validate_site_config--raises","level":5,"title":"Raises","text":"<p>ValueError     If configuration is invalid.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def validate_site_config(self) -&gt; bool:\n    \"\"\"\n    Validate that the site configuration is consistent.\n\n    Returns\n    -------\n    bool\n        True if configuration is valid.\n\n    Raises\n    ------\n    ValueError\n        If configuration is invalid.\n    \"\"\"\n    # Check that all VOD analyses reference valid receivers\n    # Build set of valid reference store groups (e.g. reference_01_canopy_01)\n    valid_ref_groups = {\n        f\"{ref}_{canopy}\" for ref, canopy in self.get_reference_canopy_pairs()\n    }\n\n    for analysis_name, analysis_config in self.vod_analyses.items():\n        canopy_rx = analysis_config[\"canopy_receiver\"]\n        ref_rx = analysis_config[\"reference_receiver\"]\n\n        if canopy_rx not in self.receivers:\n            raise ValueError(\n                f\"VOD analysis '{analysis_name}' references \"\n                f\"unknown canopy receiver: {canopy_rx}\"\n            )\n        # ref_rx can be either a raw receiver name or a store group name\n        if ref_rx not in self.receivers and ref_rx not in valid_ref_groups:\n            raise ValueError(\n                f\"VOD analysis '{analysis_name}' references \"\n                f\"unknown reference receiver/group: {ref_rx}\"\n            )\n\n        # Check canopy type\n        canopy_type = self.receivers[canopy_rx][\"type\"]\n        if canopy_type != \"canopy\":\n            raise ValueError(\n                f\"Receiver '{canopy_rx}' used as canopy but type is '{canopy_type}'\"\n            )\n        # Check reference type (only if it's a raw receiver name)\n        if ref_rx in self.receivers:\n            ref_type = self.receivers[ref_rx][\"type\"]\n            if ref_type != \"reference\":\n                raise ValueError(\n                    f\"Receiver '{ref_rx}' used as reference but type is '{ref_type}'\"\n                )\n\n    self._logger.debug(\"Site configuration validation passed\")\n    return True\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.get_receiver_groups","level":3,"title":"<code>get_receiver_groups()</code>","text":"<p>Get list of receiver groups that exist in the RINEX store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.get_receiver_groups--returns","level":5,"title":"Returns","text":"<p>list[str]     Existing receiver group names.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def get_receiver_groups(self) -&gt; list[str]:\n    \"\"\"\n    Get list of receiver groups that exist in the RINEX store.\n\n    Returns\n    -------\n    list[str]\n        Existing receiver group names.\n    \"\"\"\n    return self.rinex_store.list_groups()\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.get_vod_analysis_groups","level":3,"title":"<code>get_vod_analysis_groups()</code>","text":"<p>Get list of VOD analysis groups that exist in the VOD store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.get_vod_analysis_groups--returns","level":5,"title":"Returns","text":"<p>list[str]     Existing VOD analysis group names.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def get_vod_analysis_groups(self) -&gt; list[str]:\n    \"\"\"\n    Get list of VOD analysis groups that exist in the VOD store.\n\n    Returns\n    -------\n    list[str]\n        Existing VOD analysis group names.\n    \"\"\"\n    return self.vod_store.list_groups()\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.ingest_rinex_data","level":3,"title":"<code>ingest_rinex_data(dataset, receiver_name, commit_message=None)</code>","text":"<p>Ingest RINEX data for a specific receiver.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.ingest_rinex_data--parameters","level":5,"title":"Parameters","text":"<p>dataset : xr.Dataset     Processed RINEX dataset to store. receiver_name : str     Name of the receiver (must be configured). commit_message : str, optional     Commit message to store with the data.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.ingest_rinex_data--raises","level":5,"title":"Raises","text":"<p>ValueError     If <code>receiver_name</code> is not configured.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def ingest_rinex_data(\n    self, dataset: xr.Dataset, receiver_name: str, commit_message: str | None = None\n) -&gt; None:\n    \"\"\"\n    Ingest RINEX data for a specific receiver.\n\n    Parameters\n    ----------\n    dataset : xr.Dataset\n        Processed RINEX dataset to store.\n    receiver_name : str\n        Name of the receiver (must be configured).\n    commit_message : str, optional\n        Commit message to store with the data.\n\n    Raises\n    ------\n    ValueError\n        If ``receiver_name`` is not configured.\n    \"\"\"\n    if receiver_name not in self.receivers:\n        available_receivers = list(self.receivers.keys())\n        raise ValueError(\n            f\"Receiver '{receiver_name}' not configured. \"\n            f\"Available: {available_receivers}\"\n        )\n\n    self._logger.info(f\"Ingesting RINEX data for receiver '{receiver_name}'\")\n\n    self.rinex_store.write_or_append_group(\n        dataset=dataset, group_name=receiver_name, commit_message=commit_message\n    )\n\n    self._logger.info(f\"Successfully ingested data for receiver '{receiver_name}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.read_receiver_data","level":3,"title":"<code>read_receiver_data(receiver_name, time_range=None)</code>","text":"<p>Read data from a specific receiver.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.read_receiver_data--parameters","level":5,"title":"Parameters","text":"<p>receiver_name : str     Name of the receiver. time_range : tuple of datetime, optional     (start_time, end_time) for filtering the data.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.read_receiver_data--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset containing receiver observations.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.read_receiver_data--raises","level":5,"title":"Raises","text":"<p>ValueError     If the receiver group does not exist.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def read_receiver_data(\n    self, receiver_name: str, time_range: tuple[datetime, datetime] | None = None\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read data from a specific receiver.\n\n    Parameters\n    ----------\n    receiver_name : str\n        Name of the receiver.\n    time_range : tuple of datetime, optional\n        (start_time, end_time) for filtering the data.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing receiver observations.\n\n    Raises\n    ------\n    ValueError\n        If the receiver group does not exist.\n    \"\"\"\n    if not self.rinex_store.group_exists(receiver_name):\n        available_groups = self.get_receiver_groups()\n        raise ValueError(\n            f\"No data found for receiver '{receiver_name}'. \"\n            f\"Available: {available_groups}\"\n        )\n\n    self._logger.info(f\"Reading data for receiver '{receiver_name}'\")\n\n    if self.rinex_store._rinex_store_strategy == \"append\":\n        ds = self.rinex_store.read_group_deduplicated(receiver_name, keep=\"last\")\n    else:\n        ds = self.rinex_store.read_group(receiver_name)\n\n    # Apply time filtering if specified\n    if time_range is not None:\n        start_time, end_time = time_range\n        ds = ds.where(\n            (ds.epoch &gt;= np.datetime64(start_time, \"ns\"))\n            &amp; (ds.epoch &lt;= np.datetime64(end_time, \"ns\")),\n            drop=True,\n        )\n\n        self._logger.debug(f\"Applied time filter: {start_time} to {end_time}\")\n\n    return ds\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.store_vod_analysis","level":3,"title":"<code>store_vod_analysis(vod_dataset, analysis_name, commit_message=None)</code>","text":"<p>Store VOD analysis results.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.store_vod_analysis--parameters","level":5,"title":"Parameters","text":"<p>vod_dataset : xr.Dataset     Dataset containing VOD analysis results. analysis_name : str     Name of the analysis (must be configured). commit_message : str, optional     Commit message to store with the results.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.store_vod_analysis--raises","level":5,"title":"Raises","text":"<p>ValueError     If <code>analysis_name</code> is not configured.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def store_vod_analysis(\n    self,\n    vod_dataset: xr.Dataset,\n    analysis_name: str,\n    commit_message: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Store VOD analysis results.\n\n    Parameters\n    ----------\n    vod_dataset : xr.Dataset\n        Dataset containing VOD analysis results.\n    analysis_name : str\n        Name of the analysis (must be configured).\n    commit_message : str, optional\n        Commit message to store with the results.\n\n    Raises\n    ------\n    ValueError\n        If ``analysis_name`` is not configured.\n    \"\"\"\n    if analysis_name not in self.vod_analyses:\n        available_analyses = list(self.vod_analyses.keys())\n        raise ValueError(\n            f\"VOD analysis '{analysis_name}' not configured. \"\n            f\"Available: {available_analyses}\"\n        )\n\n    self._logger.info(f\"Storing VOD analysis results: '{analysis_name}'\")\n\n    self.vod_store.write_or_append_group(\n        dataset=vod_dataset, group_name=analysis_name, commit_message=commit_message\n    )\n\n    self._logger.info(f\"Successfully stored VOD analysis: '{analysis_name}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.read_vod_analysis","level":3,"title":"<code>read_vod_analysis(analysis_name, time_range=None)</code>","text":"<p>Read VOD analysis results.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.read_vod_analysis--parameters","level":5,"title":"Parameters","text":"<p>analysis_name : str     Name of the analysis. time_range : tuple of datetime, optional     (start_time, end_time) for filtering the results.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.read_vod_analysis--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset containing VOD analysis results.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.read_vod_analysis--raises","level":5,"title":"Raises","text":"<p>ValueError     If the analysis group does not exist.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def read_vod_analysis(\n    self, analysis_name: str, time_range: tuple[datetime, datetime] | None = None\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read VOD analysis results.\n\n    Parameters\n    ----------\n    analysis_name : str\n        Name of the analysis.\n    time_range : tuple of datetime, optional\n        (start_time, end_time) for filtering the results.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing VOD analysis results.\n\n    Raises\n    ------\n    ValueError\n        If the analysis group does not exist.\n    \"\"\"\n    if not self.vod_store.group_exists(analysis_name):\n        available_groups = self.get_vod_analysis_groups()\n        raise ValueError(\n            f\"No VOD results found for analysis '{analysis_name}'. \"\n            f\"Available: {available_groups}\"\n        )\n\n    self._logger.info(f\"Reading VOD analysis: '{analysis_name}'\")\n\n    ds = self.vod_store.read_group(analysis_name)\n\n    # Apply time filtering if specified\n    if time_range is not None:\n        start_time, end_time = time_range\n        ds = ds.sel(epoch=slice(start_time, end_time))\n        self._logger.debug(f\"Applied time filter: {start_time} to {end_time}\")\n\n    return ds\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.prepare_vod_input_data","level":3,"title":"<code>prepare_vod_input_data(analysis_name, time_range=None)</code>","text":"<p>Prepare aligned input data for VOD analysis.</p> <p>Reads data from both receivers specified in the analysis configuration and returns them aligned for VOD processing.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.prepare_vod_input_data--parameters","level":5,"title":"Parameters","text":"<p>analysis_name : str     Name of the VOD analysis configuration. time_range : tuple of datetime, optional     (start_time, end_time) for filtering the data.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.prepare_vod_input_data--returns","level":5,"title":"Returns","text":"<p>tuple of (xr.Dataset, xr.Dataset)     Tuple of (canopy_dataset, reference_dataset).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.prepare_vod_input_data--raises","level":5,"title":"Raises","text":"<p>ValueError     If the analysis is not configured or data is missing.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def prepare_vod_input_data(\n    self, analysis_name: str, time_range: tuple[datetime, datetime] | None = None\n) -&gt; tuple[xr.Dataset, xr.Dataset]:\n    \"\"\"\n    Prepare aligned input data for VOD analysis.\n\n    Reads data from both receivers specified in the analysis configuration\n    and returns them aligned for VOD processing.\n\n    Parameters\n    ----------\n    analysis_name : str\n        Name of the VOD analysis configuration.\n    time_range : tuple of datetime, optional\n        (start_time, end_time) for filtering the data.\n\n    Returns\n    -------\n    tuple of (xr.Dataset, xr.Dataset)\n        Tuple of (canopy_dataset, reference_dataset).\n\n    Raises\n    ------\n    ValueError\n        If the analysis is not configured or data is missing.\n    \"\"\"\n    if analysis_name not in self.vod_analyses:\n        available_analyses = list(self.vod_analyses.keys())\n        raise ValueError(\n            f\"VOD analysis '{analysis_name}' not configured. \"\n            f\"Available: {available_analyses}\"\n        )\n\n    analysis_config = self.vod_analyses[analysis_name]\n    canopy_receiver = analysis_config[\"canopy_receiver\"]\n    reference_receiver = analysis_config[\"reference_receiver\"]\n\n    self._logger.info(\n        f\"Preparing VOD input data: {canopy_receiver} vs {reference_receiver}\"\n    )\n\n    # Read data from both receivers\n    canopy_data = self.read_receiver_data(canopy_receiver, time_range)\n    reference_data = self.read_receiver_data(reference_receiver, time_range)\n\n    self._logger.info(\n        f\"Loaded data - Canopy: {dict(canopy_data.dims)}, \"\n        f\"Reference: {dict(reference_data.dims)}\"\n    )\n\n    return canopy_data, reference_data\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.calculate_vod","level":3,"title":"<code>calculate_vod(analysis_name, calculator_class=None, time_range=None)</code>","text":"<p>Calculate VOD for a configured analysis pair.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.calculate_vod--parameters","level":5,"title":"Parameters","text":"<p>analysis_name : str     Analysis name from config (e.g., 'canopy_01_vs_reference_01') calculator_class : type[VODCalculator], optional     VOD calculator class to use. If None, uses TauOmegaZerothOrder. time_range : tuple of datetime, optional     (start_time, end_time) for filtering the data</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.calculate_vod--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     VOD dataset</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.calculate_vod--note","level":5,"title":"Note","text":"<p>Requires canvod-vod to be installed.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def calculate_vod(\n    self,\n    analysis_name: str,\n    calculator_class: type[VODCalculator] | None = None,\n    time_range: tuple[datetime, datetime] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Calculate VOD for a configured analysis pair.\n\n    Parameters\n    ----------\n    analysis_name : str\n        Analysis name from config (e.g., 'canopy_01_vs_reference_01')\n    calculator_class : type[VODCalculator], optional\n        VOD calculator class to use. If None, uses TauOmegaZerothOrder.\n    time_range : tuple of datetime, optional\n        (start_time, end_time) for filtering the data\n\n    Returns\n    -------\n    xr.Dataset\n        VOD dataset\n\n    Note\n    ----\n    Requires canvod-vod to be installed.\n    \"\"\"\n    if calculator_class is None:\n        try:\n            from canvod.vod import TauOmegaZerothOrder\n\n            calculator_class = TauOmegaZerothOrder\n        except ImportError as e:\n            raise ImportError(\n                \"canvod-vod package required for VOD calculation. \"\n                \"Install with: pip install canvod-vod\"\n            ) from e\n\n    canopy_ds, reference_ds = self.prepare_vod_input_data(analysis_name, time_range)\n\n    # Use the calculator's class method for calculation\n    vod_ds = calculator_class.from_datasets(canopy_ds, reference_ds, align=True)\n\n    # Add metadata\n    analysis_config = self.vod_analyses[analysis_name]\n    vod_ds.attrs[\"analysis_name\"] = analysis_name\n    vod_ds.attrs[\"canopy_receiver\"] = analysis_config[\"canopy_receiver\"]\n    vod_ds.attrs[\"reference_receiver\"] = analysis_config[\"reference_receiver\"]\n    vod_ds.attrs[\"calculator\"] = calculator_class.__name__\n    vod_ds.attrs[\"canopy_hash\"] = canopy_ds.attrs.get(\"RINEX File Hash\", \"unknown\")\n    vod_ds.attrs[\"reference_hash\"] = reference_ds.attrs.get(\n        \"RINEX File Hash\", \"unknown\"\n    )\n\n    self._logger.info(\n        f\"VOD calculated for {analysis_name} using {calculator_class.__name__}\"\n    )\n    return vod_ds\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.store_vod","level":3,"title":"<code>store_vod(vod_ds, analysis_name)</code>","text":"<p>Store VOD dataset in VOD store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.store_vod--parameters","level":5,"title":"Parameters","text":"<p>vod_ds : xr.Dataset     VOD dataset to store analysis_name : str     Analysis name (group name in store)</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.store_vod--returns","level":5,"title":"Returns","text":"<p>str     Snapshot ID</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def store_vod(\n    self,\n    vod_ds: xr.Dataset,\n    analysis_name: str,\n) -&gt; str:\n    \"\"\"\n    Store VOD dataset in VOD store.\n\n    Parameters\n    ----------\n    vod_ds : xr.Dataset\n        VOD dataset to store\n    analysis_name : str\n        Analysis name (group name in store)\n\n    Returns\n    -------\n    str\n        Snapshot ID\n    \"\"\"\n    from gnssvodpy.utils.tools import get_version_from_pyproject\n    from icechunk.xarray import to_icechunk\n\n    canopy_hash = vod_ds.attrs.get(\"canopy_hash\", \"unknown\")\n    reference_hash = vod_ds.attrs.get(\"reference_hash\", \"unknown\")\n    combined_hash = f\"{canopy_hash}_{reference_hash}\"\n\n    with self.vod_store.writable_session() as session:\n        groups = self.vod_store.list_groups() or []\n\n        if analysis_name not in groups:\n            to_icechunk(vod_ds, session, group=analysis_name, mode=\"w\")\n            action = \"write\"\n        else:\n            to_icechunk(vod_ds, session, group=analysis_name, append_dim=\"epoch\")\n            action = \"append\"\n\n        version = get_version_from_pyproject()\n        commit_msg = f\"[v{version}] VOD for {analysis_name}\"\n        snapshot_id = session.commit(commit_msg)\n\n    self.vod_store.append_metadata(\n        group_name=analysis_name,\n        rinex_hash=combined_hash,\n        start=vod_ds[\"epoch\"].values[0],\n        end=vod_ds[\"epoch\"].values[-1],\n        snapshot_id=snapshot_id,\n        action=action,\n        commit_msg=commit_msg,\n        dataset_attrs=dict(vod_ds.attrs),\n    )\n\n    self._logger.info(\n        f\"VOD stored for {analysis_name}, snapshot={snapshot_id[:8]}...\"\n    )\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.get_site_summary","level":3,"title":"<code>get_site_summary()</code>","text":"<p>Get a comprehensive summary of the research site.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.get_site_summary--returns","level":5,"title":"Returns","text":"<p>dict     Dictionary with site statistics, data availability, and store paths.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def get_site_summary(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get a comprehensive summary of the research site.\n\n    Returns\n    -------\n    dict\n        Dictionary with site statistics, data availability, and store paths.\n    \"\"\"\n    rinex_groups = self.get_receiver_groups()\n    vod_groups = self.get_vod_analysis_groups()\n\n    summary = {\n        \"site_name\": self.site_name,\n        \"site_config\": {\n            \"total_receivers\": len(self.receivers),\n            \"active_receivers\": len(self.active_receivers),\n            \"total_vod_analyses\": len(self.vod_analyses),\n            \"active_vod_analyses\": len(self.active_vod_analyses),\n        },\n        \"data_status\": {\n            \"rinex_groups_exist\": len(rinex_groups),\n            \"rinex_groups\": rinex_groups,\n            \"vod_groups_exist\": len(vod_groups),\n            \"vod_groups\": vod_groups,\n        },\n        \"stores\": {\n            \"rinex_store_path\": str(self.rinex_store.store_path),\n            \"vod_store_path\": str(self.vod_store.store_path),\n        },\n    }\n\n    # Add receiver details\n    summary[\"receivers\"] = {}\n    for receiver_name, receiver_config in self.active_receivers.items():\n        has_data = receiver_name in rinex_groups\n        summary[\"receivers\"][receiver_name] = {\n            \"type\": receiver_config[\"type\"],\n            \"description\": receiver_config[\"description\"],\n            \"has_data\": has_data,\n        }\n\n        if has_data:\n            try:\n                info = self.rinex_store.get_group_info(receiver_name)\n                summary[\"receivers\"][receiver_name][\"data_info\"] = {\n                    \"dimensions\": info[\"dimensions\"],\n                    \"variables\": len(info[\"variables\"]),\n                    \"temporal_info\": info.get(\"temporal_info\", {}),\n                }\n            except Exception as e:\n                self._logger.warning(f\"Failed to get info for {receiver_name}: {e}\")\n\n    # Add VOD analysis details\n    summary[\"vod_analyses\"] = {}\n    for analysis_name, analysis_config in self.active_vod_analyses.items():\n        has_results = analysis_name in vod_groups\n        summary[\"vod_analyses\"][analysis_name] = {\n            \"canopy_receiver\": analysis_config[\"canopy_receiver\"],\n            \"reference_receiver\": analysis_config[\"reference_receiver\"],\n            \"description\": analysis_config[\"description\"],\n            \"has_results\": has_results,\n        }\n\n        if has_results:\n            try:\n                info = self.vod_store.get_group_info(analysis_name)\n                summary[\"vod_analyses\"][analysis_name][\"results_info\"] = {\n                    \"dimensions\": info[\"dimensions\"],\n                    \"variables\": len(info[\"variables\"]),\n                    \"temporal_info\": info.get(\"temporal_info\", {}),\n                }\n            except Exception as e:\n                self._logger.warning(\n                    f\"Failed to get VOD info for {analysis_name}: {e}\"\n                )\n\n    return summary\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.is_day_complete","level":3,"title":"<code>is_day_complete(yyyydoy, receiver_types=None, completeness_threshold=0.95)</code>","text":"<p>Check if a day has complete data coverage for all receiver types.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.is_day_complete--parameters","level":5,"title":"Parameters","text":"<p>yyyydoy : str     Date in YYYYDOY format (e.g., \"2024256\") receiver_types : List[str], optional     Receiver types to check. Defaults to ['canopy', 'reference'] completeness_threshold : float     Fraction of expected epochs that must exist (default 0.95 = 95%)     Allows for small gaps due to receiver issues</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.is_day_complete--returns","level":5,"title":"Returns","text":"<p>bool     True if all receiver types have complete data for this day</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def is_day_complete(\n    self,\n    yyyydoy: str,\n    receiver_types: list[str] | None = None,\n    completeness_threshold: float = 0.95,\n) -&gt; bool:\n    \"\"\"\n    Check if a day has complete data coverage for all receiver types.\n\n    Parameters\n    ----------\n    yyyydoy : str\n        Date in YYYYDOY format (e.g., \"2024256\")\n    receiver_types : List[str], optional\n        Receiver types to check. Defaults to ['canopy', 'reference']\n    completeness_threshold : float\n        Fraction of expected epochs that must exist (default 0.95 = 95%)\n        Allows for small gaps due to receiver issues\n\n    Returns\n    -------\n    bool\n        True if all receiver types have complete data for this day\n    \"\"\"\n    if receiver_types is None:\n        receiver_types = [\"canopy\", \"reference\"]\n\n    from gnssvodpy.utils.date_time import YYYYDOY\n\n    yyyydoy_obj = YYYYDOY.from_str(yyyydoy)\n\n    # Expected epochs for 24h at 30s sampling\n    expected_epochs = int(24 * 3600 / 30)  # 2880 epochs\n    required_epochs = int(expected_epochs * completeness_threshold)\n\n    for receiver_type in receiver_types:\n        # Get receiver name for this type\n        receiver_name = None\n        for name, config in self.active_receivers.items():\n            if config.get(\"type\") == receiver_type:\n                receiver_name = name\n                break\n\n        if not receiver_name:\n            self._logger.warning(f\"No receiver configured for type {receiver_type}\")\n            return False\n\n        try:\n            # Try to read data for this day\n            time_range = (yyyydoy_obj.start_datetime(), yyyydoy_obj.end_datetime())\n\n            ds = self.read_receiver_data(\n                receiver_name=receiver_name, time_range=time_range\n            )\n\n            # Check epoch count\n            n_epochs = ds.sizes.get(\"epoch\", 0)\n\n            if n_epochs &lt; required_epochs:\n                self._logger.info(\n                    f\"{receiver_name} {yyyydoy}: Only \"\n                    f\"{n_epochs}/{expected_epochs} epochs \"\n                    f\"({n_epochs / expected_epochs * 100:.1f}%) - incomplete\"\n                )\n                return False\n\n            self._logger.debug(\n                f\"{receiver_name} {yyyydoy}: \"\n                f\"{n_epochs}/{expected_epochs} epochs - complete\"\n            )\n\n        except (ValueError, KeyError, Exception) as e:\n            # No data exists or error reading\n            self._logger.debug(f\"{receiver_name} {yyyydoy}: No data found - {e}\")\n            return False\n\n    # All receiver types have complete data\n    return True\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return the developer-facing representation.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.__repr__--returns","level":5,"title":"Returns","text":"<p>str     Representation string.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the developer-facing representation.\n\n    Returns\n    -------\n    str\n        Representation string.\n    \"\"\"\n    return f\"GnssResearchSite(site_name='{self.site_name}')\"\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.__str__","level":3,"title":"<code>__str__()</code>","text":"<p>Return a human-readable summary.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.GnssResearchSite.__str__--returns","level":5,"title":"Returns","text":"<p>str     Summary string.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable summary.\n\n    Returns\n    -------\n    str\n        Summary string.\n    \"\"\"\n    rinex_groups = len(self.get_receiver_groups())\n    vod_groups = len(self.get_vod_analysis_groups())\n    return (\n        f\"GNSS Research Site: {self.site_name}\\n\"\n        f\"  Receivers: {len(self.active_receivers)} configured, \"\n        f\"{rinex_groups} with data\\n\"\n        f\"  VOD Analyses: {len(self.active_vod_analyses)} configured, \"\n        f\"{vod_groups} with results\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.create_default_site","level":2,"title":"<code>create_default_site()</code>","text":"<p>Create a <code>GnssResearchSite</code> instance for the default site.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.manager.create_default_site--returns","level":4,"title":"Returns","text":"<p>GnssResearchSite     Instance for the <code>DEFAULT_RESEARCH_SITE</code>.</p> Source code in <code>packages/canvod-store/src/canvod/store/manager.py</code> <pre><code>def create_default_site() -&gt; GnssResearchSite:\n    \"\"\"\n    Create a `GnssResearchSite` instance for the default site.\n\n    Returns\n    -------\n    GnssResearchSite\n        Instance for the ``DEFAULT_RESEARCH_SITE``.\n    \"\"\"\n    from canvod.utils.config import load_config\n\n    return GnssResearchSite(next(iter(load_config().sites.sites)))\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#data-store","level":2,"title":"Data Store","text":"","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore","level":2,"title":"<code>MyIcechunkStore</code>","text":"<p>Core Icechunk store manager for GNSS data.</p> <p>This class encapsulates all operations on a single Icechunk repository, providing a clean interface for GNSS data storage and retrieval with integrated logging and proper resource management.</p> <p>Features: - Automatic repository creation/connection - Group management with validation - Session management with context managers - Integrated logging with file contexts - Configurable compression and chunking</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore--parameters","level":4,"title":"Parameters","text":"<p>store_path : Path     Path to the Icechunk store directory. store_type : str, default \"rinex_store\"     Type of store (\"rinex_store\" or \"vod_store\"). compression_level : int | None, optional     Override default compression level. compression_algorithm : str | None, optional     Override default compression algorithm.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore--attributes","level":4,"title":"Attributes","text":"<p>store_path : Path     Path to the Icechunk store directory. store_type : str     Type of store (\"rinex_store\" or \"vod_store\"). compression_level : int     Compression level (1-9). compression_algorithm : icechunk.CompressionAlgorithm     Compression algorithm enum. repo : icechunk.Repository     The Icechunk repository instance.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>@add_rich_display_to_store\nclass MyIcechunkStore:\n    \"\"\"\n    Core Icechunk store manager for GNSS data.\n\n    This class encapsulates all operations on a single Icechunk repository,\n    providing a clean interface for GNSS data storage and retrieval with\n    integrated logging and proper resource management.\n\n    Features:\n    - Automatic repository creation/connection\n    - Group management with validation\n    - Session management with context managers\n    - Integrated logging with file contexts\n    - Configurable compression and chunking\n\n    Parameters\n    ----------\n    store_path : Path\n        Path to the Icechunk store directory.\n    store_type : str, default \"rinex_store\"\n        Type of store (\"rinex_store\" or \"vod_store\").\n    compression_level : int | None, optional\n        Override default compression level.\n    compression_algorithm : str | None, optional\n        Override default compression algorithm.\n\n    Attributes\n    ----------\n    store_path : Path\n        Path to the Icechunk store directory.\n    store_type : str\n        Type of store (\"rinex_store\" or \"vod_store\").\n    compression_level : int\n        Compression level (1-9).\n    compression_algorithm : icechunk.CompressionAlgorithm\n        Compression algorithm enum.\n    repo : icechunk.Repository\n        The Icechunk repository instance.\n    \"\"\"\n\n    def __init__(\n        self,\n        store_path: Path,\n        store_type: str = \"rinex_store\",\n        compression_level: int | None = None,\n        compression_algorithm: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Icechunk store manager.\n\n        Parameters\n        ----------\n        store_path : Path\n            Path to the Icechunk store directory.\n        store_type : str, default \"rinex_store\"\n            Type of store (\"rinex_store\" or \"vod_store\").\n        compression_level : int | None, optional\n            Override default compression level.\n        compression_algorithm : str | None, optional\n            Override default compression algorithm.\n        \"\"\"\n        from canvod.utils.config import load_config\n\n        cfg = load_config()\n        ic_cfg = cfg.processing.icechunk\n        st_cfg = cfg.processing.storage\n\n        self.store_path = Path(store_path)\n        self.store_type = store_type\n        # Site name is parent directory name\n        self.site_name = self.store_path.parent.name\n\n        # Compression\n        self.compression_level = compression_level or ic_cfg.compression_level\n        compression_alg = compression_algorithm or ic_cfg.compression_algorithm\n        self.compression_algorithm = getattr(\n            icechunk.CompressionAlgorithm, compression_alg.capitalize()\n        )\n\n        # Chunk strategy\n        chunk_strategies = {\n            k: {\"epoch\": v.epoch, \"sid\": v.sid}\n            for k, v in ic_cfg.chunk_strategies.items()\n        }\n        self.chunk_strategy = chunk_strategies.get(store_type, {})\n\n        # Storage config cached for metadata rows\n        self._rinex_store_strategy = st_cfg.rinex_store_strategy\n        self._rinex_store_expire_days = st_cfg.rinex_store_expire_days\n        self._vod_store_strategy = st_cfg.vod_store_strategy\n\n        # Configure repository\n        self.config = icechunk.RepositoryConfig.default()\n        self.config.compression = icechunk.CompressionConfig(\n            level=self.compression_level, algorithm=self.compression_algorithm\n        )\n        self.config.inline_chunk_threshold_bytes = ic_cfg.inline_threshold\n        self.config.get_partial_values_concurrency = ic_cfg.get_concurrency\n\n        if ic_cfg.manifest_preload_enabled:\n            self.config.manifest = icechunk.ManifestConfig(\n                preload=icechunk.ManifestPreloadConfig(\n                    max_total_refs=ic_cfg.manifest_preload_max_refs,\n                    preload_if=icechunk.ManifestPreloadCondition.name_matches(\n                        ic_cfg.manifest_preload_pattern\n                    ),\n                )\n            )\n            self._logger.info(\n                f\"Manifest preload enabled: {ic_cfg.manifest_preload_pattern}\"\n            )\n\n        self._repo = None\n        self._logger = get_logger(__name__)\n\n        # Remove .DS_Store files that corrupt icechunk ref listing on macOS\n        self._clean_ds_store()\n        self._ensure_store_exists()\n\n    def _clean_ds_store(self) -&gt; None:\n        \"\"\"Remove .DS_Store files from the store directory tree.\n\n        macOS creates these files automatically and they corrupt icechunk's\n        ref listing, causing 'invalid ref type `.DS_Store`' errors.\n        \"\"\"\n        if not self.store_path.exists():\n            return\n        for ds_store in self.store_path.rglob(\".DS_Store\"):\n            ds_store.unlink()\n            self._logger.debug(f\"Removed {ds_store}\")\n\n    def _normalize_encodings(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"Normalize dataset encodings for Icechunk.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset to normalize.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with normalized encodings.\n        \"\"\"\n        for v in ds.data_vars:\n            if \"dtype\" in ds[v].encoding:\n                ds[v].encoding[\"dtype\"] = np.dtype(ds[v].dtype)\n        return ds\n\n    def _ensure_store_exists(self) -&gt; None:\n        \"\"\"Ensure the store exists, creating if necessary.\"\"\"\n        storage = icechunk.local_filesystem_storage(str(self.store_path))\n\n        if self.store_path.exists() and any(self.store_path.iterdir()):\n            self._logger.info(f\"Opening existing Icechunk store at {self.store_path}\")\n            self._repo = icechunk.Repository.open(storage=storage, config=self.config)\n        else:\n            self._logger.info(f\"Creating new Icechunk store at {self.store_path}\")\n            self.store_path.mkdir(parents=True, exist_ok=True)\n            self._repo = icechunk.Repository.create(storage=storage, config=self.config)\n\n    @property\n    def repo(self) -&gt; icechunk.Repository:\n        \"\"\"Get the repository instance.\"\"\"\n        if self._repo is None:\n            self._ensure_store_exists()\n        return self._repo\n\n    @contextlib.contextmanager\n    def readonly_session(\n        self,\n        branch: str = \"main\",\n    ) -&gt; Generator[\"icechunk.ReadonlySession\"]:\n        \"\"\"Context manager for readonly sessions.\n\n        Parameters\n        ----------\n        branch : str, default \"main\"\n            Branch name.\n\n        Returns\n        -------\n        Generator[icechunk.ReadonlySession, None, None]\n            Readonly session context manager.\n        \"\"\"\n        session = self.repo.readonly_session(branch)\n        try:\n            self._logger.debug(f\"Opened readonly session for branch '{branch}'\")\n            yield session\n        finally:\n            self._logger.debug(f\"Closed readonly session for branch '{branch}'\")\n\n    @contextlib.contextmanager\n    def writable_session(\n        self,\n        branch: str = \"main\",\n    ) -&gt; Generator[\"icechunk.WritableSession\"]:\n        \"\"\"Context manager for writable sessions.\n\n        Parameters\n        ----------\n        branch : str, default \"main\"\n            Branch name.\n\n        Returns\n        -------\n        Generator[icechunk.WritableSession, None, None]\n            Writable session context manager.\n        \"\"\"\n        session = self.repo.writable_session(branch)\n        try:\n            self._logger.debug(f\"Opened writable session for branch '{branch}'\")\n            yield session\n        finally:\n            self._logger.debug(f\"Closed writable session for branch '{branch}'\")\n\n    def get_branch_names(self) -&gt; list[str]:\n        \"\"\"\n        List all branches in the store.\n\n        Returns\n        -------\n        list[str]\n            List of branch names.\n        \"\"\"\n        try:\n            storage_config = icechunk.local_filesystem_storage(self.store_path)\n            repo = icechunk.Repository.open(\n                storage=storage_config,\n            )\n\n            return list(repo.list_branches())\n        except Exception as e:\n            self._logger.warning(f\"Failed to list branches in {repr(self)}: {e}\")\n            warnings.warn(f\"Failed to list branches in {repr(self)}: {e}\")\n            return []\n\n    def get_group_names(self, branch: str | None = None) -&gt; dict[str, list[str]]:\n        \"\"\"\n        List all groups in the store.\n\n        Parameters\n        ----------\n        branch: Optional[str]\n            Repository branch to examine. Defaults to listing groups from all branches.\n\n        Returns\n        -------\n        dict[str, list[str]]\n            Dictionary mapping branch names to lists of group names.\n\n        \"\"\"\n        try:\n            if not branch:\n                branches = self.get_branch_names()\n            else:\n                branches = [branch]\n\n            storage_config = icechunk.local_filesystem_storage(self.store_path)\n            repo = icechunk.Repository.open(\n                storage=storage_config,\n            )\n\n            group_dict = {}\n            for br in branches:\n                with self.readonly_session(br) as session:\n                    session = repo.readonly_session(br)\n                    root = zarr.open(session.store, mode=\"r\")\n                    group_dict[br] = list(root.group_keys())\n\n            return group_dict\n\n        except Exception as e:\n            self._logger.warning(f\"Failed to list groups in {repr(self)}: {e}\")\n            return {}\n\n    def list_groups(self, branch: str = \"main\") -&gt; list[str]:\n        \"\"\"\n        List all groups in a branch.\n\n        Parameters\n        ----------\n        branch : str\n            Branch name (default: \"main\")\n\n        Returns\n        -------\n        list[str]\n            List of group names in the branch\n        \"\"\"\n        group_dict = self.get_group_names(branch=branch)\n        if branch in group_dict:\n            return group_dict[branch]\n        return []\n\n    @property\n    def tree(self) -&gt; None:\n        \"\"\"\n        Display hierarchical tree of all branches, groups, and subgroups.\n        \"\"\"\n        self.print_tree(max_depth=None)\n\n    def print_tree(self, max_depth: int | None = None) -&gt; None:\n        \"\"\"\n        Display hierarchical tree of all branches, groups, and subgroups.\n\n        Parameters\n        ----------\n        max_depth : int | None\n            Maximum depth to display. None for unlimited depth.\n            - 0: Only show branches\n            - 1: Show branches and top-level groups\n            - 2: Show branches, groups, and first level of subgroups/arrays\n            - etc.\n        \"\"\"\n        try:\n            branches = self.get_branch_names()\n\n            for i, branch in enumerate(branches):\n                is_last_branch = i == len(branches) - 1\n                branch_prefix = \"└── \" if is_last_branch else \"├── \"\n\n                if max_depth is not None and max_depth &lt; 1:\n                    continue\n\n                session = self.repo.readonly_session(branch)\n                root = zarr.open(session.store, mode=\"r\")\n\n                if i == 0:\n                    sys.stdout.write(f\"{self.store_path}\\n\")\n\n                sys.stdout.write(f\"{branch_prefix}{branch}\\n\")\n                # Build tree recursively\n                branch_indent = \"    \" if is_last_branch else \"│   \"\n                self._build_tree(root, branch_indent, max_depth, current_depth=1)\n\n        except Exception as e:\n            self._logger.warning(f\"Failed to generate tree for {repr(self)}: {e}\")\n            sys.stdout.write(f\"Error generating tree: {e}\\n\")\n\n    def _build_tree(\n        self,\n        group: zarr.Group,\n        prefix: str,\n        max_depth: int | None,\n        current_depth: int = 0,\n    ) -&gt; None:\n        \"\"\"Recursively build a tree structure.\n\n        Parameters\n        ----------\n        group : zarr.Group\n            Root group to traverse.\n        prefix : str\n            Prefix string for tree formatting.\n        max_depth : int | None\n            Maximum depth to display. None for unlimited.\n        current_depth : int, default 0\n            Current recursion depth.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if max_depth is not None and current_depth &gt;= max_depth:\n            return\n\n        # Get all groups and arrays\n        groups = list(group.group_keys())\n        arrays = list(group.array_keys())\n        items = groups + arrays\n\n        for i, item_name in enumerate(items):\n            is_last = i == len(items) - 1\n            connector = \"└── \" if is_last else \"├── \"\n\n            if item_name in groups:\n                # It's a group\n                sys.stdout.write(f\"{prefix}{connector}{item_name}\\n\")\n\n                # Recurse into subgroup\n                subgroup = group[item_name]\n                new_prefix = prefix + (\"    \" if is_last else \"│   \")\n                self._build_tree(subgroup, new_prefix, max_depth, current_depth + 1)\n            else:\n                # It's an array\n                arr = group[item_name]\n                shape_str = str(arr.shape)\n                dtype_str = str(arr.dtype)\n                sys.stdout.write(\n                    f\"{prefix}{connector}{item_name} {shape_str} {dtype_str}\\n\"\n                )\n\n    def group_exists(self, group_name: str, branch: str = \"main\") -&gt; bool:\n        \"\"\"\n        Check if a group exists.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group to check.\n        branch : str, default \"main\"\n            Repository branch to examine.\n\n        Returns\n        -------\n        bool\n            True if the group exists, False otherwise.\n        \"\"\"\n        group_dict = self.get_group_names(branch)\n\n        # get_group_names returns dict like {'main': ['canopy_01', ...]}\n        if branch in group_dict:\n            exists = group_name in group_dict[branch]\n        else:\n            exists = False\n\n        self._logger.debug(\n            f\"Group '{group_name}' exists on branch '{branch}': {exists}\"\n        )\n        return exists\n\n    def read_group(\n        self,\n        group_name: str,\n        branch: str = \"main\",\n        time_slice: slice | None = None,\n        chunks: dict[str, Any] | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Read data from a group.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group to read.\n        branch : str, default \"main\"\n            Repository branch.\n        time_slice : slice | None, optional\n            Optional time slice for filtering.\n        chunks : dict[str, Any] | None, optional\n            Chunking specification (uses config defaults if None).\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset from the group.\n        \"\"\"\n        self._logger.info(f\"Reading group '{group_name}' from branch '{branch}'\")\n\n        with self.readonly_session(branch) as session:\n            # Use default chunking strategy if none provided\n            if chunks is None:\n                chunks = self.chunk_strategy or {\"epoch\": 34560, \"sid\": -1}\n\n            ds = xr.open_zarr(\n                session.store,\n                group=group_name,\n                chunks=chunks,\n                consolidated=False,\n            )\n\n            if time_slice is not None:\n                ds = ds.isel(epoch=time_slice)\n                self._logger.debug(f\"Applied time slice: {time_slice}\")\n\n            self._logger.info(\n                f\"Successfully read group '{group_name}' - shape: {dict(ds.sizes)}\"\n            )\n            return ds\n\n    def read_group_deduplicated(\n        self,\n        group_name: str,\n        branch: str = \"main\",\n        keep: str = \"last\",\n        time_slice: slice | None = None,\n        chunks: dict[str, Any] | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Read data from a group with automatic deduplication.\n\n        This method calls read_group() then removes duplicates using metadata table\n        intelligence when available, falling back to simple epoch deduplication.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group to read.\n        branch : str, default \"main\"\n            Repository branch.\n        keep : str, default \"last\"\n            Deduplication strategy for duplicate epochs.\n        time_slice : slice | None, optional\n            Optional time slice for filtering.\n        chunks : dict[str, Any] | None, optional\n            Chunking specification (uses config defaults if None).\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with duplicates removed (latest data only).\n        \"\"\"\n\n        if keep not in [\"last\"]:\n            raise ValueError(\"Currently only 'last' is supported for keep parameter.\")\n\n        self._logger.info(f\"Reading group '{group_name}' with deduplication\")\n\n        # First, read the raw data\n        ds = self.read_group(group_name, branch, time_slice, chunks)\n\n        # Then deduplicate using metadata table intelligence\n        with self.readonly_session(branch) as session:\n            try:\n                zmeta = zarr.open_group(session.store, mode=\"r\")[\n                    f\"{group_name}/metadata/table\"\n                ]\n\n                # Load metadata and get latest entries for each time range\n                data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n                df = pl.DataFrame(data)\n\n                # Ensure datetime dtypes\n                df = df.with_columns(\n                    [\n                        pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n                        pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n                    ]\n                )\n\n                # Get latest entry for each unique (start, end) combination\n                latest_entries = df.sort(\"written_at\").unique(\n                    subset=[\"start\", \"end\"], keep=keep\n                )\n\n                if latest_entries.height &gt; 0:\n                    # Create time masks for latest data only\n                    time_masks = []\n                    for row in latest_entries.iter_rows(named=True):\n                        start_time = row[\"start\"]\n                        end_time = row[\"end\"]\n                        mask = (ds.epoch &gt;= start_time) &amp; (ds.epoch &lt;= end_time)\n                        time_masks.append(mask)\n\n                    # Combine all masks with OR logic\n                    if time_masks:\n                        combined_mask = time_masks[0]\n                        for mask in time_masks[1:]:\n                            combined_mask = combined_mask | mask\n                        ds = ds.isel(epoch=combined_mask)\n\n                        self._logger.info(\n                            \"Deduplicated using metadata table: kept \"\n                            f\"{len(latest_entries)} time ranges\"\n                        )\n\n            except Exception as e:\n                # Fall back to simple deduplication\n                self._logger.warning(\n                    f\"Metadata-based deduplication failed, using simple approach: {e}\"\n                )\n                ds = ds.drop_duplicates(\"epoch\", keep=\"last\")\n                self._logger.info(\"Applied simple epoch deduplication (keep='last')\")\n\n        return ds\n\n    def _cleanse_dataset_attrs(self, dataset: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"Remove any attributes that might interfere with Icechunk storage.\"\"\"\n\n        attrs_to_remove = [\n            \"Created\",\n            \"File Path\",\n            \"File Type\",\n            \"Date\",\n            \"institution\",\n            \"Time of First Observation\",\n            \"GLONASS COD\",\n            \"GLONASS PHS\",\n            \"GLONASS BIS\",\n            \"Leap Seconds\",\n            # \"RINEX File Hash\",\n        ]\n        for attr in attrs_to_remove:\n            if attr in dataset.attrs:\n                del dataset.attrs[attr]\n        return dataset\n\n    def write_dataset(\n        self,\n        dataset: xr.Dataset,\n        group_name: str,\n        session: Any,\n        mode: str = \"a\",\n        chunks: dict[str, int] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Write a dataset to Icechunk with proper chunking.\n\n        Parameters\n        ----------\n        dataset : xr.Dataset\n            Dataset to write\n        group_name : str\n            Group path in store\n        session : Any\n            Active writable session or store handle.\n        mode : str\n            Write mode: 'w' (overwrite) or 'a' (append)\n        chunks : dict[str, int] | None\n            Chunking spec. If None, uses store's chunk_strategy.\n            Example: {'epoch': 34560, 'sid': -1}\n        \"\"\"\n        # Use explicit chunks, or fall back to store's chunk strategy\n        if chunks is None:\n            chunks = self.chunk_strategy\n\n        # Apply chunking if strategy defined\n        if chunks:\n            dataset = dataset.chunk(chunks)\n            self._logger.info(f\"Rechunked to {dict(dataset.chunks)} before write\")\n\n        # Normalize encodings\n        dataset = self._normalize_encodings(dataset)\n\n        # Calculate dataset metrics for tracing\n        dataset_size_mb = dataset.nbytes / 1024 / 1024\n        num_variables = len(dataset.data_vars)\n\n        # Write to Icechunk with OpenTelemetry tracing\n        try:\n            from canvodpy.utils.telemetry import trace_icechunk_write\n\n            with trace_icechunk_write(\n                group_name=group_name,\n                dataset_size_mb=dataset_size_mb,\n                num_variables=num_variables,\n            ):\n                to_icechunk(dataset, session, group=group_name, mode=mode)\n        except ImportError:\n            # Fallback if telemetry not available\n            to_icechunk(dataset, session, group=group_name, mode=mode)\n\n        self._logger.info(f\"Wrote dataset to group '{group_name}' (mode={mode})\")\n\n    def write_initial_group(\n        self,\n        dataset: xr.Dataset,\n        group_name: str,\n        branch: str = \"main\",\n        commit_message: str | None = None,\n    ) -&gt; None:\n        \"\"\"Write initial data to a new group.\"\"\"\n        if self.group_exists(group_name, branch):\n            raise ValueError(\n                f\"Group '{group_name}' already exists. Use append_to_group() instead.\"\n            )\n\n        with self.writable_session(branch) as session:\n            dataset = self._normalize_encodings(dataset)\n\n            rinex_hash = dataset.attrs.get(\"RINEX File Hash\")\n            if rinex_hash is None:\n                raise ValueError(\"Dataset missing 'RINEX File Hash' attribute\")\n            start = dataset.epoch.min().values\n            end = dataset.epoch.max().values\n\n            to_icechunk(dataset, session, group=group_name, mode=\"w\")\n\n            if commit_message is None:\n                version = get_version_from_pyproject()\n                commit_message = f\"[v{version}] Initial commit to group '{group_name}'\"\n\n            snapshot_id = session.commit(commit_message)\n\n            self.append_metadata(\n                group_name=group_name,\n                rinex_hash=rinex_hash,\n                start=start,\n                end=end,\n                snapshot_id=snapshot_id,\n                action=\"write\",  # Correct action for initial data\n                commit_msg=commit_message,\n                dataset_attrs=dataset.attrs,\n            )\n\n        self._logger.info(\n            f\"Created group '{group_name}' with {len(dataset.epoch)} epochs, \"\n            f\"hash={rinex_hash}\"\n        )\n\n    def backup_metadata_table(\n        self,\n        group_name: str,\n        session: Any,\n    ) -&gt; pl.DataFrame | None:\n        \"\"\"Backup the metadata table to a Polars DataFrame.\n\n        Parameters\n        ----------\n        group_name : str\n            Group name.\n        session : Any\n            Active session for reading.\n\n        Returns\n        -------\n        pl.DataFrame | None\n            DataFrame with metadata rows, or None if missing.\n        \"\"\"\n        try:\n            zroot = zarr.open_group(session.store, mode=\"r\")\n            meta_group_path = f\"{group_name}/metadata/table\"\n\n            if (\n                \"metadata\" not in zroot[group_name]\n                or \"table\" not in zroot[group_name][\"metadata\"]\n            ):\n                self._logger.info(\n                    \"No metadata table found for group \"\n                    f\"'{group_name}' - nothing to backup\"\n                )\n                return None\n\n            zmeta = zroot[meta_group_path]\n\n            # Load all columns into a dictionary\n            data = {}\n            for col_name in zmeta.array_keys():\n                data[col_name] = zmeta[col_name][:]\n\n            # Convert to Polars DataFrame\n            df = pl.DataFrame(data)\n\n            self._logger.info(\n                \"Backed up metadata table with \"\n                f\"{df.height} rows for group '{group_name}'\"\n            )\n            return df\n\n        except Exception as e:\n            self._logger.warning(\n                f\"Failed to backup metadata table for group '{group_name}': {e}\"\n            )\n            return None\n\n    def restore_metadata_table(\n        self,\n        group_name: str,\n        df: pl.DataFrame,\n        session: Any,\n    ) -&gt; None:\n        \"\"\"Restore the metadata table from a Polars DataFrame.\n\n        This recreates the full Zarr structure for the metadata table.\n\n        Parameters\n        ----------\n        group_name : str\n            Group name.\n        df : pl.DataFrame\n            Metadata table to restore.\n        session : Any\n            Active session for writing.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if df is None or df.height == 0:\n            self._logger.info(f\"No metadata to restore for group '{group_name}'\")\n            return\n\n        try:\n            zroot = zarr.open_group(session.store, mode=\"a\")\n            meta_group_path = f\"{group_name}/metadata/table\"\n\n            # Create the metadata subgroup\n            zmeta = zroot.require_group(meta_group_path)\n\n            # Create all arrays from the DataFrame\n            for col_name in df.columns:\n                col_data = df[col_name]\n\n                if col_name == \"index\":\n                    # Index column as int64\n                    arr = col_data.to_numpy().astype(\"i8\")\n                    dtype = \"i8\"\n                elif col_name in (\"start\", \"end\"):\n                    # Datetime columns\n                    arr = col_data.to_numpy().astype(\"datetime64[ns]\")\n                    dtype = \"M8[ns]\"\n                else:\n                    # String columns - use VariableLengthUTF8\n                    arr = col_data.to_list()  # Convert to list for VariableLengthUTF8\n                    dtype = VariableLengthUTF8()\n\n                # Create the array\n                zmeta.create_array(\n                    name=col_name,\n                    shape=(len(arr),),\n                    dtype=dtype,\n                    chunks=(1024,),\n                    overwrite=True,\n                )\n\n                # Write the data\n                zmeta[col_name][:] = arr\n\n            self._logger.info(\n                \"Restored metadata table with \"\n                f\"{df.height} rows for group '{group_name}'\"\n            )\n\n        except Exception as e:\n            self._logger.error(\n                f\"Failed to restore metadata table for group '{group_name}': {e}\"\n            )\n            raise RuntimeError(f\"Critical error: could not restore metadata table: {e}\")\n\n    def overwrite_file_in_group(\n        self,\n        dataset: xr.Dataset,\n        group_name: str,\n        rinex_hash: str,\n        start: np.datetime64,\n        end: np.datetime64,\n        branch: str = \"main\",\n        commit_message: str | None = None,\n    ) -&gt; None:\n        \"\"\"Overwrite a file's contribution to the group (same hash, new epoch range).\"\"\"\n\n        dataset = self._normalize_encodings(dataset)\n\n        # --- Step 3: rewrite store ---\n        with self.writable_session(branch) as session:\n            ds_from_store = xr.open_zarr(\n                session.store, group=group_name, consolidated=False\n            )\n\n            # Backup the existing metadata table\n            metadata_backup = self.backup_metadata_table(group_name, session)\n\n            mask = (ds_from_store.epoch.values &lt; start) | (\n                ds_from_store.epoch.values &gt; end\n            )\n            ds_from_store_cleansed = ds_from_store.isel(epoch=mask)\n            ds_from_store_cleansed = self._normalize_encodings(ds_from_store_cleansed)\n\n            # Check if any epochs remain after cleansing, then write leftovers.\n            if ds_from_store_cleansed.sizes.get(\"epoch\", 0) &gt; 0:\n                to_icechunk(ds_from_store_cleansed, session, group=group_name, mode=\"w\")\n            # no epochs left, reset group to empty\n            else:\n                to_icechunk(dataset.isel(epoch=[]), session, group=group_name, mode=\"w\")\n\n            # write back the backed up metadata table\n            self.restore_metadata_table(group_name, metadata_backup, session)\n\n            # Append the new dataset\n            to_icechunk(dataset, session, group=group_name, append_dim=\"epoch\")\n\n            if commit_message is None:\n                version = get_version_from_pyproject()\n                commit_message = (\n                    f\"[v{version}] Overwrote file {rinex_hash} in group '{group_name}'\"\n                )\n\n            snapshot_id = session.commit(commit_message)\n\n            self.append_metadata(\n                group_name=group_name,\n                rinex_hash=rinex_hash,\n                start=start,\n                end=end,\n                snapshot_id=snapshot_id,\n                action=\"overwrite\",\n                commit_msg=commit_message,\n                dataset_attrs=dataset.attrs,\n            )\n\n    def get_group_info(self, group_name: str, branch: str = \"main\") -&gt; dict[str, Any]:\n        \"\"\"\n        Get metadata about a group.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group.\n        branch : str, default \"main\"\n            Repository branch to examine.\n\n        Returns\n        -------\n        dict[str, Any]\n            Group metadata.\n\n        Raises\n        ------\n        ValueError\n            If the group does not exist.\n        \"\"\"\n        if not self.group_exists(group_name, branch):\n            raise ValueError(f\"Group '{group_name}' does not exist\")\n\n        ds = self.read_group(group_name, branch)\n\n        info = {\n            \"group_name\": group_name,\n            \"store_type\": self.store_type,\n            \"dimensions\": dict(ds.sizes),\n            \"variables\": list(ds.data_vars.keys()),\n            \"coordinates\": list(ds.coords.keys()),\n            \"attributes\": dict(ds.attrs),\n        }\n\n        # Add temporal information if epoch dimension exists\n        if \"epoch\" in ds.sizes:\n            info[\"temporal_info\"] = {\n                \"start\": str(ds.epoch.min().values),\n                \"end\": str(ds.epoch.max().values),\n                \"count\": ds.sizes[\"epoch\"],\n                \"resolution\": str(ds.epoch.diff(\"epoch\").median().values),\n            }\n\n        return info\n\n    def rel_path_for_commit(self, file_path: Path) -&gt; str:\n        \"\"\"\n        Generate relative path for commit messages.\n\n        Parameters\n        ----------\n        file_path : Path\n            Full file path.\n\n        Returns\n        -------\n        str\n            Relative path string with log_path_depth parts.\n        \"\"\"\n        depth = load_config().processing.logging.log_path_depth\n        return str(Path(*file_path.parts[-depth:]))\n\n    def get_store_stats(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get statistics about the store.\n\n        Returns\n        -------\n        dict[str, Any]\n            Store statistics.\n        \"\"\"\n        groups = self.get_group_names()\n        stats = {\n            \"store_path\": str(self.store_path),\n            \"store_type\": self.store_type,\n            \"compression_level\": self.compression_level,\n            \"compression_algorithm\": self.compression_algorithm.name,\n            \"total_groups\": len(groups),\n            \"groups\": groups,\n        }\n\n        # Add group-specific stats\n        for group_name in groups:\n            try:\n                info = self.get_group_info(group_name)\n                stats[f\"group_{group_name}\"] = {\n                    \"dimensions\": info[\"dimensions\"],\n                    \"variables_count\": len(info[\"variables\"]),\n                    \"has_temporal_data\": \"temporal_info\" in info,\n                }\n            except Exception as e:\n                self._logger.warning(\n                    f\"Failed to get stats for group '{group_name}': {e}\"\n                )\n\n        return stats\n\n    def append_to_group(\n        self,\n        dataset: xr.Dataset,\n        group_name: str,\n        append_dim: str = \"epoch\",\n        branch: str = \"main\",\n        action: str = \"write\",\n        commit_message: str | None = None,\n    ) -&gt; None:\n        \"\"\"Append data to an existing group.\"\"\"\n        if not self.group_exists(group_name, branch):\n            raise ValueError(\n                f\"Group '{group_name}' does not exist. Use write_initial_group() first.\"\n            )\n\n        dataset = self._normalize_encodings(dataset)\n\n        rinex_hash = dataset.attrs.get(\"RINEX File Hash\")\n        if rinex_hash is None:\n            raise ValueError(\"Dataset missing 'RINEX File Hash' attribute\")\n        start = dataset.epoch.min().values\n        end = dataset.epoch.max().values\n\n        with self.writable_session(branch) as session:\n            to_icechunk(dataset, session, group=group_name, append_dim=append_dim)\n\n            if commit_message is None and action == \"write\":\n                version = get_version_from_pyproject()\n                commit_message = f\"[v{version}] Wrote to group '{group_name}'\"\n            elif commit_message is None and action != \"append\":\n                version = get_version_from_pyproject()\n                commit_message = f\"[v{version}] Appended to group '{group_name}'\"\n\n            snapshot_id = session.commit(commit_message)\n\n            self.append_metadata(\n                group_name=group_name,\n                rinex_hash=rinex_hash,\n                start=start,\n                end=end,\n                snapshot_id=snapshot_id,\n                action=action,\n                commit_msg=commit_message,\n                dataset_attrs=dataset.attrs,\n            )\n\n        if action == \"append\":\n            self._logger.info(\n                f\"Appended {len(dataset.epoch)} epochs to group '{group_name}', \"\n                f\"hash={rinex_hash}\"\n            )\n        elif action == \"write\":\n            self._logger.info(\n                f\"Wrote {len(dataset.epoch)} epochs to group '{group_name}', \"\n                f\"hash={rinex_hash}\"\n            )\n        else:\n            self._logger.info(\n                f\"Action '{action}' completed for group '{group_name}', \"\n                f\"hash={rinex_hash}\"\n            )\n\n    def append_metadata(\n        self,\n        group_name: str,\n        rinex_hash: str,\n        start: np.datetime64,\n        end: np.datetime64,\n        snapshot_id: str,\n        action: str,\n        commit_msg: str,\n        dataset_attrs: dict,\n        branch: str = \"main\",\n    ) -&gt; None:\n        \"\"\"\n        Append a metadata row into the group_name/metadata/table.\n\n        Schema:\n            index           int64 (continuous row id)\n            rinex_hash      str   (UTF-8, VariableLengthUTF8)\n            start           datetime64[ns]\n            end             datetime64[ns]\n            snapshot_id     str   (UTF-8)\n            action          str   (UTF-8, e.g. \"insert\"|\"append\"|\"overwrite\"|\"skip\")\n            commit_msg      str   (UTF-8)\n            written_at      str   (UTF-8, ISO8601 with timezone)\n            write_strategy  str   (UTF-8, RINEX_STORE_STRATEGY or VOD_STORE_STRATEGY)\n            attrs           str   (UTF-8, JSON dump of dataset attrs)\n        \"\"\"\n        written_at = datetime.now().astimezone().isoformat()\n\n        row = {\n            \"rinex_hash\": str(rinex_hash),\n            \"start\": np.datetime64(start, \"ns\"),\n            \"end\": np.datetime64(end, \"ns\"),\n            \"snapshot_id\": str(snapshot_id),\n            \"action\": str(action),\n            \"commit_msg\": str(commit_msg),\n            \"written_at\": written_at,\n            \"write_strategy\": str(self._rinex_store_strategy)\n            if self.store_type == \"rinex_store\"\n            else str(self._vod_store_strategy),\n            \"attrs\": json.dumps(dataset_attrs, default=str),\n        }\n        df_row = pl.DataFrame([row])\n\n        with self.writable_session(branch) as session:\n            zroot = zarr.open_group(session.store, mode=\"a\")\n            meta_group_path = f\"{group_name}/metadata/table\"\n\n            if (\n                \"metadata\" not in zroot[group_name]\n                or \"table\" not in zroot[group_name][\"metadata\"]\n            ):\n                # --- First time: create arrays with correct dtypes ---\n                zmeta = zroot.require_group(meta_group_path)\n\n                # index counter\n                zmeta.create_array(\n                    name=\"index\", shape=(0,), dtype=\"i8\", chunks=(1024,), overwrite=True\n                )\n                zmeta[\"index\"].append([0])\n\n                for col in df_row.columns:\n                    if col in (\"start\", \"end\"):\n                        dtype = \"M8[ns]\"\n                        arr = np.array(df_row[col].to_numpy(), dtype=dtype)\n                    else:\n                        dtype = VariableLengthUTF8()\n                        arr = df_row[col].to_list()\n\n                    zmeta.create_array(\n                        name=col,\n                        shape=(0,),\n                        dtype=dtype,\n                        chunks=(1024,),\n                        overwrite=True,\n                    )\n                    zmeta[col].append(arr)\n\n            else:\n                # --- Append to existing ---\n                zmeta = zroot[meta_group_path]\n\n                # index increment\n                current_len = zmeta[\"index\"].shape[0]\n                next_idx = current_len\n                zmeta[\"index\"].append([next_idx])\n\n                for col in df_row.columns:\n                    if col in (\"start\", \"end\"):\n                        arr = np.array(df_row[col].to_numpy(), dtype=\"M8[ns]\")\n                    else:\n                        arr = df_row[col].to_list()\n                    zmeta[col].append(arr)\n\n            session.commit(f\"Appended metadata row for {group_name}, hash={rinex_hash}\")\n\n        self._logger.info(\n            f\"Metadata appended for group '{group_name}': \"\n            f\"hash={rinex_hash}, snapshot={snapshot_id}, action={action}\"\n        )\n\n    def append_metadata_bulk(\n        self,\n        group_name: str,\n        rows: list[dict[str, Any]],\n        session: Optional[\"icechunk.WritableSession\"] = None,\n    ) -&gt; None:\n        \"\"\"\n        Append multiple metadata rows in one commit.\n\n        Parameters\n        ----------\n        group_name : str\n            Group name (e.g. \"canopy\", \"reference\")\n        rows : list[dict[str, Any]]\n            List of metadata records matching the schema used in\n            append_metadata().\n        session : icechunk.WritableSession, optional\n            If provided, rows are written into this session (caller commits later).\n            If None, this method opens its own writable session and commits once.\n        \"\"\"\n        if not rows:\n            self._logger.info(f\"No metadata rows to append for group '{group_name}'\")\n            return\n\n        # Ensure datetime conversions for consistency\n        for row in rows:\n            if isinstance(row.get(\"start\"), str):\n                row[\"start\"] = np.datetime64(row[\"start\"])\n            if isinstance(row.get(\"end\"), str):\n                row[\"end\"] = np.datetime64(row[\"end\"])\n            if \"written_at\" not in row:\n                row[\"written_at\"] = datetime.now(UTC).isoformat()\n\n        # Prepare the Polars DataFrame\n        df = pl.DataFrame(rows)\n\n        def _do_append(session_obj: \"icechunk.WritableSession\") -&gt; None:\n            \"\"\"Append metadata rows to a writable session.\n\n            Parameters\n            ----------\n            session_obj : icechunk.WritableSession\n                Writable session to update.\n\n            Returns\n            -------\n            None\n            \"\"\"\n            zroot = zarr.open_group(session_obj.store, mode=\"a\")\n            meta_group_path = f\"{group_name}/metadata/table\"\n            zmeta = zroot.require_group(meta_group_path)\n\n            start_index = 0\n            if \"index\" in zmeta:\n                existing_len = zmeta[\"index\"].shape[0]\n                start_index = (\n                    int(zmeta[\"index\"][-1].item()) + 1 if existing_len &gt; 0 else 0\n                )\n\n            # Assign sequential indices\n            df_with_index = df.with_columns(\n                (pl.arange(start_index, start_index + df.height)).alias(\"index\")\n            )\n\n            # Write each column\n            for col_name in df_with_index.columns:\n                col_data = df_with_index[col_name]\n\n                if col_name == \"index\":\n                    dtype = \"i8\"\n                    arr = col_data.to_numpy().astype(dtype)\n                elif col_name in (\"start\", \"end\"):\n                    dtype = \"M8[ns]\"\n                    arr = col_data.to_numpy().astype(dtype)\n                else:\n                    # strings / jsons / ids\n                    dtype = VariableLengthUTF8()\n                    arr = col_data.to_list()\n\n                if col_name not in zmeta:\n                    # Create array if it doesn't exist\n                    zmeta.create_array(\n                        name=col_name,\n                        shape=(0,),\n                        dtype=dtype,\n                        chunks=(1024,),\n                        overwrite=True,\n                    )\n\n                # Resize and append\n                old_len = zmeta[col_name].shape[0]\n                new_len = old_len + len(arr)\n                zmeta[col_name].resize(new_len)\n                zmeta[col_name][old_len:new_len] = arr\n\n            self._logger.info(\n                f\"Appended {df_with_index.height} metadata rows to group '{group_name}'\"\n            )\n\n        if session is not None:\n            _do_append(session)\n        else:\n            with self.writable_session() as sess:\n                _do_append(sess)\n                sess.commit(f\"Bulk metadata append for {group_name}\")\n\n    def load_metadata(self, store: Any, group_name: str) -&gt; pl.DataFrame:\n        \"\"\"Load metadata directly from Zarr into a Polars DataFrame.\n\n        Parameters\n        ----------\n        store : Any\n            Zarr store or session store handle.\n        group_name : str\n            Group name.\n\n        Returns\n        -------\n        pl.DataFrame\n            Metadata table.\n        \"\"\"\n        zroot = zarr.open_group(store, mode=\"r\")\n        zmeta = zroot[f\"{group_name}/metadata/table\"]\n\n        # Read all columns into a dict of numpy arrays\n        data = {col: zmeta[col][...] for col in zmeta.array_keys()}\n\n        # Build Polars DataFrame\n        df = pl.DataFrame(data)\n\n        # Convert numeric datetime64 columns back to proper Polars datetimes\n        if df[\"start\"].dtype in (pl.Int64, pl.Float64):\n            df = df.with_columns(pl.col(\"start\").cast(pl.Datetime(\"ns\")))\n        if df[\"end\"].dtype in (pl.Int64, pl.Float64):\n            df = df.with_columns(pl.col(\"end\").cast(pl.Datetime(\"ns\")))\n        if df[\"written_at\"].dtype == pl.Utf8:\n            df = df.with_columns(pl.col(\"written_at\").str.to_datetime(\"%+\"))\n        return df\n\n    def read_metadata_table(self, session: Any, group_name: str) -&gt; pl.DataFrame:\n        \"\"\"Read the metadata table from a session.\n\n        Parameters\n        ----------\n        session : Any\n            Active session for reading.\n        group_name : str\n            Group name.\n\n        Returns\n        -------\n        pl.DataFrame\n            Metadata table.\n        \"\"\"\n        zmeta = zarr.open_group(\n            session.store,\n            mode=\"r\",\n        )[f\"{group_name}/metadata/table\"]\n\n        data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n        df = pl.DataFrame(data)\n\n        # Ensure start/end are proper datetime\n        df = df.with_columns(\n            [\n                pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n                pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n            ]\n        )\n        return df\n\n    def metadata_row_exists(\n        self,\n        group_name: str,\n        rinex_hash: str,\n        start: np.datetime64,\n        end: np.datetime64,\n        branch: str = \"main\",\n    ) -&gt; tuple[bool, pl.DataFrame]:\n        \"\"\"\n        Check whether a (start, end) interval exists in group metadata.\n\n        Parameters\n        ----------\n        group_name : str\n            Icechunk group name.\n        rinex_hash : str\n            Hash of the current RINEX dataset.\n        start : np.datetime64\n            Start time for the interval.\n        end : np.datetime64\n            End time for the interval.\n        branch : str, default \"main\"\n            Branch name in the Icechunk repository.\n\n        Returns\n        -------\n        tuple[bool, pl.DataFrame]\n            Existence flag and the matching metadata rows.\n\n        Raises\n        ------\n        ValueError\n            If a conflicting hash is found for the same interval.\n\n        Notes\n        -----\n        The metadata table is cast to `Datetime(\"ns\")` for `start` and `end`\n        before filtering.\n        \"\"\"\n        with self.readonly_session(branch) as session:\n            try:\n                zmeta = zarr.open_group(session.store, mode=\"r\")[\n                    f\"{group_name}/metadata/table\"\n                ]\n            except Exception:\n                return False, pl.DataFrame()\n\n            # Load all arrays into a dict\n            data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n            df = pl.DataFrame(data)\n\n            # Ensure datetime dtypes\n            df = df.with_columns(\n                [\n                    pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n                    pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n                ]\n            )\n\n            # Step 1: filter by start+end\n            matches = df.filter(\n                (pl.col(\"start\") == np.datetime64(start, \"ns\"))\n                &amp; (pl.col(\"end\") == np.datetime64(end, \"ns\"))\n            )\n\n            if matches.is_empty():\n                return False, matches\n\n            # Step 2: check hash consistency\n            unique_hashes = matches.select(\"rinex_hash\").unique()\n\n            if (\n                unique_hashes.height &gt; 1\n                or unique_hashes.item(0, \"rinex_hash\") != rinex_hash\n            ):\n                existing_hashes = unique_hashes.to_series().to_list()\n                raise ValueError(\n                    \"Metadata conflict: rows with start=\"\n                    f\"{start}, end={end} exist but hash differs \"\n                    f\"(existing={existing_hashes}, new={rinex_hash})\"\n                )\n\n            return True, matches\n\n    def batch_check_existing(self, group_name: str, file_hashes: list[str]) -&gt; set[str]:\n        \"\"\"Check which file hashes already exist in metadata.\"\"\"\n\n        try:\n            with self.readonly_session(\"main\") as session:\n                df = self.load_metadata(session.store, group_name)\n\n                # Filter to matching hashes\n                existing = df.filter(pl.col(\"rinex_hash\").is_in(file_hashes))\n                return set(existing[\"rinex_hash\"].to_list())\n\n        except (KeyError, zarr.errors.GroupNotFoundError, Exception):\n            # Branch/group/metadata doesn't exist yet (fresh store)\n            return set()\n\n    def append_metadata_bulk_store(\n        self,\n        group_name: str,\n        rows: list[dict[str, Any]],\n        store: Any,\n    ) -&gt; None:\n        \"\"\"\n        Append metadata rows into an open transaction store.\n\n        Parameters\n        ----------\n        group_name : str\n            Group name (e.g. \"canopy\", \"reference\").\n        rows : list[dict[str, Any]]\n            Metadata rows to append.\n        store : Any\n            Open Icechunk transaction store.\n        \"\"\"\n        if not rows:\n            return\n\n        zroot = zarr.open_group(store, mode=\"a\")\n        zmeta = zroot.require_group(f\"{group_name}/metadata/table\")\n\n        # Find next index\n        start_index = 0\n        if \"index\" in zmeta:\n            start_index = (\n                int(zmeta[\"index\"][-1]) + 1 if zmeta[\"index\"].shape[0] &gt; 0 else 0\n            )\n\n        for i, row in enumerate(rows, start=start_index):\n            row[\"index\"] = i\n\n        import polars as pl\n\n        df = pl.DataFrame(rows)\n\n        for col in df.columns:\n            list_only_cols = {\n                \"attrs\",\n                \"commit_msg\",\n                \"action\",\n                \"write_strategy\",\n                \"rinex_hash\",\n                \"snapshot_id\",\n            }\n            if col in list_only_cols:\n                values = df[col].to_list()\n            else:\n                values = df[col].to_numpy()\n\n            if col == \"index\":\n                dtype = \"i8\"\n            elif col in (\"start\", \"end\"):\n                dtype = \"M8[ns]\"\n            else:\n                dtype = VariableLengthUTF8()\n\n            if col not in zmeta:\n                zmeta.create_array(\n                    name=col, shape=(0,), dtype=dtype, chunks=(1024,), overwrite=True\n                )\n\n            arr = zmeta[col]\n            old_len = arr.shape[0]\n            new_len = old_len + len(values)\n            arr.resize(new_len)\n            arr[old_len:new_len] = values\n\n        self._logger.info(f\"Appended {df.height} metadata rows to group '{group_name}'\")\n\n    def expire_old_snapshots(\n        self,\n        days: int | None = None,\n        branch: str = \"main\",\n        delete_expired_branches: bool = True,\n        delete_expired_tags: bool = True,\n    ) -&gt; set[str]:\n        \"\"\"\n        Expire and garbage-collect snapshots older than the given retention period.\n\n        Parameters\n        ----------\n        days : int | None, optional\n            Number of days to retain snapshots. Defaults to config value.\n        branch : str, default \"main\"\n            Branch to apply expiration on.\n        delete_expired_branches : bool, default True\n            Whether to delete branches pointing to expired snapshots.\n        delete_expired_tags : bool, default True\n            Whether to delete tags pointing to expired snapshots.\n\n        Returns\n        -------\n        set[str]\n            Expired snapshot IDs.\n        \"\"\"\n        if days is None:\n            days = self._rinex_store_expire_days\n        cutoff = datetime.now(UTC) - timedelta(days=days)\n\n        # cutoff = datetime(2025, 10, 3, 16, 44, 1, tzinfo=timezone.utc)\n        self._logger.info(\n            f\"Running expiration on store '{self.store_type}' \"\n            f\"(branch '{branch}') with cutoff {cutoff.isoformat()}\"\n        )\n\n        # Expire snapshots older than cutoff\n        expired_ids = self.repo.expire_snapshots(\n            older_than=cutoff,\n            delete_expired_branches=delete_expired_branches,\n            delete_expired_tags=delete_expired_tags,\n        )\n\n        if expired_ids:\n            self._logger.info(\n                f\"Expired {len(expired_ids)} snapshots: {sorted(expired_ids)}\"\n            )\n        else:\n            self._logger.info(\"No snapshots to expire.\")\n\n        # Garbage-collect expired objects to reclaim storage\n        summary = self.repo.garbage_collect(delete_object_older_than=cutoff)\n        self._logger.info(\n            f\"Garbage collection summary: \"\n            f\"deleted_bytes={summary.bytes_deleted}, \"\n            f\"deleted_chunks={summary.chunks_deleted}, \"\n            f\"deleted_manifests={summary.manifests_deleted}, \"\n            f\"deleted_snapshots={summary.snapshots_deleted}, \"\n            f\"deleted_attributes={summary.attributes_deleted}, \"\n            f\"deleted_transaction_logs={summary.transaction_logs_deleted}\"\n        )\n\n        return expired_ids\n\n    def get_history(self, branch: str = \"main\", limit: int | None = None) -&gt; list[dict]:\n        \"\"\"\n        Return commit ancestry (history) for a branch.\n\n        Parameters\n        ----------\n        branch : str, default \"main\"\n            Branch name.\n        limit : int | None, optional\n            Maximum number of commits to return.\n\n        Returns\n        -------\n        list[dict]\n            Commit info dictionaries (id, message, written_at, parent_ids).\n        \"\"\"\n        self._logger.info(f\"Fetching ancestry for branch '{branch}'\")\n\n        history = []\n        for i, ancestor in enumerate(self.repo.ancestry(branch=branch)):\n            history.append(\n                {\n                    \"snapshot_id\": ancestor.id,\n                    \"commit_msg\": ancestor.message,\n                    \"written_at\": ancestor.written_at,\n                    \"parent_ids\": ancestor.parent_id,\n                }\n            )\n            if limit is not None and i + 1 &gt;= limit:\n                break\n\n        return history\n\n    def print_history(self, branch: str = \"main\", limit: int | None = 100) -&gt; None:\n        \"\"\"\n        Pretty-print the ancestry for quick inspection.\n        \"\"\"\n        for entry in self.get_history(branch=branch, limit=limit):\n            ts = entry[\"written_at\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n            print(f\"{ts} {entry['snapshot_id'][:8]} {entry['commit_msg']}\")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the developer-facing representation.\n\n        Returns\n        -------\n        str\n            Representation string.\n        \"\"\"\n        return (\n            \"MyIcechunkStore(\"\n            f\"store_path={self.store_path}, store_type={self.store_type})\"\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable summary.\n\n        Returns\n        -------\n        str\n            Summary string.\n        \"\"\"\n\n        # Capture tree output\n        old_stdout = sys.stdout\n        sys.stdout = buffer = io.StringIO()\n\n        try:\n            self.print_tree()\n            tree_output = buffer.getvalue()\n        finally:\n            sys.stdout = old_stdout\n\n        branches = self.get_branch_names()\n        group_dict = self.get_group_names()\n        total_groups = sum(len(groups) for groups in group_dict.values())\n\n        return (\n            f\"MyIcechunkStore: {self.store_path}\\n\"\n            f\"Branches: {len(branches)} | Total Groups: {total_groups}\\n\\n\"\n            f\"{tree_output}\"\n        )\n\n    def rechunk_group(\n        self,\n        group_name: str,\n        chunks: dict[str, int],\n        source_branch: str = \"main\",\n        temp_branch: str | None = None,\n        promote_to_main: bool = True,\n        delete_temp_branch: bool = True,\n    ) -&gt; str:\n        \"\"\"\n        Rechunk a group with optimal chunk sizes.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group to rechunk\n        chunks : dict[str, int]\n            Chunking specification, e.g. {'epoch': 34560, 'sid': -1}\n        source_branch : str\n            Branch to read original data from (default: \"main\")\n        temp_branch : str | None\n            Temporary branch name for rechunked data. If None, uses\n            \"{group_name}_rechunked\".\n        promote_to_main : bool\n            If True, reset main branch to rechunked snapshot after writing\n        delete_temp_branch : bool\n            If True, delete temporary branch after promotion (only if\n            promote_to_main=True).\n\n        Returns\n        -------\n        str\n            Snapshot ID of the rechunked data\n        \"\"\"\n        if temp_branch is None:\n            temp_branch = f\"{group_name}_rechunked_temp\"\n\n        self._logger.info(\n            f\"Starting rechunk of group '{group_name}' with chunks={chunks}\"\n        )\n\n        # Get CURRENT snapshot from source branch to preserve all other groups\n        current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n\n        # Create temp branch from current snapshot (preserves all existing groups)\n        try:\n            self.repo.create_branch(temp_branch, current_snapshot)\n            self._logger.info(\n                f\"Created temporary branch '{temp_branch}' from current {source_branch}\"\n            )\n        except Exception as e:\n            self._logger.warning(f\"Branch '{temp_branch}' may already exist: {e}\")\n\n        # Read original data\n        ds_original = self.read_group(group_name, branch=source_branch)\n        self._logger.info(f\"Original chunks: {ds_original.chunks}\")\n\n        # Rechunk\n        ds_rechunked = ds_original.chunk(chunks)\n        self._logger.info(f\"New chunks: {ds_rechunked.chunks}\")\n\n        # Clear encoding to avoid conflicts\n        for var in ds_rechunked.data_vars:\n            ds_rechunked[var].encoding = {}\n\n        # Write rechunked data (overwrites only this group)\n        with self.writable_session(temp_branch) as session:\n            to_icechunk(ds_rechunked, session, group=group_name, mode=\"w\")\n            snapshot_id = session.commit(f\"Rechunked {group_name} with chunks={chunks}\")\n\n        self._logger.info(\n            f\"Rechunked data written to branch '{temp_branch}', snapshot={snapshot_id}\"\n        )\n\n        # Promote to main if requested\n        if promote_to_main:\n            rechunked_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n            self.repo.reset_branch(source_branch, rechunked_snapshot)\n            self._logger.info(\n                f\"Reset branch '{source_branch}' to rechunked snapshot \"\n                f\"{rechunked_snapshot}\"\n            )\n\n            # Delete temp branch if requested\n            if delete_temp_branch:\n                self.repo.delete_branch(temp_branch)\n                self._logger.info(f\"Deleted temporary branch '{temp_branch}'\")\n\n        return snapshot_id\n\n    def rechunk_group_verbose(\n        self,\n        group_name: str,\n        chunks: dict[str, int] | None = None,\n        source_branch: str = \"main\",\n        temp_branch: str | None = None,\n        promote_to_main: bool = True,\n        delete_temp_branch: bool = True,\n    ) -&gt; str:\n        \"\"\"\n        Rechunk a group with optimal chunk sizes.\n\n        Parameters\n        ----------\n        group_name : str\n            Name of the group to rechunk\n        chunks : dict[str, int] | None\n            Chunking specification, e.g. {'epoch': 34560, 'sid': -1}. Defaults\n            to `gnnsvodpy.globals.ICECHUNK_CHUNK_STRATEGIES`.\n        source_branch : str\n            Branch to read original data from (default: \"main\")\n        temp_branch : str | None\n            Temporary branch name for rechunked data. If None, uses\n            \"{group_name}_rechunked\".\n        promote_to_main : bool\n            If True, reset main branch to rechunked snapshot after writing\n        delete_temp_branch : bool\n            If True, delete temporary branch after promotion (only if\n            promote_to_main=True).\n\n        Returns\n        -------\n        str\n            Snapshot ID of the rechunked data\n        \"\"\"\n        if temp_branch is None:\n            temp_branch = f\"{group_name}_rechunked_temp\"\n\n        if chunks is None:\n            chunks = self.chunk_strategy or {\"epoch\": 34560, \"sid\": -1}\n\n        print(f\"\\n{'=' * 60}\")\n        print(f\"Starting rechunk of group '{group_name}'\")\n        print(f\"Target chunks: {chunks}\")\n        print(f\"{'=' * 60}\\n\")\n\n        self._logger.info(\n            f\"Starting rechunk of group '{group_name}' with chunks={chunks}\"\n        )\n\n        # Get CURRENT snapshot from source branch to preserve all other groups\n        print(f\"[1/7] Getting current snapshot from branch '{source_branch}'...\")\n        current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n        print(f\"      ✓ Current snapshot: {current_snapshot[:12]}\")\n\n        # Create temp branch from current snapshot (preserves all existing groups)\n        print(f\"\\n[2/7] Creating temporary branch '{temp_branch}'...\")\n        try:\n            self.repo.create_branch(temp_branch, current_snapshot)\n            print(f\"      ✓ Branch '{temp_branch}' created\")\n            self._logger.info(\n                f\"Created temporary branch '{temp_branch}' from current {source_branch}\"\n            )\n        except Exception as e:\n            print(\n                f\"      ⚠ Branch '{temp_branch}' already exists, using existing branch\"\n            )\n            self._logger.warning(f\"Branch '{temp_branch}' may already exist: {e}\")\n\n        # Read original data\n        print(f\"\\n[3/7] Reading original data from '{group_name}'...\")\n        ds_original = self.read_group(group_name, branch=source_branch)\n\n        # Unify chunks if inconsistent\n        try:\n            ds_original = ds_original.unify_chunks()\n            print(\"      ✓ Unified inconsistent chunks\")\n        except (TypeError, ValueError):\n            pass  # Chunks are already consistent\n\n        print(f\"      ✓ Data shape: {dict(ds_original.sizes)}\")\n        print(f\"      ✓ Original chunks: {ds_original.chunks}\")\n        self._logger.info(f\"Original chunks: {ds_original.chunks}\")\n\n        # Rechunk\n        print(\"\\n[4/7] Rechunking data...\")\n        ds_rechunked = ds_original.chunk(chunks)\n        ds_rechunked = ds_rechunked.unify_chunks()\n        print(f\"      ✓ New chunks: {ds_rechunked.chunks}\")\n        self._logger.info(f\"New chunks: {ds_rechunked.chunks}\")\n\n        # Clear encoding to avoid conflicts\n        for var in ds_rechunked.data_vars:\n            ds_rechunked[var].encoding = {}\n        for coord in ds_rechunked.coords:\n            if \"chunks\" in ds_rechunked[coord].encoding:\n                del ds_rechunked[coord].encoding[\"chunks\"]\n\n        # Write rechunked data first (overwrites entire group)\n        print(f\"\\n[5/7] Writing rechunked data to branch '{temp_branch}'...\")\n        print(\"      This may take several minutes for large datasets...\")\n        with self.writable_session(temp_branch) as session:\n            to_icechunk(ds_rechunked, session, group=group_name, mode=\"w\")\n            session.commit(f\"Wrote rechunked data for {group_name}\")\n        print(\"      ✓ Data written successfully\")\n\n        # Copy subgroups after writing rechunked data\n        print(f\"\\n[6/7] Copying subgroups from '{group_name}'...\")\n        with self.writable_session(temp_branch) as session:\n            with self.readonly_session(source_branch) as icsession:\n                source_group = zarr.open_group(icsession.store, mode=\"r\")[group_name]\n            target_group = zarr.open_group(session.store, mode=\"a\")[group_name]\n\n            subgroup_count = 0\n            for subgroup_name in source_group.group_keys():\n                print(f\"      ✓ Copying subgroup '{subgroup_name}'...\")\n                source_subgroup = source_group[subgroup_name]\n                target_subgroup = target_group.create_group(\n                    subgroup_name, overwrite=True\n                )\n\n                # Copy arrays from subgroup\n                for array_name in source_subgroup.array_keys():\n                    source_array = source_subgroup[array_name]\n                    target_array = target_subgroup.create_array(\n                        array_name,\n                        shape=source_array.shape,\n                        dtype=source_array.dtype,\n                        chunks=source_array.chunks,\n                        overwrite=True,\n                    )\n                    target_array[:] = source_array[:]\n\n                # Copy subgroup attributes\n                target_subgroup.attrs.update(source_subgroup.attrs)\n                subgroup_count += 1\n\n            if subgroup_count &gt; 0:\n                snapshot_id = session.commit(\n                    f\"Rechunked {group_name} with chunks={chunks}\"\n                )\n                print(f\"      ✓ {subgroup_count} subgroups copied\")\n            else:\n                snapshot_id = next(self.repo.ancestry(branch=temp_branch)).id\n                print(\"      ✓ No subgroups to copy\")\n\n        print(f\"      ✓ Snapshot ID: {snapshot_id[:12]}\")\n        self._logger.info(\n            f\"Rechunked data written to branch '{temp_branch}', snapshot={snapshot_id}\"\n        )\n\n        # Promote to main if requested\n        if promote_to_main:\n            print(f\"\\n[7/7] Promoting to '{source_branch}' branch...\")\n            rechunked_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n            self.repo.reset_branch(source_branch, rechunked_snapshot)\n            print(\n                f\"      ✓ Branch '{source_branch}' reset to {rechunked_snapshot[:12]}\"\n            )\n            self._logger.info(\n                f\"Reset branch '{source_branch}' to rechunked snapshot \"\n                f\"{rechunked_snapshot}\"\n            )\n\n            # Delete temp branch if requested\n            if delete_temp_branch:\n                print(f\"      ✓ Deleting temporary branch '{temp_branch}'...\")\n                self.delete_branch(temp_branch)\n                print(\"      ✓ Temporary branch deleted\")\n                self._logger.info(f\"Deleted temporary branch '{temp_branch}'\")\n        else:\n            print(\"\\n[7/7] Skipping promotion (promote_to_main=False)\")\n            print(f\"      Rechunked data available on branch '{temp_branch}'\")\n\n        print(f\"\\n{'=' * 60}\")\n        print(f\"✓ Rechunking complete for '{group_name}'\")\n        print(f\"{'=' * 60}\\n\")\n\n        return snapshot_id\n\n    def create_release_tag(self, tag_name: str, snapshot_id: str | None = None) -&gt; None:\n        \"\"\"\n        Create an immutable tag for an important version.\n\n        Parameters\n        ----------\n        tag_name : str\n            Name for the tag (e.g., \"v2024_complete\", \"before_reprocess\")\n        snapshot_id : str | None\n            Snapshot to tag. If None, uses current tip of main branch.\n        \"\"\"\n        if snapshot_id is None:\n            # Tag current main branch tip\n            snapshot_id = next(self.repo.ancestry(branch=\"main\")).id\n\n        self.repo.create_tag(tag_name, snapshot_id)\n        self._logger.info(f\"Created tag '{tag_name}' at snapshot {snapshot_id[:8]}\")\n\n    def list_tags(self) -&gt; list[str]:\n        \"\"\"List all tags in the repository.\"\"\"\n        return list(self.repo.list_tags())\n\n    def delete_tag(self, tag_name: str) -&gt; None:\n        \"\"\"Delete a tag (use with caution - tags are meant to be permanent).\"\"\"\n        self.repo.delete_tag(tag_name)\n        self._logger.warning(f\"Deleted tag '{tag_name}'\")\n\n    def plot_commit_graph(self, max_commits: int = 100) -&gt; \"Figure\":\n        \"\"\"\n        Visualize commit history as an interactive git-like graph.\n\n        Creates an interactive visualization showing:\n        - Branches with different colors\n        - Chronological commit ordering\n        - Branch divergence points\n        - Commit messages on hover\n        - Click to see commit details\n\n        Parameters\n        ----------\n        max_commits : int\n            Maximum number of commits to display (default: 100).\n\n        Returns\n        -------\n        Figure\n            Interactive plotly figure (works in marimo and Jupyter).\n        \"\"\"\n        from collections import defaultdict\n        from datetime import datetime\n\n        import plotly.graph_objects as go\n\n        # Collect all commits with full metadata\n        commit_map = {}  # id -&gt; commit data\n        branch_tips = {}  # branch -&gt; latest commit id\n\n        for branch in self.repo.list_branches():\n            ancestors = list(self.repo.ancestry(branch=branch))\n            if ancestors:\n                branch_tips[branch] = ancestors[0].id\n\n            for ancestor in ancestors:\n                if ancestor.id not in commit_map:\n                    commit_map[ancestor.id] = {\n                        \"id\": ancestor.id,\n                        \"parent_id\": ancestor.parent_id,\n                        \"message\": ancestor.message,\n                        \"written_at\": ancestor.written_at,\n                        \"branches\": [branch],\n                    }\n                else:\n                    # Multiple branches point to same commit\n                    commit_map[ancestor.id][\"branches\"].append(branch)\n\n                if len(commit_map) &gt;= max_commits:\n                    break\n            if len(commit_map) &gt;= max_commits:\n                break\n\n        # Build parent-child relationships\n        commits_list = list(commit_map.values())\n        commits_list.sort(key=lambda c: c[\"written_at\"])  # Oldest first\n\n        # Assign horizontal positions (chronological)\n        commit_x_positions = {}\n        for idx, commit in enumerate(commits_list):\n            commit[\"x\"] = idx\n            commit_x_positions[commit[\"id\"]] = idx\n\n        # Assign vertical positions: commits shared by branches stay on same Y\n        # Only diverge when branches have different commits\n        branch_names = sorted(\n            self.repo.list_branches(), key=lambda b: (b != \"main\", b)\n        )  # main first\n\n        # Build a set of all commit IDs for each branch\n        branch_commits = {}\n        for branch in branch_names:\n            history = list(self.repo.ancestry(branch=branch))\n            branch_commits[branch] = {h.id for h in history if h.id in commit_map}\n\n        # Find where branches diverge\n        def branches_share_commit(\n            commit_id: str,\n            branches: list[str],\n        ) -&gt; list[str]:\n            \"\"\"Return branches that contain a commit.\n\n            Parameters\n            ----------\n            commit_id : str\n                Commit identifier to check.\n            branches : list[str]\n                Branch names to search.\n\n            Returns\n            -------\n            list[str]\n                Branches that contain the commit.\n            \"\"\"\n            return [b for b in branches if commit_id in branch_commits[b]]\n\n        # Assign Y position: all commits on a single horizontal line initially\n        # We'll use vertical offset for parallel branch indicators\n        for commit in commits_list:\n            commit[\"y\"] = 0  # All on same timeline\n            commit[\"branch_set\"] = frozenset(commit[\"branches\"])\n\n        # Color palette for branches\n        colors = [\n            \"#4a9a4a\",  # green (main)\n            \"#5580c8\",  # blue\n            \"#d97643\",  # orange\n            \"#9b59b6\",  # purple\n            \"#e74c3c\",  # red\n            \"#1abc9c\",  # turquoise\n            \"#f39c12\",  # yellow\n            \"#34495e\",  # dark gray\n        ]\n        branch_colors = {b: colors[i % len(colors)] for i, b in enumerate(branch_names)}\n\n        # Build edges: draw parallel lines for shared commits (metro-style)\n        edges_by_branch = defaultdict(list)  # branch -&gt; list of edge dicts\n\n        for commit in commits_list:\n            if commit[\"parent_id\"] and commit[\"parent_id\"] in commit_map:\n                parent = commit_map[commit[\"parent_id\"]]\n\n                # Find which branches share both this commit and its parent\n                shared_branches = [\n                    b for b in commit[\"branches\"] if b in parent[\"branches\"]\n                ]\n\n                for branch in shared_branches:\n                    edges_by_branch[branch].append(\n                        {\n                            \"x0\": parent[\"x\"],\n                            \"y0\": parent[\"y\"],\n                            \"x1\": commit[\"x\"],\n                            \"y1\": commit[\"y\"],\n                        }\n                    )\n\n        # Create plotly figure\n        fig = go.Figure()\n\n        # Draw edges grouped by branch (parallel lines for shared paths)\n        for branch_idx, branch in enumerate(branch_names):\n            if branch not in edges_by_branch:\n                continue\n\n            color = branch_colors[branch]\n\n            # Draw each edge as a separate line\n            for edge in edges_by_branch[branch]:\n                # Vertical offset for parallel lines (metro-style)\n                offset = (branch_idx - (len(branch_names) - 1) / 2) * 0.15\n\n                fig.add_trace(\n                    go.Scatter(\n                        x=[edge[\"x0\"], edge[\"x1\"]],\n                        y=[edge[\"y0\"] + offset, edge[\"y1\"] + offset],\n                        mode=\"lines\",\n                        line=dict(color=color, width=3),\n                        hoverinfo=\"skip\",\n                        showlegend=False,\n                        opacity=0.7,\n                    )\n                )\n\n        # Draw commits (nodes) - one trace per unique commit\n        # Color by which branches include it\n        x_vals = [c[\"x\"] for c in commits_list]\n        y_vals = [c[\"y\"] for c in commits_list]\n\n        # Format hover text\n        hover_texts = []\n        marker_colors = []\n        marker_symbols = []\n\n        for c in commits_list:\n            # Handle both string and datetime objects\n            if isinstance(c[\"written_at\"], str):\n                time_str = datetime.fromisoformat(c[\"written_at\"]).strftime(\n                    \"%Y-%m-%d %H:%M\"\n                )\n            else:\n                time_str = c[\"written_at\"].strftime(\"%Y-%m-%d %H:%M\")\n\n            branches_str = \", \".join(c[\"branches\"])\n            hover_texts.append(\n                f\"&lt;b&gt;{c['message'] or 'No message'}&lt;/b&gt;&lt;br&gt;\"\n                f\"Commit: {c['id'][:12]}&lt;br&gt;\"\n                f\"Branches: {branches_str}&lt;br&gt;\"\n                f\"Time: {time_str}\"\n            )\n\n            # Color by first branch (priority: main)\n            if \"main\" in c[\"branches\"]:\n                marker_colors.append(branch_colors[\"main\"])\n            else:\n                marker_colors.append(branch_colors[c[\"branches\"][0]])\n\n            # Star for branch tips\n            if c[\"id\"] in branch_tips.values():\n                marker_symbols.append(\"star\")\n            else:\n                marker_symbols.append(\"circle\")\n\n        fig.add_trace(\n            go.Scatter(\n                x=x_vals,\n                y=y_vals,\n                mode=\"markers\",\n                name=\"Commits\",\n                marker=dict(\n                    size=14,\n                    color=marker_colors,\n                    symbol=marker_symbols,\n                    line=dict(color=\"white\", width=2),\n                ),\n                hovertext=hover_texts,\n                hoverinfo=\"text\",\n                showlegend=False,\n            )\n        )\n\n        # Add legend traces (invisible points just for legend)\n        for branch_idx, branch in enumerate(branch_names):\n            fig.add_trace(\n                go.Scatter(\n                    x=[None],\n                    y=[None],\n                    mode=\"markers\",\n                    name=branch,\n                    marker=dict(\n                        size=10,\n                        color=branch_colors[branch],\n                        line=dict(color=\"white\", width=2),\n                    ),\n                    showlegend=True,\n                )\n            )\n\n        # Layout styling\n        title_text = (\n            f\"Commit Graph: {self.site_name} ({len(commits_list)} commits, \"\n            f\"{len(branch_names)} branches)\"\n        )\n        fig.update_layout(\n            title=dict(\n                text=title_text,\n                font=dict(size=16, color=\"#e5e5e5\"),\n            ),\n            xaxis=dict(\n                title=\"Time (oldest ← → newest)\",\n                showticklabels=False,\n                showgrid=True,\n                gridcolor=\"rgba(255,255,255,0.1)\",\n                zeroline=False,\n            ),\n            yaxis=dict(\n                title=\"\",\n                showticklabels=False,\n                showgrid=False,\n                zeroline=False,\n                range=[-1, 1],  # Fixed range for single timeline\n            ),\n            plot_bgcolor=\"#1a1a1a\",\n            paper_bgcolor=\"#1a1a1a\",\n            font=dict(color=\"#e5e5e5\"),\n            hovermode=\"closest\",\n            height=400,\n            width=max(800, len(commits_list) * 50),\n            legend=dict(\n                title=\"Branches\",\n                orientation=\"h\",\n                x=0,\n                y=-0.15,\n                bgcolor=\"rgba(30,30,30,0.8)\",\n                bordercolor=\"rgba(255,255,255,0.2)\",\n                borderwidth=1,\n            ),\n        )\n\n        return fig\n\n    def cleanup_stale_branches(\n        self, keep_patterns: list[str] | None = None\n    ) -&gt; list[str]:\n        \"\"\"\n        Delete stale temporary branches (e.g., from failed rechunking).\n\n        Parameters\n        ----------\n        keep_patterns : list[str] | None\n            Patterns to preserve. Default: [\"main\", \"dev\"]\n\n        Returns\n        -------\n        list[str]\n            Names of deleted branches\n        \"\"\"\n        if keep_patterns is None:\n            keep_patterns = [\"main\", \"dev\"]\n\n        deleted = []\n\n        for branch in self.repo.list_branches():\n            # Keep if matches any pattern\n            should_keep = any(pattern in branch for pattern in keep_patterns)\n\n            if not should_keep:\n                # Check if it's a temp branch from rechunking\n                if \"_rechunked_temp\" in branch or \"_temp\" in branch:\n                    try:\n                        self.repo.delete_branch(branch)\n                        deleted.append(branch)\n                        self._logger.info(f\"Deleted stale branch: {branch}\")\n                    except Exception as e:\n                        self._logger.warning(f\"Failed to delete branch {branch}: {e}\")\n\n        return deleted\n\n    def delete_branch(self, branch_name: str) -&gt; None:\n        \"\"\"Delete a branch.\"\"\"\n        if branch_name == \"main\":\n            raise ValueError(\"Cannot delete 'main' branch\")\n\n        self.repo.delete_branch(branch_name)\n        self._logger.info(f\"Deleted branch '{branch_name}'\")\n\n    def get_snapshot_info(self, snapshot_id: str) -&gt; dict:\n        \"\"\"\n        Get detailed information about a specific snapshot.\n\n        Parameters\n        ----------\n        snapshot_id : str\n            Snapshot ID to inspect\n\n        Returns\n        -------\n        dict\n            Snapshot metadata and statistics\n        \"\"\"\n        # Find the snapshot in ancestry\n        for ancestor in self.repo.ancestry(branch=\"main\"):\n            if ancestor.id == snapshot_id or ancestor.id.startswith(snapshot_id):\n                info = {\n                    \"snapshot_id\": ancestor.id,\n                    \"message\": ancestor.message,\n                    \"written_at\": ancestor.written_at,\n                    \"parent_id\": ancestor.parent_id,\n                }\n\n                # Try to get groups at this snapshot\n                try:\n                    session = self.repo.readonly_session(snapshot_id=ancestor.id)\n                    root = zarr.open(session.store, mode=\"r\")\n                    info[\"groups\"] = list(root.group_keys())\n                    info[\"arrays\"] = list(root.array_keys())\n                except Exception as e:\n                    self._logger.warning(f\"Could not inspect snapshot contents: {e}\")\n\n                return info\n\n        raise ValueError(f\"Snapshot {snapshot_id} not found in history\")\n\n    def compare_snapshots(self, snapshot_id_1: str, snapshot_id_2: str) -&gt; dict:\n        \"\"\"\n        Compare two snapshots to see what changed.\n\n        Parameters\n        ----------\n        snapshot_id_1 : str\n            First snapshot (older)\n        snapshot_id_2 : str\n            Second snapshot (newer)\n\n        Returns\n        -------\n        dict\n            Comparison results showing added/removed/modified groups\n        \"\"\"\n        info_1 = self.get_snapshot_info(snapshot_id_1)\n        info_2 = self.get_snapshot_info(snapshot_id_2)\n\n        groups_1 = set(info_1.get(\"groups\", []))\n        groups_2 = set(info_2.get(\"groups\", []))\n\n        return {\n            \"snapshot_1\": snapshot_id_1[:8],\n            \"snapshot_2\": snapshot_id_2[:8],\n            \"added_groups\": list(groups_2 - groups_1),\n            \"removed_groups\": list(groups_1 - groups_2),\n            \"common_groups\": list(groups_1 &amp; groups_2),\n            \"time_diff\": (info_2[\"written_at\"] - info_1[\"written_at\"]).total_seconds(),\n        }\n\n    def maintenance(\n        self, expire_days: int = 7, cleanup_branches: bool = True, run_gc: bool = True\n    ) -&gt; dict:\n        \"\"\"\n        Run full maintenance on the store.\n\n        Parameters\n        ----------\n        expire_days : int\n            Days of snapshot history to keep\n        cleanup_branches : bool\n            Remove stale temporary branches\n        run_gc : bool\n            Run garbage collection after expiration\n\n        Returns\n        -------\n        dict\n            Summary of maintenance actions\n        \"\"\"\n        self._logger.info(f\"Starting maintenance on {self.store_type}\")\n\n        results = {\"expired_snapshots\": 0, \"deleted_branches\": [], \"gc_summary\": None}\n\n        # Expire old snapshots\n        expired_ids = self.expire_old_snapshots(days=expire_days)\n        results[\"expired_snapshots\"] = len(expired_ids)\n\n        # Cleanup stale branches\n        if cleanup_branches:\n            deleted_branches = self.cleanup_stale_branches()\n            results[\"deleted_branches\"] = deleted_branches\n\n        # Garbage collection\n        if run_gc:\n            from datetime import datetime, timedelta\n\n            cutoff = datetime.now(UTC) - timedelta(days=expire_days)\n            gc_summary = self.repo.garbage_collect(delete_object_older_than=cutoff)\n            results[\"gc_summary\"] = {\n                \"bytes_deleted\": gc_summary.bytes_deleted,\n                \"chunks_deleted\": gc_summary.chunks_deleted,\n                \"manifests_deleted\": gc_summary.manifests_deleted,\n            }\n\n        self._logger.info(f\"Maintenance complete: {results}\")\n        return results\n\n    def sanitize_store(\n        self,\n        source_branch: str = \"main\",\n        temp_branch: str = \"sanitize_temp\",\n        promote_to_main: bool = True,\n        delete_temp_branch: bool = True,\n    ) -&gt; str:\n        \"\"\"\n        Sanitize all groups by removing NaN-only SIDs and cleaning coordinates.\n\n        Creates a temporary branch, applies sanitization to all groups, then\n        optionally promotes to main and cleans up.\n\n        Parameters\n        ----------\n        source_branch : str, default \"main\"\n            Branch to read original data from.\n        temp_branch : str, default \"sanitize_temp\"\n            Temporary branch name for sanitized data.\n        promote_to_main : bool, default True\n            If True, reset main branch to sanitized snapshot after writing.\n        delete_temp_branch : bool, default True\n            If True, delete temporary branch after promotion.\n\n        Returns\n        -------\n        str\n            Snapshot ID of the sanitized data.\n        \"\"\"\n        import time\n\n        from icechunk.xarray import to_icechunk\n\n        print(f\"\\n{'=' * 60}\")\n        print(\"Starting store sanitization\")\n        print(f\"{'=' * 60}\\n\")\n\n        # Step 1: Get current snapshot\n        print(f\"[1/6] Getting current snapshot from '{source_branch}'...\")\n        current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n        print(f\"      ✓ Current snapshot: {current_snapshot[:12]}\")\n\n        # Step 2: Create temp branch\n        print(f\"\\n[2/6] Creating temporary branch '{temp_branch}'...\")\n        try:\n            self.repo.create_branch(temp_branch, current_snapshot)\n            print(f\"      ✓ Branch '{temp_branch}' created\")\n        except Exception:\n            print(\"      ⚠ Branch exists, deleting and recreating...\")\n            self.delete_branch(temp_branch)\n            self.repo.create_branch(temp_branch, current_snapshot)\n            print(f\"      ✓ Branch '{temp_branch}' created\")\n\n        # Step 3: Get all groups\n        print(\"\\n[3/6] Discovering groups...\")\n        groups = self.list_groups()\n        print(f\"      ✓ Found {len(groups)} groups: {groups}\")\n\n        # Step 4: Sanitize each group\n        print(\"\\n[4/6] Sanitizing groups...\")\n        sanitized_count = 0\n\n        for group_name in groups:\n            print(f\"\\n      Processing '{group_name}'...\")\n            t_start = time.time()\n\n            try:\n                # Read original data\n                ds_original = self.read_group(group_name, branch=source_branch)\n                original_sids = len(ds_original.sid)\n                print(f\"        • Original: {original_sids} SIDs\")\n\n                # Sanitize: remove SIDs with all-NaN data\n                ds_sanitized = self._sanitize_dataset(ds_original)\n                sanitized_sids = len(ds_sanitized.sid)\n                removed_sids = original_sids - sanitized_sids\n\n                print(\n                    f\"        • Sanitized: {sanitized_sids} SIDs \"\n                    f\"(removed {removed_sids})\"\n                )\n\n                # Write sanitized data\n                with self.writable_session(temp_branch) as session:\n                    to_icechunk(ds_sanitized, session, group=group_name, mode=\"w\")\n\n                    # Copy metadata subgroups if they exist\n                    try:\n                        with self.readonly_session(source_branch) as read_session:\n                            source_group = zarr.open_group(\n                                read_session.store, mode=\"r\"\n                            )[group_name]\n                            if \"metadata\" in source_group.group_keys():\n                                # Copy entire metadata subgroup\n                                dest_group = zarr.open_group(session.store)[group_name]\n                                zarr.copy(\n                                    source_group[\"metadata\"],\n                                    dest_group,\n                                    name=\"metadata\",\n                                )\n                                print(\"        • Copied metadata subgroup\")\n                    except Exception as e:\n                        print(f\"        ⚠ Could not copy metadata: {e}\")\n\n                    session.commit(\n                        f\"Sanitized {group_name}: removed {removed_sids} empty SIDs\"\n                    )\n\n                t_elapsed = time.time() - t_start\n                print(f\"        ✓ Completed in {t_elapsed:.2f}s\")\n                sanitized_count += 1\n\n            except Exception as e:\n                print(f\"        ✗ Failed: {e}\")\n                continue\n\n        print(f\"\\n      ✓ Sanitized {sanitized_count}/{len(groups)} groups\")\n\n        # Step 5: Get final snapshot\n        print(\"\\n[5/6] Getting sanitized snapshot...\")\n        sanitized_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n        print(f\"      ✓ Snapshot: {sanitized_snapshot[:12]}\")\n\n        # Step 6: Promote to main\n        if promote_to_main:\n            print(f\"\\n[6/6] Promoting to '{source_branch}' branch...\")\n            self.repo.reset_branch(source_branch, sanitized_snapshot)\n            print(\n                f\"      ✓ Branch '{source_branch}' reset to {sanitized_snapshot[:12]}\"\n            )\n\n            if delete_temp_branch:\n                print(f\"      ✓ Deleting temporary branch '{temp_branch}'...\")\n                self.delete_branch(temp_branch)\n                print(\"      ✓ Temporary branch deleted\")\n        else:\n            print(\"\\n[6/6] Skipping promotion (promote_to_main=False)\")\n            print(f\"      Sanitized data available on branch '{temp_branch}'\")\n\n        print(f\"\\n{'=' * 60}\")\n        print(\"✓ Sanitization complete\")\n        print(f\"{'=' * 60}\\n\")\n\n        return sanitized_snapshot\n\n    def _sanitize_dataset(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"\n        Remove SIDs that have all-NaN data and clean coordinate metadata.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset to sanitize\n\n        Returns\n        -------\n        xr.Dataset\n            Sanitized dataset with NaN-only SIDs removed\n        \"\"\"\n        # Find SIDs that have at least some non-NaN data across all variables\n        has_data = ds.to_array().notnull().any(dim=[\"variable\", \"epoch\"])\n\n        # Keep only SIDs with data\n        sids_with_data = ds.sid.values[has_data.values]\n        ds_clean = ds.sel(sid=sids_with_data)\n\n        # Clean coordinate metadata - remove NaN values from string coordinates\n        for coord in [\"band\", \"system\", \"code\", \"sv\"]:\n            if coord in ds_clean.coords:\n                coord_values = ds_clean[coord].values\n                # Convert object arrays, handling NaN\n                if coord_values.dtype == object:\n                    clean_values = []\n                    for val in coord_values:\n                        if isinstance(val, float) and np.isnan(val):\n                            clean_values.append(\"\")\n                        elif val is None or (isinstance(val, str) and val == \"nan\"):\n                            clean_values.append(\"\")\n                        else:\n                            clean_values.append(str(val))\n                    ds_clean = ds_clean.assign_coords({coord: (\"sid\", clean_values)})\n\n        # Numeric coordinates can keep NaN if needed\n        for coord in [\"freq_center\", \"freq_min\", \"freq_max\"]:\n            if coord in ds_clean.coords:\n                # These are fine as-is since they're numeric\n                pass\n\n        return ds_clean\n\n    def safe_temporal_aggregate(\n        self,\n        group: str,\n        freq: str = \"1D\",\n        vars_to_aggregate: Sequence[str] = (\"VOD\",),\n        geometry_vars: Sequence[str] = (\"phi\", \"theta\"),\n        drop_empty: bool = True,\n        branch: str = \"main\",\n    ) -&gt; xr.Dataset:\n        \"\"\"Safely aggregate temporally irregular VOD data.\n\n        Parameters\n        ----------\n        group : str\n            Group name to aggregate.\n        freq : str, default \"1D\"\n            Resample frequency string.\n        vars_to_aggregate : Sequence[str], optional\n            Variables to aggregate using mean.\n        geometry_vars : Sequence[str], optional\n            Geometry variables to preserve via first() per bin.\n        drop_empty : bool, default True\n            Drop empty epochs after aggregation.\n        branch : str, default \"main\"\n            Branch name to read from.\n\n        Returns\n        -------\n        xr.Dataset\n            Aggregated dataset.\n        \"\"\"\n\n        with self.readonly_session(branch=branch) as session:\n            ds = xr.open_zarr(session.store, group=group, consolidated=False)\n\n            print(\n                f\"📦 Aggregating group '{group}' from branch '{branch}' → freq={freq}\"\n            )\n\n            # 1️⃣ Aggregate numeric variables\n            merged_vars = []\n            for var in vars_to_aggregate:\n                if var in ds:\n                    merged_vars.append(ds[var].resample(epoch=freq).mean())\n                else:\n                    print(f\"⚠️ Skipping missing variable: {var}\")\n            ds_agg = xr.merge(merged_vars)\n\n            # 2️⃣ Preserve geometry variables (use first() per bin)\n            for var in geometry_vars:\n                if var in ds:\n                    ds_agg[var] = ds[var].resample(epoch=freq).first()\n\n            # 3️⃣ Add remaining coordinates\n            for coord in ds.coords:\n                if coord not in ds_agg.coords and coord != \"epoch\":\n                    ds_agg[coord] = ds[coord]\n\n            # 4️⃣ Drop all-NaN epochs if requested\n            if drop_empty and \"VOD\" in ds_agg:\n                valid_mask = ds_agg[\"VOD\"].notnull().any(dim=\"sid\").compute()\n                ds_agg = ds_agg.isel(epoch=valid_mask)\n\n            print(f\"✅ Aggregation done: {dict(ds_agg.sizes)}\")\n            return ds_agg\n\n    def safe_temporal_aggregate_to_branch(\n        self,\n        source_group: str,\n        target_group: str,\n        target_branch: str,\n        freq: str = \"1D\",\n        overwrite: bool = False,\n        **kwargs: Any,\n    ) -&gt; xr.Dataset:\n        \"\"\"Aggregate a group and save to a new Icechunk branch/group.\n\n        Parameters\n        ----------\n        source_group : str\n            Source group name.\n        target_group : str\n            Target group name.\n        target_branch : str\n            Target branch name.\n        freq : str, default \"1D\"\n            Resample frequency string.\n        overwrite : bool, default False\n            Whether to overwrite an existing branch.\n        **kwargs : Any\n            Additional keyword args passed to safe_temporal_aggregate().\n\n        Returns\n        -------\n        xr.Dataset\n            Aggregated dataset written to the target branch.\n        \"\"\"\n\n        print(\n            f\"🚀 Creating new aggregated branch '{target_branch}' at '{target_group}'\"\n        )\n\n        # Compute safe aggregation\n        ds_agg = self.safe_temporal_aggregate(\n            group=source_group,\n            freq=freq,\n            **kwargs,\n        )\n\n        # Write to new branch\n        current_snapshot = next(self.repo.ancestry(branch=\"main\")).id\n        self.delete_branch(target_branch)\n        self.repo.create_branch(target_branch, current_snapshot)\n        with self.writable_session(target_branch) as session:\n            to_icechunk(\n                obj=ds_agg,\n                session=session,\n                group=target_group,\n                mode=\"w\",\n            )\n            session.commit(f\"Saved aggregated data to {target_group} at freq={freq}\")\n\n        print(\n            f\"✅ Saved aggregated dataset to branch '{target_branch}' \"\n            f\"(group '{target_group}')\"\n        )\n        return ds_agg\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.repo","level":3,"title":"<code>repo</code>  <code>property</code>","text":"<p>Get the repository instance.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.tree","level":3,"title":"<code>tree</code>  <code>property</code>","text":"<p>Display hierarchical tree of all branches, groups, and subgroups.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.__init__","level":3,"title":"<code>__init__(store_path, store_type='rinex_store', compression_level=None, compression_algorithm=None)</code>","text":"<p>Initialize the Icechunk store manager.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.__init__--parameters","level":5,"title":"Parameters","text":"<p>store_path : Path     Path to the Icechunk store directory. store_type : str, default \"rinex_store\"     Type of store (\"rinex_store\" or \"vod_store\"). compression_level : int | None, optional     Override default compression level. compression_algorithm : str | None, optional     Override default compression algorithm.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def __init__(\n    self,\n    store_path: Path,\n    store_type: str = \"rinex_store\",\n    compression_level: int | None = None,\n    compression_algorithm: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Icechunk store manager.\n\n    Parameters\n    ----------\n    store_path : Path\n        Path to the Icechunk store directory.\n    store_type : str, default \"rinex_store\"\n        Type of store (\"rinex_store\" or \"vod_store\").\n    compression_level : int | None, optional\n        Override default compression level.\n    compression_algorithm : str | None, optional\n        Override default compression algorithm.\n    \"\"\"\n    from canvod.utils.config import load_config\n\n    cfg = load_config()\n    ic_cfg = cfg.processing.icechunk\n    st_cfg = cfg.processing.storage\n\n    self.store_path = Path(store_path)\n    self.store_type = store_type\n    # Site name is parent directory name\n    self.site_name = self.store_path.parent.name\n\n    # Compression\n    self.compression_level = compression_level or ic_cfg.compression_level\n    compression_alg = compression_algorithm or ic_cfg.compression_algorithm\n    self.compression_algorithm = getattr(\n        icechunk.CompressionAlgorithm, compression_alg.capitalize()\n    )\n\n    # Chunk strategy\n    chunk_strategies = {\n        k: {\"epoch\": v.epoch, \"sid\": v.sid}\n        for k, v in ic_cfg.chunk_strategies.items()\n    }\n    self.chunk_strategy = chunk_strategies.get(store_type, {})\n\n    # Storage config cached for metadata rows\n    self._rinex_store_strategy = st_cfg.rinex_store_strategy\n    self._rinex_store_expire_days = st_cfg.rinex_store_expire_days\n    self._vod_store_strategy = st_cfg.vod_store_strategy\n\n    # Configure repository\n    self.config = icechunk.RepositoryConfig.default()\n    self.config.compression = icechunk.CompressionConfig(\n        level=self.compression_level, algorithm=self.compression_algorithm\n    )\n    self.config.inline_chunk_threshold_bytes = ic_cfg.inline_threshold\n    self.config.get_partial_values_concurrency = ic_cfg.get_concurrency\n\n    if ic_cfg.manifest_preload_enabled:\n        self.config.manifest = icechunk.ManifestConfig(\n            preload=icechunk.ManifestPreloadConfig(\n                max_total_refs=ic_cfg.manifest_preload_max_refs,\n                preload_if=icechunk.ManifestPreloadCondition.name_matches(\n                    ic_cfg.manifest_preload_pattern\n                ),\n            )\n        )\n        self._logger.info(\n            f\"Manifest preload enabled: {ic_cfg.manifest_preload_pattern}\"\n        )\n\n    self._repo = None\n    self._logger = get_logger(__name__)\n\n    # Remove .DS_Store files that corrupt icechunk ref listing on macOS\n    self._clean_ds_store()\n    self._ensure_store_exists()\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.readonly_session","level":3,"title":"<code>readonly_session(branch='main')</code>","text":"<p>Context manager for readonly sessions.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.readonly_session--parameters","level":5,"title":"Parameters","text":"<p>branch : str, default \"main\"     Branch name.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.readonly_session--returns","level":5,"title":"Returns","text":"<p>Generator[icechunk.ReadonlySession, None, None]     Readonly session context manager.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>@contextlib.contextmanager\ndef readonly_session(\n    self,\n    branch: str = \"main\",\n) -&gt; Generator[\"icechunk.ReadonlySession\"]:\n    \"\"\"Context manager for readonly sessions.\n\n    Parameters\n    ----------\n    branch : str, default \"main\"\n        Branch name.\n\n    Returns\n    -------\n    Generator[icechunk.ReadonlySession, None, None]\n        Readonly session context manager.\n    \"\"\"\n    session = self.repo.readonly_session(branch)\n    try:\n        self._logger.debug(f\"Opened readonly session for branch '{branch}'\")\n        yield session\n    finally:\n        self._logger.debug(f\"Closed readonly session for branch '{branch}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.writable_session","level":3,"title":"<code>writable_session(branch='main')</code>","text":"<p>Context manager for writable sessions.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.writable_session--parameters","level":5,"title":"Parameters","text":"<p>branch : str, default \"main\"     Branch name.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.writable_session--returns","level":5,"title":"Returns","text":"<p>Generator[icechunk.WritableSession, None, None]     Writable session context manager.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>@contextlib.contextmanager\ndef writable_session(\n    self,\n    branch: str = \"main\",\n) -&gt; Generator[\"icechunk.WritableSession\"]:\n    \"\"\"Context manager for writable sessions.\n\n    Parameters\n    ----------\n    branch : str, default \"main\"\n        Branch name.\n\n    Returns\n    -------\n    Generator[icechunk.WritableSession, None, None]\n        Writable session context manager.\n    \"\"\"\n    session = self.repo.writable_session(branch)\n    try:\n        self._logger.debug(f\"Opened writable session for branch '{branch}'\")\n        yield session\n    finally:\n        self._logger.debug(f\"Closed writable session for branch '{branch}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_branch_names","level":3,"title":"<code>get_branch_names()</code>","text":"<p>List all branches in the store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_branch_names--returns","level":5,"title":"Returns","text":"<p>list[str]     List of branch names.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_branch_names(self) -&gt; list[str]:\n    \"\"\"\n    List all branches in the store.\n\n    Returns\n    -------\n    list[str]\n        List of branch names.\n    \"\"\"\n    try:\n        storage_config = icechunk.local_filesystem_storage(self.store_path)\n        repo = icechunk.Repository.open(\n            storage=storage_config,\n        )\n\n        return list(repo.list_branches())\n    except Exception as e:\n        self._logger.warning(f\"Failed to list branches in {repr(self)}: {e}\")\n        warnings.warn(f\"Failed to list branches in {repr(self)}: {e}\")\n        return []\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_group_names","level":3,"title":"<code>get_group_names(branch=None)</code>","text":"<p>List all groups in the store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_group_names--parameters","level":5,"title":"Parameters","text":"<p>branch: Optional[str]     Repository branch to examine. Defaults to listing groups from all branches.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_group_names--returns","level":5,"title":"Returns","text":"<p>dict[str, list[str]]     Dictionary mapping branch names to lists of group names.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_group_names(self, branch: str | None = None) -&gt; dict[str, list[str]]:\n    \"\"\"\n    List all groups in the store.\n\n    Parameters\n    ----------\n    branch: Optional[str]\n        Repository branch to examine. Defaults to listing groups from all branches.\n\n    Returns\n    -------\n    dict[str, list[str]]\n        Dictionary mapping branch names to lists of group names.\n\n    \"\"\"\n    try:\n        if not branch:\n            branches = self.get_branch_names()\n        else:\n            branches = [branch]\n\n        storage_config = icechunk.local_filesystem_storage(self.store_path)\n        repo = icechunk.Repository.open(\n            storage=storage_config,\n        )\n\n        group_dict = {}\n        for br in branches:\n            with self.readonly_session(br) as session:\n                session = repo.readonly_session(br)\n                root = zarr.open(session.store, mode=\"r\")\n                group_dict[br] = list(root.group_keys())\n\n        return group_dict\n\n    except Exception as e:\n        self._logger.warning(f\"Failed to list groups in {repr(self)}: {e}\")\n        return {}\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.list_groups","level":3,"title":"<code>list_groups(branch='main')</code>","text":"<p>List all groups in a branch.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.list_groups--parameters","level":5,"title":"Parameters","text":"<p>branch : str     Branch name (default: \"main\")</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.list_groups--returns","level":5,"title":"Returns","text":"<p>list[str]     List of group names in the branch</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def list_groups(self, branch: str = \"main\") -&gt; list[str]:\n    \"\"\"\n    List all groups in a branch.\n\n    Parameters\n    ----------\n    branch : str\n        Branch name (default: \"main\")\n\n    Returns\n    -------\n    list[str]\n        List of group names in the branch\n    \"\"\"\n    group_dict = self.get_group_names(branch=branch)\n    if branch in group_dict:\n        return group_dict[branch]\n    return []\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.print_tree","level":3,"title":"<code>print_tree(max_depth=None)</code>","text":"<p>Display hierarchical tree of all branches, groups, and subgroups.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.print_tree--parameters","level":5,"title":"Parameters","text":"<p>max_depth : int | None     Maximum depth to display. None for unlimited depth.     - 0: Only show branches     - 1: Show branches and top-level groups     - 2: Show branches, groups, and first level of subgroups/arrays     - etc.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def print_tree(self, max_depth: int | None = None) -&gt; None:\n    \"\"\"\n    Display hierarchical tree of all branches, groups, and subgroups.\n\n    Parameters\n    ----------\n    max_depth : int | None\n        Maximum depth to display. None for unlimited depth.\n        - 0: Only show branches\n        - 1: Show branches and top-level groups\n        - 2: Show branches, groups, and first level of subgroups/arrays\n        - etc.\n    \"\"\"\n    try:\n        branches = self.get_branch_names()\n\n        for i, branch in enumerate(branches):\n            is_last_branch = i == len(branches) - 1\n            branch_prefix = \"└── \" if is_last_branch else \"├── \"\n\n            if max_depth is not None and max_depth &lt; 1:\n                continue\n\n            session = self.repo.readonly_session(branch)\n            root = zarr.open(session.store, mode=\"r\")\n\n            if i == 0:\n                sys.stdout.write(f\"{self.store_path}\\n\")\n\n            sys.stdout.write(f\"{branch_prefix}{branch}\\n\")\n            # Build tree recursively\n            branch_indent = \"    \" if is_last_branch else \"│   \"\n            self._build_tree(root, branch_indent, max_depth, current_depth=1)\n\n    except Exception as e:\n        self._logger.warning(f\"Failed to generate tree for {repr(self)}: {e}\")\n        sys.stdout.write(f\"Error generating tree: {e}\\n\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.group_exists","level":3,"title":"<code>group_exists(group_name, branch='main')</code>","text":"<p>Check if a group exists.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.group_exists--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group to check. branch : str, default \"main\"     Repository branch to examine.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.group_exists--returns","level":5,"title":"Returns","text":"<p>bool     True if the group exists, False otherwise.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def group_exists(self, group_name: str, branch: str = \"main\") -&gt; bool:\n    \"\"\"\n    Check if a group exists.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group to check.\n    branch : str, default \"main\"\n        Repository branch to examine.\n\n    Returns\n    -------\n    bool\n        True if the group exists, False otherwise.\n    \"\"\"\n    group_dict = self.get_group_names(branch)\n\n    # get_group_names returns dict like {'main': ['canopy_01', ...]}\n    if branch in group_dict:\n        exists = group_name in group_dict[branch]\n    else:\n        exists = False\n\n    self._logger.debug(\n        f\"Group '{group_name}' exists on branch '{branch}': {exists}\"\n    )\n    return exists\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.read_group","level":3,"title":"<code>read_group(group_name, branch='main', time_slice=None, chunks=None)</code>","text":"<p>Read data from a group.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.read_group--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group to read. branch : str, default \"main\"     Repository branch. time_slice : slice | None, optional     Optional time slice for filtering. chunks : dict[str, Any] | None, optional     Chunking specification (uses config defaults if None).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.read_group--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset from the group.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def read_group(\n    self,\n    group_name: str,\n    branch: str = \"main\",\n    time_slice: slice | None = None,\n    chunks: dict[str, Any] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read data from a group.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group to read.\n    branch : str, default \"main\"\n        Repository branch.\n    time_slice : slice | None, optional\n        Optional time slice for filtering.\n    chunks : dict[str, Any] | None, optional\n        Chunking specification (uses config defaults if None).\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset from the group.\n    \"\"\"\n    self._logger.info(f\"Reading group '{group_name}' from branch '{branch}'\")\n\n    with self.readonly_session(branch) as session:\n        # Use default chunking strategy if none provided\n        if chunks is None:\n            chunks = self.chunk_strategy or {\"epoch\": 34560, \"sid\": -1}\n\n        ds = xr.open_zarr(\n            session.store,\n            group=group_name,\n            chunks=chunks,\n            consolidated=False,\n        )\n\n        if time_slice is not None:\n            ds = ds.isel(epoch=time_slice)\n            self._logger.debug(f\"Applied time slice: {time_slice}\")\n\n        self._logger.info(\n            f\"Successfully read group '{group_name}' - shape: {dict(ds.sizes)}\"\n        )\n        return ds\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.read_group_deduplicated","level":3,"title":"<code>read_group_deduplicated(group_name, branch='main', keep='last', time_slice=None, chunks=None)</code>","text":"<p>Read data from a group with automatic deduplication.</p> <p>This method calls read_group() then removes duplicates using metadata table intelligence when available, falling back to simple epoch deduplication.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.read_group_deduplicated--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group to read. branch : str, default \"main\"     Repository branch. keep : str, default \"last\"     Deduplication strategy for duplicate epochs. time_slice : slice | None, optional     Optional time slice for filtering. chunks : dict[str, Any] | None, optional     Chunking specification (uses config defaults if None).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.read_group_deduplicated--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with duplicates removed (latest data only).</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def read_group_deduplicated(\n    self,\n    group_name: str,\n    branch: str = \"main\",\n    keep: str = \"last\",\n    time_slice: slice | None = None,\n    chunks: dict[str, Any] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read data from a group with automatic deduplication.\n\n    This method calls read_group() then removes duplicates using metadata table\n    intelligence when available, falling back to simple epoch deduplication.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group to read.\n    branch : str, default \"main\"\n        Repository branch.\n    keep : str, default \"last\"\n        Deduplication strategy for duplicate epochs.\n    time_slice : slice | None, optional\n        Optional time slice for filtering.\n    chunks : dict[str, Any] | None, optional\n        Chunking specification (uses config defaults if None).\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with duplicates removed (latest data only).\n    \"\"\"\n\n    if keep not in [\"last\"]:\n        raise ValueError(\"Currently only 'last' is supported for keep parameter.\")\n\n    self._logger.info(f\"Reading group '{group_name}' with deduplication\")\n\n    # First, read the raw data\n    ds = self.read_group(group_name, branch, time_slice, chunks)\n\n    # Then deduplicate using metadata table intelligence\n    with self.readonly_session(branch) as session:\n        try:\n            zmeta = zarr.open_group(session.store, mode=\"r\")[\n                f\"{group_name}/metadata/table\"\n            ]\n\n            # Load metadata and get latest entries for each time range\n            data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n            df = pl.DataFrame(data)\n\n            # Ensure datetime dtypes\n            df = df.with_columns(\n                [\n                    pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n                    pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n                ]\n            )\n\n            # Get latest entry for each unique (start, end) combination\n            latest_entries = df.sort(\"written_at\").unique(\n                subset=[\"start\", \"end\"], keep=keep\n            )\n\n            if latest_entries.height &gt; 0:\n                # Create time masks for latest data only\n                time_masks = []\n                for row in latest_entries.iter_rows(named=True):\n                    start_time = row[\"start\"]\n                    end_time = row[\"end\"]\n                    mask = (ds.epoch &gt;= start_time) &amp; (ds.epoch &lt;= end_time)\n                    time_masks.append(mask)\n\n                # Combine all masks with OR logic\n                if time_masks:\n                    combined_mask = time_masks[0]\n                    for mask in time_masks[1:]:\n                        combined_mask = combined_mask | mask\n                    ds = ds.isel(epoch=combined_mask)\n\n                    self._logger.info(\n                        \"Deduplicated using metadata table: kept \"\n                        f\"{len(latest_entries)} time ranges\"\n                    )\n\n        except Exception as e:\n            # Fall back to simple deduplication\n            self._logger.warning(\n                f\"Metadata-based deduplication failed, using simple approach: {e}\"\n            )\n            ds = ds.drop_duplicates(\"epoch\", keep=\"last\")\n            self._logger.info(\"Applied simple epoch deduplication (keep='last')\")\n\n    return ds\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.write_dataset","level":3,"title":"<code>write_dataset(dataset, group_name, session, mode='a', chunks=None)</code>","text":"<p>Write a dataset to Icechunk with proper chunking.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.write_dataset--parameters","level":5,"title":"Parameters","text":"<p>dataset : xr.Dataset     Dataset to write group_name : str     Group path in store session : Any     Active writable session or store handle. mode : str     Write mode: 'w' (overwrite) or 'a' (append) chunks : dict[str, int] | None     Chunking spec. If None, uses store's chunk_strategy.     Example: {'epoch': 34560, 'sid': -1}</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def write_dataset(\n    self,\n    dataset: xr.Dataset,\n    group_name: str,\n    session: Any,\n    mode: str = \"a\",\n    chunks: dict[str, int] | None = None,\n) -&gt; None:\n    \"\"\"\n    Write a dataset to Icechunk with proper chunking.\n\n    Parameters\n    ----------\n    dataset : xr.Dataset\n        Dataset to write\n    group_name : str\n        Group path in store\n    session : Any\n        Active writable session or store handle.\n    mode : str\n        Write mode: 'w' (overwrite) or 'a' (append)\n    chunks : dict[str, int] | None\n        Chunking spec. If None, uses store's chunk_strategy.\n        Example: {'epoch': 34560, 'sid': -1}\n    \"\"\"\n    # Use explicit chunks, or fall back to store's chunk strategy\n    if chunks is None:\n        chunks = self.chunk_strategy\n\n    # Apply chunking if strategy defined\n    if chunks:\n        dataset = dataset.chunk(chunks)\n        self._logger.info(f\"Rechunked to {dict(dataset.chunks)} before write\")\n\n    # Normalize encodings\n    dataset = self._normalize_encodings(dataset)\n\n    # Calculate dataset metrics for tracing\n    dataset_size_mb = dataset.nbytes / 1024 / 1024\n    num_variables = len(dataset.data_vars)\n\n    # Write to Icechunk with OpenTelemetry tracing\n    try:\n        from canvodpy.utils.telemetry import trace_icechunk_write\n\n        with trace_icechunk_write(\n            group_name=group_name,\n            dataset_size_mb=dataset_size_mb,\n            num_variables=num_variables,\n        ):\n            to_icechunk(dataset, session, group=group_name, mode=mode)\n    except ImportError:\n        # Fallback if telemetry not available\n        to_icechunk(dataset, session, group=group_name, mode=mode)\n\n    self._logger.info(f\"Wrote dataset to group '{group_name}' (mode={mode})\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.write_initial_group","level":3,"title":"<code>write_initial_group(dataset, group_name, branch='main', commit_message=None)</code>","text":"<p>Write initial data to a new group.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def write_initial_group(\n    self,\n    dataset: xr.Dataset,\n    group_name: str,\n    branch: str = \"main\",\n    commit_message: str | None = None,\n) -&gt; None:\n    \"\"\"Write initial data to a new group.\"\"\"\n    if self.group_exists(group_name, branch):\n        raise ValueError(\n            f\"Group '{group_name}' already exists. Use append_to_group() instead.\"\n        )\n\n    with self.writable_session(branch) as session:\n        dataset = self._normalize_encodings(dataset)\n\n        rinex_hash = dataset.attrs.get(\"RINEX File Hash\")\n        if rinex_hash is None:\n            raise ValueError(\"Dataset missing 'RINEX File Hash' attribute\")\n        start = dataset.epoch.min().values\n        end = dataset.epoch.max().values\n\n        to_icechunk(dataset, session, group=group_name, mode=\"w\")\n\n        if commit_message is None:\n            version = get_version_from_pyproject()\n            commit_message = f\"[v{version}] Initial commit to group '{group_name}'\"\n\n        snapshot_id = session.commit(commit_message)\n\n        self.append_metadata(\n            group_name=group_name,\n            rinex_hash=rinex_hash,\n            start=start,\n            end=end,\n            snapshot_id=snapshot_id,\n            action=\"write\",  # Correct action for initial data\n            commit_msg=commit_message,\n            dataset_attrs=dataset.attrs,\n        )\n\n    self._logger.info(\n        f\"Created group '{group_name}' with {len(dataset.epoch)} epochs, \"\n        f\"hash={rinex_hash}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.backup_metadata_table","level":3,"title":"<code>backup_metadata_table(group_name, session)</code>","text":"<p>Backup the metadata table to a Polars DataFrame.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.backup_metadata_table--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Group name. session : Any     Active session for reading.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.backup_metadata_table--returns","level":5,"title":"Returns","text":"<p>pl.DataFrame | None     DataFrame with metadata rows, or None if missing.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def backup_metadata_table(\n    self,\n    group_name: str,\n    session: Any,\n) -&gt; pl.DataFrame | None:\n    \"\"\"Backup the metadata table to a Polars DataFrame.\n\n    Parameters\n    ----------\n    group_name : str\n        Group name.\n    session : Any\n        Active session for reading.\n\n    Returns\n    -------\n    pl.DataFrame | None\n        DataFrame with metadata rows, or None if missing.\n    \"\"\"\n    try:\n        zroot = zarr.open_group(session.store, mode=\"r\")\n        meta_group_path = f\"{group_name}/metadata/table\"\n\n        if (\n            \"metadata\" not in zroot[group_name]\n            or \"table\" not in zroot[group_name][\"metadata\"]\n        ):\n            self._logger.info(\n                \"No metadata table found for group \"\n                f\"'{group_name}' - nothing to backup\"\n            )\n            return None\n\n        zmeta = zroot[meta_group_path]\n\n        # Load all columns into a dictionary\n        data = {}\n        for col_name in zmeta.array_keys():\n            data[col_name] = zmeta[col_name][:]\n\n        # Convert to Polars DataFrame\n        df = pl.DataFrame(data)\n\n        self._logger.info(\n            \"Backed up metadata table with \"\n            f\"{df.height} rows for group '{group_name}'\"\n        )\n        return df\n\n    except Exception as e:\n        self._logger.warning(\n            f\"Failed to backup metadata table for group '{group_name}': {e}\"\n        )\n        return None\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.restore_metadata_table","level":3,"title":"<code>restore_metadata_table(group_name, df, session)</code>","text":"<p>Restore the metadata table from a Polars DataFrame.</p> <p>This recreates the full Zarr structure for the metadata table.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.restore_metadata_table--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Group name. df : pl.DataFrame     Metadata table to restore. session : Any     Active session for writing.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.restore_metadata_table--returns","level":5,"title":"Returns","text":"<p>None</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def restore_metadata_table(\n    self,\n    group_name: str,\n    df: pl.DataFrame,\n    session: Any,\n) -&gt; None:\n    \"\"\"Restore the metadata table from a Polars DataFrame.\n\n    This recreates the full Zarr structure for the metadata table.\n\n    Parameters\n    ----------\n    group_name : str\n        Group name.\n    df : pl.DataFrame\n        Metadata table to restore.\n    session : Any\n        Active session for writing.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if df is None or df.height == 0:\n        self._logger.info(f\"No metadata to restore for group '{group_name}'\")\n        return\n\n    try:\n        zroot = zarr.open_group(session.store, mode=\"a\")\n        meta_group_path = f\"{group_name}/metadata/table\"\n\n        # Create the metadata subgroup\n        zmeta = zroot.require_group(meta_group_path)\n\n        # Create all arrays from the DataFrame\n        for col_name in df.columns:\n            col_data = df[col_name]\n\n            if col_name == \"index\":\n                # Index column as int64\n                arr = col_data.to_numpy().astype(\"i8\")\n                dtype = \"i8\"\n            elif col_name in (\"start\", \"end\"):\n                # Datetime columns\n                arr = col_data.to_numpy().astype(\"datetime64[ns]\")\n                dtype = \"M8[ns]\"\n            else:\n                # String columns - use VariableLengthUTF8\n                arr = col_data.to_list()  # Convert to list for VariableLengthUTF8\n                dtype = VariableLengthUTF8()\n\n            # Create the array\n            zmeta.create_array(\n                name=col_name,\n                shape=(len(arr),),\n                dtype=dtype,\n                chunks=(1024,),\n                overwrite=True,\n            )\n\n            # Write the data\n            zmeta[col_name][:] = arr\n\n        self._logger.info(\n            \"Restored metadata table with \"\n            f\"{df.height} rows for group '{group_name}'\"\n        )\n\n    except Exception as e:\n        self._logger.error(\n            f\"Failed to restore metadata table for group '{group_name}': {e}\"\n        )\n        raise RuntimeError(f\"Critical error: could not restore metadata table: {e}\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.overwrite_file_in_group","level":3,"title":"<code>overwrite_file_in_group(dataset, group_name, rinex_hash, start, end, branch='main', commit_message=None)</code>","text":"<p>Overwrite a file's contribution to the group (same hash, new epoch range).</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def overwrite_file_in_group(\n    self,\n    dataset: xr.Dataset,\n    group_name: str,\n    rinex_hash: str,\n    start: np.datetime64,\n    end: np.datetime64,\n    branch: str = \"main\",\n    commit_message: str | None = None,\n) -&gt; None:\n    \"\"\"Overwrite a file's contribution to the group (same hash, new epoch range).\"\"\"\n\n    dataset = self._normalize_encodings(dataset)\n\n    # --- Step 3: rewrite store ---\n    with self.writable_session(branch) as session:\n        ds_from_store = xr.open_zarr(\n            session.store, group=group_name, consolidated=False\n        )\n\n        # Backup the existing metadata table\n        metadata_backup = self.backup_metadata_table(group_name, session)\n\n        mask = (ds_from_store.epoch.values &lt; start) | (\n            ds_from_store.epoch.values &gt; end\n        )\n        ds_from_store_cleansed = ds_from_store.isel(epoch=mask)\n        ds_from_store_cleansed = self._normalize_encodings(ds_from_store_cleansed)\n\n        # Check if any epochs remain after cleansing, then write leftovers.\n        if ds_from_store_cleansed.sizes.get(\"epoch\", 0) &gt; 0:\n            to_icechunk(ds_from_store_cleansed, session, group=group_name, mode=\"w\")\n        # no epochs left, reset group to empty\n        else:\n            to_icechunk(dataset.isel(epoch=[]), session, group=group_name, mode=\"w\")\n\n        # write back the backed up metadata table\n        self.restore_metadata_table(group_name, metadata_backup, session)\n\n        # Append the new dataset\n        to_icechunk(dataset, session, group=group_name, append_dim=\"epoch\")\n\n        if commit_message is None:\n            version = get_version_from_pyproject()\n            commit_message = (\n                f\"[v{version}] Overwrote file {rinex_hash} in group '{group_name}'\"\n            )\n\n        snapshot_id = session.commit(commit_message)\n\n        self.append_metadata(\n            group_name=group_name,\n            rinex_hash=rinex_hash,\n            start=start,\n            end=end,\n            snapshot_id=snapshot_id,\n            action=\"overwrite\",\n            commit_msg=commit_message,\n            dataset_attrs=dataset.attrs,\n        )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_group_info","level":3,"title":"<code>get_group_info(group_name, branch='main')</code>","text":"<p>Get metadata about a group.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_group_info--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group. branch : str, default \"main\"     Repository branch to examine.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_group_info--returns","level":5,"title":"Returns","text":"<p>dict[str, Any]     Group metadata.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_group_info--raises","level":5,"title":"Raises","text":"<p>ValueError     If the group does not exist.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_group_info(self, group_name: str, branch: str = \"main\") -&gt; dict[str, Any]:\n    \"\"\"\n    Get metadata about a group.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group.\n    branch : str, default \"main\"\n        Repository branch to examine.\n\n    Returns\n    -------\n    dict[str, Any]\n        Group metadata.\n\n    Raises\n    ------\n    ValueError\n        If the group does not exist.\n    \"\"\"\n    if not self.group_exists(group_name, branch):\n        raise ValueError(f\"Group '{group_name}' does not exist\")\n\n    ds = self.read_group(group_name, branch)\n\n    info = {\n        \"group_name\": group_name,\n        \"store_type\": self.store_type,\n        \"dimensions\": dict(ds.sizes),\n        \"variables\": list(ds.data_vars.keys()),\n        \"coordinates\": list(ds.coords.keys()),\n        \"attributes\": dict(ds.attrs),\n    }\n\n    # Add temporal information if epoch dimension exists\n    if \"epoch\" in ds.sizes:\n        info[\"temporal_info\"] = {\n            \"start\": str(ds.epoch.min().values),\n            \"end\": str(ds.epoch.max().values),\n            \"count\": ds.sizes[\"epoch\"],\n            \"resolution\": str(ds.epoch.diff(\"epoch\").median().values),\n        }\n\n    return info\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.rel_path_for_commit","level":3,"title":"<code>rel_path_for_commit(file_path)</code>","text":"<p>Generate relative path for commit messages.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.rel_path_for_commit--parameters","level":5,"title":"Parameters","text":"<p>file_path : Path     Full file path.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.rel_path_for_commit--returns","level":5,"title":"Returns","text":"<p>str     Relative path string with log_path_depth parts.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def rel_path_for_commit(self, file_path: Path) -&gt; str:\n    \"\"\"\n    Generate relative path for commit messages.\n\n    Parameters\n    ----------\n    file_path : Path\n        Full file path.\n\n    Returns\n    -------\n    str\n        Relative path string with log_path_depth parts.\n    \"\"\"\n    depth = load_config().processing.logging.log_path_depth\n    return str(Path(*file_path.parts[-depth:]))\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_store_stats","level":3,"title":"<code>get_store_stats()</code>","text":"<p>Get statistics about the store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_store_stats--returns","level":5,"title":"Returns","text":"<p>dict[str, Any]     Store statistics.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_store_stats(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get statistics about the store.\n\n    Returns\n    -------\n    dict[str, Any]\n        Store statistics.\n    \"\"\"\n    groups = self.get_group_names()\n    stats = {\n        \"store_path\": str(self.store_path),\n        \"store_type\": self.store_type,\n        \"compression_level\": self.compression_level,\n        \"compression_algorithm\": self.compression_algorithm.name,\n        \"total_groups\": len(groups),\n        \"groups\": groups,\n    }\n\n    # Add group-specific stats\n    for group_name in groups:\n        try:\n            info = self.get_group_info(group_name)\n            stats[f\"group_{group_name}\"] = {\n                \"dimensions\": info[\"dimensions\"],\n                \"variables_count\": len(info[\"variables\"]),\n                \"has_temporal_data\": \"temporal_info\" in info,\n            }\n        except Exception as e:\n            self._logger.warning(\n                f\"Failed to get stats for group '{group_name}': {e}\"\n            )\n\n    return stats\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.append_to_group","level":3,"title":"<code>append_to_group(dataset, group_name, append_dim='epoch', branch='main', action='write', commit_message=None)</code>","text":"<p>Append data to an existing group.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def append_to_group(\n    self,\n    dataset: xr.Dataset,\n    group_name: str,\n    append_dim: str = \"epoch\",\n    branch: str = \"main\",\n    action: str = \"write\",\n    commit_message: str | None = None,\n) -&gt; None:\n    \"\"\"Append data to an existing group.\"\"\"\n    if not self.group_exists(group_name, branch):\n        raise ValueError(\n            f\"Group '{group_name}' does not exist. Use write_initial_group() first.\"\n        )\n\n    dataset = self._normalize_encodings(dataset)\n\n    rinex_hash = dataset.attrs.get(\"RINEX File Hash\")\n    if rinex_hash is None:\n        raise ValueError(\"Dataset missing 'RINEX File Hash' attribute\")\n    start = dataset.epoch.min().values\n    end = dataset.epoch.max().values\n\n    with self.writable_session(branch) as session:\n        to_icechunk(dataset, session, group=group_name, append_dim=append_dim)\n\n        if commit_message is None and action == \"write\":\n            version = get_version_from_pyproject()\n            commit_message = f\"[v{version}] Wrote to group '{group_name}'\"\n        elif commit_message is None and action != \"append\":\n            version = get_version_from_pyproject()\n            commit_message = f\"[v{version}] Appended to group '{group_name}'\"\n\n        snapshot_id = session.commit(commit_message)\n\n        self.append_metadata(\n            group_name=group_name,\n            rinex_hash=rinex_hash,\n            start=start,\n            end=end,\n            snapshot_id=snapshot_id,\n            action=action,\n            commit_msg=commit_message,\n            dataset_attrs=dataset.attrs,\n        )\n\n    if action == \"append\":\n        self._logger.info(\n            f\"Appended {len(dataset.epoch)} epochs to group '{group_name}', \"\n            f\"hash={rinex_hash}\"\n        )\n    elif action == \"write\":\n        self._logger.info(\n            f\"Wrote {len(dataset.epoch)} epochs to group '{group_name}', \"\n            f\"hash={rinex_hash}\"\n        )\n    else:\n        self._logger.info(\n            f\"Action '{action}' completed for group '{group_name}', \"\n            f\"hash={rinex_hash}\"\n        )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.append_metadata","level":3,"title":"<code>append_metadata(group_name, rinex_hash, start, end, snapshot_id, action, commit_msg, dataset_attrs, branch='main')</code>","text":"<p>Append a metadata row into the group_name/metadata/table.</p> Schema <p>index           int64 (continuous row id) rinex_hash      str   (UTF-8, VariableLengthUTF8) start           datetime64[ns] end             datetime64[ns] snapshot_id     str   (UTF-8) action          str   (UTF-8, e.g. \"insert\"|\"append\"|\"overwrite\"|\"skip\") commit_msg      str   (UTF-8) written_at      str   (UTF-8, ISO8601 with timezone) write_strategy  str   (UTF-8, RINEX_STORE_STRATEGY or VOD_STORE_STRATEGY) attrs           str   (UTF-8, JSON dump of dataset attrs)</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def append_metadata(\n    self,\n    group_name: str,\n    rinex_hash: str,\n    start: np.datetime64,\n    end: np.datetime64,\n    snapshot_id: str,\n    action: str,\n    commit_msg: str,\n    dataset_attrs: dict,\n    branch: str = \"main\",\n) -&gt; None:\n    \"\"\"\n    Append a metadata row into the group_name/metadata/table.\n\n    Schema:\n        index           int64 (continuous row id)\n        rinex_hash      str   (UTF-8, VariableLengthUTF8)\n        start           datetime64[ns]\n        end             datetime64[ns]\n        snapshot_id     str   (UTF-8)\n        action          str   (UTF-8, e.g. \"insert\"|\"append\"|\"overwrite\"|\"skip\")\n        commit_msg      str   (UTF-8)\n        written_at      str   (UTF-8, ISO8601 with timezone)\n        write_strategy  str   (UTF-8, RINEX_STORE_STRATEGY or VOD_STORE_STRATEGY)\n        attrs           str   (UTF-8, JSON dump of dataset attrs)\n    \"\"\"\n    written_at = datetime.now().astimezone().isoformat()\n\n    row = {\n        \"rinex_hash\": str(rinex_hash),\n        \"start\": np.datetime64(start, \"ns\"),\n        \"end\": np.datetime64(end, \"ns\"),\n        \"snapshot_id\": str(snapshot_id),\n        \"action\": str(action),\n        \"commit_msg\": str(commit_msg),\n        \"written_at\": written_at,\n        \"write_strategy\": str(self._rinex_store_strategy)\n        if self.store_type == \"rinex_store\"\n        else str(self._vod_store_strategy),\n        \"attrs\": json.dumps(dataset_attrs, default=str),\n    }\n    df_row = pl.DataFrame([row])\n\n    with self.writable_session(branch) as session:\n        zroot = zarr.open_group(session.store, mode=\"a\")\n        meta_group_path = f\"{group_name}/metadata/table\"\n\n        if (\n            \"metadata\" not in zroot[group_name]\n            or \"table\" not in zroot[group_name][\"metadata\"]\n        ):\n            # --- First time: create arrays with correct dtypes ---\n            zmeta = zroot.require_group(meta_group_path)\n\n            # index counter\n            zmeta.create_array(\n                name=\"index\", shape=(0,), dtype=\"i8\", chunks=(1024,), overwrite=True\n            )\n            zmeta[\"index\"].append([0])\n\n            for col in df_row.columns:\n                if col in (\"start\", \"end\"):\n                    dtype = \"M8[ns]\"\n                    arr = np.array(df_row[col].to_numpy(), dtype=dtype)\n                else:\n                    dtype = VariableLengthUTF8()\n                    arr = df_row[col].to_list()\n\n                zmeta.create_array(\n                    name=col,\n                    shape=(0,),\n                    dtype=dtype,\n                    chunks=(1024,),\n                    overwrite=True,\n                )\n                zmeta[col].append(arr)\n\n        else:\n            # --- Append to existing ---\n            zmeta = zroot[meta_group_path]\n\n            # index increment\n            current_len = zmeta[\"index\"].shape[0]\n            next_idx = current_len\n            zmeta[\"index\"].append([next_idx])\n\n            for col in df_row.columns:\n                if col in (\"start\", \"end\"):\n                    arr = np.array(df_row[col].to_numpy(), dtype=\"M8[ns]\")\n                else:\n                    arr = df_row[col].to_list()\n                zmeta[col].append(arr)\n\n        session.commit(f\"Appended metadata row for {group_name}, hash={rinex_hash}\")\n\n    self._logger.info(\n        f\"Metadata appended for group '{group_name}': \"\n        f\"hash={rinex_hash}, snapshot={snapshot_id}, action={action}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.append_metadata_bulk","level":3,"title":"<code>append_metadata_bulk(group_name, rows, session=None)</code>","text":"<p>Append multiple metadata rows in one commit.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.append_metadata_bulk--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Group name (e.g. \"canopy\", \"reference\") rows : list[dict[str, Any]]     List of metadata records matching the schema used in     append_metadata(). session : icechunk.WritableSession, optional     If provided, rows are written into this session (caller commits later).     If None, this method opens its own writable session and commits once.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def append_metadata_bulk(\n    self,\n    group_name: str,\n    rows: list[dict[str, Any]],\n    session: Optional[\"icechunk.WritableSession\"] = None,\n) -&gt; None:\n    \"\"\"\n    Append multiple metadata rows in one commit.\n\n    Parameters\n    ----------\n    group_name : str\n        Group name (e.g. \"canopy\", \"reference\")\n    rows : list[dict[str, Any]]\n        List of metadata records matching the schema used in\n        append_metadata().\n    session : icechunk.WritableSession, optional\n        If provided, rows are written into this session (caller commits later).\n        If None, this method opens its own writable session and commits once.\n    \"\"\"\n    if not rows:\n        self._logger.info(f\"No metadata rows to append for group '{group_name}'\")\n        return\n\n    # Ensure datetime conversions for consistency\n    for row in rows:\n        if isinstance(row.get(\"start\"), str):\n            row[\"start\"] = np.datetime64(row[\"start\"])\n        if isinstance(row.get(\"end\"), str):\n            row[\"end\"] = np.datetime64(row[\"end\"])\n        if \"written_at\" not in row:\n            row[\"written_at\"] = datetime.now(UTC).isoformat()\n\n    # Prepare the Polars DataFrame\n    df = pl.DataFrame(rows)\n\n    def _do_append(session_obj: \"icechunk.WritableSession\") -&gt; None:\n        \"\"\"Append metadata rows to a writable session.\n\n        Parameters\n        ----------\n        session_obj : icechunk.WritableSession\n            Writable session to update.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        zroot = zarr.open_group(session_obj.store, mode=\"a\")\n        meta_group_path = f\"{group_name}/metadata/table\"\n        zmeta = zroot.require_group(meta_group_path)\n\n        start_index = 0\n        if \"index\" in zmeta:\n            existing_len = zmeta[\"index\"].shape[0]\n            start_index = (\n                int(zmeta[\"index\"][-1].item()) + 1 if existing_len &gt; 0 else 0\n            )\n\n        # Assign sequential indices\n        df_with_index = df.with_columns(\n            (pl.arange(start_index, start_index + df.height)).alias(\"index\")\n        )\n\n        # Write each column\n        for col_name in df_with_index.columns:\n            col_data = df_with_index[col_name]\n\n            if col_name == \"index\":\n                dtype = \"i8\"\n                arr = col_data.to_numpy().astype(dtype)\n            elif col_name in (\"start\", \"end\"):\n                dtype = \"M8[ns]\"\n                arr = col_data.to_numpy().astype(dtype)\n            else:\n                # strings / jsons / ids\n                dtype = VariableLengthUTF8()\n                arr = col_data.to_list()\n\n            if col_name not in zmeta:\n                # Create array if it doesn't exist\n                zmeta.create_array(\n                    name=col_name,\n                    shape=(0,),\n                    dtype=dtype,\n                    chunks=(1024,),\n                    overwrite=True,\n                )\n\n            # Resize and append\n            old_len = zmeta[col_name].shape[0]\n            new_len = old_len + len(arr)\n            zmeta[col_name].resize(new_len)\n            zmeta[col_name][old_len:new_len] = arr\n\n        self._logger.info(\n            f\"Appended {df_with_index.height} metadata rows to group '{group_name}'\"\n        )\n\n    if session is not None:\n        _do_append(session)\n    else:\n        with self.writable_session() as sess:\n            _do_append(sess)\n            sess.commit(f\"Bulk metadata append for {group_name}\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.load_metadata","level":3,"title":"<code>load_metadata(store, group_name)</code>","text":"<p>Load metadata directly from Zarr into a Polars DataFrame.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.load_metadata--parameters","level":5,"title":"Parameters","text":"<p>store : Any     Zarr store or session store handle. group_name : str     Group name.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.load_metadata--returns","level":5,"title":"Returns","text":"<p>pl.DataFrame     Metadata table.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def load_metadata(self, store: Any, group_name: str) -&gt; pl.DataFrame:\n    \"\"\"Load metadata directly from Zarr into a Polars DataFrame.\n\n    Parameters\n    ----------\n    store : Any\n        Zarr store or session store handle.\n    group_name : str\n        Group name.\n\n    Returns\n    -------\n    pl.DataFrame\n        Metadata table.\n    \"\"\"\n    zroot = zarr.open_group(store, mode=\"r\")\n    zmeta = zroot[f\"{group_name}/metadata/table\"]\n\n    # Read all columns into a dict of numpy arrays\n    data = {col: zmeta[col][...] for col in zmeta.array_keys()}\n\n    # Build Polars DataFrame\n    df = pl.DataFrame(data)\n\n    # Convert numeric datetime64 columns back to proper Polars datetimes\n    if df[\"start\"].dtype in (pl.Int64, pl.Float64):\n        df = df.with_columns(pl.col(\"start\").cast(pl.Datetime(\"ns\")))\n    if df[\"end\"].dtype in (pl.Int64, pl.Float64):\n        df = df.with_columns(pl.col(\"end\").cast(pl.Datetime(\"ns\")))\n    if df[\"written_at\"].dtype == pl.Utf8:\n        df = df.with_columns(pl.col(\"written_at\").str.to_datetime(\"%+\"))\n    return df\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.read_metadata_table","level":3,"title":"<code>read_metadata_table(session, group_name)</code>","text":"<p>Read the metadata table from a session.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.read_metadata_table--parameters","level":5,"title":"Parameters","text":"<p>session : Any     Active session for reading. group_name : str     Group name.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.read_metadata_table--returns","level":5,"title":"Returns","text":"<p>pl.DataFrame     Metadata table.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def read_metadata_table(self, session: Any, group_name: str) -&gt; pl.DataFrame:\n    \"\"\"Read the metadata table from a session.\n\n    Parameters\n    ----------\n    session : Any\n        Active session for reading.\n    group_name : str\n        Group name.\n\n    Returns\n    -------\n    pl.DataFrame\n        Metadata table.\n    \"\"\"\n    zmeta = zarr.open_group(\n        session.store,\n        mode=\"r\",\n    )[f\"{group_name}/metadata/table\"]\n\n    data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n    df = pl.DataFrame(data)\n\n    # Ensure start/end are proper datetime\n    df = df.with_columns(\n        [\n            pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n            pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n        ]\n    )\n    return df\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.metadata_row_exists","level":3,"title":"<code>metadata_row_exists(group_name, rinex_hash, start, end, branch='main')</code>","text":"<p>Check whether a (start, end) interval exists in group metadata.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.metadata_row_exists--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Icechunk group name. rinex_hash : str     Hash of the current RINEX dataset. start : np.datetime64     Start time for the interval. end : np.datetime64     End time for the interval. branch : str, default \"main\"     Branch name in the Icechunk repository.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.metadata_row_exists--returns","level":5,"title":"Returns","text":"<p>tuple[bool, pl.DataFrame]     Existence flag and the matching metadata rows.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.metadata_row_exists--raises","level":5,"title":"Raises","text":"<p>ValueError     If a conflicting hash is found for the same interval.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.metadata_row_exists--notes","level":5,"title":"Notes","text":"<p>The metadata table is cast to <code>Datetime(\"ns\")</code> for <code>start</code> and <code>end</code> before filtering.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def metadata_row_exists(\n    self,\n    group_name: str,\n    rinex_hash: str,\n    start: np.datetime64,\n    end: np.datetime64,\n    branch: str = \"main\",\n) -&gt; tuple[bool, pl.DataFrame]:\n    \"\"\"\n    Check whether a (start, end) interval exists in group metadata.\n\n    Parameters\n    ----------\n    group_name : str\n        Icechunk group name.\n    rinex_hash : str\n        Hash of the current RINEX dataset.\n    start : np.datetime64\n        Start time for the interval.\n    end : np.datetime64\n        End time for the interval.\n    branch : str, default \"main\"\n        Branch name in the Icechunk repository.\n\n    Returns\n    -------\n    tuple[bool, pl.DataFrame]\n        Existence flag and the matching metadata rows.\n\n    Raises\n    ------\n    ValueError\n        If a conflicting hash is found for the same interval.\n\n    Notes\n    -----\n    The metadata table is cast to `Datetime(\"ns\")` for `start` and `end`\n    before filtering.\n    \"\"\"\n    with self.readonly_session(branch) as session:\n        try:\n            zmeta = zarr.open_group(session.store, mode=\"r\")[\n                f\"{group_name}/metadata/table\"\n            ]\n        except Exception:\n            return False, pl.DataFrame()\n\n        # Load all arrays into a dict\n        data = {col: zmeta[col][:] for col in zmeta.array_keys()}\n        df = pl.DataFrame(data)\n\n        # Ensure datetime dtypes\n        df = df.with_columns(\n            [\n                pl.col(\"start\").cast(pl.Datetime(\"ns\")),\n                pl.col(\"end\").cast(pl.Datetime(\"ns\")),\n            ]\n        )\n\n        # Step 1: filter by start+end\n        matches = df.filter(\n            (pl.col(\"start\") == np.datetime64(start, \"ns\"))\n            &amp; (pl.col(\"end\") == np.datetime64(end, \"ns\"))\n        )\n\n        if matches.is_empty():\n            return False, matches\n\n        # Step 2: check hash consistency\n        unique_hashes = matches.select(\"rinex_hash\").unique()\n\n        if (\n            unique_hashes.height &gt; 1\n            or unique_hashes.item(0, \"rinex_hash\") != rinex_hash\n        ):\n            existing_hashes = unique_hashes.to_series().to_list()\n            raise ValueError(\n                \"Metadata conflict: rows with start=\"\n                f\"{start}, end={end} exist but hash differs \"\n                f\"(existing={existing_hashes}, new={rinex_hash})\"\n            )\n\n        return True, matches\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.batch_check_existing","level":3,"title":"<code>batch_check_existing(group_name, file_hashes)</code>","text":"<p>Check which file hashes already exist in metadata.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def batch_check_existing(self, group_name: str, file_hashes: list[str]) -&gt; set[str]:\n    \"\"\"Check which file hashes already exist in metadata.\"\"\"\n\n    try:\n        with self.readonly_session(\"main\") as session:\n            df = self.load_metadata(session.store, group_name)\n\n            # Filter to matching hashes\n            existing = df.filter(pl.col(\"rinex_hash\").is_in(file_hashes))\n            return set(existing[\"rinex_hash\"].to_list())\n\n    except (KeyError, zarr.errors.GroupNotFoundError, Exception):\n        # Branch/group/metadata doesn't exist yet (fresh store)\n        return set()\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.append_metadata_bulk_store","level":3,"title":"<code>append_metadata_bulk_store(group_name, rows, store)</code>","text":"<p>Append metadata rows into an open transaction store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.append_metadata_bulk_store--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Group name (e.g. \"canopy\", \"reference\"). rows : list[dict[str, Any]]     Metadata rows to append. store : Any     Open Icechunk transaction store.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def append_metadata_bulk_store(\n    self,\n    group_name: str,\n    rows: list[dict[str, Any]],\n    store: Any,\n) -&gt; None:\n    \"\"\"\n    Append metadata rows into an open transaction store.\n\n    Parameters\n    ----------\n    group_name : str\n        Group name (e.g. \"canopy\", \"reference\").\n    rows : list[dict[str, Any]]\n        Metadata rows to append.\n    store : Any\n        Open Icechunk transaction store.\n    \"\"\"\n    if not rows:\n        return\n\n    zroot = zarr.open_group(store, mode=\"a\")\n    zmeta = zroot.require_group(f\"{group_name}/metadata/table\")\n\n    # Find next index\n    start_index = 0\n    if \"index\" in zmeta:\n        start_index = (\n            int(zmeta[\"index\"][-1]) + 1 if zmeta[\"index\"].shape[0] &gt; 0 else 0\n        )\n\n    for i, row in enumerate(rows, start=start_index):\n        row[\"index\"] = i\n\n    import polars as pl\n\n    df = pl.DataFrame(rows)\n\n    for col in df.columns:\n        list_only_cols = {\n            \"attrs\",\n            \"commit_msg\",\n            \"action\",\n            \"write_strategy\",\n            \"rinex_hash\",\n            \"snapshot_id\",\n        }\n        if col in list_only_cols:\n            values = df[col].to_list()\n        else:\n            values = df[col].to_numpy()\n\n        if col == \"index\":\n            dtype = \"i8\"\n        elif col in (\"start\", \"end\"):\n            dtype = \"M8[ns]\"\n        else:\n            dtype = VariableLengthUTF8()\n\n        if col not in zmeta:\n            zmeta.create_array(\n                name=col, shape=(0,), dtype=dtype, chunks=(1024,), overwrite=True\n            )\n\n        arr = zmeta[col]\n        old_len = arr.shape[0]\n        new_len = old_len + len(values)\n        arr.resize(new_len)\n        arr[old_len:new_len] = values\n\n    self._logger.info(f\"Appended {df.height} metadata rows to group '{group_name}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.expire_old_snapshots","level":3,"title":"<code>expire_old_snapshots(days=None, branch='main', delete_expired_branches=True, delete_expired_tags=True)</code>","text":"<p>Expire and garbage-collect snapshots older than the given retention period.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.expire_old_snapshots--parameters","level":5,"title":"Parameters","text":"<p>days : int | None, optional     Number of days to retain snapshots. Defaults to config value. branch : str, default \"main\"     Branch to apply expiration on. delete_expired_branches : bool, default True     Whether to delete branches pointing to expired snapshots. delete_expired_tags : bool, default True     Whether to delete tags pointing to expired snapshots.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.expire_old_snapshots--returns","level":5,"title":"Returns","text":"<p>set[str]     Expired snapshot IDs.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def expire_old_snapshots(\n    self,\n    days: int | None = None,\n    branch: str = \"main\",\n    delete_expired_branches: bool = True,\n    delete_expired_tags: bool = True,\n) -&gt; set[str]:\n    \"\"\"\n    Expire and garbage-collect snapshots older than the given retention period.\n\n    Parameters\n    ----------\n    days : int | None, optional\n        Number of days to retain snapshots. Defaults to config value.\n    branch : str, default \"main\"\n        Branch to apply expiration on.\n    delete_expired_branches : bool, default True\n        Whether to delete branches pointing to expired snapshots.\n    delete_expired_tags : bool, default True\n        Whether to delete tags pointing to expired snapshots.\n\n    Returns\n    -------\n    set[str]\n        Expired snapshot IDs.\n    \"\"\"\n    if days is None:\n        days = self._rinex_store_expire_days\n    cutoff = datetime.now(UTC) - timedelta(days=days)\n\n    # cutoff = datetime(2025, 10, 3, 16, 44, 1, tzinfo=timezone.utc)\n    self._logger.info(\n        f\"Running expiration on store '{self.store_type}' \"\n        f\"(branch '{branch}') with cutoff {cutoff.isoformat()}\"\n    )\n\n    # Expire snapshots older than cutoff\n    expired_ids = self.repo.expire_snapshots(\n        older_than=cutoff,\n        delete_expired_branches=delete_expired_branches,\n        delete_expired_tags=delete_expired_tags,\n    )\n\n    if expired_ids:\n        self._logger.info(\n            f\"Expired {len(expired_ids)} snapshots: {sorted(expired_ids)}\"\n        )\n    else:\n        self._logger.info(\"No snapshots to expire.\")\n\n    # Garbage-collect expired objects to reclaim storage\n    summary = self.repo.garbage_collect(delete_object_older_than=cutoff)\n    self._logger.info(\n        f\"Garbage collection summary: \"\n        f\"deleted_bytes={summary.bytes_deleted}, \"\n        f\"deleted_chunks={summary.chunks_deleted}, \"\n        f\"deleted_manifests={summary.manifests_deleted}, \"\n        f\"deleted_snapshots={summary.snapshots_deleted}, \"\n        f\"deleted_attributes={summary.attributes_deleted}, \"\n        f\"deleted_transaction_logs={summary.transaction_logs_deleted}\"\n    )\n\n    return expired_ids\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_history","level":3,"title":"<code>get_history(branch='main', limit=None)</code>","text":"<p>Return commit ancestry (history) for a branch.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_history--parameters","level":5,"title":"Parameters","text":"<p>branch : str, default \"main\"     Branch name. limit : int | None, optional     Maximum number of commits to return.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_history--returns","level":5,"title":"Returns","text":"<p>list[dict]     Commit info dictionaries (id, message, written_at, parent_ids).</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_history(self, branch: str = \"main\", limit: int | None = None) -&gt; list[dict]:\n    \"\"\"\n    Return commit ancestry (history) for a branch.\n\n    Parameters\n    ----------\n    branch : str, default \"main\"\n        Branch name.\n    limit : int | None, optional\n        Maximum number of commits to return.\n\n    Returns\n    -------\n    list[dict]\n        Commit info dictionaries (id, message, written_at, parent_ids).\n    \"\"\"\n    self._logger.info(f\"Fetching ancestry for branch '{branch}'\")\n\n    history = []\n    for i, ancestor in enumerate(self.repo.ancestry(branch=branch)):\n        history.append(\n            {\n                \"snapshot_id\": ancestor.id,\n                \"commit_msg\": ancestor.message,\n                \"written_at\": ancestor.written_at,\n                \"parent_ids\": ancestor.parent_id,\n            }\n        )\n        if limit is not None and i + 1 &gt;= limit:\n            break\n\n    return history\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.print_history","level":3,"title":"<code>print_history(branch='main', limit=100)</code>","text":"<p>Pretty-print the ancestry for quick inspection.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def print_history(self, branch: str = \"main\", limit: int | None = 100) -&gt; None:\n    \"\"\"\n    Pretty-print the ancestry for quick inspection.\n    \"\"\"\n    for entry in self.get_history(branch=branch, limit=limit):\n        ts = entry[\"written_at\"].strftime(\"%Y-%m-%d %H:%M:%S\")\n        print(f\"{ts} {entry['snapshot_id'][:8]} {entry['commit_msg']}\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return the developer-facing representation.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.__repr__--returns","level":5,"title":"Returns","text":"<p>str     Representation string.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the developer-facing representation.\n\n    Returns\n    -------\n    str\n        Representation string.\n    \"\"\"\n    return (\n        \"MyIcechunkStore(\"\n        f\"store_path={self.store_path}, store_type={self.store_type})\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.__str__","level":3,"title":"<code>__str__()</code>","text":"<p>Return a human-readable summary.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.__str__--returns","level":5,"title":"Returns","text":"<p>str     Summary string.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable summary.\n\n    Returns\n    -------\n    str\n        Summary string.\n    \"\"\"\n\n    # Capture tree output\n    old_stdout = sys.stdout\n    sys.stdout = buffer = io.StringIO()\n\n    try:\n        self.print_tree()\n        tree_output = buffer.getvalue()\n    finally:\n        sys.stdout = old_stdout\n\n    branches = self.get_branch_names()\n    group_dict = self.get_group_names()\n    total_groups = sum(len(groups) for groups in group_dict.values())\n\n    return (\n        f\"MyIcechunkStore: {self.store_path}\\n\"\n        f\"Branches: {len(branches)} | Total Groups: {total_groups}\\n\\n\"\n        f\"{tree_output}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.rechunk_group","level":3,"title":"<code>rechunk_group(group_name, chunks, source_branch='main', temp_branch=None, promote_to_main=True, delete_temp_branch=True)</code>","text":"<p>Rechunk a group with optimal chunk sizes.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.rechunk_group--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group to rechunk chunks : dict[str, int]     Chunking specification, e.g. {'epoch': 34560, 'sid': -1} source_branch : str     Branch to read original data from (default: \"main\") temp_branch : str | None     Temporary branch name for rechunked data. If None, uses     \"{group_name}_rechunked\". promote_to_main : bool     If True, reset main branch to rechunked snapshot after writing delete_temp_branch : bool     If True, delete temporary branch after promotion (only if     promote_to_main=True).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.rechunk_group--returns","level":5,"title":"Returns","text":"<p>str     Snapshot ID of the rechunked data</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def rechunk_group(\n    self,\n    group_name: str,\n    chunks: dict[str, int],\n    source_branch: str = \"main\",\n    temp_branch: str | None = None,\n    promote_to_main: bool = True,\n    delete_temp_branch: bool = True,\n) -&gt; str:\n    \"\"\"\n    Rechunk a group with optimal chunk sizes.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group to rechunk\n    chunks : dict[str, int]\n        Chunking specification, e.g. {'epoch': 34560, 'sid': -1}\n    source_branch : str\n        Branch to read original data from (default: \"main\")\n    temp_branch : str | None\n        Temporary branch name for rechunked data. If None, uses\n        \"{group_name}_rechunked\".\n    promote_to_main : bool\n        If True, reset main branch to rechunked snapshot after writing\n    delete_temp_branch : bool\n        If True, delete temporary branch after promotion (only if\n        promote_to_main=True).\n\n    Returns\n    -------\n    str\n        Snapshot ID of the rechunked data\n    \"\"\"\n    if temp_branch is None:\n        temp_branch = f\"{group_name}_rechunked_temp\"\n\n    self._logger.info(\n        f\"Starting rechunk of group '{group_name}' with chunks={chunks}\"\n    )\n\n    # Get CURRENT snapshot from source branch to preserve all other groups\n    current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n\n    # Create temp branch from current snapshot (preserves all existing groups)\n    try:\n        self.repo.create_branch(temp_branch, current_snapshot)\n        self._logger.info(\n            f\"Created temporary branch '{temp_branch}' from current {source_branch}\"\n        )\n    except Exception as e:\n        self._logger.warning(f\"Branch '{temp_branch}' may already exist: {e}\")\n\n    # Read original data\n    ds_original = self.read_group(group_name, branch=source_branch)\n    self._logger.info(f\"Original chunks: {ds_original.chunks}\")\n\n    # Rechunk\n    ds_rechunked = ds_original.chunk(chunks)\n    self._logger.info(f\"New chunks: {ds_rechunked.chunks}\")\n\n    # Clear encoding to avoid conflicts\n    for var in ds_rechunked.data_vars:\n        ds_rechunked[var].encoding = {}\n\n    # Write rechunked data (overwrites only this group)\n    with self.writable_session(temp_branch) as session:\n        to_icechunk(ds_rechunked, session, group=group_name, mode=\"w\")\n        snapshot_id = session.commit(f\"Rechunked {group_name} with chunks={chunks}\")\n\n    self._logger.info(\n        f\"Rechunked data written to branch '{temp_branch}', snapshot={snapshot_id}\"\n    )\n\n    # Promote to main if requested\n    if promote_to_main:\n        rechunked_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n        self.repo.reset_branch(source_branch, rechunked_snapshot)\n        self._logger.info(\n            f\"Reset branch '{source_branch}' to rechunked snapshot \"\n            f\"{rechunked_snapshot}\"\n        )\n\n        # Delete temp branch if requested\n        if delete_temp_branch:\n            self.repo.delete_branch(temp_branch)\n            self._logger.info(f\"Deleted temporary branch '{temp_branch}'\")\n\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.rechunk_group_verbose","level":3,"title":"<code>rechunk_group_verbose(group_name, chunks=None, source_branch='main', temp_branch=None, promote_to_main=True, delete_temp_branch=True)</code>","text":"<p>Rechunk a group with optimal chunk sizes.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.rechunk_group_verbose--parameters","level":5,"title":"Parameters","text":"<p>group_name : str     Name of the group to rechunk chunks : dict[str, int] | None     Chunking specification, e.g. {'epoch': 34560, 'sid': -1}. Defaults     to <code>gnnsvodpy.globals.ICECHUNK_CHUNK_STRATEGIES</code>. source_branch : str     Branch to read original data from (default: \"main\") temp_branch : str | None     Temporary branch name for rechunked data. If None, uses     \"{group_name}_rechunked\". promote_to_main : bool     If True, reset main branch to rechunked snapshot after writing delete_temp_branch : bool     If True, delete temporary branch after promotion (only if     promote_to_main=True).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.rechunk_group_verbose--returns","level":5,"title":"Returns","text":"<p>str     Snapshot ID of the rechunked data</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def rechunk_group_verbose(\n    self,\n    group_name: str,\n    chunks: dict[str, int] | None = None,\n    source_branch: str = \"main\",\n    temp_branch: str | None = None,\n    promote_to_main: bool = True,\n    delete_temp_branch: bool = True,\n) -&gt; str:\n    \"\"\"\n    Rechunk a group with optimal chunk sizes.\n\n    Parameters\n    ----------\n    group_name : str\n        Name of the group to rechunk\n    chunks : dict[str, int] | None\n        Chunking specification, e.g. {'epoch': 34560, 'sid': -1}. Defaults\n        to `gnnsvodpy.globals.ICECHUNK_CHUNK_STRATEGIES`.\n    source_branch : str\n        Branch to read original data from (default: \"main\")\n    temp_branch : str | None\n        Temporary branch name for rechunked data. If None, uses\n        \"{group_name}_rechunked\".\n    promote_to_main : bool\n        If True, reset main branch to rechunked snapshot after writing\n    delete_temp_branch : bool\n        If True, delete temporary branch after promotion (only if\n        promote_to_main=True).\n\n    Returns\n    -------\n    str\n        Snapshot ID of the rechunked data\n    \"\"\"\n    if temp_branch is None:\n        temp_branch = f\"{group_name}_rechunked_temp\"\n\n    if chunks is None:\n        chunks = self.chunk_strategy or {\"epoch\": 34560, \"sid\": -1}\n\n    print(f\"\\n{'=' * 60}\")\n    print(f\"Starting rechunk of group '{group_name}'\")\n    print(f\"Target chunks: {chunks}\")\n    print(f\"{'=' * 60}\\n\")\n\n    self._logger.info(\n        f\"Starting rechunk of group '{group_name}' with chunks={chunks}\"\n    )\n\n    # Get CURRENT snapshot from source branch to preserve all other groups\n    print(f\"[1/7] Getting current snapshot from branch '{source_branch}'...\")\n    current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n    print(f\"      ✓ Current snapshot: {current_snapshot[:12]}\")\n\n    # Create temp branch from current snapshot (preserves all existing groups)\n    print(f\"\\n[2/7] Creating temporary branch '{temp_branch}'...\")\n    try:\n        self.repo.create_branch(temp_branch, current_snapshot)\n        print(f\"      ✓ Branch '{temp_branch}' created\")\n        self._logger.info(\n            f\"Created temporary branch '{temp_branch}' from current {source_branch}\"\n        )\n    except Exception as e:\n        print(\n            f\"      ⚠ Branch '{temp_branch}' already exists, using existing branch\"\n        )\n        self._logger.warning(f\"Branch '{temp_branch}' may already exist: {e}\")\n\n    # Read original data\n    print(f\"\\n[3/7] Reading original data from '{group_name}'...\")\n    ds_original = self.read_group(group_name, branch=source_branch)\n\n    # Unify chunks if inconsistent\n    try:\n        ds_original = ds_original.unify_chunks()\n        print(\"      ✓ Unified inconsistent chunks\")\n    except (TypeError, ValueError):\n        pass  # Chunks are already consistent\n\n    print(f\"      ✓ Data shape: {dict(ds_original.sizes)}\")\n    print(f\"      ✓ Original chunks: {ds_original.chunks}\")\n    self._logger.info(f\"Original chunks: {ds_original.chunks}\")\n\n    # Rechunk\n    print(\"\\n[4/7] Rechunking data...\")\n    ds_rechunked = ds_original.chunk(chunks)\n    ds_rechunked = ds_rechunked.unify_chunks()\n    print(f\"      ✓ New chunks: {ds_rechunked.chunks}\")\n    self._logger.info(f\"New chunks: {ds_rechunked.chunks}\")\n\n    # Clear encoding to avoid conflicts\n    for var in ds_rechunked.data_vars:\n        ds_rechunked[var].encoding = {}\n    for coord in ds_rechunked.coords:\n        if \"chunks\" in ds_rechunked[coord].encoding:\n            del ds_rechunked[coord].encoding[\"chunks\"]\n\n    # Write rechunked data first (overwrites entire group)\n    print(f\"\\n[5/7] Writing rechunked data to branch '{temp_branch}'...\")\n    print(\"      This may take several minutes for large datasets...\")\n    with self.writable_session(temp_branch) as session:\n        to_icechunk(ds_rechunked, session, group=group_name, mode=\"w\")\n        session.commit(f\"Wrote rechunked data for {group_name}\")\n    print(\"      ✓ Data written successfully\")\n\n    # Copy subgroups after writing rechunked data\n    print(f\"\\n[6/7] Copying subgroups from '{group_name}'...\")\n    with self.writable_session(temp_branch) as session:\n        with self.readonly_session(source_branch) as icsession:\n            source_group = zarr.open_group(icsession.store, mode=\"r\")[group_name]\n        target_group = zarr.open_group(session.store, mode=\"a\")[group_name]\n\n        subgroup_count = 0\n        for subgroup_name in source_group.group_keys():\n            print(f\"      ✓ Copying subgroup '{subgroup_name}'...\")\n            source_subgroup = source_group[subgroup_name]\n            target_subgroup = target_group.create_group(\n                subgroup_name, overwrite=True\n            )\n\n            # Copy arrays from subgroup\n            for array_name in source_subgroup.array_keys():\n                source_array = source_subgroup[array_name]\n                target_array = target_subgroup.create_array(\n                    array_name,\n                    shape=source_array.shape,\n                    dtype=source_array.dtype,\n                    chunks=source_array.chunks,\n                    overwrite=True,\n                )\n                target_array[:] = source_array[:]\n\n            # Copy subgroup attributes\n            target_subgroup.attrs.update(source_subgroup.attrs)\n            subgroup_count += 1\n\n        if subgroup_count &gt; 0:\n            snapshot_id = session.commit(\n                f\"Rechunked {group_name} with chunks={chunks}\"\n            )\n            print(f\"      ✓ {subgroup_count} subgroups copied\")\n        else:\n            snapshot_id = next(self.repo.ancestry(branch=temp_branch)).id\n            print(\"      ✓ No subgroups to copy\")\n\n    print(f\"      ✓ Snapshot ID: {snapshot_id[:12]}\")\n    self._logger.info(\n        f\"Rechunked data written to branch '{temp_branch}', snapshot={snapshot_id}\"\n    )\n\n    # Promote to main if requested\n    if promote_to_main:\n        print(f\"\\n[7/7] Promoting to '{source_branch}' branch...\")\n        rechunked_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n        self.repo.reset_branch(source_branch, rechunked_snapshot)\n        print(\n            f\"      ✓ Branch '{source_branch}' reset to {rechunked_snapshot[:12]}\"\n        )\n        self._logger.info(\n            f\"Reset branch '{source_branch}' to rechunked snapshot \"\n            f\"{rechunked_snapshot}\"\n        )\n\n        # Delete temp branch if requested\n        if delete_temp_branch:\n            print(f\"      ✓ Deleting temporary branch '{temp_branch}'...\")\n            self.delete_branch(temp_branch)\n            print(\"      ✓ Temporary branch deleted\")\n            self._logger.info(f\"Deleted temporary branch '{temp_branch}'\")\n    else:\n        print(\"\\n[7/7] Skipping promotion (promote_to_main=False)\")\n        print(f\"      Rechunked data available on branch '{temp_branch}'\")\n\n    print(f\"\\n{'=' * 60}\")\n    print(f\"✓ Rechunking complete for '{group_name}'\")\n    print(f\"{'=' * 60}\\n\")\n\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.create_release_tag","level":3,"title":"<code>create_release_tag(tag_name, snapshot_id=None)</code>","text":"<p>Create an immutable tag for an important version.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.create_release_tag--parameters","level":5,"title":"Parameters","text":"<p>tag_name : str     Name for the tag (e.g., \"v2024_complete\", \"before_reprocess\") snapshot_id : str | None     Snapshot to tag. If None, uses current tip of main branch.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def create_release_tag(self, tag_name: str, snapshot_id: str | None = None) -&gt; None:\n    \"\"\"\n    Create an immutable tag for an important version.\n\n    Parameters\n    ----------\n    tag_name : str\n        Name for the tag (e.g., \"v2024_complete\", \"before_reprocess\")\n    snapshot_id : str | None\n        Snapshot to tag. If None, uses current tip of main branch.\n    \"\"\"\n    if snapshot_id is None:\n        # Tag current main branch tip\n        snapshot_id = next(self.repo.ancestry(branch=\"main\")).id\n\n    self.repo.create_tag(tag_name, snapshot_id)\n    self._logger.info(f\"Created tag '{tag_name}' at snapshot {snapshot_id[:8]}\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.list_tags","level":3,"title":"<code>list_tags()</code>","text":"<p>List all tags in the repository.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def list_tags(self) -&gt; list[str]:\n    \"\"\"List all tags in the repository.\"\"\"\n    return list(self.repo.list_tags())\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.delete_tag","level":3,"title":"<code>delete_tag(tag_name)</code>","text":"<p>Delete a tag (use with caution - tags are meant to be permanent).</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def delete_tag(self, tag_name: str) -&gt; None:\n    \"\"\"Delete a tag (use with caution - tags are meant to be permanent).\"\"\"\n    self.repo.delete_tag(tag_name)\n    self._logger.warning(f\"Deleted tag '{tag_name}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.plot_commit_graph","level":3,"title":"<code>plot_commit_graph(max_commits=100)</code>","text":"<p>Visualize commit history as an interactive git-like graph.</p> <p>Creates an interactive visualization showing: - Branches with different colors - Chronological commit ordering - Branch divergence points - Commit messages on hover - Click to see commit details</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.plot_commit_graph--parameters","level":5,"title":"Parameters","text":"<p>max_commits : int     Maximum number of commits to display (default: 100).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.plot_commit_graph--returns","level":5,"title":"Returns","text":"<p>Figure     Interactive plotly figure (works in marimo and Jupyter).</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def plot_commit_graph(self, max_commits: int = 100) -&gt; \"Figure\":\n    \"\"\"\n    Visualize commit history as an interactive git-like graph.\n\n    Creates an interactive visualization showing:\n    - Branches with different colors\n    - Chronological commit ordering\n    - Branch divergence points\n    - Commit messages on hover\n    - Click to see commit details\n\n    Parameters\n    ----------\n    max_commits : int\n        Maximum number of commits to display (default: 100).\n\n    Returns\n    -------\n    Figure\n        Interactive plotly figure (works in marimo and Jupyter).\n    \"\"\"\n    from collections import defaultdict\n    from datetime import datetime\n\n    import plotly.graph_objects as go\n\n    # Collect all commits with full metadata\n    commit_map = {}  # id -&gt; commit data\n    branch_tips = {}  # branch -&gt; latest commit id\n\n    for branch in self.repo.list_branches():\n        ancestors = list(self.repo.ancestry(branch=branch))\n        if ancestors:\n            branch_tips[branch] = ancestors[0].id\n\n        for ancestor in ancestors:\n            if ancestor.id not in commit_map:\n                commit_map[ancestor.id] = {\n                    \"id\": ancestor.id,\n                    \"parent_id\": ancestor.parent_id,\n                    \"message\": ancestor.message,\n                    \"written_at\": ancestor.written_at,\n                    \"branches\": [branch],\n                }\n            else:\n                # Multiple branches point to same commit\n                commit_map[ancestor.id][\"branches\"].append(branch)\n\n            if len(commit_map) &gt;= max_commits:\n                break\n        if len(commit_map) &gt;= max_commits:\n            break\n\n    # Build parent-child relationships\n    commits_list = list(commit_map.values())\n    commits_list.sort(key=lambda c: c[\"written_at\"])  # Oldest first\n\n    # Assign horizontal positions (chronological)\n    commit_x_positions = {}\n    for idx, commit in enumerate(commits_list):\n        commit[\"x\"] = idx\n        commit_x_positions[commit[\"id\"]] = idx\n\n    # Assign vertical positions: commits shared by branches stay on same Y\n    # Only diverge when branches have different commits\n    branch_names = sorted(\n        self.repo.list_branches(), key=lambda b: (b != \"main\", b)\n    )  # main first\n\n    # Build a set of all commit IDs for each branch\n    branch_commits = {}\n    for branch in branch_names:\n        history = list(self.repo.ancestry(branch=branch))\n        branch_commits[branch] = {h.id for h in history if h.id in commit_map}\n\n    # Find where branches diverge\n    def branches_share_commit(\n        commit_id: str,\n        branches: list[str],\n    ) -&gt; list[str]:\n        \"\"\"Return branches that contain a commit.\n\n        Parameters\n        ----------\n        commit_id : str\n            Commit identifier to check.\n        branches : list[str]\n            Branch names to search.\n\n        Returns\n        -------\n        list[str]\n            Branches that contain the commit.\n        \"\"\"\n        return [b for b in branches if commit_id in branch_commits[b]]\n\n    # Assign Y position: all commits on a single horizontal line initially\n    # We'll use vertical offset for parallel branch indicators\n    for commit in commits_list:\n        commit[\"y\"] = 0  # All on same timeline\n        commit[\"branch_set\"] = frozenset(commit[\"branches\"])\n\n    # Color palette for branches\n    colors = [\n        \"#4a9a4a\",  # green (main)\n        \"#5580c8\",  # blue\n        \"#d97643\",  # orange\n        \"#9b59b6\",  # purple\n        \"#e74c3c\",  # red\n        \"#1abc9c\",  # turquoise\n        \"#f39c12\",  # yellow\n        \"#34495e\",  # dark gray\n    ]\n    branch_colors = {b: colors[i % len(colors)] for i, b in enumerate(branch_names)}\n\n    # Build edges: draw parallel lines for shared commits (metro-style)\n    edges_by_branch = defaultdict(list)  # branch -&gt; list of edge dicts\n\n    for commit in commits_list:\n        if commit[\"parent_id\"] and commit[\"parent_id\"] in commit_map:\n            parent = commit_map[commit[\"parent_id\"]]\n\n            # Find which branches share both this commit and its parent\n            shared_branches = [\n                b for b in commit[\"branches\"] if b in parent[\"branches\"]\n            ]\n\n            for branch in shared_branches:\n                edges_by_branch[branch].append(\n                    {\n                        \"x0\": parent[\"x\"],\n                        \"y0\": parent[\"y\"],\n                        \"x1\": commit[\"x\"],\n                        \"y1\": commit[\"y\"],\n                    }\n                )\n\n    # Create plotly figure\n    fig = go.Figure()\n\n    # Draw edges grouped by branch (parallel lines for shared paths)\n    for branch_idx, branch in enumerate(branch_names):\n        if branch not in edges_by_branch:\n            continue\n\n        color = branch_colors[branch]\n\n        # Draw each edge as a separate line\n        for edge in edges_by_branch[branch]:\n            # Vertical offset for parallel lines (metro-style)\n            offset = (branch_idx - (len(branch_names) - 1) / 2) * 0.15\n\n            fig.add_trace(\n                go.Scatter(\n                    x=[edge[\"x0\"], edge[\"x1\"]],\n                    y=[edge[\"y0\"] + offset, edge[\"y1\"] + offset],\n                    mode=\"lines\",\n                    line=dict(color=color, width=3),\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                    opacity=0.7,\n                )\n            )\n\n    # Draw commits (nodes) - one trace per unique commit\n    # Color by which branches include it\n    x_vals = [c[\"x\"] for c in commits_list]\n    y_vals = [c[\"y\"] for c in commits_list]\n\n    # Format hover text\n    hover_texts = []\n    marker_colors = []\n    marker_symbols = []\n\n    for c in commits_list:\n        # Handle both string and datetime objects\n        if isinstance(c[\"written_at\"], str):\n            time_str = datetime.fromisoformat(c[\"written_at\"]).strftime(\n                \"%Y-%m-%d %H:%M\"\n            )\n        else:\n            time_str = c[\"written_at\"].strftime(\"%Y-%m-%d %H:%M\")\n\n        branches_str = \", \".join(c[\"branches\"])\n        hover_texts.append(\n            f\"&lt;b&gt;{c['message'] or 'No message'}&lt;/b&gt;&lt;br&gt;\"\n            f\"Commit: {c['id'][:12]}&lt;br&gt;\"\n            f\"Branches: {branches_str}&lt;br&gt;\"\n            f\"Time: {time_str}\"\n        )\n\n        # Color by first branch (priority: main)\n        if \"main\" in c[\"branches\"]:\n            marker_colors.append(branch_colors[\"main\"])\n        else:\n            marker_colors.append(branch_colors[c[\"branches\"][0]])\n\n        # Star for branch tips\n        if c[\"id\"] in branch_tips.values():\n            marker_symbols.append(\"star\")\n        else:\n            marker_symbols.append(\"circle\")\n\n    fig.add_trace(\n        go.Scatter(\n            x=x_vals,\n            y=y_vals,\n            mode=\"markers\",\n            name=\"Commits\",\n            marker=dict(\n                size=14,\n                color=marker_colors,\n                symbol=marker_symbols,\n                line=dict(color=\"white\", width=2),\n            ),\n            hovertext=hover_texts,\n            hoverinfo=\"text\",\n            showlegend=False,\n        )\n    )\n\n    # Add legend traces (invisible points just for legend)\n    for branch_idx, branch in enumerate(branch_names):\n        fig.add_trace(\n            go.Scatter(\n                x=[None],\n                y=[None],\n                mode=\"markers\",\n                name=branch,\n                marker=dict(\n                    size=10,\n                    color=branch_colors[branch],\n                    line=dict(color=\"white\", width=2),\n                ),\n                showlegend=True,\n            )\n        )\n\n    # Layout styling\n    title_text = (\n        f\"Commit Graph: {self.site_name} ({len(commits_list)} commits, \"\n        f\"{len(branch_names)} branches)\"\n    )\n    fig.update_layout(\n        title=dict(\n            text=title_text,\n            font=dict(size=16, color=\"#e5e5e5\"),\n        ),\n        xaxis=dict(\n            title=\"Time (oldest ← → newest)\",\n            showticklabels=False,\n            showgrid=True,\n            gridcolor=\"rgba(255,255,255,0.1)\",\n            zeroline=False,\n        ),\n        yaxis=dict(\n            title=\"\",\n            showticklabels=False,\n            showgrid=False,\n            zeroline=False,\n            range=[-1, 1],  # Fixed range for single timeline\n        ),\n        plot_bgcolor=\"#1a1a1a\",\n        paper_bgcolor=\"#1a1a1a\",\n        font=dict(color=\"#e5e5e5\"),\n        hovermode=\"closest\",\n        height=400,\n        width=max(800, len(commits_list) * 50),\n        legend=dict(\n            title=\"Branches\",\n            orientation=\"h\",\n            x=0,\n            y=-0.15,\n            bgcolor=\"rgba(30,30,30,0.8)\",\n            bordercolor=\"rgba(255,255,255,0.2)\",\n            borderwidth=1,\n        ),\n    )\n\n    return fig\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.cleanup_stale_branches","level":3,"title":"<code>cleanup_stale_branches(keep_patterns=None)</code>","text":"<p>Delete stale temporary branches (e.g., from failed rechunking).</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.cleanup_stale_branches--parameters","level":5,"title":"Parameters","text":"<p>keep_patterns : list[str] | None     Patterns to preserve. Default: [\"main\", \"dev\"]</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.cleanup_stale_branches--returns","level":5,"title":"Returns","text":"<p>list[str]     Names of deleted branches</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def cleanup_stale_branches(\n    self, keep_patterns: list[str] | None = None\n) -&gt; list[str]:\n    \"\"\"\n    Delete stale temporary branches (e.g., from failed rechunking).\n\n    Parameters\n    ----------\n    keep_patterns : list[str] | None\n        Patterns to preserve. Default: [\"main\", \"dev\"]\n\n    Returns\n    -------\n    list[str]\n        Names of deleted branches\n    \"\"\"\n    if keep_patterns is None:\n        keep_patterns = [\"main\", \"dev\"]\n\n    deleted = []\n\n    for branch in self.repo.list_branches():\n        # Keep if matches any pattern\n        should_keep = any(pattern in branch for pattern in keep_patterns)\n\n        if not should_keep:\n            # Check if it's a temp branch from rechunking\n            if \"_rechunked_temp\" in branch or \"_temp\" in branch:\n                try:\n                    self.repo.delete_branch(branch)\n                    deleted.append(branch)\n                    self._logger.info(f\"Deleted stale branch: {branch}\")\n                except Exception as e:\n                    self._logger.warning(f\"Failed to delete branch {branch}: {e}\")\n\n    return deleted\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.delete_branch","level":3,"title":"<code>delete_branch(branch_name)</code>","text":"<p>Delete a branch.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def delete_branch(self, branch_name: str) -&gt; None:\n    \"\"\"Delete a branch.\"\"\"\n    if branch_name == \"main\":\n        raise ValueError(\"Cannot delete 'main' branch\")\n\n    self.repo.delete_branch(branch_name)\n    self._logger.info(f\"Deleted branch '{branch_name}'\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_snapshot_info","level":3,"title":"<code>get_snapshot_info(snapshot_id)</code>","text":"<p>Get detailed information about a specific snapshot.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_snapshot_info--parameters","level":5,"title":"Parameters","text":"<p>snapshot_id : str     Snapshot ID to inspect</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.get_snapshot_info--returns","level":5,"title":"Returns","text":"<p>dict     Snapshot metadata and statistics</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def get_snapshot_info(self, snapshot_id: str) -&gt; dict:\n    \"\"\"\n    Get detailed information about a specific snapshot.\n\n    Parameters\n    ----------\n    snapshot_id : str\n        Snapshot ID to inspect\n\n    Returns\n    -------\n    dict\n        Snapshot metadata and statistics\n    \"\"\"\n    # Find the snapshot in ancestry\n    for ancestor in self.repo.ancestry(branch=\"main\"):\n        if ancestor.id == snapshot_id or ancestor.id.startswith(snapshot_id):\n            info = {\n                \"snapshot_id\": ancestor.id,\n                \"message\": ancestor.message,\n                \"written_at\": ancestor.written_at,\n                \"parent_id\": ancestor.parent_id,\n            }\n\n            # Try to get groups at this snapshot\n            try:\n                session = self.repo.readonly_session(snapshot_id=ancestor.id)\n                root = zarr.open(session.store, mode=\"r\")\n                info[\"groups\"] = list(root.group_keys())\n                info[\"arrays\"] = list(root.array_keys())\n            except Exception as e:\n                self._logger.warning(f\"Could not inspect snapshot contents: {e}\")\n\n            return info\n\n    raise ValueError(f\"Snapshot {snapshot_id} not found in history\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.compare_snapshots","level":3,"title":"<code>compare_snapshots(snapshot_id_1, snapshot_id_2)</code>","text":"<p>Compare two snapshots to see what changed.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.compare_snapshots--parameters","level":5,"title":"Parameters","text":"<p>snapshot_id_1 : str     First snapshot (older) snapshot_id_2 : str     Second snapshot (newer)</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.compare_snapshots--returns","level":5,"title":"Returns","text":"<p>dict     Comparison results showing added/removed/modified groups</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def compare_snapshots(self, snapshot_id_1: str, snapshot_id_2: str) -&gt; dict:\n    \"\"\"\n    Compare two snapshots to see what changed.\n\n    Parameters\n    ----------\n    snapshot_id_1 : str\n        First snapshot (older)\n    snapshot_id_2 : str\n        Second snapshot (newer)\n\n    Returns\n    -------\n    dict\n        Comparison results showing added/removed/modified groups\n    \"\"\"\n    info_1 = self.get_snapshot_info(snapshot_id_1)\n    info_2 = self.get_snapshot_info(snapshot_id_2)\n\n    groups_1 = set(info_1.get(\"groups\", []))\n    groups_2 = set(info_2.get(\"groups\", []))\n\n    return {\n        \"snapshot_1\": snapshot_id_1[:8],\n        \"snapshot_2\": snapshot_id_2[:8],\n        \"added_groups\": list(groups_2 - groups_1),\n        \"removed_groups\": list(groups_1 - groups_2),\n        \"common_groups\": list(groups_1 &amp; groups_2),\n        \"time_diff\": (info_2[\"written_at\"] - info_1[\"written_at\"]).total_seconds(),\n    }\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.maintenance","level":3,"title":"<code>maintenance(expire_days=7, cleanup_branches=True, run_gc=True)</code>","text":"<p>Run full maintenance on the store.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.maintenance--parameters","level":5,"title":"Parameters","text":"<p>expire_days : int     Days of snapshot history to keep cleanup_branches : bool     Remove stale temporary branches run_gc : bool     Run garbage collection after expiration</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.maintenance--returns","level":5,"title":"Returns","text":"<p>dict     Summary of maintenance actions</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def maintenance(\n    self, expire_days: int = 7, cleanup_branches: bool = True, run_gc: bool = True\n) -&gt; dict:\n    \"\"\"\n    Run full maintenance on the store.\n\n    Parameters\n    ----------\n    expire_days : int\n        Days of snapshot history to keep\n    cleanup_branches : bool\n        Remove stale temporary branches\n    run_gc : bool\n        Run garbage collection after expiration\n\n    Returns\n    -------\n    dict\n        Summary of maintenance actions\n    \"\"\"\n    self._logger.info(f\"Starting maintenance on {self.store_type}\")\n\n    results = {\"expired_snapshots\": 0, \"deleted_branches\": [], \"gc_summary\": None}\n\n    # Expire old snapshots\n    expired_ids = self.expire_old_snapshots(days=expire_days)\n    results[\"expired_snapshots\"] = len(expired_ids)\n\n    # Cleanup stale branches\n    if cleanup_branches:\n        deleted_branches = self.cleanup_stale_branches()\n        results[\"deleted_branches\"] = deleted_branches\n\n    # Garbage collection\n    if run_gc:\n        from datetime import datetime, timedelta\n\n        cutoff = datetime.now(UTC) - timedelta(days=expire_days)\n        gc_summary = self.repo.garbage_collect(delete_object_older_than=cutoff)\n        results[\"gc_summary\"] = {\n            \"bytes_deleted\": gc_summary.bytes_deleted,\n            \"chunks_deleted\": gc_summary.chunks_deleted,\n            \"manifests_deleted\": gc_summary.manifests_deleted,\n        }\n\n    self._logger.info(f\"Maintenance complete: {results}\")\n    return results\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.sanitize_store","level":3,"title":"<code>sanitize_store(source_branch='main', temp_branch='sanitize_temp', promote_to_main=True, delete_temp_branch=True)</code>","text":"<p>Sanitize all groups by removing NaN-only SIDs and cleaning coordinates.</p> <p>Creates a temporary branch, applies sanitization to all groups, then optionally promotes to main and cleans up.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.sanitize_store--parameters","level":5,"title":"Parameters","text":"<p>source_branch : str, default \"main\"     Branch to read original data from. temp_branch : str, default \"sanitize_temp\"     Temporary branch name for sanitized data. promote_to_main : bool, default True     If True, reset main branch to sanitized snapshot after writing. delete_temp_branch : bool, default True     If True, delete temporary branch after promotion.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.sanitize_store--returns","level":5,"title":"Returns","text":"<p>str     Snapshot ID of the sanitized data.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def sanitize_store(\n    self,\n    source_branch: str = \"main\",\n    temp_branch: str = \"sanitize_temp\",\n    promote_to_main: bool = True,\n    delete_temp_branch: bool = True,\n) -&gt; str:\n    \"\"\"\n    Sanitize all groups by removing NaN-only SIDs and cleaning coordinates.\n\n    Creates a temporary branch, applies sanitization to all groups, then\n    optionally promotes to main and cleans up.\n\n    Parameters\n    ----------\n    source_branch : str, default \"main\"\n        Branch to read original data from.\n    temp_branch : str, default \"sanitize_temp\"\n        Temporary branch name for sanitized data.\n    promote_to_main : bool, default True\n        If True, reset main branch to sanitized snapshot after writing.\n    delete_temp_branch : bool, default True\n        If True, delete temporary branch after promotion.\n\n    Returns\n    -------\n    str\n        Snapshot ID of the sanitized data.\n    \"\"\"\n    import time\n\n    from icechunk.xarray import to_icechunk\n\n    print(f\"\\n{'=' * 60}\")\n    print(\"Starting store sanitization\")\n    print(f\"{'=' * 60}\\n\")\n\n    # Step 1: Get current snapshot\n    print(f\"[1/6] Getting current snapshot from '{source_branch}'...\")\n    current_snapshot = next(self.repo.ancestry(branch=source_branch)).id\n    print(f\"      ✓ Current snapshot: {current_snapshot[:12]}\")\n\n    # Step 2: Create temp branch\n    print(f\"\\n[2/6] Creating temporary branch '{temp_branch}'...\")\n    try:\n        self.repo.create_branch(temp_branch, current_snapshot)\n        print(f\"      ✓ Branch '{temp_branch}' created\")\n    except Exception:\n        print(\"      ⚠ Branch exists, deleting and recreating...\")\n        self.delete_branch(temp_branch)\n        self.repo.create_branch(temp_branch, current_snapshot)\n        print(f\"      ✓ Branch '{temp_branch}' created\")\n\n    # Step 3: Get all groups\n    print(\"\\n[3/6] Discovering groups...\")\n    groups = self.list_groups()\n    print(f\"      ✓ Found {len(groups)} groups: {groups}\")\n\n    # Step 4: Sanitize each group\n    print(\"\\n[4/6] Sanitizing groups...\")\n    sanitized_count = 0\n\n    for group_name in groups:\n        print(f\"\\n      Processing '{group_name}'...\")\n        t_start = time.time()\n\n        try:\n            # Read original data\n            ds_original = self.read_group(group_name, branch=source_branch)\n            original_sids = len(ds_original.sid)\n            print(f\"        • Original: {original_sids} SIDs\")\n\n            # Sanitize: remove SIDs with all-NaN data\n            ds_sanitized = self._sanitize_dataset(ds_original)\n            sanitized_sids = len(ds_sanitized.sid)\n            removed_sids = original_sids - sanitized_sids\n\n            print(\n                f\"        • Sanitized: {sanitized_sids} SIDs \"\n                f\"(removed {removed_sids})\"\n            )\n\n            # Write sanitized data\n            with self.writable_session(temp_branch) as session:\n                to_icechunk(ds_sanitized, session, group=group_name, mode=\"w\")\n\n                # Copy metadata subgroups if they exist\n                try:\n                    with self.readonly_session(source_branch) as read_session:\n                        source_group = zarr.open_group(\n                            read_session.store, mode=\"r\"\n                        )[group_name]\n                        if \"metadata\" in source_group.group_keys():\n                            # Copy entire metadata subgroup\n                            dest_group = zarr.open_group(session.store)[group_name]\n                            zarr.copy(\n                                source_group[\"metadata\"],\n                                dest_group,\n                                name=\"metadata\",\n                            )\n                            print(\"        • Copied metadata subgroup\")\n                except Exception as e:\n                    print(f\"        ⚠ Could not copy metadata: {e}\")\n\n                session.commit(\n                    f\"Sanitized {group_name}: removed {removed_sids} empty SIDs\"\n                )\n\n            t_elapsed = time.time() - t_start\n            print(f\"        ✓ Completed in {t_elapsed:.2f}s\")\n            sanitized_count += 1\n\n        except Exception as e:\n            print(f\"        ✗ Failed: {e}\")\n            continue\n\n    print(f\"\\n      ✓ Sanitized {sanitized_count}/{len(groups)} groups\")\n\n    # Step 5: Get final snapshot\n    print(\"\\n[5/6] Getting sanitized snapshot...\")\n    sanitized_snapshot = next(self.repo.ancestry(branch=temp_branch)).id\n    print(f\"      ✓ Snapshot: {sanitized_snapshot[:12]}\")\n\n    # Step 6: Promote to main\n    if promote_to_main:\n        print(f\"\\n[6/6] Promoting to '{source_branch}' branch...\")\n        self.repo.reset_branch(source_branch, sanitized_snapshot)\n        print(\n            f\"      ✓ Branch '{source_branch}' reset to {sanitized_snapshot[:12]}\"\n        )\n\n        if delete_temp_branch:\n            print(f\"      ✓ Deleting temporary branch '{temp_branch}'...\")\n            self.delete_branch(temp_branch)\n            print(\"      ✓ Temporary branch deleted\")\n    else:\n        print(\"\\n[6/6] Skipping promotion (promote_to_main=False)\")\n        print(f\"      Sanitized data available on branch '{temp_branch}'\")\n\n    print(f\"\\n{'=' * 60}\")\n    print(\"✓ Sanitization complete\")\n    print(f\"{'=' * 60}\\n\")\n\n    return sanitized_snapshot\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.safe_temporal_aggregate","level":3,"title":"<code>safe_temporal_aggregate(group, freq='1D', vars_to_aggregate=('VOD',), geometry_vars=('phi', 'theta'), drop_empty=True, branch='main')</code>","text":"<p>Safely aggregate temporally irregular VOD data.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.safe_temporal_aggregate--parameters","level":5,"title":"Parameters","text":"<p>group : str     Group name to aggregate. freq : str, default \"1D\"     Resample frequency string. vars_to_aggregate : Sequence[str], optional     Variables to aggregate using mean. geometry_vars : Sequence[str], optional     Geometry variables to preserve via first() per bin. drop_empty : bool, default True     Drop empty epochs after aggregation. branch : str, default \"main\"     Branch name to read from.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.safe_temporal_aggregate--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Aggregated dataset.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def safe_temporal_aggregate(\n    self,\n    group: str,\n    freq: str = \"1D\",\n    vars_to_aggregate: Sequence[str] = (\"VOD\",),\n    geometry_vars: Sequence[str] = (\"phi\", \"theta\"),\n    drop_empty: bool = True,\n    branch: str = \"main\",\n) -&gt; xr.Dataset:\n    \"\"\"Safely aggregate temporally irregular VOD data.\n\n    Parameters\n    ----------\n    group : str\n        Group name to aggregate.\n    freq : str, default \"1D\"\n        Resample frequency string.\n    vars_to_aggregate : Sequence[str], optional\n        Variables to aggregate using mean.\n    geometry_vars : Sequence[str], optional\n        Geometry variables to preserve via first() per bin.\n    drop_empty : bool, default True\n        Drop empty epochs after aggregation.\n    branch : str, default \"main\"\n        Branch name to read from.\n\n    Returns\n    -------\n    xr.Dataset\n        Aggregated dataset.\n    \"\"\"\n\n    with self.readonly_session(branch=branch) as session:\n        ds = xr.open_zarr(session.store, group=group, consolidated=False)\n\n        print(\n            f\"📦 Aggregating group '{group}' from branch '{branch}' → freq={freq}\"\n        )\n\n        # 1️⃣ Aggregate numeric variables\n        merged_vars = []\n        for var in vars_to_aggregate:\n            if var in ds:\n                merged_vars.append(ds[var].resample(epoch=freq).mean())\n            else:\n                print(f\"⚠️ Skipping missing variable: {var}\")\n        ds_agg = xr.merge(merged_vars)\n\n        # 2️⃣ Preserve geometry variables (use first() per bin)\n        for var in geometry_vars:\n            if var in ds:\n                ds_agg[var] = ds[var].resample(epoch=freq).first()\n\n        # 3️⃣ Add remaining coordinates\n        for coord in ds.coords:\n            if coord not in ds_agg.coords and coord != \"epoch\":\n                ds_agg[coord] = ds[coord]\n\n        # 4️⃣ Drop all-NaN epochs if requested\n        if drop_empty and \"VOD\" in ds_agg:\n            valid_mask = ds_agg[\"VOD\"].notnull().any(dim=\"sid\").compute()\n            ds_agg = ds_agg.isel(epoch=valid_mask)\n\n        print(f\"✅ Aggregation done: {dict(ds_agg.sizes)}\")\n        return ds_agg\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.safe_temporal_aggregate_to_branch","level":3,"title":"<code>safe_temporal_aggregate_to_branch(source_group, target_group, target_branch, freq='1D', overwrite=False, **kwargs)</code>","text":"<p>Aggregate a group and save to a new Icechunk branch/group.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.safe_temporal_aggregate_to_branch--parameters","level":5,"title":"Parameters","text":"<p>source_group : str     Source group name. target_group : str     Target group name. target_branch : str     Target branch name. freq : str, default \"1D\"     Resample frequency string. overwrite : bool, default False     Whether to overwrite an existing branch. **kwargs : Any     Additional keyword args passed to safe_temporal_aggregate().</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.MyIcechunkStore.safe_temporal_aggregate_to_branch--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Aggregated dataset written to the target branch.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def safe_temporal_aggregate_to_branch(\n    self,\n    source_group: str,\n    target_group: str,\n    target_branch: str,\n    freq: str = \"1D\",\n    overwrite: bool = False,\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"Aggregate a group and save to a new Icechunk branch/group.\n\n    Parameters\n    ----------\n    source_group : str\n        Source group name.\n    target_group : str\n        Target group name.\n    target_branch : str\n        Target branch name.\n    freq : str, default \"1D\"\n        Resample frequency string.\n    overwrite : bool, default False\n        Whether to overwrite an existing branch.\n    **kwargs : Any\n        Additional keyword args passed to safe_temporal_aggregate().\n\n    Returns\n    -------\n    xr.Dataset\n        Aggregated dataset written to the target branch.\n    \"\"\"\n\n    print(\n        f\"🚀 Creating new aggregated branch '{target_branch}' at '{target_group}'\"\n    )\n\n    # Compute safe aggregation\n    ds_agg = self.safe_temporal_aggregate(\n        group=source_group,\n        freq=freq,\n        **kwargs,\n    )\n\n    # Write to new branch\n    current_snapshot = next(self.repo.ancestry(branch=\"main\")).id\n    self.delete_branch(target_branch)\n    self.repo.create_branch(target_branch, current_snapshot)\n    with self.writable_session(target_branch) as session:\n        to_icechunk(\n            obj=ds_agg,\n            session=session,\n            group=target_group,\n            mode=\"w\",\n        )\n        session.commit(f\"Saved aggregated data to {target_group} at freq={freq}\")\n\n    print(\n        f\"✅ Saved aggregated dataset to branch '{target_branch}' \"\n        f\"(group '{target_group}')\"\n    )\n    return ds_agg\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.create_rinex_store","level":2,"title":"<code>create_rinex_store(store_path)</code>","text":"<p>Create a RINEX Icechunk store with appropriate configuration.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.create_rinex_store--parameters","level":4,"title":"Parameters","text":"<p>store_path : Path     Path to the store directory.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.create_rinex_store--returns","level":4,"title":"Returns","text":"<p>MyIcechunkStore     Configured store for RINEX data.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def create_rinex_store(store_path: Path) -&gt; MyIcechunkStore:\n    \"\"\"\n    Create a RINEX Icechunk store with appropriate configuration.\n\n    Parameters\n    ----------\n    store_path : Path\n        Path to the store directory.\n\n    Returns\n    -------\n    MyIcechunkStore\n        Configured store for RINEX data.\n    \"\"\"\n    return MyIcechunkStore(store_path=store_path, store_type=\"rinex_store\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.create_vod_store","level":2,"title":"<code>create_vod_store(store_path)</code>","text":"<p>Create a VOD Icechunk store with appropriate configuration.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.create_vod_store--parameters","level":4,"title":"Parameters","text":"<p>store_path : Path     Path to the store directory.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.create_vod_store--returns","level":4,"title":"Returns","text":"<p>MyIcechunkStore     Configured store for VOD analysis data.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def create_vod_store(store_path: Path) -&gt; MyIcechunkStore:\n    \"\"\"\n    Create a VOD Icechunk store with appropriate configuration.\n\n    Parameters\n    ----------\n    store_path : Path\n        Path to the store directory.\n\n    Returns\n    -------\n    MyIcechunkStore\n        Configured store for VOD analysis data.\n    \"\"\"\n    return MyIcechunkStore(store_path=store_path, store_type=\"vod_store\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.store.write_vod_to_store","level":2,"title":"<code>write_vod_to_store(vod_store, group_name, vod_ds, canopy_hash, sky_hash, commit_msg='VOD calculation')</code>","text":"<p>Write VOD data to store with metadata tracking.</p> Source code in <code>packages/canvod-store/src/canvod/store/store.py</code> <pre><code>def write_vod_to_store(\n    vod_store: MyIcechunkStore,\n    group_name: str,\n    vod_ds: xr.Dataset,\n    canopy_hash: str,\n    sky_hash: str,\n    commit_msg: str = \"VOD calculation\",\n) -&gt; str:\n    \"\"\"Write VOD data to store with metadata tracking.\"\"\"\n\n    with vod_store.writable_session() as session:\n        vod_store.write_dataset(dataset=vod_ds, group_name=group_name, session=session)\n\n        start = vod_ds[\"epoch\"].values[0]\n        end = vod_ds[\"epoch\"].values[-1]\n\n        vod_store.append_metadata(\n            group_name=group_name,\n            rinex_hash=f\"{canopy_hash}_{sky_hash}\",\n            start=start,\n            end=end,\n            snapshot_id=session.snapshot_id,\n            action=\"insert\",\n            commit_msg=commit_msg,\n            dataset_attrs=dict(vod_ds.attrs),\n        )\n\n        snapshot_id = session.commit(commit_msg)\n\n    return snapshot_id\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#data-reader","level":2,"title":"Data Reader","text":"<p>Icechunk-backed readers for RINEX datasets.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader","level":2,"title":"<code>IcechunkDataReader</code>","text":"<p>Replacement for RinexFilesParser that reads from Icechunk stores.</p> <p>This class provides a similar interface to the old parser but reads pre-processed data from Icechunk stores instead of processing RINEX files.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader--parameters","level":4,"title":"Parameters","text":"<p>matched_dirs : MatchedDirs     Directory information for date/location matching site_name : str, optional     Name of research site (defaults to DEFAULT_RESEARCH_SITE) n_max_workers : int, optional     Maximum number of workers for parallel operations (defaults to N_MAX_THREADS) enable_gc : bool, optional     Whether to enable garbage collection between operations (default True) gc_delay : float, optional     Delay in seconds after garbage collection (default 1.0)</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>class IcechunkDataReader:\n    \"\"\"\n    Replacement for RinexFilesParser that reads from Icechunk stores.\n\n    This class provides a similar interface to the old parser but reads\n    pre-processed data from Icechunk stores instead of processing RINEX files.\n\n    Parameters\n    ----------\n    matched_dirs : MatchedDirs\n        Directory information for date/location matching\n    site_name : str, optional\n        Name of research site (defaults to DEFAULT_RESEARCH_SITE)\n    n_max_workers : int, optional\n        Maximum number of workers for parallel operations (defaults to N_MAX_THREADS)\n    enable_gc : bool, optional\n        Whether to enable garbage collection between operations (default True)\n    gc_delay : float, optional\n        Delay in seconds after garbage collection (default 1.0)\n    \"\"\"\n\n    def __init__(\n        self,\n        matched_dirs: MatchedDirs,\n        site_name: str | None = None,\n        n_max_workers: int | None = None,\n        enable_gc: bool = True,\n        gc_delay: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the Icechunk data reader.\n\n        Parameters\n        ----------\n        matched_dirs : MatchedDirs\n            Directory information for date/location matching.\n        site_name : str | None, optional\n            Name of research site. If ``None``, uses the first site from config.\n        n_max_workers : int | None, optional\n            Maximum number of workers for parallel operations. Defaults to config.\n        enable_gc : bool, optional\n            Whether to enable garbage collection between operations.\n        gc_delay : float, optional\n            Delay in seconds after garbage collection.\n        \"\"\"\n        config = load_config()\n        if site_name is None:\n            site_name = next(iter(config.sites.sites))\n        if n_max_workers is None:\n            n_max_workers = config.processing.processing.n_max_threads\n\n        self.matched_dirs = matched_dirs\n        self.site_name = site_name\n        self.n_max_workers = n_max_workers\n        self.enable_gc = enable_gc\n        self.gc_delay = gc_delay\n\n        self._logger = get_logger(__name__).bind(\n            site=site_name,\n            date=matched_dirs.yyyydoy.to_str(),\n        )\n        self._site = GnssResearchSite(site_name)\n\n        date_obj = self.matched_dirs.yyyydoy.date\n        self._start_time = datetime.combine(date_obj, datetime.min.time())\n        self._end_time = datetime.combine(date_obj, datetime.max.time())\n        self._time_range = (self._start_time, self._end_time)\n\n        # ✅ single persistent pool\n        self._pool = ProcessPoolExecutor(\n            max_workers=min(self.n_max_workers, 16),\n        )\n\n        self._logger.info(\n            f\"Initialized Icechunk data reader for {matched_dirs.yyyydoy.to_str()}\"\n        )\n\n    def __del__(self) -&gt; None:\n        \"\"\"Ensure the pool is shut down when the reader is deleted.\"\"\"\n        try:\n            self._pool.shutdown(wait=True)\n        except Exception:\n            pass\n\n    def _memory_cleanup(self) -&gt; None:\n        \"\"\"Perform memory cleanup if enabled.\"\"\"\n        if self.enable_gc:\n            gc.collect()\n            if self.gc_delay &gt; 0:\n                time.sleep(self.gc_delay)\n\n    def get_receiver_by_type(self, receiver_type: str) -&gt; list[str]:\n        \"\"\"\n        Get list of receiver names by type.\n\n        Parameters\n        ----------\n        receiver_type : str\n            Type of receiver ('canopy', 'reference')\n\n        Returns\n        -------\n        list[str]\n            List of receiver names of the specified type\n        \"\"\"\n        return [\n            name\n            for name, config in self._site.active_receivers.items()\n            if config[\"type\"] == receiver_type\n        ]\n\n    def _get_receiver_name_for_type(self, receiver_type: str) -&gt; str | None:\n        \"\"\"Get the first configured receiver name for a given type.\"\"\"\n        for name, config in self._site.active_receivers.items():\n            if config[\"type\"] == receiver_type:\n                return name\n        return None\n\n    def _memory_cleanup(self) -&gt; None:\n        \"\"\"Perform memory cleanup if enabled.\"\"\"\n        if self.enable_gc:\n            gc.collect()\n            if self.gc_delay &gt; 0:\n                time.sleep(self.gc_delay)\n\n    def parsed_rinex_data_gen_v2(\n        self,\n        keep_vars: list[str] | None = None,\n        receiver_types: list[str] | None = None,\n    ) -&gt; Generator[xr.Dataset]:\n        \"\"\"\n        Generator that processes RINEX files, augments them (φ, θ, r),\n        and appends to Icechunk stores on-the-fly.\n\n        Yields enriched daily datasets (already augmented).\n        \"\"\"\n\n        if receiver_types is None:\n            receiver_types = [\"canopy\", \"reference\"]\n\n        self._logger.info(\n            f\"Starting RINEX processing and ingestion for types: {receiver_types}\"\n        )\n\n        # --- 1) Cache auxiliaries once per day ---\n        from canvodpy.orchestrator import RinexDataProcessor\n\n        processor = RinexDataProcessor(\n            matched_data_dirs=self.matched_dirs, icechunk_reader=self\n        )\n\n        ephem_ds = prep_aux_ds(processor.get_ephemeride_ds())\n        clk_ds = prep_aux_ds(processor.get_clk_ds())\n\n        aux_ds_dict = {\"ephem\": ephem_ds, \"clk\": clk_ds}\n        approx_pos = None  # computed on first canopy dataset\n\n        for receiver_type in receiver_types:\n            # --- resolve dirs and names ---\n            if receiver_type == \"canopy\":\n                rinex_dir = self.matched_dirs.canopy_data_dir\n                receiver_name = self._get_receiver_name_for_type(\"canopy\")\n                store_group = \"canopy\"\n            elif receiver_type == \"reference\":\n                rinex_dir = self.matched_dirs.reference_data_dir\n                receiver_name = self._get_receiver_name_for_type(\"reference\")\n                store_group = \"reference\"\n            else:\n                self._logger.warning(f\"Unknown receiver type: {receiver_type}\")\n                continue\n\n            if not receiver_name:\n                self._logger.warning(f\"No configured receiver for type {receiver_type}\")\n                continue\n\n            rinex_files = self._get_rinex_files(rinex_dir)\n            if not rinex_files:\n                self._logger.warning(f\"No RINEX files found in {rinex_dir}\")\n                continue\n\n            self._logger.info(\n                f\"Processing {len(rinex_files)} RINEX files for {receiver_type}\"\n            )\n\n            # --- parallel preprocessing ---\n            futures = {\n                self._pool.submit(preprocess_rnx, f, keep_vars): f for f in rinex_files\n            }\n            results: list[tuple[Path, xr.Dataset]] = []\n\n            for fut in tqdm(\n                as_completed(futures),\n                total=len(futures),\n                desc=f\"Processing {receiver_type}\",\n            ):\n                try:\n                    fname, ds = fut.result()\n                    results.append((fname, ds))\n                except Exception as e:\n                    self._logger.error(f\"Failed preprocessing: {e}\")\n\n            results.sort(key=lambda x: x[0].name)  # chronological order\n\n            # --- per-file commit ---\n            for idx, (fname, ds) in enumerate(results):\n                log = self._logger.bind(file=str(fname))\n                try:\n                    rel_path = self._site.rinex_store.rel_path_for_commit(fname)\n                    version = get_version_from_pyproject()\n\n                    rinex_hash = ds.attrs.get(\"RINEX File Hash\")\n                    if not rinex_hash:\n                        log.warning(\"Dataset missing hash → skipping\")\n                        continue\n\n                    start_epoch = np.datetime64(ds.epoch.min().values)\n                    end_epoch = np.datetime64(ds.epoch.max().values)\n\n                    exists, matches = self._site.rinex_store.metadata_row_exists(\n                        store_group, rinex_hash, start_epoch, end_epoch\n                    )\n\n                    ds = self._site.rinex_store._cleanse_dataset_attrs(ds)\n\n                    # --- 2) Compute approx_pos once from first canopy file ---\n                    if receiver_type == \"canopy\" and approx_pos is None:\n                        approx_pos = processor.get_approx_position(ds)\n\n                    # --- 3) Augment with φ, θ, r ---\n                    matched = processor.match_datasets(ds, **aux_ds_dict)\n                    ephem_matched = matched[\"ephem\"]\n                    ds = processor.add_azi_ele(\n                        rnx_obs_ds=ds,\n                        ephem_ds=ephem_matched,\n                        rx_x=approx_pos.x,\n                        rx_y=approx_pos.y,\n                        rx_z=approx_pos.z,\n                    )\n\n                    # --- 4) Store to Icechunk ---\n                    existing_groups = self._site.rinex_store.list_groups()\n                    if not exists and store_group not in existing_groups and idx == 0:\n                        msg = (\n                            f\"[v{version}] Initial commit {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch})\"\n                        )\n                        self._site.rinex_store.write_initial_group(\n                            dataset=ds, group_name=store_group, commit_message=msg\n                        )\n                        log.info(msg)\n                        continue\n\n                    match (exists, self._site.rinex_store._rinex_store_strategy):\n                        case (True, \"skip\"):\n                            log.info(f\"[v{version}] Skipped {rel_path}\")\n                            # just metadata row\n                            self._site.rinex_store.append_metadata(\n                                group_name=store_group,\n                                rinex_hash=rinex_hash,\n                                start=start_epoch,\n                                end=end_epoch,\n                                snapshot_id=\"none\",\n                                action=\"skip\",\n                                commit_msg=\"skip\",\n                                dataset_attrs=ds.attrs,\n                            )\n\n                        case (True, \"overwrite\"):\n                            msg = f\"[v{version}] Overwrote {rel_path}\"\n                            self._site.rinex_store.overwrite_file_in_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                rinex_hash=rinex_hash,\n                                start=start_epoch,\n                                end=end_epoch,\n                                commit_message=msg,\n                            )\n\n                        case (True, \"append\"):\n                            msg = f\"[v{version}] Appended {rel_path}\"\n                            self._site.rinex_store.append_to_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                append_dim=\"epoch\",\n                                action=\"append\",\n                                commit_message=msg,\n                            )\n\n                        case (False, _):\n                            msg = f\"[v{version}] Wrote {rel_path}\"\n                            self._site.rinex_store.append_to_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                append_dim=\"epoch\",\n                                action=\"write\",\n                                commit_message=msg,\n                            )\n                except Exception as e:\n                    log.exception(\"file_commit_failed\", error=str(e))\n                    raise\n\n                self._memory_cleanup()\n\n            # --- 5) Yield full daily dataset (already enriched) ---\n            final_ds = self._site.read_receiver_data(store_group, self._time_range)\n            self._logger.info(\n                f\"Yielding {receiver_type} dataset: {dict(final_ds.sizes)}\"\n            )\n            yield final_ds\n            self._memory_cleanup()\n\n        self._logger.info(\"RINEX processing and ingestion completed\")\n\n    def parsed_rinex_data_gen(\n        self,\n        keep_vars: list[str] | None = None,\n        receiver_types: list[str] | None = None,\n    ) -&gt; Generator[xr.Dataset]:\n        \"\"\"\n        Generator that processes RINEX files and appends to Icechunk stores on-the-fly.\n\n        Parameters\n        ----------\n        keep_vars : list[str], optional\n            List of variables to keep in datasets\n        receiver_types : list[str], optional\n            List of receiver types to process ('canopy', 'reference').\n            If None, defaults to ['canopy', 'reference']\n\n        Yields\n        ------\n        xr.Dataset\n            Processed and ingested datasets for each receiver type, in order\n        \"\"\"\n\n        if receiver_types is None:\n            receiver_types = [\"canopy\", \"reference\"]\n\n        self._logger.info(\n            f\"Starting RINEX processing and ingestion for types: {receiver_types}\"\n        )\n\n        for receiver_type in receiver_types:\n            # --- resolve dirs and names ---\n            if receiver_type == \"canopy\":\n                rinex_dir = self.matched_dirs.canopy_data_dir\n                receiver_name = self._get_receiver_name_for_type(\"canopy\")\n                store_group = \"canopy\"\n            elif receiver_type == \"reference\":\n                rinex_dir = self.matched_dirs.reference_data_dir\n                receiver_name = self._get_receiver_name_for_type(\"reference\")\n                store_group = \"reference\"\n            else:\n                self._logger.warning(f\"Unknown receiver type: {receiver_type}\")\n                continue\n\n            if not receiver_name:\n                self._logger.warning(f\"No configured receiver for type {receiver_type}\")\n                continue\n\n            rinex_files = self._get_rinex_files(rinex_dir)\n            if not rinex_files:\n                self._logger.warning(f\"No RINEX files found in {rinex_dir}\")\n                continue\n\n            self._logger.info(\n                f\"Processing {len(rinex_files)} RINEX files for {receiver_type}\"\n            )\n\n            groups = self._site.rinex_store.list_groups() or []\n\n            # --- one pool per receiver type ---\n            futures = {\n                self._pool.submit(preprocess_rnx, f, keep_vars): f for f in rinex_files\n            }\n            results: list[tuple[Path, xr.Dataset]] = []\n\n            # ✅ progressbar over all files, not per batch\n            for fut in tqdm(\n                as_completed(futures),\n                total=len(futures),\n                desc=f\"Processing {receiver_type}\",\n            ):\n                try:\n                    fname, ds = fut.result()\n                    results.append((fname, ds))\n                except Exception as e:\n                    self._logger.error(f\"Failed preprocessing: {e}\")\n\n            # --- sort all results once (chronological order) ---\n            results.sort(key=lambda x: x[0].name)\n\n            # --- sequential append to Icechunk ---\n            for idx, (fname, ds) in enumerate(results):\n                log = self._logger.bind(file=str(fname))\n                try:\n                    rel_path = self._site.rinex_store.rel_path_for_commit(fname)\n                    version = get_version_from_pyproject()\n\n                    rinex_hash = ds.attrs.get(\"RINEX File Hash\")\n                    if not rinex_hash:\n                        log.warning(\n                            f\"No RINEX hash found in dataset from {fname}. \"\n                            \"Skipping duplicate detection for this file.\"\n                        )\n                        continue\n\n                    start_epoch = np.datetime64(ds.epoch.min().values)\n                    end_epoch = np.datetime64(ds.epoch.max().values)\n\n                    exists, matches = self._site.rinex_store.metadata_row_exists(\n                        store_group, rinex_hash, start_epoch, end_epoch\n                    )\n\n                    ds = self._site.rinex_store._cleanse_dataset_attrs(ds)\n\n                    # --- Initial commit ---\n                    if not exists and store_group not in groups and idx == 0:\n                        msg = (\n                            f\"[v{version}] Initial commit with {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                            f\"to group '{store_group}'\"\n                        )\n                        self._site.rinex_store.write_initial_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            commit_message=msg,\n                        )\n                        groups.append(store_group)\n                        log.info(msg)\n                        continue\n\n                    # --- Handle strategies with match ---\n                    match (exists, self._site.rinex_store._rinex_store_strategy):\n                        case (True, \"skip\"):\n                            msg = (\n                                f\"[v{version}] Skipped {rel_path} \"\n                                f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                                f\"in group '{store_group}'\"\n                            )\n                            log.info(msg)\n                            self._site.rinex_store.append_metadata(\n                                group_name=store_group,\n                                rinex_hash=rinex_hash,\n                                start=start_epoch,\n                                end=end_epoch,\n                                snapshot_id=\"none\",\n                                action=\"skip\",\n                                commit_msg=msg,\n                                dataset_attrs=ds.attrs,\n                            )\n\n                        case (True, \"overwrite\"):\n                            msg = (\n                                f\"[v{version}] Overwrote {rel_path} \"\n                                f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                                f\"in group '{store_group}'\"\n                            )\n                            log.info(msg)\n                            self._site.rinex_store.overwrite_file_in_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                rinex_hash=rinex_hash,\n                                start=start_epoch,\n                                end=end_epoch,\n                                commit_message=msg,\n                            )\n\n                        case (True, \"append\"):\n                            msg = (\n                                f\"[v{version}] Appended {rel_path} \"\n                                f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                                f\"to group '{store_group}'\"\n                            )\n                            self._site.rinex_store.append_to_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                append_dim=\"epoch\",\n                                action=\"append\",\n                                commit_message=msg,\n                            )\n                            log.info(msg)\n\n                        case (False, _):\n                            msg = (\n                                f\"[v{version}] Wrote {rel_path} \"\n                                f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                                f\"to group '{store_group}'\"\n                            )\n                            self._site.rinex_store.append_to_group(\n                                dataset=ds,\n                                group_name=store_group,\n                                append_dim=\"epoch\",\n                                action=\"write\",\n                                commit_message=msg,\n                            )\n                            log.info(msg)\n\n                except Exception as e:\n                    log.exception(\"file_commit_failed\", error=str(e))\n                    raise\n\n                self._memory_cleanup()\n\n            # --- read back final dataset ---\n            final_ds = self._site.read_receiver_data(store_group, self._time_range)\n            self._logger.info(\n                f\"Yielding {receiver_type} dataset: {dict(final_ds.sizes)}\"\n            )\n            yield final_ds\n\n            self._memory_cleanup()\n\n        self._logger.info(\"RINEX processing and ingestion completed\")\n\n    def _get_receiver_name_for_type(self, receiver_type: str) -&gt; str | None:\n        \"\"\"Get the first configured receiver name for a given type.\"\"\"\n        for name, config in self._site.active_receivers.items():\n            if config[\"type\"] == receiver_type:\n                return name\n        return None\n\n    def _get_rinex_files(self, rinex_dir: Path) -&gt; list[Path]:\n        \"\"\"Get sorted list of RINEX files from directory.\"\"\"\n\n        if not rinex_dir.exists():\n            self._logger.warning(f\"Directory does not exist: {rinex_dir}\")\n            return []\n\n        # Look for common RINEX file patterns\n        patterns = [\"*.??o\", \"*.??O\", \"*.rnx\", \"*.RNX\"]\n        rinex_files = []\n\n        for pattern in patterns:\n            files = list(rinex_dir.glob(pattern))\n            rinex_files.extend(files)\n\n        return natsorted(rinex_files)\n\n    def _append_to_icechunk_store(\n        self,\n        dataset: xr.Dataset,\n        receiver_name: str,\n        receiver_type: str,\n    ) -&gt; None:\n        \"\"\"Append dataset to the appropriate Icechunk store.\"\"\"\n        from gnssvodpy.utils.tools import get_version_from_pyproject\n\n        try:\n            version = get_version_from_pyproject()\n            date_str = self.matched_dirs.yyyydoy.to_str()\n\n            commit_message = (\n                f\"[v{version}] Processed and ingested {receiver_type} data \"\n                f\"for {date_str}\"\n            )\n\n            # Use site's ingestion method\n            self._site.ingest_rinex_data(dataset, receiver_name, commit_message)\n\n            self._logger.info(\n                f\"Successfully appended {receiver_type} data to store as \"\n                f\"'{receiver_name}'\"\n            )\n\n        except Exception as e:\n            self._logger.exception(\n                f\"Failed to append {receiver_type} data to store\", error=str(e)\n            )\n            raise\n\n    def get_available_receivers(self) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Get available receivers grouped by type.\n\n        Returns\n        -------\n        dict[str, list[str]]\n            Dictionary mapping receiver types to lists of receiver names.\n        \"\"\"\n        available = {}\n        for receiver_type in [\"canopy\", \"reference\"]:\n            receivers = self.get_receiver_by_type(receiver_type)\n            # Filter to only receivers that have data\n            with_data = [r for r in receivers if self._site.rinex_store.group_exists(r)]\n            available[receiver_type] = with_data\n\n        return available\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a human-readable summary.\n\n        Returns\n        -------\n        str\n            Summary string.\n        \"\"\"\n        available = self.get_available_receivers()\n        return (\n            f\"IcechunkDataReader for {self.matched_dirs.yyyydoy.to_str()}\\n\"\n            f\"  Site: {self.site_name}\\n\"\n            f\"  Available receivers: {dict(available)}\\n\"\n            f\"  Workers: {self.n_max_workers}, GC enabled: {self.enable_gc}\"\n        )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.__init__","level":3,"title":"<code>__init__(matched_dirs, site_name=None, n_max_workers=None, enable_gc=True, gc_delay=1.0)</code>","text":"<p>Initialize the Icechunk data reader.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.__init__--parameters","level":5,"title":"Parameters","text":"<p>matched_dirs : MatchedDirs     Directory information for date/location matching. site_name : str | None, optional     Name of research site. If <code>None</code>, uses the first site from config. n_max_workers : int | None, optional     Maximum number of workers for parallel operations. Defaults to config. enable_gc : bool, optional     Whether to enable garbage collection between operations. gc_delay : float, optional     Delay in seconds after garbage collection.</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def __init__(\n    self,\n    matched_dirs: MatchedDirs,\n    site_name: str | None = None,\n    n_max_workers: int | None = None,\n    enable_gc: bool = True,\n    gc_delay: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the Icechunk data reader.\n\n    Parameters\n    ----------\n    matched_dirs : MatchedDirs\n        Directory information for date/location matching.\n    site_name : str | None, optional\n        Name of research site. If ``None``, uses the first site from config.\n    n_max_workers : int | None, optional\n        Maximum number of workers for parallel operations. Defaults to config.\n    enable_gc : bool, optional\n        Whether to enable garbage collection between operations.\n    gc_delay : float, optional\n        Delay in seconds after garbage collection.\n    \"\"\"\n    config = load_config()\n    if site_name is None:\n        site_name = next(iter(config.sites.sites))\n    if n_max_workers is None:\n        n_max_workers = config.processing.processing.n_max_threads\n\n    self.matched_dirs = matched_dirs\n    self.site_name = site_name\n    self.n_max_workers = n_max_workers\n    self.enable_gc = enable_gc\n    self.gc_delay = gc_delay\n\n    self._logger = get_logger(__name__).bind(\n        site=site_name,\n        date=matched_dirs.yyyydoy.to_str(),\n    )\n    self._site = GnssResearchSite(site_name)\n\n    date_obj = self.matched_dirs.yyyydoy.date\n    self._start_time = datetime.combine(date_obj, datetime.min.time())\n    self._end_time = datetime.combine(date_obj, datetime.max.time())\n    self._time_range = (self._start_time, self._end_time)\n\n    # ✅ single persistent pool\n    self._pool = ProcessPoolExecutor(\n        max_workers=min(self.n_max_workers, 16),\n    )\n\n    self._logger.info(\n        f\"Initialized Icechunk data reader for {matched_dirs.yyyydoy.to_str()}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.__del__","level":3,"title":"<code>__del__()</code>","text":"<p>Ensure the pool is shut down when the reader is deleted.</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Ensure the pool is shut down when the reader is deleted.\"\"\"\n    try:\n        self._pool.shutdown(wait=True)\n    except Exception:\n        pass\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.get_receiver_by_type","level":3,"title":"<code>get_receiver_by_type(receiver_type)</code>","text":"<p>Get list of receiver names by type.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.get_receiver_by_type--parameters","level":5,"title":"Parameters","text":"<p>receiver_type : str     Type of receiver ('canopy', 'reference')</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.get_receiver_by_type--returns","level":5,"title":"Returns","text":"<p>list[str]     List of receiver names of the specified type</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def get_receiver_by_type(self, receiver_type: str) -&gt; list[str]:\n    \"\"\"\n    Get list of receiver names by type.\n\n    Parameters\n    ----------\n    receiver_type : str\n        Type of receiver ('canopy', 'reference')\n\n    Returns\n    -------\n    list[str]\n        List of receiver names of the specified type\n    \"\"\"\n    return [\n        name\n        for name, config in self._site.active_receivers.items()\n        if config[\"type\"] == receiver_type\n    ]\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.parsed_rinex_data_gen_v2","level":3,"title":"<code>parsed_rinex_data_gen_v2(keep_vars=None, receiver_types=None)</code>","text":"<p>Generator that processes RINEX files, augments them (φ, θ, r), and appends to Icechunk stores on-the-fly.</p> <p>Yields enriched daily datasets (already augmented).</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def parsed_rinex_data_gen_v2(\n    self,\n    keep_vars: list[str] | None = None,\n    receiver_types: list[str] | None = None,\n) -&gt; Generator[xr.Dataset]:\n    \"\"\"\n    Generator that processes RINEX files, augments them (φ, θ, r),\n    and appends to Icechunk stores on-the-fly.\n\n    Yields enriched daily datasets (already augmented).\n    \"\"\"\n\n    if receiver_types is None:\n        receiver_types = [\"canopy\", \"reference\"]\n\n    self._logger.info(\n        f\"Starting RINEX processing and ingestion for types: {receiver_types}\"\n    )\n\n    # --- 1) Cache auxiliaries once per day ---\n    from canvodpy.orchestrator import RinexDataProcessor\n\n    processor = RinexDataProcessor(\n        matched_data_dirs=self.matched_dirs, icechunk_reader=self\n    )\n\n    ephem_ds = prep_aux_ds(processor.get_ephemeride_ds())\n    clk_ds = prep_aux_ds(processor.get_clk_ds())\n\n    aux_ds_dict = {\"ephem\": ephem_ds, \"clk\": clk_ds}\n    approx_pos = None  # computed on first canopy dataset\n\n    for receiver_type in receiver_types:\n        # --- resolve dirs and names ---\n        if receiver_type == \"canopy\":\n            rinex_dir = self.matched_dirs.canopy_data_dir\n            receiver_name = self._get_receiver_name_for_type(\"canopy\")\n            store_group = \"canopy\"\n        elif receiver_type == \"reference\":\n            rinex_dir = self.matched_dirs.reference_data_dir\n            receiver_name = self._get_receiver_name_for_type(\"reference\")\n            store_group = \"reference\"\n        else:\n            self._logger.warning(f\"Unknown receiver type: {receiver_type}\")\n            continue\n\n        if not receiver_name:\n            self._logger.warning(f\"No configured receiver for type {receiver_type}\")\n            continue\n\n        rinex_files = self._get_rinex_files(rinex_dir)\n        if not rinex_files:\n            self._logger.warning(f\"No RINEX files found in {rinex_dir}\")\n            continue\n\n        self._logger.info(\n            f\"Processing {len(rinex_files)} RINEX files for {receiver_type}\"\n        )\n\n        # --- parallel preprocessing ---\n        futures = {\n            self._pool.submit(preprocess_rnx, f, keep_vars): f for f in rinex_files\n        }\n        results: list[tuple[Path, xr.Dataset]] = []\n\n        for fut in tqdm(\n            as_completed(futures),\n            total=len(futures),\n            desc=f\"Processing {receiver_type}\",\n        ):\n            try:\n                fname, ds = fut.result()\n                results.append((fname, ds))\n            except Exception as e:\n                self._logger.error(f\"Failed preprocessing: {e}\")\n\n        results.sort(key=lambda x: x[0].name)  # chronological order\n\n        # --- per-file commit ---\n        for idx, (fname, ds) in enumerate(results):\n            log = self._logger.bind(file=str(fname))\n            try:\n                rel_path = self._site.rinex_store.rel_path_for_commit(fname)\n                version = get_version_from_pyproject()\n\n                rinex_hash = ds.attrs.get(\"RINEX File Hash\")\n                if not rinex_hash:\n                    log.warning(\"Dataset missing hash → skipping\")\n                    continue\n\n                start_epoch = np.datetime64(ds.epoch.min().values)\n                end_epoch = np.datetime64(ds.epoch.max().values)\n\n                exists, matches = self._site.rinex_store.metadata_row_exists(\n                    store_group, rinex_hash, start_epoch, end_epoch\n                )\n\n                ds = self._site.rinex_store._cleanse_dataset_attrs(ds)\n\n                # --- 2) Compute approx_pos once from first canopy file ---\n                if receiver_type == \"canopy\" and approx_pos is None:\n                    approx_pos = processor.get_approx_position(ds)\n\n                # --- 3) Augment with φ, θ, r ---\n                matched = processor.match_datasets(ds, **aux_ds_dict)\n                ephem_matched = matched[\"ephem\"]\n                ds = processor.add_azi_ele(\n                    rnx_obs_ds=ds,\n                    ephem_ds=ephem_matched,\n                    rx_x=approx_pos.x,\n                    rx_y=approx_pos.y,\n                    rx_z=approx_pos.z,\n                )\n\n                # --- 4) Store to Icechunk ---\n                existing_groups = self._site.rinex_store.list_groups()\n                if not exists and store_group not in existing_groups and idx == 0:\n                    msg = (\n                        f\"[v{version}] Initial commit {rel_path} \"\n                        f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch})\"\n                    )\n                    self._site.rinex_store.write_initial_group(\n                        dataset=ds, group_name=store_group, commit_message=msg\n                    )\n                    log.info(msg)\n                    continue\n\n                match (exists, self._site.rinex_store._rinex_store_strategy):\n                    case (True, \"skip\"):\n                        log.info(f\"[v{version}] Skipped {rel_path}\")\n                        # just metadata row\n                        self._site.rinex_store.append_metadata(\n                            group_name=store_group,\n                            rinex_hash=rinex_hash,\n                            start=start_epoch,\n                            end=end_epoch,\n                            snapshot_id=\"none\",\n                            action=\"skip\",\n                            commit_msg=\"skip\",\n                            dataset_attrs=ds.attrs,\n                        )\n\n                    case (True, \"overwrite\"):\n                        msg = f\"[v{version}] Overwrote {rel_path}\"\n                        self._site.rinex_store.overwrite_file_in_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            rinex_hash=rinex_hash,\n                            start=start_epoch,\n                            end=end_epoch,\n                            commit_message=msg,\n                        )\n\n                    case (True, \"append\"):\n                        msg = f\"[v{version}] Appended {rel_path}\"\n                        self._site.rinex_store.append_to_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            append_dim=\"epoch\",\n                            action=\"append\",\n                            commit_message=msg,\n                        )\n\n                    case (False, _):\n                        msg = f\"[v{version}] Wrote {rel_path}\"\n                        self._site.rinex_store.append_to_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            append_dim=\"epoch\",\n                            action=\"write\",\n                            commit_message=msg,\n                        )\n            except Exception as e:\n                log.exception(\"file_commit_failed\", error=str(e))\n                raise\n\n            self._memory_cleanup()\n\n        # --- 5) Yield full daily dataset (already enriched) ---\n        final_ds = self._site.read_receiver_data(store_group, self._time_range)\n        self._logger.info(\n            f\"Yielding {receiver_type} dataset: {dict(final_ds.sizes)}\"\n        )\n        yield final_ds\n        self._memory_cleanup()\n\n    self._logger.info(\"RINEX processing and ingestion completed\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.parsed_rinex_data_gen","level":3,"title":"<code>parsed_rinex_data_gen(keep_vars=None, receiver_types=None)</code>","text":"<p>Generator that processes RINEX files and appends to Icechunk stores on-the-fly.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.parsed_rinex_data_gen--parameters","level":5,"title":"Parameters","text":"<p>keep_vars : list[str], optional     List of variables to keep in datasets receiver_types : list[str], optional     List of receiver types to process ('canopy', 'reference').     If None, defaults to ['canopy', 'reference']</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.parsed_rinex_data_gen--yields","level":5,"title":"Yields","text":"<p>xr.Dataset     Processed and ingested datasets for each receiver type, in order</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def parsed_rinex_data_gen(\n    self,\n    keep_vars: list[str] | None = None,\n    receiver_types: list[str] | None = None,\n) -&gt; Generator[xr.Dataset]:\n    \"\"\"\n    Generator that processes RINEX files and appends to Icechunk stores on-the-fly.\n\n    Parameters\n    ----------\n    keep_vars : list[str], optional\n        List of variables to keep in datasets\n    receiver_types : list[str], optional\n        List of receiver types to process ('canopy', 'reference').\n        If None, defaults to ['canopy', 'reference']\n\n    Yields\n    ------\n    xr.Dataset\n        Processed and ingested datasets for each receiver type, in order\n    \"\"\"\n\n    if receiver_types is None:\n        receiver_types = [\"canopy\", \"reference\"]\n\n    self._logger.info(\n        f\"Starting RINEX processing and ingestion for types: {receiver_types}\"\n    )\n\n    for receiver_type in receiver_types:\n        # --- resolve dirs and names ---\n        if receiver_type == \"canopy\":\n            rinex_dir = self.matched_dirs.canopy_data_dir\n            receiver_name = self._get_receiver_name_for_type(\"canopy\")\n            store_group = \"canopy\"\n        elif receiver_type == \"reference\":\n            rinex_dir = self.matched_dirs.reference_data_dir\n            receiver_name = self._get_receiver_name_for_type(\"reference\")\n            store_group = \"reference\"\n        else:\n            self._logger.warning(f\"Unknown receiver type: {receiver_type}\")\n            continue\n\n        if not receiver_name:\n            self._logger.warning(f\"No configured receiver for type {receiver_type}\")\n            continue\n\n        rinex_files = self._get_rinex_files(rinex_dir)\n        if not rinex_files:\n            self._logger.warning(f\"No RINEX files found in {rinex_dir}\")\n            continue\n\n        self._logger.info(\n            f\"Processing {len(rinex_files)} RINEX files for {receiver_type}\"\n        )\n\n        groups = self._site.rinex_store.list_groups() or []\n\n        # --- one pool per receiver type ---\n        futures = {\n            self._pool.submit(preprocess_rnx, f, keep_vars): f for f in rinex_files\n        }\n        results: list[tuple[Path, xr.Dataset]] = []\n\n        # ✅ progressbar over all files, not per batch\n        for fut in tqdm(\n            as_completed(futures),\n            total=len(futures),\n            desc=f\"Processing {receiver_type}\",\n        ):\n            try:\n                fname, ds = fut.result()\n                results.append((fname, ds))\n            except Exception as e:\n                self._logger.error(f\"Failed preprocessing: {e}\")\n\n        # --- sort all results once (chronological order) ---\n        results.sort(key=lambda x: x[0].name)\n\n        # --- sequential append to Icechunk ---\n        for idx, (fname, ds) in enumerate(results):\n            log = self._logger.bind(file=str(fname))\n            try:\n                rel_path = self._site.rinex_store.rel_path_for_commit(fname)\n                version = get_version_from_pyproject()\n\n                rinex_hash = ds.attrs.get(\"RINEX File Hash\")\n                if not rinex_hash:\n                    log.warning(\n                        f\"No RINEX hash found in dataset from {fname}. \"\n                        \"Skipping duplicate detection for this file.\"\n                    )\n                    continue\n\n                start_epoch = np.datetime64(ds.epoch.min().values)\n                end_epoch = np.datetime64(ds.epoch.max().values)\n\n                exists, matches = self._site.rinex_store.metadata_row_exists(\n                    store_group, rinex_hash, start_epoch, end_epoch\n                )\n\n                ds = self._site.rinex_store._cleanse_dataset_attrs(ds)\n\n                # --- Initial commit ---\n                if not exists and store_group not in groups and idx == 0:\n                    msg = (\n                        f\"[v{version}] Initial commit with {rel_path} \"\n                        f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                        f\"to group '{store_group}'\"\n                    )\n                    self._site.rinex_store.write_initial_group(\n                        dataset=ds,\n                        group_name=store_group,\n                        commit_message=msg,\n                    )\n                    groups.append(store_group)\n                    log.info(msg)\n                    continue\n\n                # --- Handle strategies with match ---\n                match (exists, self._site.rinex_store._rinex_store_strategy):\n                    case (True, \"skip\"):\n                        msg = (\n                            f\"[v{version}] Skipped {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                            f\"in group '{store_group}'\"\n                        )\n                        log.info(msg)\n                        self._site.rinex_store.append_metadata(\n                            group_name=store_group,\n                            rinex_hash=rinex_hash,\n                            start=start_epoch,\n                            end=end_epoch,\n                            snapshot_id=\"none\",\n                            action=\"skip\",\n                            commit_msg=msg,\n                            dataset_attrs=ds.attrs,\n                        )\n\n                    case (True, \"overwrite\"):\n                        msg = (\n                            f\"[v{version}] Overwrote {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                            f\"in group '{store_group}'\"\n                        )\n                        log.info(msg)\n                        self._site.rinex_store.overwrite_file_in_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            rinex_hash=rinex_hash,\n                            start=start_epoch,\n                            end=end_epoch,\n                            commit_message=msg,\n                        )\n\n                    case (True, \"append\"):\n                        msg = (\n                            f\"[v{version}] Appended {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                            f\"to group '{store_group}'\"\n                        )\n                        self._site.rinex_store.append_to_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            append_dim=\"epoch\",\n                            action=\"append\",\n                            commit_message=msg,\n                        )\n                        log.info(msg)\n\n                    case (False, _):\n                        msg = (\n                            f\"[v{version}] Wrote {rel_path} \"\n                            f\"(hash={rinex_hash}, epoch={start_epoch}→{end_epoch}) \"\n                            f\"to group '{store_group}'\"\n                        )\n                        self._site.rinex_store.append_to_group(\n                            dataset=ds,\n                            group_name=store_group,\n                            append_dim=\"epoch\",\n                            action=\"write\",\n                            commit_message=msg,\n                        )\n                        log.info(msg)\n\n            except Exception as e:\n                log.exception(\"file_commit_failed\", error=str(e))\n                raise\n\n            self._memory_cleanup()\n\n        # --- read back final dataset ---\n        final_ds = self._site.read_receiver_data(store_group, self._time_range)\n        self._logger.info(\n            f\"Yielding {receiver_type} dataset: {dict(final_ds.sizes)}\"\n        )\n        yield final_ds\n\n        self._memory_cleanup()\n\n    self._logger.info(\"RINEX processing and ingestion completed\")\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.get_available_receivers","level":3,"title":"<code>get_available_receivers()</code>","text":"<p>Get available receivers grouped by type.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.get_available_receivers--returns","level":5,"title":"Returns","text":"<p>dict[str, list[str]]     Dictionary mapping receiver types to lists of receiver names.</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def get_available_receivers(self) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Get available receivers grouped by type.\n\n    Returns\n    -------\n    dict[str, list[str]]\n        Dictionary mapping receiver types to lists of receiver names.\n    \"\"\"\n    available = {}\n    for receiver_type in [\"canopy\", \"reference\"]:\n        receivers = self.get_receiver_by_type(receiver_type)\n        # Filter to only receivers that have data\n        with_data = [r for r in receivers if self._site.rinex_store.group_exists(r)]\n        available[receiver_type] = with_data\n\n    return available\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.__str__","level":3,"title":"<code>__str__()</code>","text":"<p>Return a human-readable summary.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.IcechunkDataReader.__str__--returns","level":5,"title":"Returns","text":"<p>str     Summary string.</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a human-readable summary.\n\n    Returns\n    -------\n    str\n        Summary string.\n    \"\"\"\n    available = self.get_available_receivers()\n    return (\n        f\"IcechunkDataReader for {self.matched_dirs.yyyydoy.to_str()}\\n\"\n        f\"  Site: {self.site_name}\\n\"\n        f\"  Available receivers: {dict(available)}\\n\"\n        f\"  Workers: {self.n_max_workers}, GC enabled: {self.enable_gc}\"\n    )\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.preprocess_rnx","level":2,"title":"<code>preprocess_rnx(rnx_file, keep_vars=None)</code>","text":"<p>Preprocess a single RINEX file and attach file hash metadata.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.preprocess_rnx--parameters","level":4,"title":"Parameters","text":"<p>rnx_file : Path     RINEX file to preprocess. keep_vars : list[str] | None, optional     Variables to retain in the dataset.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.reader.preprocess_rnx--returns","level":4,"title":"Returns","text":"<p>tuple[Path, xr.Dataset]     The input file path and the processed dataset.</p> Source code in <code>packages/canvod-store/src/canvod/store/reader.py</code> <pre><code>def preprocess_rnx(\n    rnx_file: Path,\n    keep_vars: list[str] | None = None,\n) -&gt; tuple[Path, xr.Dataset]:\n    \"\"\"\n    Preprocess a single RINEX file and attach file hash metadata.\n\n    Parameters\n    ----------\n    rnx_file : Path\n        RINEX file to preprocess.\n    keep_vars : list[str] | None, optional\n        Variables to retain in the dataset.\n\n    Returns\n    -------\n    tuple[Path, xr.Dataset]\n        The input file path and the processed dataset.\n    \"\"\"\n    log = get_logger(__name__).bind(file=str(rnx_file))\n    log.info(\"preprocessing_started\")\n\n    try:\n        rnx = Rnxv3Obs(fpath=rnx_file, include_auxiliary=False)\n        ds = rnx.to_ds(write_global_attrs=True)\n\n        # ✅ Attach cached file hash\n        ds.attrs[\"RINEX File Hash\"] = rnx.file_hash\n\n        # Filter variables if specified\n        if keep_vars:\n            available_vars = [var for var in keep_vars if var in ds.data_vars]\n            if available_vars:\n                ds = ds[available_vars]\n\n        log.info(\"preprocessing_complete\")\n        return rnx_file, ds\n    except Exception as e:\n        log.exception(\"preprocessing_failed\", error=str(e))\n        raise\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#preprocessing","level":2,"title":"Preprocessing","text":"<p>Preprocessing wrapper for Icechunk storage.</p> <p>This module provides the IcechunkPreprocessor class which wraps preprocessing functions from canvod.auxiliary.preprocessing. It maintains backward compatibility with gnssvodpy code while delegating to the new modular implementation.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor","level":2,"title":"<code>IcechunkPreprocessor</code>","text":"<p>Handles preprocessing of RINEX-converted datasets before writing to Icechunk.</p> <p>This class wraps functions from canvod.auxiliary.preprocessing to provide backward compatibility with existing gnssvodpy code.</p> Note <p>All methods now delegate to canvod.auxiliary.preprocessing functions. The aggregate_glonass_fdma parameter should be passed from configuration.</p> Source code in <code>packages/canvod-store/src/canvod/store/preprocessing.py</code> <pre><code>class IcechunkPreprocessor:\n    \"\"\"\n    Handles preprocessing of RINEX-converted datasets before writing to Icechunk.\n\n    This class wraps functions from canvod.auxiliary.preprocessing to provide\n    backward compatibility with existing gnssvodpy code.\n\n    Note:\n        All methods now delegate to canvod.auxiliary.preprocessing functions.\n        The aggregate_glonass_fdma parameter should be passed from configuration.\n    \"\"\"\n\n    @staticmethod\n    def map_aux_sv_to_sid(\n        aux_ds: xr.Dataset,\n        fill_value: float = None,\n        aggregate_glonass_fdma: bool = True,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Transform auxiliary dataset from sv → sid dimension.\n\n        Delegates to canvod.auxiliary.preprocessing.map_aux_sv_to_sid()\n\n        Parameters\n        ----------\n        aux_ds : xr.Dataset\n            Dataset with 'sv' dimension\n        fill_value : float, optional\n            Fill value for missing entries (default: np.nan)\n        aggregate_glonass_fdma : bool, default True\n            Whether to aggregate GLONASS FDMA bands\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with 'sid' dimension replacing 'sv'\n        \"\"\"\n        import numpy as np\n\n        if fill_value is None:\n            fill_value = np.nan\n        return map_aux_sv_to_sid(aux_ds, fill_value, aggregate_glonass_fdma)\n\n    @staticmethod\n    def pad_to_global_sid(\n        ds: xr.Dataset,\n        keep_sids: list[str] | None = None,\n        aggregate_glonass_fdma: bool = True,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Pad dataset so it has all possible SIDs across all constellations.\n\n        Delegates to canvod.auxiliary.preprocessing.pad_to_global_sid()\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset with 'sid' dimension\n        keep_sids : list[str] | None\n            Optional list of specific SIDs to keep\n        aggregate_glonass_fdma : bool, default True\n            Whether to aggregate GLONASS FDMA bands\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset padded with NaN for missing SIDs\n        \"\"\"\n        return pad_to_global_sid(ds, keep_sids, aggregate_glonass_fdma)\n\n    @staticmethod\n    def normalize_sid_dtype(ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"\n        Ensure sid coordinate uses object dtype.\n\n        Delegates to canvod.auxiliary.preprocessing.normalize_sid_dtype()\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset with 'sid' coordinate\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with sid as object dtype\n        \"\"\"\n        return normalize_sid_dtype(ds)\n\n    @staticmethod\n    def strip_fillvalue(ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"\n        Remove _FillValue attrs/encodings.\n\n        Delegates to canvod.auxiliary.preprocessing.strip_fillvalue()\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset to clean\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with _FillValue attributes removed\n        \"\"\"\n        return strip_fillvalue(ds)\n\n    @staticmethod\n    def add_future_datavars(\n        ds: xr.Dataset,\n        var_config: dict[str, dict[str, Any]],\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Add placeholder data variables from configuration.\n\n        Delegates to canvod.auxiliary.preprocessing.add_future_datavars()\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset to add variables to\n        var_config : dict[str, dict[str, Any]]\n            Variable configuration dictionary\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with new variables added\n        \"\"\"\n        return add_future_datavars(ds, var_config)\n\n    @staticmethod\n    def prep_aux_ds(\n        aux_ds: xr.Dataset,\n        fill_value: float = None,\n        aggregate_glonass_fdma: bool = True,\n        keep_sids: list[str] | None = None,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocess auxiliary dataset before writing to Icechunk.\n\n        Delegates to canvod.auxiliary.preprocessing.prep_aux_ds()\n\n        Performs complete 4-step preprocessing:\n        1. Convert sv → sid dimension\n        2. Pad to global sid list or filter to keep_sids\n        3. Normalize sid dtype to object\n        4. Strip _FillValue attributes\n\n        Parameters\n        ----------\n        aux_ds : xr.Dataset\n            Dataset with 'sv' dimension\n        fill_value : float, optional\n            Fill value for missing entries (default: np.nan)\n        aggregate_glonass_fdma : bool, default True\n            Whether to aggregate GLONASS FDMA bands\n        keep_sids : list[str] | None, default None\n            List of specific SIDs to keep. If None, keeps all possible SIDs.\n\n        Returns\n        -------\n        xr.Dataset\n            Fully preprocessed dataset ready for Icechunk\n        \"\"\"\n        import numpy as np\n\n        if fill_value is None:\n            fill_value = np.nan\n        return prep_aux_ds(aux_ds, fill_value, aggregate_glonass_fdma, keep_sids)\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.map_aux_sv_to_sid","level":3,"title":"<code>map_aux_sv_to_sid(aux_ds, fill_value=None, aggregate_glonass_fdma=True)</code>  <code>staticmethod</code>","text":"<p>Transform auxiliary dataset from sv → sid dimension.</p> <p>Delegates to canvod.auxiliary.preprocessing.map_aux_sv_to_sid()</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.map_aux_sv_to_sid--parameters","level":5,"title":"Parameters","text":"<p>aux_ds : xr.Dataset     Dataset with 'sv' dimension fill_value : float, optional     Fill value for missing entries (default: np.nan) aggregate_glonass_fdma : bool, default True     Whether to aggregate GLONASS FDMA bands</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.map_aux_sv_to_sid--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with 'sid' dimension replacing 'sv'</p> Source code in <code>packages/canvod-store/src/canvod/store/preprocessing.py</code> <pre><code>@staticmethod\ndef map_aux_sv_to_sid(\n    aux_ds: xr.Dataset,\n    fill_value: float = None,\n    aggregate_glonass_fdma: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Transform auxiliary dataset from sv → sid dimension.\n\n    Delegates to canvod.auxiliary.preprocessing.map_aux_sv_to_sid()\n\n    Parameters\n    ----------\n    aux_ds : xr.Dataset\n        Dataset with 'sv' dimension\n    fill_value : float, optional\n        Fill value for missing entries (default: np.nan)\n    aggregate_glonass_fdma : bool, default True\n        Whether to aggregate GLONASS FDMA bands\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with 'sid' dimension replacing 'sv'\n    \"\"\"\n    import numpy as np\n\n    if fill_value is None:\n        fill_value = np.nan\n    return map_aux_sv_to_sid(aux_ds, fill_value, aggregate_glonass_fdma)\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.pad_to_global_sid","level":3,"title":"<code>pad_to_global_sid(ds, keep_sids=None, aggregate_glonass_fdma=True)</code>  <code>staticmethod</code>","text":"<p>Pad dataset so it has all possible SIDs across all constellations.</p> <p>Delegates to canvod.auxiliary.preprocessing.pad_to_global_sid()</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.pad_to_global_sid--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with 'sid' dimension keep_sids : list[str] | None     Optional list of specific SIDs to keep aggregate_glonass_fdma : bool, default True     Whether to aggregate GLONASS FDMA bands</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.pad_to_global_sid--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset padded with NaN for missing SIDs</p> Source code in <code>packages/canvod-store/src/canvod/store/preprocessing.py</code> <pre><code>@staticmethod\ndef pad_to_global_sid(\n    ds: xr.Dataset,\n    keep_sids: list[str] | None = None,\n    aggregate_glonass_fdma: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Pad dataset so it has all possible SIDs across all constellations.\n\n    Delegates to canvod.auxiliary.preprocessing.pad_to_global_sid()\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with 'sid' dimension\n    keep_sids : list[str] | None\n        Optional list of specific SIDs to keep\n    aggregate_glonass_fdma : bool, default True\n        Whether to aggregate GLONASS FDMA bands\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset padded with NaN for missing SIDs\n    \"\"\"\n    return pad_to_global_sid(ds, keep_sids, aggregate_glonass_fdma)\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.normalize_sid_dtype","level":3,"title":"<code>normalize_sid_dtype(ds)</code>  <code>staticmethod</code>","text":"<p>Ensure sid coordinate uses object dtype.</p> <p>Delegates to canvod.auxiliary.preprocessing.normalize_sid_dtype()</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.normalize_sid_dtype--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with 'sid' coordinate</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.normalize_sid_dtype--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with sid as object dtype</p> Source code in <code>packages/canvod-store/src/canvod/store/preprocessing.py</code> <pre><code>@staticmethod\ndef normalize_sid_dtype(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Ensure sid coordinate uses object dtype.\n\n    Delegates to canvod.auxiliary.preprocessing.normalize_sid_dtype()\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with 'sid' coordinate\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with sid as object dtype\n    \"\"\"\n    return normalize_sid_dtype(ds)\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.strip_fillvalue","level":3,"title":"<code>strip_fillvalue(ds)</code>  <code>staticmethod</code>","text":"<p>Remove _FillValue attrs/encodings.</p> <p>Delegates to canvod.auxiliary.preprocessing.strip_fillvalue()</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.strip_fillvalue--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset to clean</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.strip_fillvalue--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with _FillValue attributes removed</p> Source code in <code>packages/canvod-store/src/canvod/store/preprocessing.py</code> <pre><code>@staticmethod\ndef strip_fillvalue(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Remove _FillValue attrs/encodings.\n\n    Delegates to canvod.auxiliary.preprocessing.strip_fillvalue()\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset to clean\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with _FillValue attributes removed\n    \"\"\"\n    return strip_fillvalue(ds)\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.add_future_datavars","level":3,"title":"<code>add_future_datavars(ds, var_config)</code>  <code>staticmethod</code>","text":"<p>Add placeholder data variables from configuration.</p> <p>Delegates to canvod.auxiliary.preprocessing.add_future_datavars()</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.add_future_datavars--parameters","level":5,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset to add variables to var_config : dict[str, dict[str, Any]]     Variable configuration dictionary</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.add_future_datavars--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset with new variables added</p> Source code in <code>packages/canvod-store/src/canvod/store/preprocessing.py</code> <pre><code>@staticmethod\ndef add_future_datavars(\n    ds: xr.Dataset,\n    var_config: dict[str, dict[str, Any]],\n) -&gt; xr.Dataset:\n    \"\"\"\n    Add placeholder data variables from configuration.\n\n    Delegates to canvod.auxiliary.preprocessing.add_future_datavars()\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset to add variables to\n    var_config : dict[str, dict[str, Any]]\n        Variable configuration dictionary\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with new variables added\n    \"\"\"\n    return add_future_datavars(ds, var_config)\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.prep_aux_ds","level":3,"title":"<code>prep_aux_ds(aux_ds, fill_value=None, aggregate_glonass_fdma=True, keep_sids=None)</code>  <code>staticmethod</code>","text":"<p>Preprocess auxiliary dataset before writing to Icechunk.</p> <p>Delegates to canvod.auxiliary.preprocessing.prep_aux_ds()</p> <p>Performs complete 4-step preprocessing: 1. Convert sv → sid dimension 2. Pad to global sid list or filter to keep_sids 3. Normalize sid dtype to object 4. Strip _FillValue attributes</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.prep_aux_ds--parameters","level":5,"title":"Parameters","text":"<p>aux_ds : xr.Dataset     Dataset with 'sv' dimension fill_value : float, optional     Fill value for missing entries (default: np.nan) aggregate_glonass_fdma : bool, default True     Whether to aggregate GLONASS FDMA bands keep_sids : list[str] | None, default None     List of specific SIDs to keep. If None, keeps all possible SIDs.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.preprocessing.IcechunkPreprocessor.prep_aux_ds--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Fully preprocessed dataset ready for Icechunk</p> Source code in <code>packages/canvod-store/src/canvod/store/preprocessing.py</code> <pre><code>@staticmethod\ndef prep_aux_ds(\n    aux_ds: xr.Dataset,\n    fill_value: float = None,\n    aggregate_glonass_fdma: bool = True,\n    keep_sids: list[str] | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocess auxiliary dataset before writing to Icechunk.\n\n    Delegates to canvod.auxiliary.preprocessing.prep_aux_ds()\n\n    Performs complete 4-step preprocessing:\n    1. Convert sv → sid dimension\n    2. Pad to global sid list or filter to keep_sids\n    3. Normalize sid dtype to object\n    4. Strip _FillValue attributes\n\n    Parameters\n    ----------\n    aux_ds : xr.Dataset\n        Dataset with 'sv' dimension\n    fill_value : float, optional\n        Fill value for missing entries (default: np.nan)\n    aggregate_glonass_fdma : bool, default True\n        Whether to aggregate GLONASS FDMA bands\n    keep_sids : list[str] | None, default None\n        List of specific SIDs to keep. If None, keeps all possible SIDs.\n\n    Returns\n    -------\n    xr.Dataset\n        Fully preprocessed dataset ready for Icechunk\n    \"\"\"\n    import numpy as np\n\n    if fill_value is None:\n        fill_value = np.nan\n    return prep_aux_ds(aux_ds, fill_value, aggregate_glonass_fdma, keep_sids)\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#metadata","level":2,"title":"Metadata","text":"<p>Metadata table helpers for Icechunk-backed stores.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.metadata.MetadataManager","level":2,"title":"<code>MetadataManager</code>","text":"<p>Manage metadata table CRUD, backups, and deduplication for groups.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.metadata.MetadataManager--parameters","level":4,"title":"Parameters","text":"<p>logger : Any, optional     Logger-like object to use. Defaults to the configured context logger.</p> Source code in <code>packages/canvod-store/src/canvod/store/metadata.py</code> <pre><code>class MetadataManager:\n    \"\"\"Manage metadata table CRUD, backups, and deduplication for groups.\n\n    Parameters\n    ----------\n    logger : Any, optional\n        Logger-like object to use. Defaults to the configured context logger.\n    \"\"\"\n\n    def __init__(self, logger: Any | None = None) -&gt; None:\n        \"\"\"Initialize the metadata manager.\n\n        Parameters\n        ----------\n        logger : Any | None, optional\n            Logger-like object to use.\n        \"\"\"\n        self._logger = logger or get_logger(__name__)\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.metadata.MetadataManager.__init__","level":3,"title":"<code>__init__(logger=None)</code>","text":"<p>Initialize the metadata manager.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#canvod.store.metadata.MetadataManager.__init__--parameters","level":5,"title":"Parameters","text":"<p>logger : Any | None, optional     Logger-like object to use.</p> Source code in <code>packages/canvod-store/src/canvod/store/metadata.py</code> <pre><code>def __init__(self, logger: Any | None = None) -&gt; None:\n    \"\"\"Initialize the metadata manager.\n\n    Parameters\n    ----------\n    logger : Any | None, optional\n        Logger-like object to use.\n    \"\"\"\n    self._logger = logger or get_logger(__name__)\n</code></pre>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-store/#grid-adapters","level":2,"title":"Grid Adapters","text":"<p>Grid adapters for hemisphere grid storage in Icechunk.</p>","path":["API Reference","canvod.store API Reference"],"tags":[]},{"location":"api/canvod-utils/","level":1,"title":"canvod.utils API Reference","text":"<p>Shared utilities: configuration management, date handling, and CLI tools.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#configuration","level":2,"title":"Configuration","text":"<p>Configuration management for canvodpy.</p> <p>This package provides: - Pydantic models for type-safe configuration - YAML-based configuration loading - CLI for configuration management - Validation and error reporting</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config--examples","level":3,"title":"Examples","text":"<p>from canvod.utils.config import load_config config = load_config() print(config.nasa_earthdata_acc_mail) print(config.processing.aux_data.agency)</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.CanvodConfig","level":2,"title":"<code>CanvodConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete canvodpy configuration.</p> <p>This is the top-level configuration object that combines all configuration sections. It's fully serializable and can be used for local development (YAML files) or API-based configuration.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>class CanvodConfig(BaseModel):\n    \"\"\"\n    Complete canvodpy configuration.\n\n    This is the top-level configuration object that combines all\n    configuration sections. It's fully serializable and can be used\n    for local development (YAML files) or API-based configuration.\n    \"\"\"\n\n    processing: ProcessingConfig\n    sites: SitesConfig\n    sids: SidsConfig\n\n    model_config = {\"extra\": \"forbid\"}  # Catch typos in config files!\n\n    @property\n    def nasa_earthdata_acc_mail(self) -&gt; str | None:\n        \"\"\"Return the configured NASA Earthdata email for CDDIS authentication.\n\n        Returns\n        -------\n        str | None\n            NASA Earthdata email address.\n        \"\"\"\n        return self.processing.credentials.nasa_earthdata_acc_mail\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.CanvodConfig.nasa_earthdata_acc_mail","level":3,"title":"<code>nasa_earthdata_acc_mail</code>  <code>property</code>","text":"<p>Return the configured NASA Earthdata email for CDDIS authentication.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.CanvodConfig.nasa_earthdata_acc_mail--returns","level":5,"title":"Returns","text":"<p>str | None     NASA Earthdata email address.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.MetadataConfig","level":2,"title":"<code>MetadataConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata to be written to processed files.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.MetadataConfig--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic model for configuration validation.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>class MetadataConfig(BaseModel):\n    \"\"\"Metadata to be written to processed files.\n\n    Notes\n    -----\n    This is a Pydantic model for configuration validation.\n    \"\"\"\n\n    author: str = Field(..., description=\"Author name\")\n    email: EmailStr = Field(..., description=\"Author email\")\n    institution: str = Field(..., description=\"Institution name\")\n    department: str | None = Field(None, description=\"Department name\")\n    research_group: str | None = Field(\n        None,\n        description=\"Research group name\",\n    )\n    website: str | None = Field(\n        None,\n        description=\"Institution/group website\",\n    )\n\n    def to_attrs_dict(self) -&gt; dict[str, str]:\n        \"\"\"Convert to a dictionary for xarray attributes.\n\n        Returns\n        -------\n        dict[str, str]\n            Metadata as xarray-compatible attributes.\n        \"\"\"\n        attrs = {\n            \"author\": self.author,\n            \"email\": self.email,\n            \"institution\": self.institution,\n        }\n        if self.department:\n            attrs[\"department\"] = self.department\n        if self.research_group:\n            attrs[\"research_group\"] = self.research_group\n        if self.website:\n            attrs[\"website\"] = self.website\n        return attrs\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.MetadataConfig.to_attrs_dict","level":3,"title":"<code>to_attrs_dict()</code>","text":"<p>Convert to a dictionary for xarray attributes.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.MetadataConfig.to_attrs_dict--returns","level":5,"title":"Returns","text":"<p>dict[str, str]     Metadata as xarray-compatible attributes.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>def to_attrs_dict(self) -&gt; dict[str, str]:\n    \"\"\"Convert to a dictionary for xarray attributes.\n\n    Returns\n    -------\n    dict[str, str]\n        Metadata as xarray-compatible attributes.\n    \"\"\"\n    attrs = {\n        \"author\": self.author,\n        \"email\": self.email,\n        \"institution\": self.institution,\n    }\n    if self.department:\n        attrs[\"department\"] = self.department\n    if self.research_group:\n        attrs[\"research_group\"] = self.research_group\n    if self.website:\n        attrs[\"website\"] = self.website\n    return attrs\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.ProcessingConfig","level":2,"title":"<code>ProcessingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete processing configuration.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>class ProcessingConfig(BaseModel):\n    \"\"\"Complete processing configuration.\"\"\"\n\n    metadata: MetadataConfig\n    credentials: CredentialsConfig = Field(\n        default_factory=CredentialsConfig,\n        description=\"Credentials for external data services\",\n    )\n    aux_data: AuxDataConfig = Field(default_factory=AuxDataConfig)\n    processing: ProcessingParams = Field(default_factory=ProcessingParams)\n    compression: CompressionConfig = Field(default_factory=CompressionConfig)\n    icechunk: IcechunkConfig = Field(default_factory=IcechunkConfig)\n    storage: StorageConfig = Field(default_factory=StorageConfig)\n    logging: LoggingConfig = Field(default_factory=LoggingConfig)\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SiteConfig","level":2,"title":"<code>SiteConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Research site configuration.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>class SiteConfig(BaseModel):\n    \"\"\"Research site configuration.\"\"\"\n\n    gnss_site_data_root: str = Field(\n        ..., description=\"Root directory for site GNSS data\"\n    )\n    receivers: dict[str, ReceiverConfig] = Field(..., description=\"Site receivers\")\n    vod_analyses: dict[str, VodAnalysisConfig] | None = Field(\n        None,\n        description=\"VOD analysis pairs\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_scs_from_targets(self) -&gt; \"SiteConfig\":\n        \"\"\"Validate that scs_from entries reference existing canopy receivers.\"\"\"\n        canopy_names = self.get_canopy_receiver_names()\n        for name, cfg in self.receivers.items():\n            if cfg.type != \"reference\" or cfg.scs_from is None:\n                continue\n            if isinstance(cfg.scs_from, str) and cfg.scs_from == \"all\":\n                continue\n            targets = cfg.scs_from if isinstance(cfg.scs_from, list) else [cfg.scs_from]\n            for target in targets:\n                if target not in canopy_names:\n                    msg = (\n                        f\"Receiver '{name}' scs_from references '{target}' \"\n                        f\"which is not a canopy receiver. \"\n                        f\"Available canopy receivers: {canopy_names}\"\n                    )\n                    raise ValueError(msg)\n        return self\n\n    def get_base_path(self) -&gt; Path:\n        \"\"\"Get gnss_site_data_root as a Path.\n\n        Returns\n        -------\n        Path\n            Site data root directory as a Path object.\n        \"\"\"\n        return Path(self.gnss_site_data_root)\n\n    def get_canopy_receiver_names(self) -&gt; list[str]:\n        \"\"\"Get names of all canopy receivers.\n\n        Returns\n        -------\n        list[str]\n            Canopy receiver names.\n        \"\"\"\n        return [name for name, cfg in self.receivers.items() if cfg.type == \"canopy\"]\n\n    def resolve_scs_from(self, receiver_name: str) -&gt; list[str]:\n        \"\"\"Resolve scs_from for a reference receiver to a list of canopy names.\n\n        Parameters\n        ----------\n        receiver_name : str\n            Name of the reference receiver.\n\n        Returns\n        -------\n        list[str]\n            List of canopy receiver names for SCS computation.\n        \"\"\"\n        cfg = self.receivers[receiver_name]\n        if cfg.type != \"reference\":\n            msg = f\"resolve_scs_from only applies to reference receivers, got '{cfg.type}'\"\n            raise ValueError(msg)\n        if cfg.scs_from == \"all\":\n            return self.get_canopy_receiver_names()\n        if isinstance(cfg.scs_from, list):\n            return cfg.scs_from\n        # Single canopy name as string\n        return [cfg.scs_from]\n\n    def get_reference_canopy_pairs(self) -&gt; list[tuple[str, str]]:\n        \"\"\"Expand scs_from into (reference_name, canopy_name) pairs.\n\n        Returns\n        -------\n        list[tuple[str, str]]\n            List of (reference_name, canopy_name) pairs.\n        \"\"\"\n        pairs = []\n        for name, cfg in self.receivers.items():\n            if cfg.type != \"reference\":\n                continue\n            for canopy_name in self.resolve_scs_from(name):\n                pairs.append((name, canopy_name))\n        return pairs\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SiteConfig.validate_scs_from_targets","level":3,"title":"<code>validate_scs_from_targets()</code>","text":"<p>Validate that scs_from entries reference existing canopy receivers.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_scs_from_targets(self) -&gt; \"SiteConfig\":\n    \"\"\"Validate that scs_from entries reference existing canopy receivers.\"\"\"\n    canopy_names = self.get_canopy_receiver_names()\n    for name, cfg in self.receivers.items():\n        if cfg.type != \"reference\" or cfg.scs_from is None:\n            continue\n        if isinstance(cfg.scs_from, str) and cfg.scs_from == \"all\":\n            continue\n        targets = cfg.scs_from if isinstance(cfg.scs_from, list) else [cfg.scs_from]\n        for target in targets:\n            if target not in canopy_names:\n                msg = (\n                    f\"Receiver '{name}' scs_from references '{target}' \"\n                    f\"which is not a canopy receiver. \"\n                    f\"Available canopy receivers: {canopy_names}\"\n                )\n                raise ValueError(msg)\n    return self\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SiteConfig.get_base_path","level":3,"title":"<code>get_base_path()</code>","text":"<p>Get gnss_site_data_root as a Path.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SiteConfig.get_base_path--returns","level":5,"title":"Returns","text":"<p>Path     Site data root directory as a Path object.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>def get_base_path(self) -&gt; Path:\n    \"\"\"Get gnss_site_data_root as a Path.\n\n    Returns\n    -------\n    Path\n        Site data root directory as a Path object.\n    \"\"\"\n    return Path(self.gnss_site_data_root)\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SiteConfig.get_canopy_receiver_names","level":3,"title":"<code>get_canopy_receiver_names()</code>","text":"<p>Get names of all canopy receivers.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SiteConfig.get_canopy_receiver_names--returns","level":5,"title":"Returns","text":"<p>list[str]     Canopy receiver names.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>def get_canopy_receiver_names(self) -&gt; list[str]:\n    \"\"\"Get names of all canopy receivers.\n\n    Returns\n    -------\n    list[str]\n        Canopy receiver names.\n    \"\"\"\n    return [name for name, cfg in self.receivers.items() if cfg.type == \"canopy\"]\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SiteConfig.resolve_scs_from","level":3,"title":"<code>resolve_scs_from(receiver_name)</code>","text":"<p>Resolve scs_from for a reference receiver to a list of canopy names.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SiteConfig.resolve_scs_from--parameters","level":5,"title":"Parameters","text":"<p>receiver_name : str     Name of the reference receiver.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SiteConfig.resolve_scs_from--returns","level":5,"title":"Returns","text":"<p>list[str]     List of canopy receiver names for SCS computation.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>def resolve_scs_from(self, receiver_name: str) -&gt; list[str]:\n    \"\"\"Resolve scs_from for a reference receiver to a list of canopy names.\n\n    Parameters\n    ----------\n    receiver_name : str\n        Name of the reference receiver.\n\n    Returns\n    -------\n    list[str]\n        List of canopy receiver names for SCS computation.\n    \"\"\"\n    cfg = self.receivers[receiver_name]\n    if cfg.type != \"reference\":\n        msg = f\"resolve_scs_from only applies to reference receivers, got '{cfg.type}'\"\n        raise ValueError(msg)\n    if cfg.scs_from == \"all\":\n        return self.get_canopy_receiver_names()\n    if isinstance(cfg.scs_from, list):\n        return cfg.scs_from\n    # Single canopy name as string\n    return [cfg.scs_from]\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SiteConfig.get_reference_canopy_pairs","level":3,"title":"<code>get_reference_canopy_pairs()</code>","text":"<p>Expand scs_from into (reference_name, canopy_name) pairs.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SiteConfig.get_reference_canopy_pairs--returns","level":5,"title":"Returns","text":"<p>list[tuple[str, str]]     List of (reference_name, canopy_name) pairs.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>def get_reference_canopy_pairs(self) -&gt; list[tuple[str, str]]:\n    \"\"\"Expand scs_from into (reference_name, canopy_name) pairs.\n\n    Returns\n    -------\n    list[tuple[str, str]]\n        List of (reference_name, canopy_name) pairs.\n    \"\"\"\n    pairs = []\n    for name, cfg in self.receivers.items():\n        if cfg.type != \"reference\":\n            continue\n        for canopy_name in self.resolve_scs_from(name):\n            pairs.append((name, canopy_name))\n    return pairs\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SitesConfig","level":2,"title":"<code>SitesConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>All research sites.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>class SitesConfig(BaseModel):\n    \"\"\"All research sites.\"\"\"\n\n    sites: dict[str, SiteConfig]\n\n    @field_validator(\"sites\")\n    @classmethod\n    def validate_at_least_one_site(\n        cls,\n        v: dict[str, \"SiteConfig\"],\n    ) -&gt; dict[str, \"SiteConfig\"]:\n        \"\"\"Warn if no sites are defined.\n\n        Parameters\n        ----------\n        v : dict[str, SiteConfig]\n            Sites dictionary to validate.\n\n        Returns\n        -------\n        dict[str, SiteConfig]\n            Validated sites dictionary.\n        \"\"\"\n        if not v:\n            import warnings\n\n            warnings.warn(\n                \"No research sites defined in sites.yaml. Run: canvodpy config init\",\n                UserWarning,\n                stacklevel=2,\n            )\n        return v\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SitesConfig.validate_at_least_one_site","level":3,"title":"<code>validate_at_least_one_site(v)</code>  <code>classmethod</code>","text":"<p>Warn if no sites are defined.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SitesConfig.validate_at_least_one_site--parameters","level":5,"title":"Parameters","text":"<p>v : dict[str, SiteConfig]     Sites dictionary to validate.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SitesConfig.validate_at_least_one_site--returns","level":5,"title":"Returns","text":"<p>dict[str, SiteConfig]     Validated sites dictionary.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>@field_validator(\"sites\")\n@classmethod\ndef validate_at_least_one_site(\n    cls,\n    v: dict[str, \"SiteConfig\"],\n) -&gt; dict[str, \"SiteConfig\"]:\n    \"\"\"Warn if no sites are defined.\n\n    Parameters\n    ----------\n    v : dict[str, SiteConfig]\n        Sites dictionary to validate.\n\n    Returns\n    -------\n    dict[str, SiteConfig]\n        Validated sites dictionary.\n    \"\"\"\n    if not v:\n        import warnings\n\n        warnings.warn(\n            \"No research sites defined in sites.yaml. Run: canvodpy config init\",\n            UserWarning,\n            stacklevel=2,\n        )\n    return v\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SidsConfig","level":2,"title":"<code>SidsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Signal ID configuration.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>class SidsConfig(BaseModel):\n    \"\"\"Signal ID configuration.\"\"\"\n\n    mode: Literal[\"all\", \"preset\", \"custom\"] = Field(\n        \"all\",\n        description=\"SID selection mode\",\n    )\n    preset: str | None = Field(\n        None,\n        description=\"Preset name when mode=preset\",\n    )\n    custom_sids: list[str] = Field(\n        default_factory=list,\n        description=\"Custom SID list when mode=custom\",\n    )\n\n    @field_validator(\"preset\")\n    @classmethod\n    def validate_preset_when_mode_preset(\n        cls,\n        v: str | None,\n        info: ValidationInfo,\n    ) -&gt; str | None:\n        \"\"\"Ensure preset is set when mode is preset.\n\n        Parameters\n        ----------\n        v : str | None\n            Preset name.\n        info : ValidationInfo\n            Pydantic validation info.\n\n        Returns\n        -------\n        str | None\n            Preset value if valid.\n        \"\"\"\n        mode = info.data.get(\"mode\")\n        if mode == \"preset\" and not v:\n            msg = \"preset must be specified when mode is 'preset'\"\n            raise ValueError(msg)\n        return v\n\n    def get_sids(self) -&gt; list[str] | None:\n        \"\"\"Get the effective SID list.\n\n        Returns\n        -------\n        list[str] | None\n            None if mode is \"all\" (keep all SIDs), otherwise a SID list.\n        \"\"\"\n        if self.mode == \"all\":\n            return None\n        if self.mode == \"preset\":\n            return self._get_preset_sids()\n        # CUSTOM\n        return self.custom_sids\n\n    def _get_preset_sids(self) -&gt; list[str]:\n        \"\"\"Load the preset SID list.\n\n        Returns\n        -------\n        list[str]\n            Preset SID list.\n        \"\"\"\n        # TODO: Implement preset loading from package defaults\n        # For now, return empty list\n        return []\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SidsConfig.validate_preset_when_mode_preset","level":3,"title":"<code>validate_preset_when_mode_preset(v, info)</code>  <code>classmethod</code>","text":"<p>Ensure preset is set when mode is preset.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SidsConfig.validate_preset_when_mode_preset--parameters","level":5,"title":"Parameters","text":"<p>v : str | None     Preset name. info : ValidationInfo     Pydantic validation info.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SidsConfig.validate_preset_when_mode_preset--returns","level":5,"title":"Returns","text":"<p>str | None     Preset value if valid.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>@field_validator(\"preset\")\n@classmethod\ndef validate_preset_when_mode_preset(\n    cls,\n    v: str | None,\n    info: ValidationInfo,\n) -&gt; str | None:\n    \"\"\"Ensure preset is set when mode is preset.\n\n    Parameters\n    ----------\n    v : str | None\n        Preset name.\n    info : ValidationInfo\n        Pydantic validation info.\n\n    Returns\n    -------\n    str | None\n        Preset value if valid.\n    \"\"\"\n    mode = info.data.get(\"mode\")\n    if mode == \"preset\" and not v:\n        msg = \"preset must be specified when mode is 'preset'\"\n        raise ValueError(msg)\n    return v\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SidsConfig.get_sids","level":3,"title":"<code>get_sids()</code>","text":"<p>Get the effective SID list.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.SidsConfig.get_sids--returns","level":5,"title":"Returns","text":"<p>list[str] | None     None if mode is \"all\" (keep all SIDs), otherwise a SID list.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/models.py</code> <pre><code>def get_sids(self) -&gt; list[str] | None:\n    \"\"\"Get the effective SID list.\n\n    Returns\n    -------\n    list[str] | None\n        None if mode is \"all\" (keep all SIDs), otherwise a SID list.\n    \"\"\"\n    if self.mode == \"all\":\n        return None\n    if self.mode == \"preset\":\n        return self._get_preset_sids()\n    # CUSTOM\n    return self.custom_sids\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.load_config","level":2,"title":"<code>load_config(config_dir=None)</code>","text":"<p>Load configuration from YAML files.</p> <p>This is the main entry point for loading configuration.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.load_config--parameters","level":4,"title":"Parameters","text":"<p>config_dir : Path | None, optional     Directory containing config files. If None, automatically finds     monorepo root and uses {monorepo_root}/config.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.load_config--returns","level":4,"title":"Returns","text":"<p>CanvodConfig     Validated configuration object.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.config.load_config--examples","level":4,"title":"Examples","text":"<p>from canvod.utils.config import load_config config = load_config() print(config.nasa_earthdata_acc_mail) print(config.processing.aux_data.agency)</p> Source code in <code>packages/canvod-utils/src/canvod/utils/config/loader.py</code> <pre><code>def load_config(config_dir: Path | None = None) -&gt; CanvodConfig:\n    \"\"\"\n    Load configuration from YAML files.\n\n    This is the main entry point for loading configuration.\n\n    Parameters\n    ----------\n    config_dir : Path | None, optional\n        Directory containing config files. If None, automatically finds\n        monorepo root and uses {monorepo_root}/config.\n\n    Returns\n    -------\n    CanvodConfig\n        Validated configuration object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.utils.config import load_config\n    &gt;&gt;&gt; config = load_config()\n    &gt;&gt;&gt; print(config.nasa_earthdata_acc_mail)\n    &gt;&gt;&gt; print(config.processing.aux_data.agency)\n    \"\"\"\n    loader = ConfigLoader(config_dir)\n    return loader.load()\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#tools","level":2,"title":"Tools","text":"<p>Utility tools for canVODpy packages.</p> <p>This module provides common utilities used across all canVODpy packages: - Version management - Date/time utilities for GNSS data - Validation helpers - File hashing</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools--examples","level":3,"title":"Examples","text":"<p>from canvod.utils.tools import get_version_from_pyproject version = get_version_from_pyproject()</p> <p>from canvod.utils.tools import YYYYDOY date = YYYYDOY.from_str(\"2025024\") print(date.to_datetime())</p> <p>from canvod.utils.tools import gpsweekday week, day = gpsweekday(\"2025-01-15\")</p> <p>from canvod.utils.tools import isfloat isfloat(\"3.14\")  # True</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.gpsweekday","level":2,"title":"<code>gpsweekday = YYYYDOY.gpsweekday</code>  <code>module-attribute</code>","text":"","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY","level":2,"title":"<code>YYYYDOY</code>","text":"<p>Year and day-of-year (DOY) date representation.</p> <p>Used throughout canvodpy for GNSS data organization and file naming.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY--notes","level":4,"title":"Notes","text":"<p>This is a Pydantic dataclass and uses <code>total_ordering</code> for comparisons.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY--attributes","level":4,"title":"Attributes","text":"<p>year : int     The year (e.g., 2025). doy : int     Day of year (1-366). date : datetime.date     Calculated calendar date. yydoy : str     Short format (YYDDD, e.g., \"25001\").</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY--examples","level":4,"title":"Examples","text":"Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>@total_ordering\n@dataclass\nclass YYYYDOY:\n    \"\"\"Year and day-of-year (DOY) date representation.\n\n    Used throughout canvodpy for GNSS data organization and file naming.\n\n    Notes\n    -----\n    This is a Pydantic dataclass and uses ``total_ordering`` for comparisons.\n\n    Attributes\n    ----------\n    year : int\n        The year (e.g., 2025).\n    doy : int\n        Day of year (1-366).\n    date : datetime.date\n        Calculated calendar date.\n    yydoy : str\n        Short format (YYDDD, e.g., \"25001\").\n\n    Examples\n    --------\n    &gt;&gt;&gt; # From components\n    &gt;&gt;&gt; d = YYYYDOY(year=2025, doy=1)\n    &gt;&gt;&gt; d.to_str()\n    '2025001'\n\n    &gt;&gt;&gt; # From date object\n    &gt;&gt;&gt; import datetime\n    &gt;&gt;&gt; d = YYYYDOY.from_date(datetime.date(2025, 1, 1))\n    &gt;&gt;&gt; d.doy\n    '001'\n\n    &gt;&gt;&gt; # From string\n    &gt;&gt;&gt; d = YYYYDOY.from_str(\"2025001\")\n    &gt;&gt;&gt; d.year\n    2025\n\n    &gt;&gt;&gt; # From short string (YYDDD)\n    &gt;&gt;&gt; d = YYYYDOY.from_yydoy_str(\"25001\")\n    &gt;&gt;&gt; d.to_str()\n    '2025001'\n\n    \"\"\"\n\n    year: int = Field(..., ge=1)\n    doy: int = Field(..., ge=1, le=366)\n    yydoy: str | None = None\n    date: datetime.date | None = None\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Calculate derived fields after initialization.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self.date = self._calculate_date()\n        self.doy = f\"{self.doy:03}\"\n        self.yydoy = f\"{str(self.year)[-2:]}{self.doy}\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the developer-focused representation.\n\n        Returns\n        -------\n        str\n            Detailed representation of the date.\n        \"\"\"\n        return (\n            f\"YYYYDOY(year={self.year}, doy={self.doy}, date={self.date}, \"\n            f\"yydoy={self.yydoy}, gps_week={self.gps_week}, \"\n            f\"gps_day_of_week={self.gps_day_of_week})\"\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the user-facing date string.\n\n        Returns\n        -------\n        str\n            Date string in YYYYDDD format.\n        \"\"\"\n        return self.to_str()\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Compare equality based on the date string.\n\n        Parameters\n        ----------\n        other : object\n            Object to compare with.\n\n        Returns\n        -------\n        bool\n            True if the other object is an equal YYYYDOY instance.\n        \"\"\"\n        if not isinstance(other, YYYYDOY):\n            return False\n        return self.to_str() == other.to_str()\n\n    def __lt__(self, other: object) -&gt; bool:\n        \"\"\"Compare ordering for sorting.\n\n        Parameters\n        ----------\n        other : object\n            Object to compare with.\n\n        Returns\n        -------\n        bool\n            True if this date is earlier than the other date.\n        \"\"\"\n        if not isinstance(other, YYYYDOY):\n            return NotImplemented\n        return self.date &lt; other.date\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return the hash of the date string.\n\n        Returns\n        -------\n        int\n            Hash value for use in sets and dicts.\n        \"\"\"\n        return hash(self.to_str())\n\n    def _calculate_date(self) -&gt; datetime.date:\n        \"\"\"Calculate the calendar date from year and DOY.\n\n        Returns\n        -------\n        datetime.date\n            Calculated calendar date.\n        \"\"\"\n        return datetime.date(self.year, 1, 1) + datetime.timedelta(\n            days=int(self.doy) - 1\n        )\n\n    @staticmethod\n    def _validate_doy(doy: int) -&gt; None:\n        \"\"\"Validate day of year is in range [1, 366].\n\n        Parameters\n        ----------\n        doy : int\n            Day of year value to validate.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if not 1 &lt;= doy &lt;= 366:\n            raise ValueError(f\"Day of year (DOY) must be in range [1, 366], got {doy}\")\n\n    @classmethod\n    def from_date(cls, date: datetime.date) -&gt; \"YYYYDOY\":\n        \"\"\"Create from datetime.date object.\n\n        Parameters\n        ----------\n        date : datetime.date\n            Calendar date\n\n        Returns\n        -------\n        YYYYDOY\n        \"\"\"\n        if isinstance(date, datetime.datetime):\n            date = date.date()\n        year = date.year\n        doy = (date - datetime.date(year, 1, 1)).days + 1\n        cls._validate_doy(doy)\n        return cls(year=year, doy=doy)\n\n    @classmethod\n    def from_str(cls, yyyydoy: str | int) -&gt; \"YYYYDOY\":\n        \"\"\"Create from YYYYDDD string.\n\n        Parameters\n        ----------\n        yyyydoy : str or int\n            Date in YYYYDDD format (e.g., \"2025001\" or 2025001)\n\n        Returns\n        -------\n        YYYYDOY\n        \"\"\"\n        if isinstance(yyyydoy, int):\n            yyyydoy = str(yyyydoy)\n        if len(yyyydoy) != 7:\n            raise ValueError(f\"Invalid format. Expected 'YYYYDDD', got '{yyyydoy}'\")\n        year = int(yyyydoy[:4])\n        doy = int(yyyydoy[4:])\n        cls._validate_doy(doy)\n        jan_first = datetime.datetime(year, 1, 1)\n        final_date = jan_first + datetime.timedelta(days=doy - 1)\n        return cls.from_date(final_date.date())\n\n    @classmethod\n    def from_int(cls, yyyydoy: int) -&gt; \"YYYYDOY\":\n        \"\"\"Create from YYYYDDD integer.\n\n        Parameters\n        ----------\n        yyyydoy : int\n            Date as integer (e.g., 2025001)\n\n        Returns\n        -------\n        YYYYDOY\n        \"\"\"\n        return cls.from_str(str(yyyydoy))\n\n    @classmethod\n    def from_yydoy_str(cls, yydoy: str) -&gt; \"YYYYDOY\":\n        \"\"\"Create from YYDDD short string.\n\n        Assumes current millennium (20XX).\n\n        Parameters\n        ----------\n        yydoy : str\n            Short date format (e.g., \"25001\" for 2025 DOY 001)\n\n        Returns\n        -------\n        YYYYDOY\n        \"\"\"\n        current_millennium = str(datetime.datetime.now().year)[0:2]\n        return cls.from_str(f\"{current_millennium}{yydoy}\")\n\n    def to_str(self) -&gt; str:\n        \"\"\"Convert to YYYYDDD string.\n\n        Returns\n        -------\n        str\n            Date in YYYYDDD format (e.g., \"2025001\").\n\n        \"\"\"\n        return f\"{self.year}{self.doy}\"\n\n    @staticmethod\n    def gpsweekday(\n        input_date: datetime.datetime | datetime.date | str,\n        is_datetime: bool = False,\n    ) -&gt; tuple[int, int]:\n        \"\"\"\n        Calculate GPS week number and day of week from a given date.\n\n        GPS time started on January 6, 1980.\n\n        Parameters\n        ----------\n        input_date : datetime.datetime, datetime.date, or str\n            The date to calculate GPS week for. If string, format: \"dd-mm-yyyy\"\n        is_datetime : bool, optional\n            Whether input_date is a datetime object (default: False)\n\n        Returns\n        -------\n        tuple[int, int]\n            (GPS week number, day of week)\n        \"\"\"\n        gps_start_date = datetime.date(1980, 1, 6)\n\n        # Convert string to date if needed\n        if not is_datetime and isinstance(input_date, str):\n            input_date = datetime.datetime.strptime(input_date, \"%d-%m-%Y\").date()\n        elif isinstance(input_date, datetime.datetime):\n            input_date = input_date.date()\n\n        # Calculate weeks and days since GPS epoch\n        return divmod((input_date - gps_start_date).days, 7)\n\n    @property\n    def gps_week(self) -&gt; int:\n        \"\"\"GPS week number.\n\n        Returns\n        -------\n        int\n            GPS week number since GPS epoch (1980-01-06).\n        \"\"\"\n        return self.gpsweekday(self.date)[0]\n\n    @property\n    def gps_day_of_week(self) -&gt; int:\n        \"\"\"GPS day of week (0=Sunday, 6=Saturday).\n\n        Returns\n        -------\n        int\n            Day of week where 0=Sunday, 6=Saturday.\n        \"\"\"\n        return self.gpsweekday(self.date)[1]\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY--from-components","level":3,"title":"From components","text":"<p>d = YYYYDOY(year=2025, doy=1) d.to_str() '2025001'</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY--from-date-object","level":3,"title":"From date object","text":"<p>import datetime d = YYYYDOY.from_date(datetime.date(2025, 1, 1)) d.doy '001'</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY--from-string","level":3,"title":"From string","text":"<p>d = YYYYDOY.from_str(\"2025001\") d.year 2025</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY--from-short-string-yyddd","level":3,"title":"From short string (YYDDD)","text":"<p>d = YYYYDOY.from_yydoy_str(\"25001\") d.to_str() '2025001'</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.gps_week","level":3,"title":"<code>gps_week</code>  <code>property</code>","text":"<p>GPS week number.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.gps_week--returns","level":5,"title":"Returns","text":"<p>int     GPS week number since GPS epoch (1980-01-06).</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.gps_day_of_week","level":3,"title":"<code>gps_day_of_week</code>  <code>property</code>","text":"<p>GPS day of week (0=Sunday, 6=Saturday).</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.gps_day_of_week--returns","level":5,"title":"Returns","text":"<p>int     Day of week where 0=Sunday, 6=Saturday.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__post_init__","level":3,"title":"<code>__post_init__()</code>","text":"<p>Calculate derived fields after initialization.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__post_init__--returns","level":5,"title":"Returns","text":"<p>None</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Calculate derived fields after initialization.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    self.date = self._calculate_date()\n    self.doy = f\"{self.doy:03}\"\n    self.yydoy = f\"{str(self.year)[-2:]}{self.doy}\"\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>Return the developer-focused representation.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__repr__--returns","level":5,"title":"Returns","text":"<p>str     Detailed representation of the date.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the developer-focused representation.\n\n    Returns\n    -------\n    str\n        Detailed representation of the date.\n    \"\"\"\n    return (\n        f\"YYYYDOY(year={self.year}, doy={self.doy}, date={self.date}, \"\n        f\"yydoy={self.yydoy}, gps_week={self.gps_week}, \"\n        f\"gps_day_of_week={self.gps_day_of_week})\"\n    )\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__str__","level":3,"title":"<code>__str__()</code>","text":"<p>Return the user-facing date string.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__str__--returns","level":5,"title":"Returns","text":"<p>str     Date string in YYYYDDD format.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the user-facing date string.\n\n    Returns\n    -------\n    str\n        Date string in YYYYDDD format.\n    \"\"\"\n    return self.to_str()\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__eq__","level":3,"title":"<code>__eq__(other)</code>","text":"<p>Compare equality based on the date string.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__eq__--parameters","level":5,"title":"Parameters","text":"<p>other : object     Object to compare with.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__eq__--returns","level":5,"title":"Returns","text":"<p>bool     True if the other object is an equal YYYYDOY instance.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"Compare equality based on the date string.\n\n    Parameters\n    ----------\n    other : object\n        Object to compare with.\n\n    Returns\n    -------\n    bool\n        True if the other object is an equal YYYYDOY instance.\n    \"\"\"\n    if not isinstance(other, YYYYDOY):\n        return False\n    return self.to_str() == other.to_str()\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__lt__","level":3,"title":"<code>__lt__(other)</code>","text":"<p>Compare ordering for sorting.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__lt__--parameters","level":5,"title":"Parameters","text":"<p>other : object     Object to compare with.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__lt__--returns","level":5,"title":"Returns","text":"<p>bool     True if this date is earlier than the other date.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>def __lt__(self, other: object) -&gt; bool:\n    \"\"\"Compare ordering for sorting.\n\n    Parameters\n    ----------\n    other : object\n        Object to compare with.\n\n    Returns\n    -------\n    bool\n        True if this date is earlier than the other date.\n    \"\"\"\n    if not isinstance(other, YYYYDOY):\n        return NotImplemented\n    return self.date &lt; other.date\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__hash__","level":3,"title":"<code>__hash__()</code>","text":"<p>Return the hash of the date string.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.__hash__--returns","level":5,"title":"Returns","text":"<p>int     Hash value for use in sets and dicts.</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return the hash of the date string.\n\n    Returns\n    -------\n    int\n        Hash value for use in sets and dicts.\n    \"\"\"\n    return hash(self.to_str())\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_date","level":3,"title":"<code>from_date(date)</code>  <code>classmethod</code>","text":"<p>Create from datetime.date object.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_date--parameters","level":5,"title":"Parameters","text":"<p>date : datetime.date     Calendar date</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_date--returns","level":5,"title":"Returns","text":"<p>YYYYDOY</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>@classmethod\ndef from_date(cls, date: datetime.date) -&gt; \"YYYYDOY\":\n    \"\"\"Create from datetime.date object.\n\n    Parameters\n    ----------\n    date : datetime.date\n        Calendar date\n\n    Returns\n    -------\n    YYYYDOY\n    \"\"\"\n    if isinstance(date, datetime.datetime):\n        date = date.date()\n    year = date.year\n    doy = (date - datetime.date(year, 1, 1)).days + 1\n    cls._validate_doy(doy)\n    return cls(year=year, doy=doy)\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_str","level":3,"title":"<code>from_str(yyyydoy)</code>  <code>classmethod</code>","text":"<p>Create from YYYYDDD string.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_str--parameters","level":5,"title":"Parameters","text":"<p>yyyydoy : str or int     Date in YYYYDDD format (e.g., \"2025001\" or 2025001)</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_str--returns","level":5,"title":"Returns","text":"<p>YYYYDOY</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>@classmethod\ndef from_str(cls, yyyydoy: str | int) -&gt; \"YYYYDOY\":\n    \"\"\"Create from YYYYDDD string.\n\n    Parameters\n    ----------\n    yyyydoy : str or int\n        Date in YYYYDDD format (e.g., \"2025001\" or 2025001)\n\n    Returns\n    -------\n    YYYYDOY\n    \"\"\"\n    if isinstance(yyyydoy, int):\n        yyyydoy = str(yyyydoy)\n    if len(yyyydoy) != 7:\n        raise ValueError(f\"Invalid format. Expected 'YYYYDDD', got '{yyyydoy}'\")\n    year = int(yyyydoy[:4])\n    doy = int(yyyydoy[4:])\n    cls._validate_doy(doy)\n    jan_first = datetime.datetime(year, 1, 1)\n    final_date = jan_first + datetime.timedelta(days=doy - 1)\n    return cls.from_date(final_date.date())\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_int","level":3,"title":"<code>from_int(yyyydoy)</code>  <code>classmethod</code>","text":"<p>Create from YYYYDDD integer.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_int--parameters","level":5,"title":"Parameters","text":"<p>yyyydoy : int     Date as integer (e.g., 2025001)</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_int--returns","level":5,"title":"Returns","text":"<p>YYYYDOY</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>@classmethod\ndef from_int(cls, yyyydoy: int) -&gt; \"YYYYDOY\":\n    \"\"\"Create from YYYYDDD integer.\n\n    Parameters\n    ----------\n    yyyydoy : int\n        Date as integer (e.g., 2025001)\n\n    Returns\n    -------\n    YYYYDOY\n    \"\"\"\n    return cls.from_str(str(yyyydoy))\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_yydoy_str","level":3,"title":"<code>from_yydoy_str(yydoy)</code>  <code>classmethod</code>","text":"<p>Create from YYDDD short string.</p> <p>Assumes current millennium (20XX).</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_yydoy_str--parameters","level":5,"title":"Parameters","text":"<p>yydoy : str     Short date format (e.g., \"25001\" for 2025 DOY 001)</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.from_yydoy_str--returns","level":5,"title":"Returns","text":"<p>YYYYDOY</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>@classmethod\ndef from_yydoy_str(cls, yydoy: str) -&gt; \"YYYYDOY\":\n    \"\"\"Create from YYDDD short string.\n\n    Assumes current millennium (20XX).\n\n    Parameters\n    ----------\n    yydoy : str\n        Short date format (e.g., \"25001\" for 2025 DOY 001)\n\n    Returns\n    -------\n    YYYYDOY\n    \"\"\"\n    current_millennium = str(datetime.datetime.now().year)[0:2]\n    return cls.from_str(f\"{current_millennium}{yydoy}\")\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.to_str","level":3,"title":"<code>to_str()</code>","text":"<p>Convert to YYYYDDD string.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.to_str--returns","level":5,"title":"Returns","text":"<p>str     Date in YYYYDDD format (e.g., \"2025001\").</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>def to_str(self) -&gt; str:\n    \"\"\"Convert to YYYYDDD string.\n\n    Returns\n    -------\n    str\n        Date in YYYYDDD format (e.g., \"2025001\").\n\n    \"\"\"\n    return f\"{self.year}{self.doy}\"\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.gpsweekday","level":3,"title":"<code>gpsweekday(input_date, is_datetime=False)</code>  <code>staticmethod</code>","text":"<p>Calculate GPS week number and day of week from a given date.</p> <p>GPS time started on January 6, 1980.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.gpsweekday--parameters","level":5,"title":"Parameters","text":"<p>input_date : datetime.datetime, datetime.date, or str     The date to calculate GPS week for. If string, format: \"dd-mm-yyyy\" is_datetime : bool, optional     Whether input_date is a datetime object (default: False)</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYYYDOY.gpsweekday--returns","level":5,"title":"Returns","text":"<p>tuple[int, int]     (GPS week number, day of week)</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>@staticmethod\ndef gpsweekday(\n    input_date: datetime.datetime | datetime.date | str,\n    is_datetime: bool = False,\n) -&gt; tuple[int, int]:\n    \"\"\"\n    Calculate GPS week number and day of week from a given date.\n\n    GPS time started on January 6, 1980.\n\n    Parameters\n    ----------\n    input_date : datetime.datetime, datetime.date, or str\n        The date to calculate GPS week for. If string, format: \"dd-mm-yyyy\"\n    is_datetime : bool, optional\n        Whether input_date is a datetime object (default: False)\n\n    Returns\n    -------\n    tuple[int, int]\n        (GPS week number, day of week)\n    \"\"\"\n    gps_start_date = datetime.date(1980, 1, 6)\n\n    # Convert string to date if needed\n    if not is_datetime and isinstance(input_date, str):\n        input_date = datetime.datetime.strptime(input_date, \"%d-%m-%Y\").date()\n    elif isinstance(input_date, datetime.datetime):\n        input_date = input_date.date()\n\n    # Calculate weeks and days since GPS epoch\n    return divmod((input_date - gps_start_date).days, 7)\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYDOY","level":2,"title":"<code>YYDOY</code>","text":"<p>Two-digit year + day of year (DOY) date representation.</p> <p>This is a compact format often used in GNSS filenames.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYDOY--examples","level":4,"title":"Examples","text":"<p>date = YYDOY.from_str(\"25001\")  # 2025, day 1</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>@dataclass\nclass YYDOY:\n    \"\"\"\n    Two-digit year + day of year (DOY) date representation.\n\n    This is a compact format often used in GNSS filenames.\n\n    Examples\n    --------\n    &gt;&gt;&gt; date = YYDOY.from_str(\"25001\")  # 2025, day 1\n    \"\"\"\n\n    @classmethod\n    def from_str(cls, yydoy: str) -&gt; \"YYYYDOY\":\n        \"\"\"\n        Convert two-digit year DOY string to four-digit YYYYDOY.\n\n        Parameters\n        ----------\n        yydoy : str\n            Two-digit year + DOY (e.g., \"25001\" for 2025-01-01)\n\n        Returns\n        -------\n        YYYYDOY\n            Four-digit year DOY object\n        \"\"\"\n        date_str = f\"20{yydoy}\"\n        return YYYYDOY.from_str(date_str)\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYDOY.from_str","level":3,"title":"<code>from_str(yydoy)</code>  <code>classmethod</code>","text":"<p>Convert two-digit year DOY string to four-digit YYYYDOY.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYDOY.from_str--parameters","level":5,"title":"Parameters","text":"<p>yydoy : str     Two-digit year + DOY (e.g., \"25001\" for 2025-01-01)</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.YYDOY.from_str--returns","level":5,"title":"Returns","text":"<p>YYYYDOY     Four-digit year DOY object</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>@classmethod\ndef from_str(cls, yydoy: str) -&gt; \"YYYYDOY\":\n    \"\"\"\n    Convert two-digit year DOY string to four-digit YYYYDOY.\n\n    Parameters\n    ----------\n    yydoy : str\n        Two-digit year + DOY (e.g., \"25001\" for 2025-01-01)\n\n    Returns\n    -------\n    YYYYDOY\n        Four-digit year DOY object\n    \"\"\"\n    date_str = f\"20{yydoy}\"\n    return YYYYDOY.from_str(date_str)\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.get_gps_week_from_filename","level":2,"title":"<code>get_gps_week_from_filename(file_name)</code>","text":"<p>Extract GPS week from various GNSS product filenames.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.get_gps_week_from_filename--parameters","level":4,"title":"Parameters","text":"<p>file_name : Path     GNSS product filename (e.g., SP3, CLK, TRO, IONEX)</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.get_gps_week_from_filename--returns","level":4,"title":"Returns","text":"<p>str     GPS week as string</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.get_gps_week_from_filename--raises","level":4,"title":"Raises","text":"<p>ValueError     If file type is not recognized</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.get_gps_week_from_filename--examples","level":4,"title":"Examples","text":"<p>from pathlib import Path week = get_gps_week_from_filename(Path(\"igs12345.sp3\"))</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/date_utils.py</code> <pre><code>def get_gps_week_from_filename(file_name: Path) -&gt; str:\n    \"\"\"\n    Extract GPS week from various GNSS product filenames.\n\n    Parameters\n    ----------\n    file_name : Path\n        GNSS product filename (e.g., SP3, CLK, TRO, IONEX)\n\n    Returns\n    -------\n    str\n        GPS week as string\n\n    Raises\n    ------\n    ValueError\n        If file type is not recognized\n\n    Examples\n    --------\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; week = get_gps_week_from_filename(Path(\"igs12345.sp3\"))\n    \"\"\"\n    if file_name.suffix in [\".clk\", \".clk_05s\", \".CLK\", \".sp3\", \".SP3\"]:\n        if file_name.suffix == \".clk\":\n            return str(file_name)[3:-7]\n        else:\n            # Parse date from filename and convert to GPS week\n            date_str = str(file_name).split(\"_\")[1]\n            date = datetime.datetime.strptime(date_str, \"%Y%j%H%M\").date()\n            return str(YYYYDOY.gpsweekday(date)[0])\n    elif any(ext in str(file_name) for ext in (\"TRO\", \"IONEX\")):\n        return str(file_name).split(\"_\")[1][:4]\n\n    msg = (\n        \"Invalid file type. The filename must end with \"\n        \".clk, clk_05s, .CLK, .SP3, .TRO, or .IONEX\"\n    )\n    raise ValueError(msg)\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.isfloat","level":2,"title":"<code>isfloat(value)</code>","text":"<p>Check if a variable can be converted to float.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.isfloat--parameters","level":4,"title":"Parameters","text":"<p>value : Any     Value to check.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.isfloat--returns","level":4,"title":"Returns","text":"<p>bool     True if value can be converted to float, False otherwise.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.isfloat--examples","level":4,"title":"Examples","text":"<p>isfloat(\"3.14\") True isfloat(\"hello\") False isfloat(42) True</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/validation.py</code> <pre><code>def isfloat(value: Any) -&gt; bool:\n    \"\"\"\n    Check if a variable can be converted to float.\n\n    Parameters\n    ----------\n    value : Any\n        Value to check.\n\n    Returns\n    -------\n    bool\n        True if value can be converted to float, False otherwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; isfloat(\"3.14\")\n    True\n    &gt;&gt;&gt; isfloat(\"hello\")\n    False\n    &gt;&gt;&gt; isfloat(42)\n    True\n    \"\"\"\n    try:\n        float(value)\n        return True\n    except (ValueError, TypeError):\n        return False\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.rinex_file_hash","level":2,"title":"<code>rinex_file_hash(path, chunk_size=8192)</code>","text":"<p>Compute SHA256 hash of a RINEX file's content.</p> <p>Uses first 16 characters of the SHA256 hexdigest for a compact hash.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.rinex_file_hash--parameters","level":4,"title":"Parameters","text":"<p>path : Path     Path to RINEX file. chunk_size : int, optional     Size of chunks to read (default: 8192 bytes).</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.rinex_file_hash--returns","level":4,"title":"Returns","text":"<p>str     First 16 characters of SHA256 hash.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.rinex_file_hash--examples","level":4,"title":"Examples","text":"<p>from pathlib import Path hash_val = rinex_file_hash(Path(\"data.rnx\")) len(hash_val) 16</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/hashing.py</code> <pre><code>def rinex_file_hash(path: Path, chunk_size: int = 8192) -&gt; str:\n    \"\"\"\n    Compute SHA256 hash of a RINEX file's content.\n\n    Uses first 16 characters of the SHA256 hexdigest for a compact hash.\n\n    Parameters\n    ----------\n    path : Path\n        Path to RINEX file.\n    chunk_size : int, optional\n        Size of chunks to read (default: 8192 bytes).\n\n    Returns\n    -------\n    str\n        First 16 characters of SHA256 hash.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; hash_val = rinex_file_hash(Path(\"data.rnx\"))\n    &gt;&gt;&gt; len(hash_val)\n    16\n    \"\"\"\n    h = hashlib.sha256()\n    with path.open(\"rb\") as f:\n        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n            h.update(chunk)\n    digest = h.hexdigest()[:16]\n    return digest\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.get_version_from_pyproject","level":2,"title":"<code>get_version_from_pyproject(pyproject_path=None)</code>","text":"<p>Get version from pyproject.toml.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.get_version_from_pyproject--parameters","level":4,"title":"Parameters","text":"<p>pyproject_path : Path, optional     Path to pyproject.toml. If None, automatically finds it by traversing     up from the current file location.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.get_version_from_pyproject--returns","level":4,"title":"Returns","text":"<p>str     Version string from pyproject.toml.</p>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-utils/#canvod.utils.tools.get_version_from_pyproject--examples","level":4,"title":"Examples","text":"<p>version = get_version_from_pyproject() print(version)  # e.g., \"0.1.0\"</p> Source code in <code>packages/canvod-utils/src/canvod/utils/tools/version.py</code> <pre><code>def get_version_from_pyproject(pyproject_path: Path | None = None) -&gt; str:\n    \"\"\"\n    Get version from pyproject.toml.\n\n    Parameters\n    ----------\n    pyproject_path : Path, optional\n        Path to pyproject.toml. If None, automatically finds it by traversing\n        up from the current file location.\n\n    Returns\n    -------\n    str\n        Version string from pyproject.toml.\n\n    Examples\n    --------\n    &gt;&gt;&gt; version = get_version_from_pyproject()\n    &gt;&gt;&gt; print(version)  # e.g., \"0.1.0\"\n    \"\"\"\n    if pyproject_path is None:\n        # Automatically find pyproject.toml at package root\n        # Start from this file and go up until we find pyproject.toml\n        current = Path(__file__).resolve()\n        for parent in current.parents:\n            candidate = parent / \"pyproject.toml\"\n            if candidate.exists():\n                pyproject_path = candidate\n                break\n\n        if pyproject_path is None:\n            msg = \"Could not find pyproject.toml\"\n            raise FileNotFoundError(msg)\n\n    with open(pyproject_path, \"rb\") as f:\n        data = tomli.load(f)\n\n    return data[\"project\"][\"version\"]\n</code></pre>","path":["API Reference","canvod.utils API Reference"],"tags":[]},{"location":"api/canvod-viz/","level":1,"title":"canvod.viz API Reference","text":"<p>2D and 3D hemispheric visualization tools.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#package","level":2,"title":"Package","text":"<p>Visualization and plotting utilities for GNSS VOD data.</p> <p>This package provides 2D and 3D visualization capabilities for hemispherical GNSS grids and VOD data, with both publication-quality (matplotlib) and interactive (plotly) rendering options.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz--examples","level":3,"title":"Examples","text":"<p>2D polar visualization::</p> <pre><code>from canvod.viz import HemisphereVisualizer2D\nfrom canvod.grids import create_hemigrid\n\ngrid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\nviz = HemisphereVisualizer2D(grid)\nfig, ax = viz.plot_grid_patches(data=vod_data, title=\"VOD Distribution\")\n</code></pre> <p>Convenience function::</p> <pre><code>from canvod.viz import visualize_grid, add_tissot_indicatrix\n\nfig, ax = visualize_grid(grid, data=vod_data, cmap='viridis')\nadd_tissot_indicatrix(ax, grid, n_sample=5)\n</code></pre> <p>3D interactive visualization::</p> <pre><code>from canvod.viz import HemisphereVisualizer3D\n\nviz3d = HemisphereVisualizer3D(grid)\nfig = viz3d.plot_hemisphere_surface(data=vod_data, title=\"Interactive VOD\")\nfig.show()\n</code></pre> <p>Unified API::</p> <pre><code>from canvod.viz import HemisphereVisualizer\n\nviz = HemisphereVisualizer(grid)\nfig_2d, ax_2d = viz.plot_2d(data=vod_data)\nfig_3d = viz.plot_3d(data=vod_data)\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer","level":2,"title":"<code>HemisphereVisualizer</code>","text":"<p>Unified hemisphere visualizer combining 2D and 3D capabilities.</p> <p>Provides consistent API for both publication-quality matplotlib plots and interactive plotly visualizations. Handles styling coordination between different rendering backends.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer--parameters","level":4,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer--examples","level":4,"title":"Examples","text":"<p>Create both 2D and 3D visualizations::</p> <pre><code>from canvod.grids import create_hemigrid\nfrom canvod.viz import HemisphereVisualizer\n\ngrid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\nviz = HemisphereVisualizer(grid)\n\n# Publication-quality 2D plot\nfig_2d, ax_2d = viz.plot_2d(\n    data=vod_data,\n    title=\"VOD Distribution\",\n    save_path=\"publication.png\"\n)\n\n# Interactive 3D plot\nfig_3d = viz.plot_3d(\n    data=vod_data,\n    title=\"Interactive VOD Explorer\"\n)\nfig_3d.show()\n</code></pre> <p>Switch styles easily::</p> <pre><code># Publication style\npub_style = create_publication_style()\nviz.set_style(pub_style)\nfig, ax = viz.plot_2d(data=vod_data)\n\n# Interactive style\nint_style = create_interactive_style(dark_mode=True)\nviz.set_style(int_style)\nfig = viz.plot_3d(data=vod_data)\n</code></pre> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>class HemisphereVisualizer:\n    \"\"\"Unified hemisphere visualizer combining 2D and 3D capabilities.\n\n    Provides consistent API for both publication-quality matplotlib plots\n    and interactive plotly visualizations. Handles styling coordination\n    between different rendering backends.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    Examples\n    --------\n    Create both 2D and 3D visualizations::\n\n        from canvod.grids import create_hemigrid\n        from canvod.viz import HemisphereVisualizer\n\n        grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\n        viz = HemisphereVisualizer(grid)\n\n        # Publication-quality 2D plot\n        fig_2d, ax_2d = viz.plot_2d(\n            data=vod_data,\n            title=\"VOD Distribution\",\n            save_path=\"publication.png\"\n        )\n\n        # Interactive 3D plot\n        fig_3d = viz.plot_3d(\n            data=vod_data,\n            title=\"Interactive VOD Explorer\"\n        )\n        fig_3d.show()\n\n    Switch styles easily::\n\n        # Publication style\n        pub_style = create_publication_style()\n        viz.set_style(pub_style)\n        fig, ax = viz.plot_2d(data=vod_data)\n\n        # Interactive style\n        int_style = create_interactive_style(dark_mode=True)\n        viz.set_style(int_style)\n        fig = viz.plot_3d(data=vod_data)\n\n    \"\"\"\n\n    def __init__(self, grid: HemiGrid) -&gt; None:\n        \"\"\"Initialize unified visualizer.\n\n        Parameters\n        ----------\n        grid : HemiGrid\n            Hemisphere grid to visualize\n\n        \"\"\"\n        self.grid = grid\n\n        # Initialize specialized visualizers\n        self.viz_2d = HemisphereVisualizer2D(grid)\n        self.viz_3d = HemisphereVisualizer3D(grid)\n\n        # Default styling\n        self.style = PlotStyle()\n\n    def set_style(self, style: PlotStyle) -&gt; None:\n        \"\"\"Set unified styling for both 2D and 3D plots.\n\n        Parameters\n        ----------\n        style : PlotStyle\n            Styling configuration\n\n        Examples\n        --------\n        &gt;&gt;&gt; pub_style = create_publication_style()\n        &gt;&gt;&gt; viz.set_style(pub_style)\n        &gt;&gt;&gt; fig, ax = viz.plot_2d(data=vod_data)\n\n        \"\"\"\n        self.style = style\n\n    def plot_2d(\n        self,\n        data: np.ndarray | None = None,\n        title: str | None = None,\n        ax: Axes | None = None,\n        save_path: Path | str | None = None,\n        style: PolarPlotStyle | None = None,\n        **kwargs: Any,\n    ) -&gt; tuple[Figure, Axes]:\n        \"\"\"Create 2D publication-quality plot.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, optional\n            Plot title\n        ax : matplotlib.axes.Axes, optional\n            Existing axes to plot on\n        save_path : Path or str, optional\n            Save figure to this path\n        style : PolarPlotStyle, optional\n            Override default 2D style\n        **kwargs\n            Additional styling parameters\n\n        Returns\n        -------\n        fig : matplotlib.figure.Figure\n            Figure object\n        ax : matplotlib.axes.Axes\n            Polar axes with plot\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig, ax = viz.plot_2d(\n        ...     data=vod_data,\n        ...     title=\"VOD Distribution\",\n        ...     cmap='plasma',\n        ...     save_path=\"output.png\",\n        ...     dpi=300\n        ... )\n\n        \"\"\"\n        if style is None:\n            style = self.style.to_polar_style()\n\n        if title:\n            style.title = title\n\n        return self.viz_2d.plot_grid_patches(\n            data=data, style=style, ax=ax, save_path=save_path, **kwargs\n        )\n\n    def plot_3d(\n        self,\n        data: np.ndarray | None = None,\n        title: str | None = None,\n        style: PlotStyle | None = None,\n        **kwargs: Any,\n    ) -&gt; go.Figure:\n        \"\"\"Create 3D interactive plot.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, optional\n            Plot title\n        style : PlotStyle, optional\n            Override default 3D style\n        **kwargs\n            Additional plotly parameters\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive 3D figure\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig = viz.plot_3d(\n        ...     data=vod_data,\n        ...     title=\"Interactive VOD\",\n        ...     opacity=0.9,\n        ...     width=1000,\n        ...     height=800\n        ... )\n        &gt;&gt;&gt; fig.show()\n        &gt;&gt;&gt; fig.write_html(\"interactive.html\")\n\n        \"\"\"\n        if style is None:\n            style = self.style\n\n        return self.viz_3d.plot_hemisphere_surface(\n            data=data, style=style, title=title, **kwargs\n        )\n\n    def plot_3d_mesh(\n        self,\n        data: np.ndarray | None = None,\n        title: str | None = None,\n        **kwargs: Any,\n    ) -&gt; go.Figure:\n        \"\"\"Create 3D mesh plot showing cell boundaries.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, optional\n            Plot title\n        **kwargs\n            Additional plotly parameters\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive mesh figure\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig = viz.plot_3d_mesh(\n        ...     data=vod_data,\n        ...     title=\"VOD Mesh View\",\n        ...     opacity=0.7\n        ... )\n\n        \"\"\"\n        return self.viz_3d.plot_cell_mesh(data=data, title=title, **kwargs)\n\n    def create_comparison_plot(\n        self,\n        data: np.ndarray | None = None,\n        title_2d: str = \"2D Polar View\",\n        title_3d: str = \"3D Hemisphere View\",\n        save_2d: Path | str | None = None,\n        save_3d: Path | str | None = None,\n    ) -&gt; tuple[tuple[Figure, Axes], go.Figure]:\n        \"\"\"Create both 2D and 3D plots for comparison.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title_2d : str, default \"2D Polar View\"\n            Title for 2D plot\n        title_3d : str, default \"3D Hemisphere View\"\n            Title for 3D plot\n        save_2d : Path or str, optional\n            Save 2D figure to this path\n        save_3d : Path or str, optional\n            Save 3D figure to this path (HTML)\n\n        Returns\n        -------\n        plot_2d : tuple of Figure and Axes\n            2D matplotlib plot\n        plot_3d : plotly.graph_objects.Figure\n            3D plotly plot\n\n        Examples\n        --------\n        &gt;&gt;&gt; (fig_2d, ax_2d), fig_3d = viz.create_comparison_plot(\n        ...     data=vod_data,\n        ...     save_2d=\"comparison_2d.png\",\n        ...     save_3d=\"comparison_3d.html\"\n        ... )\n        &gt;&gt;&gt; plt.show()  # Show 2D\n        &gt;&gt;&gt; fig_3d.show()  # Show 3D\n\n        \"\"\"\n        # Create 2D plot\n        fig_2d, ax_2d = self.plot_2d(data=data, title=title_2d, save_path=save_2d)\n\n        # Create 3D plot\n        fig_3d = self.plot_3d(data=data, title=title_3d)\n\n        # Save 3D if requested\n        if save_3d:\n            save_3d = Path(save_3d)\n            save_3d.parent.mkdir(parents=True, exist_ok=True)\n            fig_3d.write_html(str(save_3d))\n\n        return (fig_2d, ax_2d), fig_3d\n\n    def create_publication_figure(\n        self,\n        data: np.ndarray | None = None,\n        title: str = \"Hemispherical Data Distribution\",\n        save_path: Path | str | None = None,\n        dpi: int = 300,\n        **kwargs: Any,\n    ) -&gt; tuple[Figure, Axes]:\n        \"\"\"Create publication-ready figure with optimal styling.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, default \"Hemispherical Data Distribution\"\n            Plot title\n        save_path : Path or str, optional\n            Save figure to this path\n        dpi : int, default 300\n            Resolution in dots per inch\n        **kwargs\n            Additional styling parameters\n\n        Returns\n        -------\n        fig : matplotlib.figure.Figure\n            Publication-ready figure\n        ax : matplotlib.axes.Axes\n            Styled polar axes\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig, ax = viz.create_publication_figure(\n        ...     data=vod_data,\n        ...     title=\"VOD Distribution Over Rosalia Site\",\n        ...     save_path=\"paper_figure_3.png\",\n        ...     dpi=600\n        ... )\n\n        \"\"\"\n        # Use publication style and convert to PolarPlotStyle\n        pub_plot_style = create_publication_style()\n        polar_style = pub_plot_style.to_polar_style()\n        polar_style.title = title\n        polar_style.dpi = dpi\n\n        # Override with kwargs\n        for key, value in kwargs.items():\n            if hasattr(polar_style, key):\n                setattr(polar_style, key, value)\n\n        return self.plot_2d(data=data, style=polar_style, save_path=save_path)\n\n    def create_interactive_explorer(\n        self,\n        data: np.ndarray | None = None,\n        title: str = \"Interactive Data Explorer\",\n        dark_mode: bool = True,\n        save_html: Path | str | None = None,\n    ) -&gt; go.Figure:\n        \"\"\"Create interactive explorer with optimal settings.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, default \"Interactive Data Explorer\"\n            Plot title\n        dark_mode : bool, default True\n            Use dark theme\n        save_html : Path or str, optional\n            Save HTML to this path\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive explorer figure\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig = viz.create_interactive_explorer(\n        ...     data=vod_data,\n        ...     title=\"VOD Explorer\",\n        ...     dark_mode=True,\n        ...     save_html=\"explorer.html\"\n        ... )\n        &gt;&gt;&gt; fig.show()\n\n        \"\"\"\n        # Use interactive style\n        int_style = create_interactive_style(dark_mode=dark_mode)\n\n        fig = self.plot_3d(data=data, title=title, style=int_style)\n\n        # Save if requested\n        if save_html:\n            save_html = Path(save_html)\n            save_html.parent.mkdir(parents=True, exist_ok=True)\n            fig.write_html(str(save_html))\n\n        return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.__init__","level":3,"title":"<code>__init__(grid)</code>","text":"<p>Initialize unified visualizer.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.__init__--parameters","level":5,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def __init__(self, grid: HemiGrid) -&gt; None:\n    \"\"\"Initialize unified visualizer.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    \"\"\"\n    self.grid = grid\n\n    # Initialize specialized visualizers\n    self.viz_2d = HemisphereVisualizer2D(grid)\n    self.viz_3d = HemisphereVisualizer3D(grid)\n\n    # Default styling\n    self.style = PlotStyle()\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.set_style","level":3,"title":"<code>set_style(style)</code>","text":"<p>Set unified styling for both 2D and 3D plots.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.set_style--parameters","level":5,"title":"Parameters","text":"<p>style : PlotStyle     Styling configuration</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.set_style--examples","level":5,"title":"Examples","text":"<p>pub_style = create_publication_style() viz.set_style(pub_style) fig, ax = viz.plot_2d(data=vod_data)</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def set_style(self, style: PlotStyle) -&gt; None:\n    \"\"\"Set unified styling for both 2D and 3D plots.\n\n    Parameters\n    ----------\n    style : PlotStyle\n        Styling configuration\n\n    Examples\n    --------\n    &gt;&gt;&gt; pub_style = create_publication_style()\n    &gt;&gt;&gt; viz.set_style(pub_style)\n    &gt;&gt;&gt; fig, ax = viz.plot_2d(data=vod_data)\n\n    \"\"\"\n    self.style = style\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_2d","level":3,"title":"<code>plot_2d(data=None, title=None, ax=None, save_path=None, style=None, **kwargs)</code>","text":"<p>Create 2D publication-quality plot.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_2d--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, optional     Plot title ax : matplotlib.axes.Axes, optional     Existing axes to plot on save_path : Path or str, optional     Save figure to this path style : PolarPlotStyle, optional     Override default 2D style **kwargs     Additional styling parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_2d--returns","level":5,"title":"Returns","text":"<p>fig : matplotlib.figure.Figure     Figure object ax : matplotlib.axes.Axes     Polar axes with plot</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_2d--examples","level":5,"title":"Examples","text":"<p>fig, ax = viz.plot_2d( ...     data=vod_data, ...     title=\"VOD Distribution\", ...     cmap='plasma', ...     save_path=\"output.png\", ...     dpi=300 ... )</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def plot_2d(\n    self,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    ax: Axes | None = None,\n    save_path: Path | str | None = None,\n    style: PolarPlotStyle | None = None,\n    **kwargs: Any,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Create 2D publication-quality plot.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    ax : matplotlib.axes.Axes, optional\n        Existing axes to plot on\n    save_path : Path or str, optional\n        Save figure to this path\n    style : PolarPlotStyle, optional\n        Override default 2D style\n    **kwargs\n        Additional styling parameters\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Figure object\n    ax : matplotlib.axes.Axes\n        Polar axes with plot\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = viz.plot_2d(\n    ...     data=vod_data,\n    ...     title=\"VOD Distribution\",\n    ...     cmap='plasma',\n    ...     save_path=\"output.png\",\n    ...     dpi=300\n    ... )\n\n    \"\"\"\n    if style is None:\n        style = self.style.to_polar_style()\n\n    if title:\n        style.title = title\n\n    return self.viz_2d.plot_grid_patches(\n        data=data, style=style, ax=ax, save_path=save_path, **kwargs\n    )\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_3d","level":3,"title":"<code>plot_3d(data=None, title=None, style=None, **kwargs)</code>","text":"<p>Create 3D interactive plot.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_3d--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, optional     Plot title style : PlotStyle, optional     Override default 3D style **kwargs     Additional plotly parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_3d--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive 3D figure</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_3d--examples","level":5,"title":"Examples","text":"<p>fig = viz.plot_3d( ...     data=vod_data, ...     title=\"Interactive VOD\", ...     opacity=0.9, ...     width=1000, ...     height=800 ... ) fig.show() fig.write_html(\"interactive.html\")</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def plot_3d(\n    self,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    style: PlotStyle | None = None,\n    **kwargs: Any,\n) -&gt; go.Figure:\n    \"\"\"Create 3D interactive plot.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    style : PlotStyle, optional\n        Override default 3D style\n    **kwargs\n        Additional plotly parameters\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive 3D figure\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = viz.plot_3d(\n    ...     data=vod_data,\n    ...     title=\"Interactive VOD\",\n    ...     opacity=0.9,\n    ...     width=1000,\n    ...     height=800\n    ... )\n    &gt;&gt;&gt; fig.show()\n    &gt;&gt;&gt; fig.write_html(\"interactive.html\")\n\n    \"\"\"\n    if style is None:\n        style = self.style\n\n    return self.viz_3d.plot_hemisphere_surface(\n        data=data, style=style, title=title, **kwargs\n    )\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_3d_mesh","level":3,"title":"<code>plot_3d_mesh(data=None, title=None, **kwargs)</code>","text":"<p>Create 3D mesh plot showing cell boundaries.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_3d_mesh--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, optional     Plot title **kwargs     Additional plotly parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_3d_mesh--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive mesh figure</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.plot_3d_mesh--examples","level":5,"title":"Examples","text":"<p>fig = viz.plot_3d_mesh( ...     data=vod_data, ...     title=\"VOD Mesh View\", ...     opacity=0.7 ... )</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def plot_3d_mesh(\n    self,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    **kwargs: Any,\n) -&gt; go.Figure:\n    \"\"\"Create 3D mesh plot showing cell boundaries.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    **kwargs\n        Additional plotly parameters\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive mesh figure\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = viz.plot_3d_mesh(\n    ...     data=vod_data,\n    ...     title=\"VOD Mesh View\",\n    ...     opacity=0.7\n    ... )\n\n    \"\"\"\n    return self.viz_3d.plot_cell_mesh(data=data, title=title, **kwargs)\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_comparison_plot","level":3,"title":"<code>create_comparison_plot(data=None, title_2d='2D Polar View', title_3d='3D Hemisphere View', save_2d=None, save_3d=None)</code>","text":"<p>Create both 2D and 3D plots for comparison.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_comparison_plot--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title_2d : str, default \"2D Polar View\"     Title for 2D plot title_3d : str, default \"3D Hemisphere View\"     Title for 3D plot save_2d : Path or str, optional     Save 2D figure to this path save_3d : Path or str, optional     Save 3D figure to this path (HTML)</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_comparison_plot--returns","level":5,"title":"Returns","text":"<p>plot_2d : tuple of Figure and Axes     2D matplotlib plot plot_3d : plotly.graph_objects.Figure     3D plotly plot</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_comparison_plot--examples","level":5,"title":"Examples","text":"<p>(fig_2d, ax_2d), fig_3d = viz.create_comparison_plot( ...     data=vod_data, ...     save_2d=\"comparison_2d.png\", ...     save_3d=\"comparison_3d.html\" ... ) plt.show()  # Show 2D fig_3d.show()  # Show 3D</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def create_comparison_plot(\n    self,\n    data: np.ndarray | None = None,\n    title_2d: str = \"2D Polar View\",\n    title_3d: str = \"3D Hemisphere View\",\n    save_2d: Path | str | None = None,\n    save_3d: Path | str | None = None,\n) -&gt; tuple[tuple[Figure, Axes], go.Figure]:\n    \"\"\"Create both 2D and 3D plots for comparison.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title_2d : str, default \"2D Polar View\"\n        Title for 2D plot\n    title_3d : str, default \"3D Hemisphere View\"\n        Title for 3D plot\n    save_2d : Path or str, optional\n        Save 2D figure to this path\n    save_3d : Path or str, optional\n        Save 3D figure to this path (HTML)\n\n    Returns\n    -------\n    plot_2d : tuple of Figure and Axes\n        2D matplotlib plot\n    plot_3d : plotly.graph_objects.Figure\n        3D plotly plot\n\n    Examples\n    --------\n    &gt;&gt;&gt; (fig_2d, ax_2d), fig_3d = viz.create_comparison_plot(\n    ...     data=vod_data,\n    ...     save_2d=\"comparison_2d.png\",\n    ...     save_3d=\"comparison_3d.html\"\n    ... )\n    &gt;&gt;&gt; plt.show()  # Show 2D\n    &gt;&gt;&gt; fig_3d.show()  # Show 3D\n\n    \"\"\"\n    # Create 2D plot\n    fig_2d, ax_2d = self.plot_2d(data=data, title=title_2d, save_path=save_2d)\n\n    # Create 3D plot\n    fig_3d = self.plot_3d(data=data, title=title_3d)\n\n    # Save 3D if requested\n    if save_3d:\n        save_3d = Path(save_3d)\n        save_3d.parent.mkdir(parents=True, exist_ok=True)\n        fig_3d.write_html(str(save_3d))\n\n    return (fig_2d, ax_2d), fig_3d\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_publication_figure","level":3,"title":"<code>create_publication_figure(data=None, title='Hemispherical Data Distribution', save_path=None, dpi=300, **kwargs)</code>","text":"<p>Create publication-ready figure with optimal styling.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_publication_figure--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, default \"Hemispherical Data Distribution\"     Plot title save_path : Path or str, optional     Save figure to this path dpi : int, default 300     Resolution in dots per inch **kwargs     Additional styling parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_publication_figure--returns","level":5,"title":"Returns","text":"<p>fig : matplotlib.figure.Figure     Publication-ready figure ax : matplotlib.axes.Axes     Styled polar axes</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_publication_figure--examples","level":5,"title":"Examples","text":"<p>fig, ax = viz.create_publication_figure( ...     data=vod_data, ...     title=\"VOD Distribution Over Rosalia Site\", ...     save_path=\"paper_figure_3.png\", ...     dpi=600 ... )</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def create_publication_figure(\n    self,\n    data: np.ndarray | None = None,\n    title: str = \"Hemispherical Data Distribution\",\n    save_path: Path | str | None = None,\n    dpi: int = 300,\n    **kwargs: Any,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Create publication-ready figure with optimal styling.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, default \"Hemispherical Data Distribution\"\n        Plot title\n    save_path : Path or str, optional\n        Save figure to this path\n    dpi : int, default 300\n        Resolution in dots per inch\n    **kwargs\n        Additional styling parameters\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Publication-ready figure\n    ax : matplotlib.axes.Axes\n        Styled polar axes\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = viz.create_publication_figure(\n    ...     data=vod_data,\n    ...     title=\"VOD Distribution Over Rosalia Site\",\n    ...     save_path=\"paper_figure_3.png\",\n    ...     dpi=600\n    ... )\n\n    \"\"\"\n    # Use publication style and convert to PolarPlotStyle\n    pub_plot_style = create_publication_style()\n    polar_style = pub_plot_style.to_polar_style()\n    polar_style.title = title\n    polar_style.dpi = dpi\n\n    # Override with kwargs\n    for key, value in kwargs.items():\n        if hasattr(polar_style, key):\n            setattr(polar_style, key, value)\n\n    return self.plot_2d(data=data, style=polar_style, save_path=save_path)\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_interactive_explorer","level":3,"title":"<code>create_interactive_explorer(data=None, title='Interactive Data Explorer', dark_mode=True, save_html=None)</code>","text":"<p>Create interactive explorer with optimal settings.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_interactive_explorer--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, default \"Interactive Data Explorer\"     Plot title dark_mode : bool, default True     Use dark theme save_html : Path or str, optional     Save HTML to this path</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_interactive_explorer--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive explorer figure</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer.create_interactive_explorer--examples","level":5,"title":"Examples","text":"<p>fig = viz.create_interactive_explorer( ...     data=vod_data, ...     title=\"VOD Explorer\", ...     dark_mode=True, ...     save_html=\"explorer.html\" ... ) fig.show()</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def create_interactive_explorer(\n    self,\n    data: np.ndarray | None = None,\n    title: str = \"Interactive Data Explorer\",\n    dark_mode: bool = True,\n    save_html: Path | str | None = None,\n) -&gt; go.Figure:\n    \"\"\"Create interactive explorer with optimal settings.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, default \"Interactive Data Explorer\"\n        Plot title\n    dark_mode : bool, default True\n        Use dark theme\n    save_html : Path or str, optional\n        Save HTML to this path\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive explorer figure\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = viz.create_interactive_explorer(\n    ...     data=vod_data,\n    ...     title=\"VOD Explorer\",\n    ...     dark_mode=True,\n    ...     save_html=\"explorer.html\"\n    ... )\n    &gt;&gt;&gt; fig.show()\n\n    \"\"\"\n    # Use interactive style\n    int_style = create_interactive_style(dark_mode=dark_mode)\n\n    fig = self.plot_3d(data=data, title=title, style=int_style)\n\n    # Save if requested\n    if save_html:\n        save_html = Path(save_html)\n        save_html.parent.mkdir(parents=True, exist_ok=True)\n        fig.write_html(str(save_html))\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer2D","level":2,"title":"<code>HemisphereVisualizer2D</code>","text":"<p>2D hemisphere visualization using matplotlib.</p> <p>Creates publication-quality polar projection plots of hemispherical grids. Supports multiple grid types and rendering methods.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer2D--parameters","level":4,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer2D--examples","level":4,"title":"Examples","text":"<p>from canvod.grids import create_hemigrid from canvod.viz import HemisphereVisualizer2D</p> <p>grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0) viz = HemisphereVisualizer2D(grid) fig, ax = viz.plot_grid_patches(data=vod_data, title=\"VOD Distribution\") plt.savefig(\"vod_plot.png\", dpi=300, bbox_inches='tight')</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_2d.py</code> <pre><code>class HemisphereVisualizer2D:\n    \"\"\"2D hemisphere visualization using matplotlib.\n\n    Creates publication-quality polar projection plots of hemispherical grids.\n    Supports multiple grid types and rendering methods.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import create_hemigrid\n    &gt;&gt;&gt; from canvod.viz import HemisphereVisualizer2D\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\n    &gt;&gt;&gt; viz = HemisphereVisualizer2D(grid)\n    &gt;&gt;&gt; fig, ax = viz.plot_grid_patches(data=vod_data, title=\"VOD Distribution\")\n    &gt;&gt;&gt; plt.savefig(\"vod_plot.png\", dpi=300, bbox_inches='tight')\n\n    \"\"\"\n\n    def __init__(self, grid: HemiGrid) -&gt; None:\n        \"\"\"Initialize 2D hemisphere visualizer.\n\n        Parameters\n        ----------\n        grid : HemiGrid\n            Hemisphere grid to visualize\n\n        \"\"\"\n        self.grid = grid\n        self._patches_cache: list[Polygon] | None = None\n        self._cell_indices_cache: np.ndarray | None = None\n\n    def plot_grid_patches(\n        self,\n        data: np.ndarray | None = None,\n        style: PolarPlotStyle | None = None,\n        ax: Axes | None = None,\n        save_path: Path | str | None = None,\n        **style_kwargs: Any,\n    ) -&gt; tuple[Figure, Axes]:\n        \"\"\"Plot hemisphere grid as colored patches in polar projection.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell. If None, plots uniform grid.\n        style : PolarPlotStyle, optional\n            Styling configuration. If None, uses defaults.\n        ax : matplotlib.axes.Axes, optional\n            Existing polar axes to plot on. If None, creates new figure.\n        save_path : Path or str, optional\n            If provided, saves figure to this path\n        **style_kwargs\n            Override individual style parameters\n\n        Returns\n        -------\n        fig : matplotlib.figure.Figure\n            Figure object\n        ax : matplotlib.axes.Axes\n            Polar axes with plot\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig, ax = viz.plot_grid_patches(\n        ...     data=vod_data,\n        ...     title=\"VOD Distribution\",\n        ...     cmap='plasma',\n        ...     save_path=\"output.png\"\n        ... )\n\n        \"\"\"\n        # Initialize style\n        if style is None:\n            style = PolarPlotStyle(**style_kwargs)\n        else:\n            # Override style with kwargs\n            for key, value in style_kwargs.items():\n                if hasattr(style, key):\n                    setattr(style, key, value)\n\n        # Create figure if needed\n        if ax is None:\n            fig, ax = plt.subplots(\n                figsize=style.figsize, dpi=style.dpi, subplot_kw={\"projection\": \"polar\"}\n            )\n        else:\n            fig = ax.figure\n\n        # Get patches for grid\n        patches, cell_indices = self._extract_grid_patches()\n\n        # Map data to patches\n        patch_data = self._map_data_to_patches(data, cell_indices)\n\n        # Determine color limits\n        vmin = style.vmin if style.vmin is not None else np.nanmin(patch_data)\n        vmax = style.vmax if style.vmax is not None else np.nanmax(patch_data)\n\n        # Create patch collection\n        pc = PatchCollection(\n            patches,\n            cmap=style.cmap,\n            edgecolor=style.edgecolor,\n            linewidth=style.linewidth,\n            alpha=style.alpha,\n        )\n        pc.set_array(np.ma.masked_invalid(patch_data))\n        pc.set_clim(vmin, vmax)\n\n        # Add to axes\n        ax.add_collection(pc)\n\n        # Style polar axes\n        self._apply_polar_styling(ax, style)\n\n        # Add colorbar\n        cbar = fig.colorbar(\n            pc,\n            ax=ax,\n            shrink=style.colorbar_shrink,\n            pad=style.colorbar_pad,\n        )\n        cbar.set_label(style.colorbar_label, fontsize=style.colorbar_fontsize)\n\n        # Set title\n        if style.title:\n            ax.set_title(style.title, y=1.08, fontsize=14)\n\n        # Save if requested\n        if save_path:\n            save_path = Path(save_path)\n            save_path.parent.mkdir(parents=True, exist_ok=True)\n            fig.savefig(\n                save_path,\n                dpi=style.dpi,\n                bbox_inches=\"tight\",\n                facecolor=\"white\",\n                edgecolor=\"none\",\n            )\n\n        return fig, ax\n\n    def _extract_grid_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract 2D polygon patches from hemispherical grid.\n\n        Returns\n        -------\n        patches : list of Polygon\n            Matplotlib polygon patches\n        cell_indices : np.ndarray\n            Corresponding cell indices in grid\n\n        \"\"\"\n        # Use cache if available\n        if self._patches_cache is not None and self._cell_indices_cache is not None:\n            return self._patches_cache, self._cell_indices_cache\n\n        grid_type = self.grid.grid_type.lower()\n\n        _rectangular_types = {\"equal_area\", \"equal_angle\", \"equirectangular\"}\n        if grid_type in _rectangular_types:\n            patches, indices = self._extract_rectangular_patches()\n        elif grid_type == \"htm\":\n            patches, indices = self._extract_htm_patches()\n        elif grid_type == \"geodesic\":\n            patches, indices = self._extract_geodesic_patches()\n        elif grid_type == \"healpix\":\n            patches, indices = self._extract_healpix_patches()\n        elif grid_type == \"fibonacci\":\n            patches, indices = self._extract_fibonacci_patches()\n        else:\n            raise ValueError(f\"Unsupported grid type: {grid_type}\")\n\n        # Cache results\n        self._patches_cache = patches\n        self._cell_indices_cache = indices\n\n        return patches, indices\n\n    def _extract_rectangular_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract patches from rectangular/equal-area grid.\"\"\"\n        patches = []\n        cell_indices = []\n\n        # Access the grid DataFrame from GridData\n        grid_df = self.grid.grid\n\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            phi_min = row[\"phi_min\"]\n            phi_max = row[\"phi_max\"]\n            theta_min = row[\"theta_min\"]\n            theta_max = row[\"theta_max\"]\n\n            # Skip cells beyond hemisphere\n            if theta_min &gt; np.pi / 2:\n                continue\n\n            # Convert to polar coordinates (rho = sin(theta))\n            rho_min = np.sin(theta_min)\n            rho_max = np.sin(theta_max)\n\n            # Create rectangular patch in polar coordinates\n            vertices = np.array(\n                [\n                    [phi_min, rho_min],\n                    [phi_max, rho_min],\n                    [phi_max, rho_max],\n                    [phi_min, rho_max],\n                ]\n            )\n\n            patches.append(Polygon(vertices, closed=True))\n            cell_indices.append(idx)\n\n        return patches, np.array(cell_indices)\n\n    def _extract_htm_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract triangular patches from HTM grid.\"\"\"\n        patches = []\n        cell_indices = []\n\n        # Access the grid DataFrame from GridData\n        grid_df = self.grid.grid\n\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                # HTM stores vertices as columns htm_vertex_0, htm_vertex_1,\n                # htm_vertex_2.\n                v0 = np.array(row[\"htm_vertex_0\"], dtype=float)\n                v1 = np.array(row[\"htm_vertex_1\"], dtype=float)\n                v2 = np.array(row[\"htm_vertex_2\"], dtype=float)\n\n                vertices_3d = np.array([v0, v1, v2])\n                x, y, z = (\n                    vertices_3d[:, 0],\n                    vertices_3d[:, 1],\n                    vertices_3d[:, 2],\n                )\n\n                # Convert to spherical coordinates\n                r = np.sqrt(x**2 + y**2 + z**2)\n                theta = np.arccos(np.clip(z / r, -1, 1))\n                phi = np.arctan2(y, x)\n                phi = np.mod(phi, 2 * np.pi)\n\n                # Skip if beyond hemisphere\n                if np.all(theta &gt; np.pi / 2):\n                    continue\n\n                # Convert to polar coordinates (rho = sin(theta))\n                rho = np.sin(theta)\n                vertices_2d = np.column_stack([phi, rho])\n\n                patches.append(Polygon(vertices_2d, closed=True))\n                cell_indices.append(idx)\n\n            except (KeyError, TypeError):\n                # Skip cells that don't have proper HTM vertex data\n                continue\n\n        return patches, np.array(cell_indices)\n\n    def _extract_geodesic_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract triangular patches from geodesic grid.\n\n        The ``geodesic_vertices`` column stores vertex **indices** into the\n        shared ``grid.vertices`` coordinate array (shape ``(n_vertices, 3)``).\n        \"\"\"\n        patches = []\n        cell_indices = []\n\n        grid_df = self.grid.grid\n        shared_vertices = self.grid.vertices  # (n_vertices, 3) or None\n\n        if shared_vertices is None or \"geodesic_vertices\" not in grid_df.columns:\n            # No vertex data — fall back to bounding-box rectangles\n            return self._extract_rectangular_patches()\n\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                v_indices = np.array(row[\"geodesic_vertices\"], dtype=int)\n                if len(v_indices) &lt; 3:\n                    continue\n\n                # Look up actual 3D coordinates from shared vertex array\n                verts_3d = shared_vertices[v_indices]  # (3, 3)\n                x, y, z = verts_3d[:, 0], verts_3d[:, 1], verts_3d[:, 2]\n\n                r = np.sqrt(x**2 + y**2 + z**2)\n                theta = np.arccos(np.clip(z / r, -1, 1))\n                phi = np.arctan2(y, x)\n                phi = np.mod(phi, 2 * np.pi)\n\n                if np.all(theta &gt; np.pi / 2):\n                    continue\n\n                rho = np.sin(theta)\n                vertices_2d = np.column_stack([phi, rho])\n                patches.append(Polygon(vertices_2d, closed=True))\n                cell_indices.append(idx)\n\n            except (IndexError, KeyError, TypeError, ValueError):\n                continue\n\n        return patches, np.array(cell_indices)\n\n    def _extract_healpix_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract patches from HEALPix grid via ``healpy.boundaries``.\"\"\"\n        try:\n            import healpy as hp\n        except ImportError:\n            raise ImportError(\n                \"healpy is required for HEALPix 2D visualization. \"\n                \"Install with: pip install healpy\"\n            )\n\n        patches = []\n        cell_indices = []\n        grid_df = self.grid.grid\n        nside = int(grid_df[\"healpix_nside\"][0])\n\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            ipix = int(row[\"healpix_ipix\"])\n            # boundaries returns shape (3, n_vertices) in Cartesian\n            boundary = hp.boundaries(nside, ipix, step=4)\n            x, y, z = boundary[0], boundary[1], boundary[2]\n\n            # Skip pixels entirely below horizon\n            if np.all(z &lt; -0.01):\n                continue\n\n            r = np.sqrt(x**2 + y**2 + z**2)\n            theta = np.arccos(np.clip(z / r, -1, 1))\n            phi = np.arctan2(y, x)\n            phi = np.mod(phi, 2 * np.pi)\n\n            # Keep only vertices in upper hemisphere\n            mask = theta &lt;= np.pi / 2 + 0.01\n            if not np.any(mask):\n                continue\n\n            rho = np.sin(theta)\n            vertices_2d = np.column_stack([phi, rho])\n            patches.append(Polygon(vertices_2d, closed=True))\n            cell_indices.append(idx)\n\n        return patches, np.array(cell_indices)\n\n    def _extract_fibonacci_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract patches from Fibonacci grid using Voronoi regions.\"\"\"\n        patches = []\n        cell_indices = []\n        grid_df = self.grid.grid\n        voronoi = self.grid.voronoi  # scipy.spatial.SphericalVoronoi or None\n\n        if voronoi is None or \"voronoi_region\" not in grid_df.columns:\n            # No Voronoi data — fall back to bounding-box rectangles\n            return self._extract_rectangular_patches()\n\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                region_indices = row[\"voronoi_region\"]\n                if region_indices is None or len(region_indices) &lt; 3:\n                    continue\n\n                verts_3d = voronoi.vertices[region_indices]\n                x, y, z = verts_3d[:, 0], verts_3d[:, 1], verts_3d[:, 2]\n\n                # Skip cells entirely below horizon\n                if np.all(z &lt; -0.01):\n                    continue\n\n                r = np.sqrt(x**2 + y**2 + z**2)\n                theta = np.arccos(np.clip(z / r, -1, 1))\n                phi = np.arctan2(y, x)\n                phi = np.mod(phi, 2 * np.pi)\n\n                # Vertices are already in polygon winding order from\n                # sort_vertices_of_regions() — use directly.\n                rho = np.sin(theta)\n                vertices_2d = np.column_stack([phi, rho])\n                patches.append(Polygon(vertices_2d, closed=True))\n                cell_indices.append(idx)\n\n            except (IndexError, KeyError, TypeError, ValueError):\n                continue\n\n        return patches, np.array(cell_indices)\n\n    def _map_data_to_patches(\n        self,\n        data: np.ndarray | None,\n        cell_indices: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"Map data values to patches.\n\n        Parameters\n        ----------\n        data : np.ndarray or None\n            Data per grid cell\n        cell_indices : np.ndarray\n            Cell indices corresponding to patches\n\n        Returns\n        -------\n        np.ndarray\n            Data values for each patch\n\n        \"\"\"\n        if data is None:\n            return np.ones(len(cell_indices)) * 0.5\n\n        return data[cell_indices]\n\n    def _apply_polar_styling(\n        self,\n        ax: Axes,\n        style: PolarPlotStyle,\n    ) -&gt; None:\n        \"\"\"Apply styling to polar axes.\n\n        Parameters\n        ----------\n        ax : matplotlib.axes.Axes\n            Polar axes to style\n        style : PolarPlotStyle\n            Styling configuration\n\n        \"\"\"\n        # Set rho limits (0 to 1 for hemisphere projection)\n        ax.set_ylim(0, 1.0)\n\n        # Configure polar axis orientation\n        ax.set_theta_zero_location(\"N\")  # North at top\n        ax.set_theta_direction(-1)  # Clockwise (azimuth convention)\n\n        # Add degree labels on radial axis\n        if style.show_degree_labels:\n            theta_labels = style.theta_labels\n            rho_ticks = [np.sin(np.radians(t)) for t in theta_labels]\n            ax.set_yticks(rho_ticks)\n            ax.set_yticklabels([f\"{t}°\" for t in theta_labels])\n\n        # Grid styling\n        if style.show_grid:\n            ax.grid(\n                True,\n                alpha=style.grid_alpha,\n                linestyle=style.grid_linestyle,\n                color=\"gray\",\n            )\n        else:\n            ax.grid(False)\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer2D.__init__","level":3,"title":"<code>__init__(grid)</code>","text":"<p>Initialize 2D hemisphere visualizer.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer2D.__init__--parameters","level":5,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_2d.py</code> <pre><code>def __init__(self, grid: HemiGrid) -&gt; None:\n    \"\"\"Initialize 2D hemisphere visualizer.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    \"\"\"\n    self.grid = grid\n    self._patches_cache: list[Polygon] | None = None\n    self._cell_indices_cache: np.ndarray | None = None\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer2D.plot_grid_patches","level":3,"title":"<code>plot_grid_patches(data=None, style=None, ax=None, save_path=None, **style_kwargs)</code>","text":"<p>Plot hemisphere grid as colored patches in polar projection.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer2D.plot_grid_patches--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell. If None, plots uniform grid. style : PolarPlotStyle, optional     Styling configuration. If None, uses defaults. ax : matplotlib.axes.Axes, optional     Existing polar axes to plot on. If None, creates new figure. save_path : Path or str, optional     If provided, saves figure to this path **style_kwargs     Override individual style parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer2D.plot_grid_patches--returns","level":5,"title":"Returns","text":"<p>fig : matplotlib.figure.Figure     Figure object ax : matplotlib.axes.Axes     Polar axes with plot</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer2D.plot_grid_patches--examples","level":5,"title":"Examples","text":"<p>fig, ax = viz.plot_grid_patches( ...     data=vod_data, ...     title=\"VOD Distribution\", ...     cmap='plasma', ...     save_path=\"output.png\" ... )</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_2d.py</code> <pre><code>def plot_grid_patches(\n    self,\n    data: np.ndarray | None = None,\n    style: PolarPlotStyle | None = None,\n    ax: Axes | None = None,\n    save_path: Path | str | None = None,\n    **style_kwargs: Any,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Plot hemisphere grid as colored patches in polar projection.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell. If None, plots uniform grid.\n    style : PolarPlotStyle, optional\n        Styling configuration. If None, uses defaults.\n    ax : matplotlib.axes.Axes, optional\n        Existing polar axes to plot on. If None, creates new figure.\n    save_path : Path or str, optional\n        If provided, saves figure to this path\n    **style_kwargs\n        Override individual style parameters\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Figure object\n    ax : matplotlib.axes.Axes\n        Polar axes with plot\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = viz.plot_grid_patches(\n    ...     data=vod_data,\n    ...     title=\"VOD Distribution\",\n    ...     cmap='plasma',\n    ...     save_path=\"output.png\"\n    ... )\n\n    \"\"\"\n    # Initialize style\n    if style is None:\n        style = PolarPlotStyle(**style_kwargs)\n    else:\n        # Override style with kwargs\n        for key, value in style_kwargs.items():\n            if hasattr(style, key):\n                setattr(style, key, value)\n\n    # Create figure if needed\n    if ax is None:\n        fig, ax = plt.subplots(\n            figsize=style.figsize, dpi=style.dpi, subplot_kw={\"projection\": \"polar\"}\n        )\n    else:\n        fig = ax.figure\n\n    # Get patches for grid\n    patches, cell_indices = self._extract_grid_patches()\n\n    # Map data to patches\n    patch_data = self._map_data_to_patches(data, cell_indices)\n\n    # Determine color limits\n    vmin = style.vmin if style.vmin is not None else np.nanmin(patch_data)\n    vmax = style.vmax if style.vmax is not None else np.nanmax(patch_data)\n\n    # Create patch collection\n    pc = PatchCollection(\n        patches,\n        cmap=style.cmap,\n        edgecolor=style.edgecolor,\n        linewidth=style.linewidth,\n        alpha=style.alpha,\n    )\n    pc.set_array(np.ma.masked_invalid(patch_data))\n    pc.set_clim(vmin, vmax)\n\n    # Add to axes\n    ax.add_collection(pc)\n\n    # Style polar axes\n    self._apply_polar_styling(ax, style)\n\n    # Add colorbar\n    cbar = fig.colorbar(\n        pc,\n        ax=ax,\n        shrink=style.colorbar_shrink,\n        pad=style.colorbar_pad,\n    )\n    cbar.set_label(style.colorbar_label, fontsize=style.colorbar_fontsize)\n\n    # Set title\n    if style.title:\n        ax.set_title(style.title, y=1.08, fontsize=14)\n\n    # Save if requested\n    if save_path:\n        save_path = Path(save_path)\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        fig.savefig(\n            save_path,\n            dpi=style.dpi,\n            bbox_inches=\"tight\",\n            facecolor=\"white\",\n            edgecolor=\"none\",\n        )\n\n    return fig, ax\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D","level":2,"title":"<code>HemisphereVisualizer3D</code>","text":"<p>3D hemisphere visualization using plotly.</p> <p>Creates interactive 3D plots with rotation, zoom, and hover capabilities. Designed for exploratory data analysis and presentations.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D--parameters","level":4,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D--examples","level":4,"title":"Examples","text":"<p>from canvod.grids import create_hemigrid from canvod.viz import HemisphereVisualizer3D</p> <p>grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0) viz = HemisphereVisualizer3D(grid) fig = viz.plot_hemisphere_surface(data=vod_data, title=\"Interactive VOD\") fig.show()</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>class HemisphereVisualizer3D:\n    \"\"\"3D hemisphere visualization using plotly.\n\n    Creates interactive 3D plots with rotation, zoom, and hover capabilities.\n    Designed for exploratory data analysis and presentations.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import create_hemigrid\n    &gt;&gt;&gt; from canvod.viz import HemisphereVisualizer3D\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\n    &gt;&gt;&gt; viz = HemisphereVisualizer3D(grid)\n    &gt;&gt;&gt; fig = viz.plot_hemisphere_surface(data=vod_data, title=\"Interactive VOD\")\n    &gt;&gt;&gt; fig.show()\n\n    \"\"\"\n\n    def __init__(self, grid: HemiGrid) -&gt; None:\n        \"\"\"Initialize 3D hemisphere visualizer.\n\n        Parameters\n        ----------\n        grid : HemiGrid\n            Hemisphere grid to visualize\n\n        \"\"\"\n        self.grid = grid\n\n    def plot_hemisphere_surface(\n        self,\n        data: np.ndarray | None = None,\n        style: PlotStyle | None = None,\n        title: str | None = None,\n        colorscale: str = \"Viridis\",\n        opacity: float = 0.8,\n        show_wireframe: bool = True,\n        show_colorbar: bool = True,\n        width: int = 800,\n        height: int = 600,\n        **kwargs: Any,\n    ) -&gt; go.Figure:\n        \"\"\"Create 3D surface plot on hemisphere with actual cell patches.\n\n        Renders grid cells as colored 3D patches (not just points).\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell. If None, shows grid structure.\n        style : PlotStyle, optional\n            Styling configuration. If None, uses defaults.\n        title : str, optional\n            Plot title\n        colorscale : str, default 'Viridis'\n            Plotly colorscale name\n        opacity : float, default 0.8\n            Surface opacity (0=transparent, 1=opaque)\n        show_wireframe : bool, default True\n            Show grid lines on surface\n        show_colorbar : bool, default True\n            Display colorbar\n        width : int, default 800\n            Figure width in pixels\n        height : int, default 600\n            Figure height in pixels\n        **kwargs\n            Additional plotly trace parameters\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive 3D figure with cell patches\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig = viz.plot_hemisphere_surface(\n        ...     data=vod_data,\n        ...     title=\"VOD Distribution 3D\",\n        ...     colorscale='Plasma',\n        ...     opacity=0.9\n        ... )\n        &gt;&gt;&gt; fig.write_html(\"vod_3d.html\")\n\n        \"\"\"\n        # Initialize style\n        if style is None:\n            style = PlotStyle()\n            colorscale = colorscale  # Use parameter\n        else:\n            colorscale = style.colorscale\n\n        # Render grid based on type\n        grid_type = self.grid.grid_type.lower()\n\n        if grid_type in [\"equal_area\", \"equal_angle\", \"equirectangular\"]:\n            trace = self._render_rectangular_cells(\n                data,\n                colorscale,\n                opacity,\n                show_colorbar,\n            )\n        elif grid_type == \"htm\":\n            trace = self._render_htm_cells(data, colorscale, opacity, show_colorbar)\n        elif grid_type == \"geodesic\":\n            trace = self._render_geodesic_cells(\n                data, colorscale, opacity, show_colorbar\n            )\n        elif grid_type == \"healpix\":\n            trace = self._render_healpix_cells(data, colorscale, opacity, show_colorbar)\n        elif grid_type == \"fibonacci\":\n            trace = self._render_fibonacci_cells(\n                data, colorscale, opacity, show_colorbar\n            )\n        else:\n            # Fallback to scatter for unknown types\n            trace = self._render_scatter_fallback(\n                data,\n                colorscale,\n                opacity,\n                show_colorbar,\n            )\n\n        fig = go.Figure(data=[trace])\n\n        # Apply layout\n        layout_config = style.to_plotly_layout() if style else {}\n        layout_config.update(\n            {\n                \"title\": title or \"Hemisphere 3D\",\n                \"scene\": dict(\n                    aspectmode=\"data\",\n                    xaxis=dict(title=\"East\", showbackground=False),\n                    yaxis=dict(title=\"North\", showbackground=False),\n                    zaxis=dict(title=\"Up\", showbackground=False),\n                    bgcolor=layout_config.get(\"plot_bgcolor\", \"white\"),\n                ),\n                \"width\": width,\n                \"height\": height,\n                \"margin\": dict(l=0, r=0, b=0, t=40),\n            }\n        )\n\n        fig.update_layout(**layout_config)\n\n        return fig\n\n    def _render_rectangular_cells(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Mesh3d:\n        \"\"\"Render rectangular grid cells as 3D mesh patches.\"\"\"\n        grid_df = self.grid.grid\n\n        all_x, all_y, all_z = [], [], []\n        all_i, all_j, all_k = [], [], []\n        all_colors = []\n\n        vertex_count = 0\n\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                phi_min, phi_max = row[\"phi_min\"], row[\"phi_max\"]\n                theta_min, theta_max = row[\"theta_min\"], row[\"theta_max\"]\n\n                # Skip if beyond hemisphere\n                if theta_min &gt; np.pi / 2:\n                    continue\n\n                # Create 4 corners of rectangular cell\n                phi_corners = [phi_min, phi_max, phi_max, phi_min]\n                theta_corners = [theta_min, theta_min, theta_max, theta_max]\n\n                patch_x, patch_y, patch_z = [], [], []\n                for phi, theta in zip(phi_corners, theta_corners):\n                    # Convert to 3D Cartesian\n                    x = np.sin(theta) * np.sin(phi)\n                    y = np.sin(theta) * np.cos(phi)\n                    z = np.cos(theta)\n                    patch_x.append(x)\n                    patch_y.append(y)\n                    patch_z.append(z)\n\n                all_x.extend(patch_x)\n                all_y.extend(patch_y)\n                all_z.extend(patch_z)\n\n                # Color value for this cell\n                color_val = data[i] if data is not None else 0.5\n                all_colors.extend([color_val] * 4)\n\n                # Two triangles per rectangle\n                all_i.extend([vertex_count, vertex_count])\n                all_j.extend([vertex_count + 1, vertex_count + 2])\n                all_k.extend([vertex_count + 2, vertex_count + 3])\n\n                vertex_count += 4\n\n            except (KeyError, IndexError):\n                continue\n\n        return go.Mesh3d(\n            x=all_x,\n            y=all_y,\n            z=all_z,\n            i=all_i,\n            j=all_j,\n            k=all_k,\n            intensity=all_colors,\n            colorscale=colorscale,\n            showscale=show_colorbar,\n            colorbar=dict(title=\"Value\") if show_colorbar else None,\n            opacity=opacity,\n            flatshading=True,\n            name=f\"{self.grid.grid_type.title()} Grid\",\n        )\n\n    def _render_htm_cells(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Mesh3d:\n        \"\"\"Render HTM triangular cells as 3D mesh.\"\"\"\n        grid_df = self.grid.grid\n\n        all_x, all_y, all_z = [], [], []\n        all_i, all_j, all_k = [], [], []\n        all_colors = []\n\n        vertex_count = 0\n\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                v0 = np.array(row[\"htm_vertex_0\"], dtype=float)\n                v1 = np.array(row[\"htm_vertex_1\"], dtype=float)\n                v2 = np.array(row[\"htm_vertex_2\"], dtype=float)\n\n                # Skip if beyond hemisphere\n                if np.all([v[2] &lt; 0 for v in [v0, v1, v2]]):\n                    continue\n\n                all_x.extend([v0[1], v1[1], v2[1]])\n                all_y.extend([v0[0], v1[0], v2[0]])\n                all_z.extend([v0[2], v1[2], v2[2]])\n\n                color_val = data[i] if data is not None else 0.5\n                all_colors.extend([color_val] * 3)\n\n                # One triangle per cell\n                all_i.append(vertex_count)\n                all_j.append(vertex_count + 1)\n                all_k.append(vertex_count + 2)\n\n                vertex_count += 3\n\n            except (KeyError, TypeError, ValueError):\n                continue\n\n        return go.Mesh3d(\n            x=all_x,\n            y=all_y,\n            z=all_z,\n            i=all_i,\n            j=all_j,\n            k=all_k,\n            intensity=all_colors,\n            colorscale=colorscale,\n            showscale=show_colorbar,\n            colorbar=dict(title=\"Value\") if show_colorbar else None,\n            opacity=opacity,\n            flatshading=True,\n            name=f\"{self.grid.grid_type.title()} Grid\",\n        )\n\n    def _render_geodesic_cells(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Mesh3d:\n        \"\"\"Render geodesic triangular cells as 3D mesh.\n\n        Reads ``geodesic_vertices`` (3 vertex indices per cell) and looks up\n        3D Cartesian coordinates from the shared ``grid.vertices`` array.\n        \"\"\"\n        grid_df = self.grid.grid\n        shared_vertices = self.grid.vertices\n\n        if shared_vertices is None or \"geodesic_vertices\" not in grid_df.columns:\n            return self._render_scatter_fallback(\n                data, colorscale, opacity, show_colorbar\n            )\n\n        all_x, all_y, all_z = [], [], []\n        all_i, all_j, all_k = [], [], []\n        all_colors = []\n        vertex_count = 0\n\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                v_indices = np.array(row[\"geodesic_vertices\"], dtype=int)\n                if len(v_indices) &lt; 3:\n                    continue\n\n                verts = shared_vertices[v_indices]  # (3, 3)\n\n                if np.all(verts[:, 2] &lt; 0):\n                    continue\n\n                all_x.extend(verts[:, 1].tolist())\n                all_y.extend(verts[:, 0].tolist())\n                all_z.extend(verts[:, 2].tolist())\n\n                color_val = data[i] if data is not None else 0.5\n                all_colors.extend([color_val] * 3)\n\n                all_i.append(vertex_count)\n                all_j.append(vertex_count + 1)\n                all_k.append(vertex_count + 2)\n                vertex_count += 3\n\n            except (KeyError, IndexError, TypeError, ValueError):\n                continue\n\n        return go.Mesh3d(\n            x=all_x,\n            y=all_y,\n            z=all_z,\n            i=all_i,\n            j=all_j,\n            k=all_k,\n            intensity=all_colors,\n            colorscale=colorscale,\n            showscale=show_colorbar,\n            colorbar=dict(title=\"Value\") if show_colorbar else None,\n            opacity=opacity,\n            flatshading=True,\n            name=\"Geodesic Grid\",\n        )\n\n    def _render_healpix_cells(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Mesh3d:\n        \"\"\"Render HEALPix curvilinear cells as 3D mesh.\n\n        Uses ``healpy.boundaries()`` to obtain true pixel boundaries,\n        then fan-triangulates each quadrilateral pixel.\n        \"\"\"\n        try:\n            import healpy as hp\n        except ImportError:\n            return self._render_scatter_fallback(\n                data, colorscale, opacity, show_colorbar\n            )\n\n        grid_df = self.grid.grid\n        if \"healpix_ipix\" not in grid_df.columns:\n            return self._render_scatter_fallback(\n                data, colorscale, opacity, show_colorbar\n            )\n\n        nside = int(grid_df[\"healpix_nside\"][0])\n        step = 4  # 4 sub-points per edge → 16 boundary vertices\n\n        all_x, all_y, all_z = [], [], []\n        all_i, all_j, all_k = [], [], []\n        all_colors = []\n        vertex_count = 0\n\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                ipix = int(row[\"healpix_ipix\"])\n                boundary = hp.boundaries(nside, ipix, step=step)\n                x, y, z = boundary[1], boundary[0], boundary[2]\n\n                if np.all(z &lt; -0.01):\n                    continue\n\n                n_verts = len(x)\n                all_x.extend(x.tolist())\n                all_y.extend(y.tolist())\n                all_z.extend(z.tolist())\n\n                color_val = data[i] if data is not None else 0.5\n                all_colors.extend([color_val] * n_verts)\n\n                # Fan triangulation from first vertex\n                for j in range(1, n_verts - 1):\n                    all_i.append(vertex_count)\n                    all_j.append(vertex_count + j)\n                    all_k.append(vertex_count + j + 1)\n\n                vertex_count += n_verts\n\n            except (KeyError, IndexError, TypeError, ValueError):\n                continue\n\n        return go.Mesh3d(\n            x=all_x,\n            y=all_y,\n            z=all_z,\n            i=all_i,\n            j=all_j,\n            k=all_k,\n            intensity=all_colors,\n            colorscale=colorscale,\n            showscale=show_colorbar,\n            colorbar=dict(title=\"Value\") if show_colorbar else None,\n            opacity=opacity,\n            flatshading=True,\n            name=\"HEALPix Grid\",\n        )\n\n    def _render_fibonacci_cells(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Mesh3d:\n        \"\"\"Render Fibonacci Voronoi cells as 3D mesh.\n\n        Reads ``voronoi_region`` (variable-length vertex index list) and\n        looks up 3D coordinates from ``grid.voronoi.vertices``.  The\n        vertex indices are already in correct polygon winding order\n        (``SphericalVoronoi.sort_vertices_of_regions()`` was called\n        during grid construction), so no re-sorting is needed.\n        Fan-triangulates each polygon for ``go.Mesh3d``.\n        \"\"\"\n        grid_df = self.grid.grid\n        voronoi = self.grid.voronoi\n\n        if voronoi is None or \"voronoi_region\" not in grid_df.columns:\n            return self._render_scatter_fallback(\n                data, colorscale, opacity, show_colorbar\n            )\n\n        all_x, all_y, all_z = [], [], []\n        all_i, all_j, all_k = [], [], []\n        all_colors = []\n        vertex_count = 0\n\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                region_indices = row[\"voronoi_region\"]\n                if region_indices is None or len(region_indices) &lt; 3:\n                    continue\n\n                verts = voronoi.vertices[region_indices]\n\n                if np.all(verts[:, 2] &lt; -0.01):\n                    continue\n\n                # Vertices are already in polygon winding order from\n                # sort_vertices_of_regions() — use directly.\n                n_verts = len(verts)\n\n                all_x.extend(verts[:, 1].tolist())\n                all_y.extend(verts[:, 0].tolist())\n                all_z.extend(verts[:, 2].tolist())\n\n                color_val = data[i] if data is not None else 0.5\n                all_colors.extend([color_val] * n_verts)\n\n                # Fan triangulation from first vertex\n                for j in range(1, n_verts - 1):\n                    all_i.append(vertex_count)\n                    all_j.append(vertex_count + j)\n                    all_k.append(vertex_count + j + 1)\n\n                vertex_count += n_verts\n\n            except (KeyError, IndexError, TypeError, ValueError):\n                continue\n\n        return go.Mesh3d(\n            x=all_x,\n            y=all_y,\n            z=all_z,\n            i=all_i,\n            j=all_j,\n            k=all_k,\n            intensity=all_colors,\n            colorscale=colorscale,\n            showscale=show_colorbar,\n            colorbar=dict(title=\"Value\") if show_colorbar else None,\n            opacity=opacity,\n            flatshading=True,\n            name=\"Fibonacci Grid\",\n        )\n\n    def _render_scatter_fallback(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Scatter3d:\n        \"\"\"Fallback scatter plot for unsupported grid types.\"\"\"\n        grid_df = self.grid.grid\n        theta = grid_df[\"theta\"].to_numpy()\n        phi = grid_df[\"phi\"].to_numpy()\n\n        # Convert to 3D Cartesian coordinates\n        x = np.sin(theta) * np.sin(phi)\n        y = np.sin(theta) * np.cos(phi)\n        z = np.cos(theta)\n\n        # Prepare data values\n        if data is None:\n            values = np.ones(self.grid.ncells) * 0.5\n        else:\n            values = data\n\n        # Filter hemisphere only\n        hemisphere_mask = theta &lt;= np.pi / 2\n        x = x[hemisphere_mask]\n        y = y[hemisphere_mask]\n        z = z[hemisphere_mask]\n        values = values[hemisphere_mask]\n\n        return go.Scatter3d(\n            x=x,\n            y=y,\n            z=z,\n            mode=\"markers\",\n            marker=dict(\n                size=6,\n                color=values,\n                colorscale=colorscale,\n                opacity=opacity,\n                colorbar=dict(title=\"Value\") if show_colorbar else None,\n                cmin=np.nanmin(values),\n                cmax=np.nanmax(values),\n            ),\n            text=[f\"Cell {i}&lt;br&gt;Value: {v:.3f}\" for i, v in enumerate(values)],\n            hoverinfo=\"text\",\n        )\n\n    def plot_hemisphere_scatter(\n        self,\n        data: np.ndarray | None = None,\n        title: str | None = None,\n        colorscale: str = \"Viridis\",\n        marker_size: int | np.ndarray = 6,\n        opacity: float = 0.8,\n        width: int = 800,\n        height: int = 600,\n    ) -&gt; go.Figure:\n        \"\"\"Create 3D scatter plot of cell centers.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, optional\n            Plot title\n        colorscale : str, default 'Viridis'\n            Plotly colorscale name\n        marker_size : int or np.ndarray, default 6\n            Marker size (constant or per-point array)\n        opacity : float, default 0.8\n            Marker opacity\n        width : int, default 800\n            Figure width\n        height : int, default 600\n            Figure height\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive scatter plot\n\n        \"\"\"\n        # Note: Now renders as mesh, not scatter points\n        # Marker size parameter is ignored\n        fig = self.plot_hemisphere_surface(\n            data=data,\n            title=title,\n            colorscale=colorscale,\n            opacity=opacity,\n            width=width,\n            height=height,\n        )\n\n        return fig\n\n    def plot_cell_mesh(\n        self,\n        data: np.ndarray | None = None,\n        title: str | None = None,\n        colorscale: str = \"Viridis\",\n        opacity: float = 0.7,\n        show_edges: bool = True,\n        width: int = 800,\n        height: int = 600,\n    ) -&gt; go.Figure:\n        \"\"\"Create 3D mesh plot showing cell boundaries.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, optional\n            Plot title\n        colorscale : str, default 'Viridis'\n            Plotly colorscale name\n        opacity : float, default 0.7\n            Mesh opacity\n        show_edges : bool, default True\n            Show cell edges\n        width : int, default 800\n            Figure width\n        height : int, default 600\n            Figure height\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive mesh plot\n\n        Notes\n        -----\n        This method requires grid cells with vertex information.\n        Currently supports HTM and geodesic grids.\n\n        \"\"\"\n        traces = []\n\n        # Prepare data\n        if data is None:\n            values = np.ones(self.grid.ncells) * 0.5\n        else:\n            values = data\n\n        grid_df = self.grid.grid\n        grid_type = self.grid.grid_type.lower()\n\n        # Check if grid supports mesh rendering\n        if grid_type == \"htm\" and \"htm_vertex_0\" in grid_df.columns:\n            # HTM triangular mesh\n            for idx, row in enumerate(grid_df.iter_rows(named=True)):\n                try:\n                    v0 = np.array(row[\"htm_vertex_0\"], dtype=float)\n                    v1 = np.array(row[\"htm_vertex_1\"], dtype=float)\n                    v2 = np.array(row[\"htm_vertex_2\"], dtype=float)\n\n                    vertices = np.array([v0, v1, v2])\n\n                    # Check hemisphere\n                    z_coords = vertices[:, 2]\n                    if np.all(z_coords &lt; 0):\n                        continue\n\n                    # Normalize color value\n                    color_val = (values[idx] - np.nanmin(values)) / (\n                        np.nanmax(values) - np.nanmin(values)\n                    )\n                    color_rgb = sample_colorscale(colorscale, [color_val])[0]\n\n                    # Create triangle mesh\n                    trace = go.Mesh3d(\n                        x=vertices[:, 0],\n                        y=vertices[:, 1],\n                        z=vertices[:, 2],\n                        i=[0],\n                        j=[1],\n                        k=[2],\n                        color=color_rgb,\n                        opacity=opacity,\n                        flatshading=True,\n                        showscale=False,\n                        hoverinfo=\"skip\",\n                    )\n                    traces.append(trace)\n                except (KeyError, TypeError, ValueError):\n                    continue\n        else:\n            # Not supported for this grid type\n            raise NotImplementedError(\n                f\"Cell mesh rendering not implemented for {grid_type} grids\"\n            )\n\n        # Sample colorscale (no longer needed above, but kept for compatibility)\n\n        # Add colorbar trace\n        if values is not None and len(traces) &gt; 0:\n            dummy_trace = go.Scatter3d(\n                x=[None],\n                y=[None],\n                z=[None],\n                mode=\"markers\",\n                marker=dict(\n                    size=0.1,\n                    color=[np.nanmin(values), np.nanmax(values)],\n                    colorscale=colorscale,\n                    colorbar=dict(title=\"Value\"),\n                ),\n                showlegend=False,\n                hoverinfo=\"skip\",\n            )\n            traces.append(dummy_trace)\n\n        fig = go.Figure(data=traces)\n\n        # Update layout\n        fig.update_layout(\n            title=title or \"Hemisphere Mesh 3D\",\n            scene=dict(\n                aspectmode=\"data\",\n                xaxis=dict(title=\"East\", showbackground=False),\n                yaxis=dict(title=\"North\", showbackground=False),\n                zaxis=dict(title=\"Up\", showbackground=False),\n            ),\n            width=width,\n            height=height,\n            margin=dict(l=0, r=0, b=0, t=40),\n        )\n\n        return fig\n\n    def add_spherical_overlays(\n        self,\n        fig: go.Figure,\n        elevation_rings: list[int] | None = None,\n        meridians_deg: list[int] | None = None,\n        overlay_color: str = \"lightgray\",\n        line_width: float = 1,\n    ) -&gt; go.Figure:\n        \"\"\"Add elevation rings and meridians to 3D plot.\n\n        Parameters\n        ----------\n        fig : plotly.graph_objects.Figure\n            Existing 3D figure to add overlays to\n        elevation_rings : list of int, optional\n            Elevation angles in degrees. Default: [15, 30, 45, 60, 75, 90]\n        meridians_deg : list of int, optional\n            Meridian angles in degrees. Default: [0, 45, 90, 135, 180, 225, 270, 315]\n        overlay_color : str, default 'lightgray'\n            Color for overlay lines\n        line_width : float, default 1\n            Width of overlay lines\n\n        Returns\n        -------\n        fig : plotly.graph_objects.Figure\n            Modified figure with overlays\n\n        \"\"\"\n        if elevation_rings is None:\n            elevation_rings = [15, 30, 45, 60, 75, 90]\n        if meridians_deg is None:\n            meridians_deg = list(range(0, 360, 45))\n\n        # Elevation rings\n        for theta_deg in elevation_rings:\n            theta = np.radians(theta_deg)\n            phi = np.linspace(0, 2 * np.pi, 200)\n            x = np.sin(theta) * np.sin(phi)\n            y = np.sin(theta) * np.cos(phi)\n            z = np.full_like(phi, np.cos(theta))\n            fig.add_trace(\n                go.Scatter3d(\n                    x=x,\n                    y=y,\n                    z=z,\n                    mode=\"lines\",\n                    line=dict(color=overlay_color, width=line_width),\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                )\n            )\n\n        # Meridians\n        for phi_deg in meridians_deg:\n            phi = np.radians(phi_deg)\n            theta = np.linspace(0, np.pi / 2, 100)\n            x = np.sin(theta) * np.sin(phi)\n            y = np.sin(theta) * np.cos(phi)\n            z = np.cos(theta)\n            fig.add_trace(\n                go.Scatter3d(\n                    x=x,\n                    y=y,\n                    z=z,\n                    mode=\"lines\",\n                    line=dict(color=overlay_color, width=line_width),\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                )\n            )\n\n        return fig\n\n    def add_custom_axes(\n        self,\n        fig: go.Figure,\n        axis_length: float = 1.2,\n        axis_color: str = \"black\",\n        show_labels: bool = True,\n    ) -&gt; go.Figure:\n        \"\"\"Add custom coordinate axes with labels.\n\n        Parameters\n        ----------\n        fig : plotly.graph_objects.Figure\n            Existing 3D figure\n        axis_length : float, default 1.2\n            Length of axis lines\n        axis_color : str, default 'black'\n            Color for axes\n        show_labels : bool, default True\n            Show axis labels (E, N, Z)\n\n        Returns\n        -------\n        fig : plotly.graph_objects.Figure\n            Modified figure with custom axes\n\n        \"\"\"\n        # Axis lines\n        axes_lines = [\n            dict(x=[0, axis_length], y=[0, 0], z=[0, 0]),  # East\n            dict(x=[0, 0], y=[0, axis_length], z=[0, 0]),  # North\n            dict(x=[0, 0], y=[0, 0], z=[0, axis_length]),  # Up\n        ]\n\n        for axis in axes_lines:\n            fig.add_trace(\n                go.Scatter3d(\n                    x=axis[\"x\"],\n                    y=axis[\"y\"],\n                    z=axis[\"z\"],\n                    mode=\"lines\",\n                    line=dict(color=axis_color, width=6),\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                )\n            )\n\n        # Arrowheads\n        arrow_tip = axis_length + 0.05\n        arrow_size = 0.1\n        for pos, direction in zip(\n            [[arrow_tip, 0, 0], [0, arrow_tip, 0], [0, 0, arrow_tip]],\n            [[arrow_size, 0, 0], [0, arrow_size, 0], [0, 0, arrow_size]],\n        ):\n            fig.add_trace(\n                go.Cone(\n                    x=[pos[0]],\n                    y=[pos[1]],\n                    z=[pos[2]],\n                    u=[direction[0]],\n                    v=[direction[1]],\n                    w=[direction[2]],\n                    sizemode=\"absolute\",\n                    sizeref=arrow_size,\n                    anchor=\"tip\",\n                    showscale=False,\n                    colorscale=[[0, axis_color], [1, axis_color]],\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                )\n            )\n\n        # Labels\n        if show_labels:\n            label_offset = axis_length + 0.15\n            for label in [\n                dict(x=label_offset, y=0, z=0, text=\"E\"),\n                dict(x=0, y=label_offset, z=0, text=\"N\"),\n                dict(x=0, y=0, z=label_offset, text=\"Z\"),\n            ]:\n                fig.add_trace(\n                    go.Scatter3d(\n                        x=[label[\"x\"]],\n                        y=[label[\"y\"]],\n                        z=[label[\"z\"]],\n                        mode=\"text\",\n                        text=[label[\"text\"]],\n                        textfont=dict(size=16, color=axis_color),\n                        hoverinfo=\"skip\",\n                        showlegend=False,\n                    )\n                )\n\n        return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.__init__","level":3,"title":"<code>__init__(grid)</code>","text":"<p>Initialize 3D hemisphere visualizer.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.__init__--parameters","level":5,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def __init__(self, grid: HemiGrid) -&gt; None:\n    \"\"\"Initialize 3D hemisphere visualizer.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    \"\"\"\n    self.grid = grid\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.plot_hemisphere_surface","level":3,"title":"<code>plot_hemisphere_surface(data=None, style=None, title=None, colorscale='Viridis', opacity=0.8, show_wireframe=True, show_colorbar=True, width=800, height=600, **kwargs)</code>","text":"<p>Create 3D surface plot on hemisphere with actual cell patches.</p> <p>Renders grid cells as colored 3D patches (not just points).</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.plot_hemisphere_surface--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell. If None, shows grid structure. style : PlotStyle, optional     Styling configuration. If None, uses defaults. title : str, optional     Plot title colorscale : str, default 'Viridis'     Plotly colorscale name opacity : float, default 0.8     Surface opacity (0=transparent, 1=opaque) show_wireframe : bool, default True     Show grid lines on surface show_colorbar : bool, default True     Display colorbar width : int, default 800     Figure width in pixels height : int, default 600     Figure height in pixels **kwargs     Additional plotly trace parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.plot_hemisphere_surface--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive 3D figure with cell patches</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.plot_hemisphere_surface--examples","level":5,"title":"Examples","text":"<p>fig = viz.plot_hemisphere_surface( ...     data=vod_data, ...     title=\"VOD Distribution 3D\", ...     colorscale='Plasma', ...     opacity=0.9 ... ) fig.write_html(\"vod_3d.html\")</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def plot_hemisphere_surface(\n    self,\n    data: np.ndarray | None = None,\n    style: PlotStyle | None = None,\n    title: str | None = None,\n    colorscale: str = \"Viridis\",\n    opacity: float = 0.8,\n    show_wireframe: bool = True,\n    show_colorbar: bool = True,\n    width: int = 800,\n    height: int = 600,\n    **kwargs: Any,\n) -&gt; go.Figure:\n    \"\"\"Create 3D surface plot on hemisphere with actual cell patches.\n\n    Renders grid cells as colored 3D patches (not just points).\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell. If None, shows grid structure.\n    style : PlotStyle, optional\n        Styling configuration. If None, uses defaults.\n    title : str, optional\n        Plot title\n    colorscale : str, default 'Viridis'\n        Plotly colorscale name\n    opacity : float, default 0.8\n        Surface opacity (0=transparent, 1=opaque)\n    show_wireframe : bool, default True\n        Show grid lines on surface\n    show_colorbar : bool, default True\n        Display colorbar\n    width : int, default 800\n        Figure width in pixels\n    height : int, default 600\n        Figure height in pixels\n    **kwargs\n        Additional plotly trace parameters\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive 3D figure with cell patches\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = viz.plot_hemisphere_surface(\n    ...     data=vod_data,\n    ...     title=\"VOD Distribution 3D\",\n    ...     colorscale='Plasma',\n    ...     opacity=0.9\n    ... )\n    &gt;&gt;&gt; fig.write_html(\"vod_3d.html\")\n\n    \"\"\"\n    # Initialize style\n    if style is None:\n        style = PlotStyle()\n        colorscale = colorscale  # Use parameter\n    else:\n        colorscale = style.colorscale\n\n    # Render grid based on type\n    grid_type = self.grid.grid_type.lower()\n\n    if grid_type in [\"equal_area\", \"equal_angle\", \"equirectangular\"]:\n        trace = self._render_rectangular_cells(\n            data,\n            colorscale,\n            opacity,\n            show_colorbar,\n        )\n    elif grid_type == \"htm\":\n        trace = self._render_htm_cells(data, colorscale, opacity, show_colorbar)\n    elif grid_type == \"geodesic\":\n        trace = self._render_geodesic_cells(\n            data, colorscale, opacity, show_colorbar\n        )\n    elif grid_type == \"healpix\":\n        trace = self._render_healpix_cells(data, colorscale, opacity, show_colorbar)\n    elif grid_type == \"fibonacci\":\n        trace = self._render_fibonacci_cells(\n            data, colorscale, opacity, show_colorbar\n        )\n    else:\n        # Fallback to scatter for unknown types\n        trace = self._render_scatter_fallback(\n            data,\n            colorscale,\n            opacity,\n            show_colorbar,\n        )\n\n    fig = go.Figure(data=[trace])\n\n    # Apply layout\n    layout_config = style.to_plotly_layout() if style else {}\n    layout_config.update(\n        {\n            \"title\": title or \"Hemisphere 3D\",\n            \"scene\": dict(\n                aspectmode=\"data\",\n                xaxis=dict(title=\"East\", showbackground=False),\n                yaxis=dict(title=\"North\", showbackground=False),\n                zaxis=dict(title=\"Up\", showbackground=False),\n                bgcolor=layout_config.get(\"plot_bgcolor\", \"white\"),\n            ),\n            \"width\": width,\n            \"height\": height,\n            \"margin\": dict(l=0, r=0, b=0, t=40),\n        }\n    )\n\n    fig.update_layout(**layout_config)\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.plot_hemisphere_scatter","level":3,"title":"<code>plot_hemisphere_scatter(data=None, title=None, colorscale='Viridis', marker_size=6, opacity=0.8, width=800, height=600)</code>","text":"<p>Create 3D scatter plot of cell centers.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.plot_hemisphere_scatter--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, optional     Plot title colorscale : str, default 'Viridis'     Plotly colorscale name marker_size : int or np.ndarray, default 6     Marker size (constant or per-point array) opacity : float, default 0.8     Marker opacity width : int, default 800     Figure width height : int, default 600     Figure height</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.plot_hemisphere_scatter--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive scatter plot</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def plot_hemisphere_scatter(\n    self,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    colorscale: str = \"Viridis\",\n    marker_size: int | np.ndarray = 6,\n    opacity: float = 0.8,\n    width: int = 800,\n    height: int = 600,\n) -&gt; go.Figure:\n    \"\"\"Create 3D scatter plot of cell centers.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    colorscale : str, default 'Viridis'\n        Plotly colorscale name\n    marker_size : int or np.ndarray, default 6\n        Marker size (constant or per-point array)\n    opacity : float, default 0.8\n        Marker opacity\n    width : int, default 800\n        Figure width\n    height : int, default 600\n        Figure height\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive scatter plot\n\n    \"\"\"\n    # Note: Now renders as mesh, not scatter points\n    # Marker size parameter is ignored\n    fig = self.plot_hemisphere_surface(\n        data=data,\n        title=title,\n        colorscale=colorscale,\n        opacity=opacity,\n        width=width,\n        height=height,\n    )\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.plot_cell_mesh","level":3,"title":"<code>plot_cell_mesh(data=None, title=None, colorscale='Viridis', opacity=0.7, show_edges=True, width=800, height=600)</code>","text":"<p>Create 3D mesh plot showing cell boundaries.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.plot_cell_mesh--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, optional     Plot title colorscale : str, default 'Viridis'     Plotly colorscale name opacity : float, default 0.7     Mesh opacity show_edges : bool, default True     Show cell edges width : int, default 800     Figure width height : int, default 600     Figure height</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.plot_cell_mesh--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive mesh plot</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.plot_cell_mesh--notes","level":5,"title":"Notes","text":"<p>This method requires grid cells with vertex information. Currently supports HTM and geodesic grids.</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def plot_cell_mesh(\n    self,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    colorscale: str = \"Viridis\",\n    opacity: float = 0.7,\n    show_edges: bool = True,\n    width: int = 800,\n    height: int = 600,\n) -&gt; go.Figure:\n    \"\"\"Create 3D mesh plot showing cell boundaries.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    colorscale : str, default 'Viridis'\n        Plotly colorscale name\n    opacity : float, default 0.7\n        Mesh opacity\n    show_edges : bool, default True\n        Show cell edges\n    width : int, default 800\n        Figure width\n    height : int, default 600\n        Figure height\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive mesh plot\n\n    Notes\n    -----\n    This method requires grid cells with vertex information.\n    Currently supports HTM and geodesic grids.\n\n    \"\"\"\n    traces = []\n\n    # Prepare data\n    if data is None:\n        values = np.ones(self.grid.ncells) * 0.5\n    else:\n        values = data\n\n    grid_df = self.grid.grid\n    grid_type = self.grid.grid_type.lower()\n\n    # Check if grid supports mesh rendering\n    if grid_type == \"htm\" and \"htm_vertex_0\" in grid_df.columns:\n        # HTM triangular mesh\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                v0 = np.array(row[\"htm_vertex_0\"], dtype=float)\n                v1 = np.array(row[\"htm_vertex_1\"], dtype=float)\n                v2 = np.array(row[\"htm_vertex_2\"], dtype=float)\n\n                vertices = np.array([v0, v1, v2])\n\n                # Check hemisphere\n                z_coords = vertices[:, 2]\n                if np.all(z_coords &lt; 0):\n                    continue\n\n                # Normalize color value\n                color_val = (values[idx] - np.nanmin(values)) / (\n                    np.nanmax(values) - np.nanmin(values)\n                )\n                color_rgb = sample_colorscale(colorscale, [color_val])[0]\n\n                # Create triangle mesh\n                trace = go.Mesh3d(\n                    x=vertices[:, 0],\n                    y=vertices[:, 1],\n                    z=vertices[:, 2],\n                    i=[0],\n                    j=[1],\n                    k=[2],\n                    color=color_rgb,\n                    opacity=opacity,\n                    flatshading=True,\n                    showscale=False,\n                    hoverinfo=\"skip\",\n                )\n                traces.append(trace)\n            except (KeyError, TypeError, ValueError):\n                continue\n    else:\n        # Not supported for this grid type\n        raise NotImplementedError(\n            f\"Cell mesh rendering not implemented for {grid_type} grids\"\n        )\n\n    # Sample colorscale (no longer needed above, but kept for compatibility)\n\n    # Add colorbar trace\n    if values is not None and len(traces) &gt; 0:\n        dummy_trace = go.Scatter3d(\n            x=[None],\n            y=[None],\n            z=[None],\n            mode=\"markers\",\n            marker=dict(\n                size=0.1,\n                color=[np.nanmin(values), np.nanmax(values)],\n                colorscale=colorscale,\n                colorbar=dict(title=\"Value\"),\n            ),\n            showlegend=False,\n            hoverinfo=\"skip\",\n        )\n        traces.append(dummy_trace)\n\n    fig = go.Figure(data=traces)\n\n    # Update layout\n    fig.update_layout(\n        title=title or \"Hemisphere Mesh 3D\",\n        scene=dict(\n            aspectmode=\"data\",\n            xaxis=dict(title=\"East\", showbackground=False),\n            yaxis=dict(title=\"North\", showbackground=False),\n            zaxis=dict(title=\"Up\", showbackground=False),\n        ),\n        width=width,\n        height=height,\n        margin=dict(l=0, r=0, b=0, t=40),\n    )\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.add_spherical_overlays","level":3,"title":"<code>add_spherical_overlays(fig, elevation_rings=None, meridians_deg=None, overlay_color='lightgray', line_width=1)</code>","text":"<p>Add elevation rings and meridians to 3D plot.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.add_spherical_overlays--parameters","level":5,"title":"Parameters","text":"<p>fig : plotly.graph_objects.Figure     Existing 3D figure to add overlays to elevation_rings : list of int, optional     Elevation angles in degrees. Default: [15, 30, 45, 60, 75, 90] meridians_deg : list of int, optional     Meridian angles in degrees. Default: [0, 45, 90, 135, 180, 225, 270, 315] overlay_color : str, default 'lightgray'     Color for overlay lines line_width : float, default 1     Width of overlay lines</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.add_spherical_overlays--returns","level":5,"title":"Returns","text":"<p>fig : plotly.graph_objects.Figure     Modified figure with overlays</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def add_spherical_overlays(\n    self,\n    fig: go.Figure,\n    elevation_rings: list[int] | None = None,\n    meridians_deg: list[int] | None = None,\n    overlay_color: str = \"lightgray\",\n    line_width: float = 1,\n) -&gt; go.Figure:\n    \"\"\"Add elevation rings and meridians to 3D plot.\n\n    Parameters\n    ----------\n    fig : plotly.graph_objects.Figure\n        Existing 3D figure to add overlays to\n    elevation_rings : list of int, optional\n        Elevation angles in degrees. Default: [15, 30, 45, 60, 75, 90]\n    meridians_deg : list of int, optional\n        Meridian angles in degrees. Default: [0, 45, 90, 135, 180, 225, 270, 315]\n    overlay_color : str, default 'lightgray'\n        Color for overlay lines\n    line_width : float, default 1\n        Width of overlay lines\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n        Modified figure with overlays\n\n    \"\"\"\n    if elevation_rings is None:\n        elevation_rings = [15, 30, 45, 60, 75, 90]\n    if meridians_deg is None:\n        meridians_deg = list(range(0, 360, 45))\n\n    # Elevation rings\n    for theta_deg in elevation_rings:\n        theta = np.radians(theta_deg)\n        phi = np.linspace(0, 2 * np.pi, 200)\n        x = np.sin(theta) * np.sin(phi)\n        y = np.sin(theta) * np.cos(phi)\n        z = np.full_like(phi, np.cos(theta))\n        fig.add_trace(\n            go.Scatter3d(\n                x=x,\n                y=y,\n                z=z,\n                mode=\"lines\",\n                line=dict(color=overlay_color, width=line_width),\n                hoverinfo=\"skip\",\n                showlegend=False,\n            )\n        )\n\n    # Meridians\n    for phi_deg in meridians_deg:\n        phi = np.radians(phi_deg)\n        theta = np.linspace(0, np.pi / 2, 100)\n        x = np.sin(theta) * np.sin(phi)\n        y = np.sin(theta) * np.cos(phi)\n        z = np.cos(theta)\n        fig.add_trace(\n            go.Scatter3d(\n                x=x,\n                y=y,\n                z=z,\n                mode=\"lines\",\n                line=dict(color=overlay_color, width=line_width),\n                hoverinfo=\"skip\",\n                showlegend=False,\n            )\n        )\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.add_custom_axes","level":3,"title":"<code>add_custom_axes(fig, axis_length=1.2, axis_color='black', show_labels=True)</code>","text":"<p>Add custom coordinate axes with labels.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.add_custom_axes--parameters","level":5,"title":"Parameters","text":"<p>fig : plotly.graph_objects.Figure     Existing 3D figure axis_length : float, default 1.2     Length of axis lines axis_color : str, default 'black'     Color for axes show_labels : bool, default True     Show axis labels (E, N, Z)</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.HemisphereVisualizer3D.add_custom_axes--returns","level":5,"title":"Returns","text":"<p>fig : plotly.graph_objects.Figure     Modified figure with custom axes</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def add_custom_axes(\n    self,\n    fig: go.Figure,\n    axis_length: float = 1.2,\n    axis_color: str = \"black\",\n    show_labels: bool = True,\n) -&gt; go.Figure:\n    \"\"\"Add custom coordinate axes with labels.\n\n    Parameters\n    ----------\n    fig : plotly.graph_objects.Figure\n        Existing 3D figure\n    axis_length : float, default 1.2\n        Length of axis lines\n    axis_color : str, default 'black'\n        Color for axes\n    show_labels : bool, default True\n        Show axis labels (E, N, Z)\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n        Modified figure with custom axes\n\n    \"\"\"\n    # Axis lines\n    axes_lines = [\n        dict(x=[0, axis_length], y=[0, 0], z=[0, 0]),  # East\n        dict(x=[0, 0], y=[0, axis_length], z=[0, 0]),  # North\n        dict(x=[0, 0], y=[0, 0], z=[0, axis_length]),  # Up\n    ]\n\n    for axis in axes_lines:\n        fig.add_trace(\n            go.Scatter3d(\n                x=axis[\"x\"],\n                y=axis[\"y\"],\n                z=axis[\"z\"],\n                mode=\"lines\",\n                line=dict(color=axis_color, width=6),\n                hoverinfo=\"skip\",\n                showlegend=False,\n            )\n        )\n\n    # Arrowheads\n    arrow_tip = axis_length + 0.05\n    arrow_size = 0.1\n    for pos, direction in zip(\n        [[arrow_tip, 0, 0], [0, arrow_tip, 0], [0, 0, arrow_tip]],\n        [[arrow_size, 0, 0], [0, arrow_size, 0], [0, 0, arrow_size]],\n    ):\n        fig.add_trace(\n            go.Cone(\n                x=[pos[0]],\n                y=[pos[1]],\n                z=[pos[2]],\n                u=[direction[0]],\n                v=[direction[1]],\n                w=[direction[2]],\n                sizemode=\"absolute\",\n                sizeref=arrow_size,\n                anchor=\"tip\",\n                showscale=False,\n                colorscale=[[0, axis_color], [1, axis_color]],\n                hoverinfo=\"skip\",\n                showlegend=False,\n            )\n        )\n\n    # Labels\n    if show_labels:\n        label_offset = axis_length + 0.15\n        for label in [\n            dict(x=label_offset, y=0, z=0, text=\"E\"),\n            dict(x=0, y=label_offset, z=0, text=\"N\"),\n            dict(x=0, y=0, z=label_offset, text=\"Z\"),\n        ]:\n            fig.add_trace(\n                go.Scatter3d(\n                    x=[label[\"x\"]],\n                    y=[label[\"y\"]],\n                    z=[label[\"z\"]],\n                    mode=\"text\",\n                    text=[label[\"text\"]],\n                    textfont=dict(size=16, color=axis_color),\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                )\n            )\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.PlotStyle","level":2,"title":"<code>PlotStyle</code>  <code>dataclass</code>","text":"<p>Unified styling configuration for both 2D and 3D plots.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.PlotStyle--parameters","level":4,"title":"Parameters","text":"<p>colormap : str, default 'viridis'     Colormap name (matplotlib or plotly) colorscale : str, default 'Viridis'     Plotly colorscale name background_color : str, default 'white'     Background color text_color : str, default 'black'     Text color grid_color : str, default 'lightgray'     Grid line color font_family : str, default 'sans-serif'     Font family font_size : int, default 11     Base font size title_size : int, default 14     Title font size label_size : int, default 12     Axis label font size edge_linewidth : float, default 0.5     Edge line width for cells opacity : float, default 0.8     3D surface opacity marker_size : int, default 8     3D marker size line_width : int, default 1     3D line width wireframe_opacity : float, default 0.2     3D wireframe transparency dark_mode : bool, default False     Use dark theme</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>@dataclass\nclass PlotStyle:\n    \"\"\"Unified styling configuration for both 2D and 3D plots.\n\n    Parameters\n    ----------\n    colormap : str, default 'viridis'\n        Colormap name (matplotlib or plotly)\n    colorscale : str, default 'Viridis'\n        Plotly colorscale name\n    background_color : str, default 'white'\n        Background color\n    text_color : str, default 'black'\n        Text color\n    grid_color : str, default 'lightgray'\n        Grid line color\n    font_family : str, default 'sans-serif'\n        Font family\n    font_size : int, default 11\n        Base font size\n    title_size : int, default 14\n        Title font size\n    label_size : int, default 12\n        Axis label font size\n    edge_linewidth : float, default 0.5\n        Edge line width for cells\n    opacity : float, default 0.8\n        3D surface opacity\n    marker_size : int, default 8\n        3D marker size\n    line_width : int, default 1\n        3D line width\n    wireframe_opacity : float, default 0.2\n        3D wireframe transparency\n    dark_mode : bool, default False\n        Use dark theme\n\n    \"\"\"\n\n    colormap: str = \"viridis\"\n    colorscale: str = \"Viridis\"\n    background_color: str = \"white\"\n    text_color: str = \"black\"\n    grid_color: str = \"lightgray\"\n    font_family: str = \"sans-serif\"\n    font_size: int = 11\n    title_size: int = 14\n    label_size: int = 12\n    edge_linewidth: float = 0.5\n    opacity: float = 0.8\n    marker_size: int = 8\n    line_width: int = 1\n    wireframe_opacity: float = 0.2\n    dark_mode: bool = False\n\n    def to_polar_style(self) -&gt; PolarPlotStyle:\n        \"\"\"Convert to PolarPlotStyle for 2D matplotlib plots.\n\n        Returns\n        -------\n        PolarPlotStyle\n            Equivalent 2D styling configuration\n\n        \"\"\"\n        return PolarPlotStyle(\n            cmap=self.colormap,\n            edgecolor=\"white\" if self.dark_mode else self.text_color,\n            linewidth=self.edge_linewidth,\n            alpha=1.0,\n            colorbar_fontsize=self.font_size,\n        )\n\n    def to_plotly_layout(self) -&gt; dict[str, Any]:\n        \"\"\"Convert to plotly layout configuration.\n\n        Returns\n        -------\n        dict\n            Plotly layout settings\n\n        \"\"\"\n        if self.dark_mode:\n            return {\n                \"template\": \"plotly_dark\",\n                \"paper_bgcolor\": \"#111111\",\n                \"plot_bgcolor\": \"#111111\",\n                \"font\": {\n                    \"family\": self.font_family,\n                    \"size\": self.font_size,\n                    \"color\": \"white\",\n                },\n            }\n        return {\n            \"template\": \"plotly\",\n            \"paper_bgcolor\": self.background_color,\n            \"plot_bgcolor\": self.background_color,\n            \"font\": {\n                \"family\": self.font_family,\n                \"size\": self.font_size,\n                \"color\": self.text_color,\n            },\n        }\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.PlotStyle.to_polar_style","level":3,"title":"<code>to_polar_style()</code>","text":"<p>Convert to PolarPlotStyle for 2D matplotlib plots.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.PlotStyle.to_polar_style--returns","level":5,"title":"Returns","text":"<p>PolarPlotStyle     Equivalent 2D styling configuration</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>def to_polar_style(self) -&gt; PolarPlotStyle:\n    \"\"\"Convert to PolarPlotStyle for 2D matplotlib plots.\n\n    Returns\n    -------\n    PolarPlotStyle\n        Equivalent 2D styling configuration\n\n    \"\"\"\n    return PolarPlotStyle(\n        cmap=self.colormap,\n        edgecolor=\"white\" if self.dark_mode else self.text_color,\n        linewidth=self.edge_linewidth,\n        alpha=1.0,\n        colorbar_fontsize=self.font_size,\n    )\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.PlotStyle.to_plotly_layout","level":3,"title":"<code>to_plotly_layout()</code>","text":"<p>Convert to plotly layout configuration.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.PlotStyle.to_plotly_layout--returns","level":5,"title":"Returns","text":"<p>dict     Plotly layout settings</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>def to_plotly_layout(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to plotly layout configuration.\n\n    Returns\n    -------\n    dict\n        Plotly layout settings\n\n    \"\"\"\n    if self.dark_mode:\n        return {\n            \"template\": \"plotly_dark\",\n            \"paper_bgcolor\": \"#111111\",\n            \"plot_bgcolor\": \"#111111\",\n            \"font\": {\n                \"family\": self.font_family,\n                \"size\": self.font_size,\n                \"color\": \"white\",\n            },\n        }\n    return {\n        \"template\": \"plotly\",\n        \"paper_bgcolor\": self.background_color,\n        \"plot_bgcolor\": self.background_color,\n        \"font\": {\n            \"family\": self.font_family,\n            \"size\": self.font_size,\n            \"color\": self.text_color,\n        },\n    }\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.PolarPlotStyle","level":2,"title":"<code>PolarPlotStyle</code>  <code>dataclass</code>","text":"<p>Configuration for 2D polar plot styling (matplotlib).</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.PolarPlotStyle--parameters","level":4,"title":"Parameters","text":"<p>cmap : str, default 'viridis'     Matplotlib colormap name edgecolor : str, default 'black'     Edge color for grid cells linewidth : float, default 0.5     Line width for cell edges alpha : float, default 1.0     Transparency (0=transparent, 1=opaque) vmin : float or None, optional     Minimum value for colormap vmax : float or None, optional     Maximum value for colormap title : str or None, optional     Plot title figsize : tuple of float, default (10, 10)     Figure size in inches (width, height) dpi : int, default 100     Dots per inch for figure colorbar_label : str, default 'Value'     Label for colorbar colorbar_shrink : float, default 0.8     Colorbar size relative to axis colorbar_pad : float, default 0.1     Space between axis and colorbar colorbar_fontsize : int, default 11     Font size for colorbar label show_grid : bool, default True     Show polar grid lines grid_alpha : float, default 0.3     Grid line transparency grid_linestyle : str, default '--'     Grid line style show_degree_labels : bool, default True     Show degree labels on radial axis theta_labels : list of int, default [0, 30, 60, 90]     Elevation angles for labels (degrees)</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>@dataclass\nclass PolarPlotStyle:\n    \"\"\"Configuration for 2D polar plot styling (matplotlib).\n\n    Parameters\n    ----------\n    cmap : str, default 'viridis'\n        Matplotlib colormap name\n    edgecolor : str, default 'black'\n        Edge color for grid cells\n    linewidth : float, default 0.5\n        Line width for cell edges\n    alpha : float, default 1.0\n        Transparency (0=transparent, 1=opaque)\n    vmin : float or None, optional\n        Minimum value for colormap\n    vmax : float or None, optional\n        Maximum value for colormap\n    title : str or None, optional\n        Plot title\n    figsize : tuple of float, default (10, 10)\n        Figure size in inches (width, height)\n    dpi : int, default 100\n        Dots per inch for figure\n    colorbar_label : str, default 'Value'\n        Label for colorbar\n    colorbar_shrink : float, default 0.8\n        Colorbar size relative to axis\n    colorbar_pad : float, default 0.1\n        Space between axis and colorbar\n    colorbar_fontsize : int, default 11\n        Font size for colorbar label\n    show_grid : bool, default True\n        Show polar grid lines\n    grid_alpha : float, default 0.3\n        Grid line transparency\n    grid_linestyle : str, default '--'\n        Grid line style\n    show_degree_labels : bool, default True\n        Show degree labels on radial axis\n    theta_labels : list of int, default [0, 30, 60, 90]\n        Elevation angles for labels (degrees)\n\n    \"\"\"\n\n    cmap: str = \"viridis\"\n    edgecolor: str = \"black\"\n    linewidth: float = 0.5\n    alpha: float = 1.0\n    vmin: float | None = None\n    vmax: float | None = None\n    title: str | None = None\n    figsize: tuple[float, float] = (10, 10)\n    dpi: int = 100\n    colorbar_label: str = \"Value\"\n    colorbar_shrink: float = 0.8\n    colorbar_pad: float = 0.1\n    colorbar_fontsize: int = 11\n    show_grid: bool = True\n    grid_alpha: float = 0.3\n    grid_linestyle: str = \"--\"\n    show_degree_labels: bool = True\n    theta_labels: list[int] = field(default_factory=lambda: [0, 30, 60, 90])\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualize_grid","level":2,"title":"<code>visualize_grid(grid, data=None, style=None, **kwargs)</code>","text":"<p>Visualize hemispherical grid in 2D polar projection.</p> <p>Convenience function providing simple interface to 2D visualization.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualize_grid--parameters","level":4,"title":"Parameters","text":"<p>grid : HemiGrid     Grid to visualize data : np.ndarray, optional     Data values per cell. If None, plots uniform grid. style : PolarPlotStyle, optional     Styling configuration. If None, uses defaults. **kwargs     Additional style parameter overrides</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualize_grid--returns","level":4,"title":"Returns","text":"<p>fig : matplotlib.figure.Figure     Figure object ax : matplotlib.axes.Axes     Polar axes object</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualize_grid--examples","level":4,"title":"Examples","text":"<p>from canvod.grids import create_hemigrid from canvod.viz import visualize_grid</p> <p>grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0) fig, ax = visualize_grid(grid, data=vod_data, cmap='viridis') plt.savefig(\"vod_plot.png\", dpi=300)</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_2d.py</code> <pre><code>def visualize_grid(\n    grid: HemiGrid,\n    data: np.ndarray | None = None,\n    style: PolarPlotStyle | None = None,\n    **kwargs: Any,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Visualize hemispherical grid in 2D polar projection.\n\n    Convenience function providing simple interface to 2D visualization.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Grid to visualize\n    data : np.ndarray, optional\n        Data values per cell. If None, plots uniform grid.\n    style : PolarPlotStyle, optional\n        Styling configuration. If None, uses defaults.\n    **kwargs\n        Additional style parameter overrides\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Figure object\n    ax : matplotlib.axes.Axes\n        Polar axes object\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import create_hemigrid\n    &gt;&gt;&gt; from canvod.viz import visualize_grid\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\n    &gt;&gt;&gt; fig, ax = visualize_grid(grid, data=vod_data, cmap='viridis')\n    &gt;&gt;&gt; plt.savefig(\"vod_plot.png\", dpi=300)\n\n    \"\"\"\n    viz = HemisphereVisualizer2D(grid)\n    return viz.plot_grid_patches(data=data, style=style, **kwargs)\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualize_grid_3d","level":2,"title":"<code>visualize_grid_3d(grid, data=None, title=None, colorscale='Viridis', add_overlays=False, add_axes=False, **kwargs)</code>","text":"<p>Visualize hemispherical grid in 3D interactive plot.</p> <p>Convenience function providing simple interface to 3D visualization.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualize_grid_3d--parameters","level":4,"title":"Parameters","text":"<p>grid : HemiGrid     Grid to visualize data : np.ndarray, optional     Data values per cell title : str, optional     Plot title colorscale : str, default 'Viridis'     Plotly colorscale name add_overlays : bool, default False     Add elevation rings and meridians add_axes : bool, default False     Add custom coordinate axes **kwargs     Additional parameters passed to plot_hemisphere_surface</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualize_grid_3d--returns","level":4,"title":"Returns","text":"<p>fig : plotly.graph_objects.Figure     Interactive 3D figure</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualize_grid_3d--examples","level":4,"title":"Examples","text":"<p>from canvod.grids import create_hemigrid from canvod.viz import visualize_grid_3d</p> <p>grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0) fig = visualize_grid_3d( ...     grid, ...     data=vod_data, ...     title=\"VOD 3D\", ...     add_overlays=True ... ) fig.show()</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def visualize_grid_3d(\n    grid: HemiGrid,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    colorscale: str = \"Viridis\",\n    add_overlays: bool = False,\n    add_axes: bool = False,\n    **kwargs: Any,\n) -&gt; go.Figure:\n    \"\"\"Visualize hemispherical grid in 3D interactive plot.\n\n    Convenience function providing simple interface to 3D visualization.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Grid to visualize\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    colorscale : str, default 'Viridis'\n        Plotly colorscale name\n    add_overlays : bool, default False\n        Add elevation rings and meridians\n    add_axes : bool, default False\n        Add custom coordinate axes\n    **kwargs\n        Additional parameters passed to plot_hemisphere_surface\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n        Interactive 3D figure\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import create_hemigrid\n    &gt;&gt;&gt; from canvod.viz import visualize_grid_3d\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\n    &gt;&gt;&gt; fig = visualize_grid_3d(\n    ...     grid,\n    ...     data=vod_data,\n    ...     title=\"VOD 3D\",\n    ...     add_overlays=True\n    ... )\n    &gt;&gt;&gt; fig.show()\n\n    \"\"\"\n    viz = HemisphereVisualizer3D(grid)\n    fig = viz.plot_hemisphere_surface(\n        data=data, title=title, colorscale=colorscale, **kwargs\n    )\n\n    if add_overlays:\n        viz.add_spherical_overlays(fig)\n\n    if add_axes:\n        viz.add_custom_axes(fig)\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.add_tissot_indicatrix","level":2,"title":"<code>add_tissot_indicatrix(ax, grid, radius_deg=None, n_sample=None, facecolor='gold', alpha=0.6, edgecolor='black', linewidth=0.5)</code>","text":"<p>Add Tissot's indicatrix circles to existing polar plot.</p> <p>Adds equal-sized circles to visualize grid distortion. In equal-area grids, circles should appear roughly equal-sized. Variation indicates distortion.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.add_tissot_indicatrix--parameters","level":4,"title":"Parameters","text":"<p>ax : matplotlib.axes.Axes     Existing polar axis to add circles to grid : HemiGrid     Grid instance radius_deg : float, optional     Angular radius of circles in degrees. If None, auto-calculated     as angular_resolution / 8. n_sample : int, optional     Subsample cells (use every nth cell) for performance.     If None, shows all cells. facecolor : str, default 'gold'     Fill color for circles alpha : float, default 0.6     Transparency (0=transparent, 1=opaque) edgecolor : str, default 'black'     Edge color for circles linewidth : float, default 0.5     Edge line width</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.add_tissot_indicatrix--returns","level":4,"title":"Returns","text":"<p>ax : matplotlib.axes.Axes     Modified axis with Tissot circles added</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.add_tissot_indicatrix--examples","level":4,"title":"Examples","text":"<p>fig, ax = visualize_grid(grid, data=vod_data) add_tissot_indicatrix(ax, grid, radius_deg=3, n_sample=5) plt.savefig(\"vod_with_tissot.png\")</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_2d.py</code> <pre><code>def add_tissot_indicatrix(\n    ax: Axes,\n    grid: HemiGrid,\n    radius_deg: float | None = None,\n    n_sample: int | None = None,\n    facecolor: str = \"gold\",\n    alpha: float = 0.6,\n    edgecolor: str = \"black\",\n    linewidth: float = 0.5,\n) -&gt; Axes:\n    \"\"\"Add Tissot's indicatrix circles to existing polar plot.\n\n    Adds equal-sized circles to visualize grid distortion. In equal-area grids,\n    circles should appear roughly equal-sized. Variation indicates distortion.\n\n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes\n        Existing polar axis to add circles to\n    grid : HemiGrid\n        Grid instance\n    radius_deg : float, optional\n        Angular radius of circles in degrees. If None, auto-calculated\n        as angular_resolution / 8.\n    n_sample : int, optional\n        Subsample cells (use every nth cell) for performance.\n        If None, shows all cells.\n    facecolor : str, default 'gold'\n        Fill color for circles\n    alpha : float, default 0.6\n        Transparency (0=transparent, 1=opaque)\n    edgecolor : str, default 'black'\n        Edge color for circles\n    linewidth : float, default 0.5\n        Edge line width\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        Modified axis with Tissot circles added\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = visualize_grid(grid, data=vod_data)\n    &gt;&gt;&gt; add_tissot_indicatrix(ax, grid, radius_deg=3, n_sample=5)\n    &gt;&gt;&gt; plt.savefig(\"vod_with_tissot.png\")\n\n    \"\"\"\n    from matplotlib.patches import Ellipse\n\n    # Auto-calculate radius if not provided\n    if radius_deg is None:\n        if hasattr(grid, \"angular_resolution\"):\n            radius_deg = grid.angular_resolution / 8\n        else:\n            theta_vals = grid.grid[\"theta\"].to_numpy()\n            theta_spacing = np.median(np.diff(np.sort(np.unique(theta_vals))))\n            radius_deg = np.rad2deg(theta_spacing) / 8\n\n    radius_rad = np.deg2rad(radius_deg)\n\n    # Generate circle points on sphere\n    n_circle_points = 32\n    circle_angles = np.linspace(0, 2 * np.pi, n_circle_points, endpoint=False)\n\n    cell_count = 0\n    grid_df = grid.grid\n\n    # Different handling for triangular vs rectangular grids\n    if grid.grid_type in [\"htm\", \"geodesic\"]:\n        # For triangular grids: create circles on sphere surface and project\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            if n_sample is not None and i % n_sample != 0:\n                continue\n\n            phi_center = row[\"phi\"]\n            theta_center = row[\"theta\"]\n\n            if theta_center &gt; np.pi / 2:\n                continue\n\n            # Convert cell center to 3D Cartesian\n            x_c = np.sin(theta_center) * np.cos(phi_center)\n            y_c = np.sin(theta_center) * np.sin(phi_center)\n            z_c = np.cos(theta_center)\n            center_3d = np.array([x_c, y_c, z_c])\n\n            # Create tangent vectors\n            if theta_center &lt; 0.01:\n                tangent_1 = np.array([1, 0, 0])\n                tangent_2 = np.array([0, 1, 0])\n            else:\n                tangent_phi = np.array([-np.sin(phi_center), np.cos(phi_center), 0])\n                tangent_phi = tangent_phi / np.linalg.norm(tangent_phi)\n\n                tangent_theta = np.array(\n                    [\n                        np.cos(theta_center) * np.cos(phi_center),\n                        np.cos(theta_center) * np.sin(phi_center),\n                        -np.sin(theta_center),\n                    ]\n                )\n                tangent_theta = tangent_theta / np.linalg.norm(tangent_theta)\n\n                tangent_1 = tangent_phi\n                tangent_2 = tangent_theta\n\n            # Create circle on sphere surface\n            circle_3d = []\n            for angle in circle_angles:\n                offset = radius_rad * (\n                    np.cos(angle) * tangent_1 + np.sin(angle) * tangent_2\n                )\n                point_3d = center_3d + offset\n                norm = np.linalg.norm(point_3d)\n                if norm &gt; 1e-10:\n                    point_3d = point_3d / norm\n                circle_3d.append(point_3d)\n\n            circle_3d = np.array(circle_3d)\n\n            # Project to 2D polar coordinates\n            x_2d, y_2d, z_2d = circle_3d[:, 0], circle_3d[:, 1], circle_3d[:, 2]\n            theta_2d = np.arccos(np.clip(z_2d, -1, 1))\n            phi_2d = np.arctan2(y_2d, x_2d)\n\n            # Convert to polar plot coordinates (rho = sin(theta))\n            rho_2d = np.sin(theta_2d)\n            angle_2d = phi_2d\n\n            vertices_2d = np.column_stack([angle_2d, rho_2d])\n\n            poly = Polygon(\n                vertices_2d,\n                facecolor=facecolor,\n                alpha=alpha,\n                edgecolor=edgecolor,\n                linewidth=linewidth,\n            )\n            ax.add_patch(poly)\n            cell_count += 1\n\n    else:\n        # Rectangular grids: use simple ellipses at grid centers\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            if n_sample is not None and i % n_sample != 0:\n                continue\n\n            phi_center = row[\"phi\"]\n            theta_center = row[\"theta\"]\n\n            if theta_center &lt;= np.pi / 2:\n                # Convert to polar plot coordinates\n                rho_center = np.sin(theta_center)\n\n                ell = Ellipse(\n                    (phi_center, rho_center),\n                    width=2 * radius_rad,\n                    height=2 * radius_rad * np.sin(theta_center),  # Scale by projection\n                    facecolor=facecolor,\n                    alpha=alpha,\n                    edgecolor=edgecolor,\n                    linewidth=linewidth,\n                )\n                ax.add_patch(ell)\n                cell_count += 1\n\n    # Update title\n    current_title = ax.get_title()\n    if current_title:\n        ax.set_title(f\"{current_title} + Tissot ({cell_count} circles)\")\n    else:\n        ax.set_title(f\"Tissot's Indicatrix - {grid.grid_type} ({cell_count} circles)\")\n\n    return ax\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.create_publication_style","level":2,"title":"<code>create_publication_style()</code>","text":"<p>Create styling optimized for publication-quality figures.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.create_publication_style--returns","level":4,"title":"Returns","text":"<p>PlotStyle     Publication-optimized styling configuration</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.create_publication_style--examples","level":4,"title":"Examples","text":"<p>style = create_publication_style() viz.plot_2d(data=vod_data, style=style)</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>def create_publication_style() -&gt; PlotStyle:\n    \"\"\"Create styling optimized for publication-quality figures.\n\n    Returns\n    -------\n    PlotStyle\n        Publication-optimized styling configuration\n\n    Examples\n    --------\n    &gt;&gt;&gt; style = create_publication_style()\n    &gt;&gt;&gt; viz.plot_2d(data=vod_data, style=style)\n\n    \"\"\"\n    return PlotStyle(\n        colormap=\"viridis\",\n        colorscale=\"Viridis\",\n        background_color=\"white\",\n        text_color=\"black\",\n        font_family=\"sans-serif\",\n        font_size=12,\n        title_size=16,\n        label_size=14,\n        edge_linewidth=0.3,\n        opacity=0.9,\n        dark_mode=False,\n    )\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.create_interactive_style","level":2,"title":"<code>create_interactive_style(dark_mode=True)</code>","text":"<p>Create styling optimized for interactive exploration.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.create_interactive_style--parameters","level":4,"title":"Parameters","text":"<p>dark_mode : bool, default True     Use dark theme for better screen viewing</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.create_interactive_style--returns","level":4,"title":"Returns","text":"<p>PlotStyle     Interactive-optimized styling configuration</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.create_interactive_style--examples","level":4,"title":"Examples","text":"<p>style = create_interactive_style(dark_mode=True) viz.plot_3d(data=vod_data, style=style)</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>def create_interactive_style(dark_mode: bool = True) -&gt; PlotStyle:\n    \"\"\"Create styling optimized for interactive exploration.\n\n    Parameters\n    ----------\n    dark_mode : bool, default True\n        Use dark theme for better screen viewing\n\n    Returns\n    -------\n    PlotStyle\n        Interactive-optimized styling configuration\n\n    Examples\n    --------\n    &gt;&gt;&gt; style = create_interactive_style(dark_mode=True)\n    &gt;&gt;&gt; viz.plot_3d(data=vod_data, style=style)\n\n    \"\"\"\n    return PlotStyle(\n        colormap=\"plasma\" if dark_mode else \"viridis\",\n        colorscale=\"Plasma\" if dark_mode else \"Viridis\",\n        background_color=\"#111111\" if dark_mode else \"white\",\n        text_color=\"white\" if dark_mode else \"black\",\n        font_family=\"Open Sans, sans-serif\",\n        font_size=11,\n        title_size=14,\n        label_size=12,\n        edge_linewidth=0.5,\n        opacity=0.85,\n        marker_size=6,\n        wireframe_opacity=0.15,\n        dark_mode=dark_mode,\n    )\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#unified-visualizer","level":2,"title":"Unified Visualizer","text":"<p>Unified hemisphere visualization API combining 2D and 3D capabilities.</p> <p>Provides a single interface for both matplotlib (publication) and plotly (interactive) plots.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer","level":2,"title":"<code>HemisphereVisualizer</code>","text":"<p>Unified hemisphere visualizer combining 2D and 3D capabilities.</p> <p>Provides consistent API for both publication-quality matplotlib plots and interactive plotly visualizations. Handles styling coordination between different rendering backends.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer--parameters","level":4,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer--examples","level":4,"title":"Examples","text":"<p>Create both 2D and 3D visualizations::</p> <pre><code>from canvod.grids import create_hemigrid\nfrom canvod.viz import HemisphereVisualizer\n\ngrid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\nviz = HemisphereVisualizer(grid)\n\n# Publication-quality 2D plot\nfig_2d, ax_2d = viz.plot_2d(\n    data=vod_data,\n    title=\"VOD Distribution\",\n    save_path=\"publication.png\"\n)\n\n# Interactive 3D plot\nfig_3d = viz.plot_3d(\n    data=vod_data,\n    title=\"Interactive VOD Explorer\"\n)\nfig_3d.show()\n</code></pre> <p>Switch styles easily::</p> <pre><code># Publication style\npub_style = create_publication_style()\nviz.set_style(pub_style)\nfig, ax = viz.plot_2d(data=vod_data)\n\n# Interactive style\nint_style = create_interactive_style(dark_mode=True)\nviz.set_style(int_style)\nfig = viz.plot_3d(data=vod_data)\n</code></pre> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>class HemisphereVisualizer:\n    \"\"\"Unified hemisphere visualizer combining 2D and 3D capabilities.\n\n    Provides consistent API for both publication-quality matplotlib plots\n    and interactive plotly visualizations. Handles styling coordination\n    between different rendering backends.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    Examples\n    --------\n    Create both 2D and 3D visualizations::\n\n        from canvod.grids import create_hemigrid\n        from canvod.viz import HemisphereVisualizer\n\n        grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\n        viz = HemisphereVisualizer(grid)\n\n        # Publication-quality 2D plot\n        fig_2d, ax_2d = viz.plot_2d(\n            data=vod_data,\n            title=\"VOD Distribution\",\n            save_path=\"publication.png\"\n        )\n\n        # Interactive 3D plot\n        fig_3d = viz.plot_3d(\n            data=vod_data,\n            title=\"Interactive VOD Explorer\"\n        )\n        fig_3d.show()\n\n    Switch styles easily::\n\n        # Publication style\n        pub_style = create_publication_style()\n        viz.set_style(pub_style)\n        fig, ax = viz.plot_2d(data=vod_data)\n\n        # Interactive style\n        int_style = create_interactive_style(dark_mode=True)\n        viz.set_style(int_style)\n        fig = viz.plot_3d(data=vod_data)\n\n    \"\"\"\n\n    def __init__(self, grid: HemiGrid) -&gt; None:\n        \"\"\"Initialize unified visualizer.\n\n        Parameters\n        ----------\n        grid : HemiGrid\n            Hemisphere grid to visualize\n\n        \"\"\"\n        self.grid = grid\n\n        # Initialize specialized visualizers\n        self.viz_2d = HemisphereVisualizer2D(grid)\n        self.viz_3d = HemisphereVisualizer3D(grid)\n\n        # Default styling\n        self.style = PlotStyle()\n\n    def set_style(self, style: PlotStyle) -&gt; None:\n        \"\"\"Set unified styling for both 2D and 3D plots.\n\n        Parameters\n        ----------\n        style : PlotStyle\n            Styling configuration\n\n        Examples\n        --------\n        &gt;&gt;&gt; pub_style = create_publication_style()\n        &gt;&gt;&gt; viz.set_style(pub_style)\n        &gt;&gt;&gt; fig, ax = viz.plot_2d(data=vod_data)\n\n        \"\"\"\n        self.style = style\n\n    def plot_2d(\n        self,\n        data: np.ndarray | None = None,\n        title: str | None = None,\n        ax: Axes | None = None,\n        save_path: Path | str | None = None,\n        style: PolarPlotStyle | None = None,\n        **kwargs: Any,\n    ) -&gt; tuple[Figure, Axes]:\n        \"\"\"Create 2D publication-quality plot.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, optional\n            Plot title\n        ax : matplotlib.axes.Axes, optional\n            Existing axes to plot on\n        save_path : Path or str, optional\n            Save figure to this path\n        style : PolarPlotStyle, optional\n            Override default 2D style\n        **kwargs\n            Additional styling parameters\n\n        Returns\n        -------\n        fig : matplotlib.figure.Figure\n            Figure object\n        ax : matplotlib.axes.Axes\n            Polar axes with plot\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig, ax = viz.plot_2d(\n        ...     data=vod_data,\n        ...     title=\"VOD Distribution\",\n        ...     cmap='plasma',\n        ...     save_path=\"output.png\",\n        ...     dpi=300\n        ... )\n\n        \"\"\"\n        if style is None:\n            style = self.style.to_polar_style()\n\n        if title:\n            style.title = title\n\n        return self.viz_2d.plot_grid_patches(\n            data=data, style=style, ax=ax, save_path=save_path, **kwargs\n        )\n\n    def plot_3d(\n        self,\n        data: np.ndarray | None = None,\n        title: str | None = None,\n        style: PlotStyle | None = None,\n        **kwargs: Any,\n    ) -&gt; go.Figure:\n        \"\"\"Create 3D interactive plot.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, optional\n            Plot title\n        style : PlotStyle, optional\n            Override default 3D style\n        **kwargs\n            Additional plotly parameters\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive 3D figure\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig = viz.plot_3d(\n        ...     data=vod_data,\n        ...     title=\"Interactive VOD\",\n        ...     opacity=0.9,\n        ...     width=1000,\n        ...     height=800\n        ... )\n        &gt;&gt;&gt; fig.show()\n        &gt;&gt;&gt; fig.write_html(\"interactive.html\")\n\n        \"\"\"\n        if style is None:\n            style = self.style\n\n        return self.viz_3d.plot_hemisphere_surface(\n            data=data, style=style, title=title, **kwargs\n        )\n\n    def plot_3d_mesh(\n        self,\n        data: np.ndarray | None = None,\n        title: str | None = None,\n        **kwargs: Any,\n    ) -&gt; go.Figure:\n        \"\"\"Create 3D mesh plot showing cell boundaries.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, optional\n            Plot title\n        **kwargs\n            Additional plotly parameters\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive mesh figure\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig = viz.plot_3d_mesh(\n        ...     data=vod_data,\n        ...     title=\"VOD Mesh View\",\n        ...     opacity=0.7\n        ... )\n\n        \"\"\"\n        return self.viz_3d.plot_cell_mesh(data=data, title=title, **kwargs)\n\n    def create_comparison_plot(\n        self,\n        data: np.ndarray | None = None,\n        title_2d: str = \"2D Polar View\",\n        title_3d: str = \"3D Hemisphere View\",\n        save_2d: Path | str | None = None,\n        save_3d: Path | str | None = None,\n    ) -&gt; tuple[tuple[Figure, Axes], go.Figure]:\n        \"\"\"Create both 2D and 3D plots for comparison.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title_2d : str, default \"2D Polar View\"\n            Title for 2D plot\n        title_3d : str, default \"3D Hemisphere View\"\n            Title for 3D plot\n        save_2d : Path or str, optional\n            Save 2D figure to this path\n        save_3d : Path or str, optional\n            Save 3D figure to this path (HTML)\n\n        Returns\n        -------\n        plot_2d : tuple of Figure and Axes\n            2D matplotlib plot\n        plot_3d : plotly.graph_objects.Figure\n            3D plotly plot\n\n        Examples\n        --------\n        &gt;&gt;&gt; (fig_2d, ax_2d), fig_3d = viz.create_comparison_plot(\n        ...     data=vod_data,\n        ...     save_2d=\"comparison_2d.png\",\n        ...     save_3d=\"comparison_3d.html\"\n        ... )\n        &gt;&gt;&gt; plt.show()  # Show 2D\n        &gt;&gt;&gt; fig_3d.show()  # Show 3D\n\n        \"\"\"\n        # Create 2D plot\n        fig_2d, ax_2d = self.plot_2d(data=data, title=title_2d, save_path=save_2d)\n\n        # Create 3D plot\n        fig_3d = self.plot_3d(data=data, title=title_3d)\n\n        # Save 3D if requested\n        if save_3d:\n            save_3d = Path(save_3d)\n            save_3d.parent.mkdir(parents=True, exist_ok=True)\n            fig_3d.write_html(str(save_3d))\n\n        return (fig_2d, ax_2d), fig_3d\n\n    def create_publication_figure(\n        self,\n        data: np.ndarray | None = None,\n        title: str = \"Hemispherical Data Distribution\",\n        save_path: Path | str | None = None,\n        dpi: int = 300,\n        **kwargs: Any,\n    ) -&gt; tuple[Figure, Axes]:\n        \"\"\"Create publication-ready figure with optimal styling.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, default \"Hemispherical Data Distribution\"\n            Plot title\n        save_path : Path or str, optional\n            Save figure to this path\n        dpi : int, default 300\n            Resolution in dots per inch\n        **kwargs\n            Additional styling parameters\n\n        Returns\n        -------\n        fig : matplotlib.figure.Figure\n            Publication-ready figure\n        ax : matplotlib.axes.Axes\n            Styled polar axes\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig, ax = viz.create_publication_figure(\n        ...     data=vod_data,\n        ...     title=\"VOD Distribution Over Rosalia Site\",\n        ...     save_path=\"paper_figure_3.png\",\n        ...     dpi=600\n        ... )\n\n        \"\"\"\n        # Use publication style and convert to PolarPlotStyle\n        pub_plot_style = create_publication_style()\n        polar_style = pub_plot_style.to_polar_style()\n        polar_style.title = title\n        polar_style.dpi = dpi\n\n        # Override with kwargs\n        for key, value in kwargs.items():\n            if hasattr(polar_style, key):\n                setattr(polar_style, key, value)\n\n        return self.plot_2d(data=data, style=polar_style, save_path=save_path)\n\n    def create_interactive_explorer(\n        self,\n        data: np.ndarray | None = None,\n        title: str = \"Interactive Data Explorer\",\n        dark_mode: bool = True,\n        save_html: Path | str | None = None,\n    ) -&gt; go.Figure:\n        \"\"\"Create interactive explorer with optimal settings.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, default \"Interactive Data Explorer\"\n            Plot title\n        dark_mode : bool, default True\n            Use dark theme\n        save_html : Path or str, optional\n            Save HTML to this path\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive explorer figure\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig = viz.create_interactive_explorer(\n        ...     data=vod_data,\n        ...     title=\"VOD Explorer\",\n        ...     dark_mode=True,\n        ...     save_html=\"explorer.html\"\n        ... )\n        &gt;&gt;&gt; fig.show()\n\n        \"\"\"\n        # Use interactive style\n        int_style = create_interactive_style(dark_mode=dark_mode)\n\n        fig = self.plot_3d(data=data, title=title, style=int_style)\n\n        # Save if requested\n        if save_html:\n            save_html = Path(save_html)\n            save_html.parent.mkdir(parents=True, exist_ok=True)\n            fig.write_html(str(save_html))\n\n        return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.__init__","level":3,"title":"<code>__init__(grid)</code>","text":"<p>Initialize unified visualizer.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.__init__--parameters","level":5,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def __init__(self, grid: HemiGrid) -&gt; None:\n    \"\"\"Initialize unified visualizer.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    \"\"\"\n    self.grid = grid\n\n    # Initialize specialized visualizers\n    self.viz_2d = HemisphereVisualizer2D(grid)\n    self.viz_3d = HemisphereVisualizer3D(grid)\n\n    # Default styling\n    self.style = PlotStyle()\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.set_style","level":3,"title":"<code>set_style(style)</code>","text":"<p>Set unified styling for both 2D and 3D plots.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.set_style--parameters","level":5,"title":"Parameters","text":"<p>style : PlotStyle     Styling configuration</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.set_style--examples","level":5,"title":"Examples","text":"<p>pub_style = create_publication_style() viz.set_style(pub_style) fig, ax = viz.plot_2d(data=vod_data)</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def set_style(self, style: PlotStyle) -&gt; None:\n    \"\"\"Set unified styling for both 2D and 3D plots.\n\n    Parameters\n    ----------\n    style : PlotStyle\n        Styling configuration\n\n    Examples\n    --------\n    &gt;&gt;&gt; pub_style = create_publication_style()\n    &gt;&gt;&gt; viz.set_style(pub_style)\n    &gt;&gt;&gt; fig, ax = viz.plot_2d(data=vod_data)\n\n    \"\"\"\n    self.style = style\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_2d","level":3,"title":"<code>plot_2d(data=None, title=None, ax=None, save_path=None, style=None, **kwargs)</code>","text":"<p>Create 2D publication-quality plot.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_2d--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, optional     Plot title ax : matplotlib.axes.Axes, optional     Existing axes to plot on save_path : Path or str, optional     Save figure to this path style : PolarPlotStyle, optional     Override default 2D style **kwargs     Additional styling parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_2d--returns","level":5,"title":"Returns","text":"<p>fig : matplotlib.figure.Figure     Figure object ax : matplotlib.axes.Axes     Polar axes with plot</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_2d--examples","level":5,"title":"Examples","text":"<p>fig, ax = viz.plot_2d( ...     data=vod_data, ...     title=\"VOD Distribution\", ...     cmap='plasma', ...     save_path=\"output.png\", ...     dpi=300 ... )</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def plot_2d(\n    self,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    ax: Axes | None = None,\n    save_path: Path | str | None = None,\n    style: PolarPlotStyle | None = None,\n    **kwargs: Any,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Create 2D publication-quality plot.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    ax : matplotlib.axes.Axes, optional\n        Existing axes to plot on\n    save_path : Path or str, optional\n        Save figure to this path\n    style : PolarPlotStyle, optional\n        Override default 2D style\n    **kwargs\n        Additional styling parameters\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Figure object\n    ax : matplotlib.axes.Axes\n        Polar axes with plot\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = viz.plot_2d(\n    ...     data=vod_data,\n    ...     title=\"VOD Distribution\",\n    ...     cmap='plasma',\n    ...     save_path=\"output.png\",\n    ...     dpi=300\n    ... )\n\n    \"\"\"\n    if style is None:\n        style = self.style.to_polar_style()\n\n    if title:\n        style.title = title\n\n    return self.viz_2d.plot_grid_patches(\n        data=data, style=style, ax=ax, save_path=save_path, **kwargs\n    )\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_3d","level":3,"title":"<code>plot_3d(data=None, title=None, style=None, **kwargs)</code>","text":"<p>Create 3D interactive plot.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_3d--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, optional     Plot title style : PlotStyle, optional     Override default 3D style **kwargs     Additional plotly parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_3d--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive 3D figure</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_3d--examples","level":5,"title":"Examples","text":"<p>fig = viz.plot_3d( ...     data=vod_data, ...     title=\"Interactive VOD\", ...     opacity=0.9, ...     width=1000, ...     height=800 ... ) fig.show() fig.write_html(\"interactive.html\")</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def plot_3d(\n    self,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    style: PlotStyle | None = None,\n    **kwargs: Any,\n) -&gt; go.Figure:\n    \"\"\"Create 3D interactive plot.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    style : PlotStyle, optional\n        Override default 3D style\n    **kwargs\n        Additional plotly parameters\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive 3D figure\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = viz.plot_3d(\n    ...     data=vod_data,\n    ...     title=\"Interactive VOD\",\n    ...     opacity=0.9,\n    ...     width=1000,\n    ...     height=800\n    ... )\n    &gt;&gt;&gt; fig.show()\n    &gt;&gt;&gt; fig.write_html(\"interactive.html\")\n\n    \"\"\"\n    if style is None:\n        style = self.style\n\n    return self.viz_3d.plot_hemisphere_surface(\n        data=data, style=style, title=title, **kwargs\n    )\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_3d_mesh","level":3,"title":"<code>plot_3d_mesh(data=None, title=None, **kwargs)</code>","text":"<p>Create 3D mesh plot showing cell boundaries.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_3d_mesh--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, optional     Plot title **kwargs     Additional plotly parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_3d_mesh--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive mesh figure</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.plot_3d_mesh--examples","level":5,"title":"Examples","text":"<p>fig = viz.plot_3d_mesh( ...     data=vod_data, ...     title=\"VOD Mesh View\", ...     opacity=0.7 ... )</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def plot_3d_mesh(\n    self,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    **kwargs: Any,\n) -&gt; go.Figure:\n    \"\"\"Create 3D mesh plot showing cell boundaries.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    **kwargs\n        Additional plotly parameters\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive mesh figure\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = viz.plot_3d_mesh(\n    ...     data=vod_data,\n    ...     title=\"VOD Mesh View\",\n    ...     opacity=0.7\n    ... )\n\n    \"\"\"\n    return self.viz_3d.plot_cell_mesh(data=data, title=title, **kwargs)\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_comparison_plot","level":3,"title":"<code>create_comparison_plot(data=None, title_2d='2D Polar View', title_3d='3D Hemisphere View', save_2d=None, save_3d=None)</code>","text":"<p>Create both 2D and 3D plots for comparison.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_comparison_plot--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title_2d : str, default \"2D Polar View\"     Title for 2D plot title_3d : str, default \"3D Hemisphere View\"     Title for 3D plot save_2d : Path or str, optional     Save 2D figure to this path save_3d : Path or str, optional     Save 3D figure to this path (HTML)</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_comparison_plot--returns","level":5,"title":"Returns","text":"<p>plot_2d : tuple of Figure and Axes     2D matplotlib plot plot_3d : plotly.graph_objects.Figure     3D plotly plot</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_comparison_plot--examples","level":5,"title":"Examples","text":"<p>(fig_2d, ax_2d), fig_3d = viz.create_comparison_plot( ...     data=vod_data, ...     save_2d=\"comparison_2d.png\", ...     save_3d=\"comparison_3d.html\" ... ) plt.show()  # Show 2D fig_3d.show()  # Show 3D</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def create_comparison_plot(\n    self,\n    data: np.ndarray | None = None,\n    title_2d: str = \"2D Polar View\",\n    title_3d: str = \"3D Hemisphere View\",\n    save_2d: Path | str | None = None,\n    save_3d: Path | str | None = None,\n) -&gt; tuple[tuple[Figure, Axes], go.Figure]:\n    \"\"\"Create both 2D and 3D plots for comparison.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title_2d : str, default \"2D Polar View\"\n        Title for 2D plot\n    title_3d : str, default \"3D Hemisphere View\"\n        Title for 3D plot\n    save_2d : Path or str, optional\n        Save 2D figure to this path\n    save_3d : Path or str, optional\n        Save 3D figure to this path (HTML)\n\n    Returns\n    -------\n    plot_2d : tuple of Figure and Axes\n        2D matplotlib plot\n    plot_3d : plotly.graph_objects.Figure\n        3D plotly plot\n\n    Examples\n    --------\n    &gt;&gt;&gt; (fig_2d, ax_2d), fig_3d = viz.create_comparison_plot(\n    ...     data=vod_data,\n    ...     save_2d=\"comparison_2d.png\",\n    ...     save_3d=\"comparison_3d.html\"\n    ... )\n    &gt;&gt;&gt; plt.show()  # Show 2D\n    &gt;&gt;&gt; fig_3d.show()  # Show 3D\n\n    \"\"\"\n    # Create 2D plot\n    fig_2d, ax_2d = self.plot_2d(data=data, title=title_2d, save_path=save_2d)\n\n    # Create 3D plot\n    fig_3d = self.plot_3d(data=data, title=title_3d)\n\n    # Save 3D if requested\n    if save_3d:\n        save_3d = Path(save_3d)\n        save_3d.parent.mkdir(parents=True, exist_ok=True)\n        fig_3d.write_html(str(save_3d))\n\n    return (fig_2d, ax_2d), fig_3d\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_publication_figure","level":3,"title":"<code>create_publication_figure(data=None, title='Hemispherical Data Distribution', save_path=None, dpi=300, **kwargs)</code>","text":"<p>Create publication-ready figure with optimal styling.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_publication_figure--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, default \"Hemispherical Data Distribution\"     Plot title save_path : Path or str, optional     Save figure to this path dpi : int, default 300     Resolution in dots per inch **kwargs     Additional styling parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_publication_figure--returns","level":5,"title":"Returns","text":"<p>fig : matplotlib.figure.Figure     Publication-ready figure ax : matplotlib.axes.Axes     Styled polar axes</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_publication_figure--examples","level":5,"title":"Examples","text":"<p>fig, ax = viz.create_publication_figure( ...     data=vod_data, ...     title=\"VOD Distribution Over Rosalia Site\", ...     save_path=\"paper_figure_3.png\", ...     dpi=600 ... )</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def create_publication_figure(\n    self,\n    data: np.ndarray | None = None,\n    title: str = \"Hemispherical Data Distribution\",\n    save_path: Path | str | None = None,\n    dpi: int = 300,\n    **kwargs: Any,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Create publication-ready figure with optimal styling.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, default \"Hemispherical Data Distribution\"\n        Plot title\n    save_path : Path or str, optional\n        Save figure to this path\n    dpi : int, default 300\n        Resolution in dots per inch\n    **kwargs\n        Additional styling parameters\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Publication-ready figure\n    ax : matplotlib.axes.Axes\n        Styled polar axes\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = viz.create_publication_figure(\n    ...     data=vod_data,\n    ...     title=\"VOD Distribution Over Rosalia Site\",\n    ...     save_path=\"paper_figure_3.png\",\n    ...     dpi=600\n    ... )\n\n    \"\"\"\n    # Use publication style and convert to PolarPlotStyle\n    pub_plot_style = create_publication_style()\n    polar_style = pub_plot_style.to_polar_style()\n    polar_style.title = title\n    polar_style.dpi = dpi\n\n    # Override with kwargs\n    for key, value in kwargs.items():\n        if hasattr(polar_style, key):\n            setattr(polar_style, key, value)\n\n    return self.plot_2d(data=data, style=polar_style, save_path=save_path)\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_interactive_explorer","level":3,"title":"<code>create_interactive_explorer(data=None, title='Interactive Data Explorer', dark_mode=True, save_html=None)</code>","text":"<p>Create interactive explorer with optimal settings.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_interactive_explorer--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, default \"Interactive Data Explorer\"     Plot title dark_mode : bool, default True     Use dark theme save_html : Path or str, optional     Save HTML to this path</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_interactive_explorer--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive explorer figure</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.visualizer.HemisphereVisualizer.create_interactive_explorer--examples","level":5,"title":"Examples","text":"<p>fig = viz.create_interactive_explorer( ...     data=vod_data, ...     title=\"VOD Explorer\", ...     dark_mode=True, ...     save_html=\"explorer.html\" ... ) fig.show()</p> Source code in <code>packages/canvod-viz/src/canvod/viz/visualizer.py</code> <pre><code>def create_interactive_explorer(\n    self,\n    data: np.ndarray | None = None,\n    title: str = \"Interactive Data Explorer\",\n    dark_mode: bool = True,\n    save_html: Path | str | None = None,\n) -&gt; go.Figure:\n    \"\"\"Create interactive explorer with optimal settings.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, default \"Interactive Data Explorer\"\n        Plot title\n    dark_mode : bool, default True\n        Use dark theme\n    save_html : Path or str, optional\n        Save HTML to this path\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive explorer figure\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = viz.create_interactive_explorer(\n    ...     data=vod_data,\n    ...     title=\"VOD Explorer\",\n    ...     dark_mode=True,\n    ...     save_html=\"explorer.html\"\n    ... )\n    &gt;&gt;&gt; fig.show()\n\n    \"\"\"\n    # Use interactive style\n    int_style = create_interactive_style(dark_mode=dark_mode)\n\n    fig = self.plot_3d(data=data, title=title, style=int_style)\n\n    # Save if requested\n    if save_html:\n        save_html = Path(save_html)\n        save_html.parent.mkdir(parents=True, exist_ok=True)\n        fig.write_html(str(save_html))\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#2d-visualization","level":2,"title":"2D Visualization","text":"<p>2D hemisphere visualization using matplotlib for publication-quality plots.</p> <p>Provides polar projection plotting of hemispherical grids with various rendering methods.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.HemisphereVisualizer2D","level":2,"title":"<code>HemisphereVisualizer2D</code>","text":"<p>2D hemisphere visualization using matplotlib.</p> <p>Creates publication-quality polar projection plots of hemispherical grids. Supports multiple grid types and rendering methods.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.HemisphereVisualizer2D--parameters","level":4,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.HemisphereVisualizer2D--examples","level":4,"title":"Examples","text":"<p>from canvod.grids import create_hemigrid from canvod.viz import HemisphereVisualizer2D</p> <p>grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0) viz = HemisphereVisualizer2D(grid) fig, ax = viz.plot_grid_patches(data=vod_data, title=\"VOD Distribution\") plt.savefig(\"vod_plot.png\", dpi=300, bbox_inches='tight')</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_2d.py</code> <pre><code>class HemisphereVisualizer2D:\n    \"\"\"2D hemisphere visualization using matplotlib.\n\n    Creates publication-quality polar projection plots of hemispherical grids.\n    Supports multiple grid types and rendering methods.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import create_hemigrid\n    &gt;&gt;&gt; from canvod.viz import HemisphereVisualizer2D\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\n    &gt;&gt;&gt; viz = HemisphereVisualizer2D(grid)\n    &gt;&gt;&gt; fig, ax = viz.plot_grid_patches(data=vod_data, title=\"VOD Distribution\")\n    &gt;&gt;&gt; plt.savefig(\"vod_plot.png\", dpi=300, bbox_inches='tight')\n\n    \"\"\"\n\n    def __init__(self, grid: HemiGrid) -&gt; None:\n        \"\"\"Initialize 2D hemisphere visualizer.\n\n        Parameters\n        ----------\n        grid : HemiGrid\n            Hemisphere grid to visualize\n\n        \"\"\"\n        self.grid = grid\n        self._patches_cache: list[Polygon] | None = None\n        self._cell_indices_cache: np.ndarray | None = None\n\n    def plot_grid_patches(\n        self,\n        data: np.ndarray | None = None,\n        style: PolarPlotStyle | None = None,\n        ax: Axes | None = None,\n        save_path: Path | str | None = None,\n        **style_kwargs: Any,\n    ) -&gt; tuple[Figure, Axes]:\n        \"\"\"Plot hemisphere grid as colored patches in polar projection.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell. If None, plots uniform grid.\n        style : PolarPlotStyle, optional\n            Styling configuration. If None, uses defaults.\n        ax : matplotlib.axes.Axes, optional\n            Existing polar axes to plot on. If None, creates new figure.\n        save_path : Path or str, optional\n            If provided, saves figure to this path\n        **style_kwargs\n            Override individual style parameters\n\n        Returns\n        -------\n        fig : matplotlib.figure.Figure\n            Figure object\n        ax : matplotlib.axes.Axes\n            Polar axes with plot\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig, ax = viz.plot_grid_patches(\n        ...     data=vod_data,\n        ...     title=\"VOD Distribution\",\n        ...     cmap='plasma',\n        ...     save_path=\"output.png\"\n        ... )\n\n        \"\"\"\n        # Initialize style\n        if style is None:\n            style = PolarPlotStyle(**style_kwargs)\n        else:\n            # Override style with kwargs\n            for key, value in style_kwargs.items():\n                if hasattr(style, key):\n                    setattr(style, key, value)\n\n        # Create figure if needed\n        if ax is None:\n            fig, ax = plt.subplots(\n                figsize=style.figsize, dpi=style.dpi, subplot_kw={\"projection\": \"polar\"}\n            )\n        else:\n            fig = ax.figure\n\n        # Get patches for grid\n        patches, cell_indices = self._extract_grid_patches()\n\n        # Map data to patches\n        patch_data = self._map_data_to_patches(data, cell_indices)\n\n        # Determine color limits\n        vmin = style.vmin if style.vmin is not None else np.nanmin(patch_data)\n        vmax = style.vmax if style.vmax is not None else np.nanmax(patch_data)\n\n        # Create patch collection\n        pc = PatchCollection(\n            patches,\n            cmap=style.cmap,\n            edgecolor=style.edgecolor,\n            linewidth=style.linewidth,\n            alpha=style.alpha,\n        )\n        pc.set_array(np.ma.masked_invalid(patch_data))\n        pc.set_clim(vmin, vmax)\n\n        # Add to axes\n        ax.add_collection(pc)\n\n        # Style polar axes\n        self._apply_polar_styling(ax, style)\n\n        # Add colorbar\n        cbar = fig.colorbar(\n            pc,\n            ax=ax,\n            shrink=style.colorbar_shrink,\n            pad=style.colorbar_pad,\n        )\n        cbar.set_label(style.colorbar_label, fontsize=style.colorbar_fontsize)\n\n        # Set title\n        if style.title:\n            ax.set_title(style.title, y=1.08, fontsize=14)\n\n        # Save if requested\n        if save_path:\n            save_path = Path(save_path)\n            save_path.parent.mkdir(parents=True, exist_ok=True)\n            fig.savefig(\n                save_path,\n                dpi=style.dpi,\n                bbox_inches=\"tight\",\n                facecolor=\"white\",\n                edgecolor=\"none\",\n            )\n\n        return fig, ax\n\n    def _extract_grid_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract 2D polygon patches from hemispherical grid.\n\n        Returns\n        -------\n        patches : list of Polygon\n            Matplotlib polygon patches\n        cell_indices : np.ndarray\n            Corresponding cell indices in grid\n\n        \"\"\"\n        # Use cache if available\n        if self._patches_cache is not None and self._cell_indices_cache is not None:\n            return self._patches_cache, self._cell_indices_cache\n\n        grid_type = self.grid.grid_type.lower()\n\n        _rectangular_types = {\"equal_area\", \"equal_angle\", \"equirectangular\"}\n        if grid_type in _rectangular_types:\n            patches, indices = self._extract_rectangular_patches()\n        elif grid_type == \"htm\":\n            patches, indices = self._extract_htm_patches()\n        elif grid_type == \"geodesic\":\n            patches, indices = self._extract_geodesic_patches()\n        elif grid_type == \"healpix\":\n            patches, indices = self._extract_healpix_patches()\n        elif grid_type == \"fibonacci\":\n            patches, indices = self._extract_fibonacci_patches()\n        else:\n            raise ValueError(f\"Unsupported grid type: {grid_type}\")\n\n        # Cache results\n        self._patches_cache = patches\n        self._cell_indices_cache = indices\n\n        return patches, indices\n\n    def _extract_rectangular_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract patches from rectangular/equal-area grid.\"\"\"\n        patches = []\n        cell_indices = []\n\n        # Access the grid DataFrame from GridData\n        grid_df = self.grid.grid\n\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            phi_min = row[\"phi_min\"]\n            phi_max = row[\"phi_max\"]\n            theta_min = row[\"theta_min\"]\n            theta_max = row[\"theta_max\"]\n\n            # Skip cells beyond hemisphere\n            if theta_min &gt; np.pi / 2:\n                continue\n\n            # Convert to polar coordinates (rho = sin(theta))\n            rho_min = np.sin(theta_min)\n            rho_max = np.sin(theta_max)\n\n            # Create rectangular patch in polar coordinates\n            vertices = np.array(\n                [\n                    [phi_min, rho_min],\n                    [phi_max, rho_min],\n                    [phi_max, rho_max],\n                    [phi_min, rho_max],\n                ]\n            )\n\n            patches.append(Polygon(vertices, closed=True))\n            cell_indices.append(idx)\n\n        return patches, np.array(cell_indices)\n\n    def _extract_htm_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract triangular patches from HTM grid.\"\"\"\n        patches = []\n        cell_indices = []\n\n        # Access the grid DataFrame from GridData\n        grid_df = self.grid.grid\n\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                # HTM stores vertices as columns htm_vertex_0, htm_vertex_1,\n                # htm_vertex_2.\n                v0 = np.array(row[\"htm_vertex_0\"], dtype=float)\n                v1 = np.array(row[\"htm_vertex_1\"], dtype=float)\n                v2 = np.array(row[\"htm_vertex_2\"], dtype=float)\n\n                vertices_3d = np.array([v0, v1, v2])\n                x, y, z = (\n                    vertices_3d[:, 0],\n                    vertices_3d[:, 1],\n                    vertices_3d[:, 2],\n                )\n\n                # Convert to spherical coordinates\n                r = np.sqrt(x**2 + y**2 + z**2)\n                theta = np.arccos(np.clip(z / r, -1, 1))\n                phi = np.arctan2(y, x)\n                phi = np.mod(phi, 2 * np.pi)\n\n                # Skip if beyond hemisphere\n                if np.all(theta &gt; np.pi / 2):\n                    continue\n\n                # Convert to polar coordinates (rho = sin(theta))\n                rho = np.sin(theta)\n                vertices_2d = np.column_stack([phi, rho])\n\n                patches.append(Polygon(vertices_2d, closed=True))\n                cell_indices.append(idx)\n\n            except (KeyError, TypeError):\n                # Skip cells that don't have proper HTM vertex data\n                continue\n\n        return patches, np.array(cell_indices)\n\n    def _extract_geodesic_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract triangular patches from geodesic grid.\n\n        The ``geodesic_vertices`` column stores vertex **indices** into the\n        shared ``grid.vertices`` coordinate array (shape ``(n_vertices, 3)``).\n        \"\"\"\n        patches = []\n        cell_indices = []\n\n        grid_df = self.grid.grid\n        shared_vertices = self.grid.vertices  # (n_vertices, 3) or None\n\n        if shared_vertices is None or \"geodesic_vertices\" not in grid_df.columns:\n            # No vertex data — fall back to bounding-box rectangles\n            return self._extract_rectangular_patches()\n\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                v_indices = np.array(row[\"geodesic_vertices\"], dtype=int)\n                if len(v_indices) &lt; 3:\n                    continue\n\n                # Look up actual 3D coordinates from shared vertex array\n                verts_3d = shared_vertices[v_indices]  # (3, 3)\n                x, y, z = verts_3d[:, 0], verts_3d[:, 1], verts_3d[:, 2]\n\n                r = np.sqrt(x**2 + y**2 + z**2)\n                theta = np.arccos(np.clip(z / r, -1, 1))\n                phi = np.arctan2(y, x)\n                phi = np.mod(phi, 2 * np.pi)\n\n                if np.all(theta &gt; np.pi / 2):\n                    continue\n\n                rho = np.sin(theta)\n                vertices_2d = np.column_stack([phi, rho])\n                patches.append(Polygon(vertices_2d, closed=True))\n                cell_indices.append(idx)\n\n            except (IndexError, KeyError, TypeError, ValueError):\n                continue\n\n        return patches, np.array(cell_indices)\n\n    def _extract_healpix_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract patches from HEALPix grid via ``healpy.boundaries``.\"\"\"\n        try:\n            import healpy as hp\n        except ImportError:\n            raise ImportError(\n                \"healpy is required for HEALPix 2D visualization. \"\n                \"Install with: pip install healpy\"\n            )\n\n        patches = []\n        cell_indices = []\n        grid_df = self.grid.grid\n        nside = int(grid_df[\"healpix_nside\"][0])\n\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            ipix = int(row[\"healpix_ipix\"])\n            # boundaries returns shape (3, n_vertices) in Cartesian\n            boundary = hp.boundaries(nside, ipix, step=4)\n            x, y, z = boundary[0], boundary[1], boundary[2]\n\n            # Skip pixels entirely below horizon\n            if np.all(z &lt; -0.01):\n                continue\n\n            r = np.sqrt(x**2 + y**2 + z**2)\n            theta = np.arccos(np.clip(z / r, -1, 1))\n            phi = np.arctan2(y, x)\n            phi = np.mod(phi, 2 * np.pi)\n\n            # Keep only vertices in upper hemisphere\n            mask = theta &lt;= np.pi / 2 + 0.01\n            if not np.any(mask):\n                continue\n\n            rho = np.sin(theta)\n            vertices_2d = np.column_stack([phi, rho])\n            patches.append(Polygon(vertices_2d, closed=True))\n            cell_indices.append(idx)\n\n        return patches, np.array(cell_indices)\n\n    def _extract_fibonacci_patches(self) -&gt; tuple[list[Polygon], np.ndarray]:\n        \"\"\"Extract patches from Fibonacci grid using Voronoi regions.\"\"\"\n        patches = []\n        cell_indices = []\n        grid_df = self.grid.grid\n        voronoi = self.grid.voronoi  # scipy.spatial.SphericalVoronoi or None\n\n        if voronoi is None or \"voronoi_region\" not in grid_df.columns:\n            # No Voronoi data — fall back to bounding-box rectangles\n            return self._extract_rectangular_patches()\n\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                region_indices = row[\"voronoi_region\"]\n                if region_indices is None or len(region_indices) &lt; 3:\n                    continue\n\n                verts_3d = voronoi.vertices[region_indices]\n                x, y, z = verts_3d[:, 0], verts_3d[:, 1], verts_3d[:, 2]\n\n                # Skip cells entirely below horizon\n                if np.all(z &lt; -0.01):\n                    continue\n\n                r = np.sqrt(x**2 + y**2 + z**2)\n                theta = np.arccos(np.clip(z / r, -1, 1))\n                phi = np.arctan2(y, x)\n                phi = np.mod(phi, 2 * np.pi)\n\n                # Vertices are already in polygon winding order from\n                # sort_vertices_of_regions() — use directly.\n                rho = np.sin(theta)\n                vertices_2d = np.column_stack([phi, rho])\n                patches.append(Polygon(vertices_2d, closed=True))\n                cell_indices.append(idx)\n\n            except (IndexError, KeyError, TypeError, ValueError):\n                continue\n\n        return patches, np.array(cell_indices)\n\n    def _map_data_to_patches(\n        self,\n        data: np.ndarray | None,\n        cell_indices: np.ndarray,\n    ) -&gt; np.ndarray:\n        \"\"\"Map data values to patches.\n\n        Parameters\n        ----------\n        data : np.ndarray or None\n            Data per grid cell\n        cell_indices : np.ndarray\n            Cell indices corresponding to patches\n\n        Returns\n        -------\n        np.ndarray\n            Data values for each patch\n\n        \"\"\"\n        if data is None:\n            return np.ones(len(cell_indices)) * 0.5\n\n        return data[cell_indices]\n\n    def _apply_polar_styling(\n        self,\n        ax: Axes,\n        style: PolarPlotStyle,\n    ) -&gt; None:\n        \"\"\"Apply styling to polar axes.\n\n        Parameters\n        ----------\n        ax : matplotlib.axes.Axes\n            Polar axes to style\n        style : PolarPlotStyle\n            Styling configuration\n\n        \"\"\"\n        # Set rho limits (0 to 1 for hemisphere projection)\n        ax.set_ylim(0, 1.0)\n\n        # Configure polar axis orientation\n        ax.set_theta_zero_location(\"N\")  # North at top\n        ax.set_theta_direction(-1)  # Clockwise (azimuth convention)\n\n        # Add degree labels on radial axis\n        if style.show_degree_labels:\n            theta_labels = style.theta_labels\n            rho_ticks = [np.sin(np.radians(t)) for t in theta_labels]\n            ax.set_yticks(rho_ticks)\n            ax.set_yticklabels([f\"{t}°\" for t in theta_labels])\n\n        # Grid styling\n        if style.show_grid:\n            ax.grid(\n                True,\n                alpha=style.grid_alpha,\n                linestyle=style.grid_linestyle,\n                color=\"gray\",\n            )\n        else:\n            ax.grid(False)\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.HemisphereVisualizer2D.__init__","level":3,"title":"<code>__init__(grid)</code>","text":"<p>Initialize 2D hemisphere visualizer.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.HemisphereVisualizer2D.__init__--parameters","level":5,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_2d.py</code> <pre><code>def __init__(self, grid: HemiGrid) -&gt; None:\n    \"\"\"Initialize 2D hemisphere visualizer.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    \"\"\"\n    self.grid = grid\n    self._patches_cache: list[Polygon] | None = None\n    self._cell_indices_cache: np.ndarray | None = None\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.HemisphereVisualizer2D.plot_grid_patches","level":3,"title":"<code>plot_grid_patches(data=None, style=None, ax=None, save_path=None, **style_kwargs)</code>","text":"<p>Plot hemisphere grid as colored patches in polar projection.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.HemisphereVisualizer2D.plot_grid_patches--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell. If None, plots uniform grid. style : PolarPlotStyle, optional     Styling configuration. If None, uses defaults. ax : matplotlib.axes.Axes, optional     Existing polar axes to plot on. If None, creates new figure. save_path : Path or str, optional     If provided, saves figure to this path **style_kwargs     Override individual style parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.HemisphereVisualizer2D.plot_grid_patches--returns","level":5,"title":"Returns","text":"<p>fig : matplotlib.figure.Figure     Figure object ax : matplotlib.axes.Axes     Polar axes with plot</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.HemisphereVisualizer2D.plot_grid_patches--examples","level":5,"title":"Examples","text":"<p>fig, ax = viz.plot_grid_patches( ...     data=vod_data, ...     title=\"VOD Distribution\", ...     cmap='plasma', ...     save_path=\"output.png\" ... )</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_2d.py</code> <pre><code>def plot_grid_patches(\n    self,\n    data: np.ndarray | None = None,\n    style: PolarPlotStyle | None = None,\n    ax: Axes | None = None,\n    save_path: Path | str | None = None,\n    **style_kwargs: Any,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Plot hemisphere grid as colored patches in polar projection.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell. If None, plots uniform grid.\n    style : PolarPlotStyle, optional\n        Styling configuration. If None, uses defaults.\n    ax : matplotlib.axes.Axes, optional\n        Existing polar axes to plot on. If None, creates new figure.\n    save_path : Path or str, optional\n        If provided, saves figure to this path\n    **style_kwargs\n        Override individual style parameters\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Figure object\n    ax : matplotlib.axes.Axes\n        Polar axes with plot\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = viz.plot_grid_patches(\n    ...     data=vod_data,\n    ...     title=\"VOD Distribution\",\n    ...     cmap='plasma',\n    ...     save_path=\"output.png\"\n    ... )\n\n    \"\"\"\n    # Initialize style\n    if style is None:\n        style = PolarPlotStyle(**style_kwargs)\n    else:\n        # Override style with kwargs\n        for key, value in style_kwargs.items():\n            if hasattr(style, key):\n                setattr(style, key, value)\n\n    # Create figure if needed\n    if ax is None:\n        fig, ax = plt.subplots(\n            figsize=style.figsize, dpi=style.dpi, subplot_kw={\"projection\": \"polar\"}\n        )\n    else:\n        fig = ax.figure\n\n    # Get patches for grid\n    patches, cell_indices = self._extract_grid_patches()\n\n    # Map data to patches\n    patch_data = self._map_data_to_patches(data, cell_indices)\n\n    # Determine color limits\n    vmin = style.vmin if style.vmin is not None else np.nanmin(patch_data)\n    vmax = style.vmax if style.vmax is not None else np.nanmax(patch_data)\n\n    # Create patch collection\n    pc = PatchCollection(\n        patches,\n        cmap=style.cmap,\n        edgecolor=style.edgecolor,\n        linewidth=style.linewidth,\n        alpha=style.alpha,\n    )\n    pc.set_array(np.ma.masked_invalid(patch_data))\n    pc.set_clim(vmin, vmax)\n\n    # Add to axes\n    ax.add_collection(pc)\n\n    # Style polar axes\n    self._apply_polar_styling(ax, style)\n\n    # Add colorbar\n    cbar = fig.colorbar(\n        pc,\n        ax=ax,\n        shrink=style.colorbar_shrink,\n        pad=style.colorbar_pad,\n    )\n    cbar.set_label(style.colorbar_label, fontsize=style.colorbar_fontsize)\n\n    # Set title\n    if style.title:\n        ax.set_title(style.title, y=1.08, fontsize=14)\n\n    # Save if requested\n    if save_path:\n        save_path = Path(save_path)\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        fig.savefig(\n            save_path,\n            dpi=style.dpi,\n            bbox_inches=\"tight\",\n            facecolor=\"white\",\n            edgecolor=\"none\",\n        )\n\n    return fig, ax\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.visualize_grid","level":2,"title":"<code>visualize_grid(grid, data=None, style=None, **kwargs)</code>","text":"<p>Visualize hemispherical grid in 2D polar projection.</p> <p>Convenience function providing simple interface to 2D visualization.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.visualize_grid--parameters","level":4,"title":"Parameters","text":"<p>grid : HemiGrid     Grid to visualize data : np.ndarray, optional     Data values per cell. If None, plots uniform grid. style : PolarPlotStyle, optional     Styling configuration. If None, uses defaults. **kwargs     Additional style parameter overrides</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.visualize_grid--returns","level":4,"title":"Returns","text":"<p>fig : matplotlib.figure.Figure     Figure object ax : matplotlib.axes.Axes     Polar axes object</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.visualize_grid--examples","level":4,"title":"Examples","text":"<p>from canvod.grids import create_hemigrid from canvod.viz import visualize_grid</p> <p>grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0) fig, ax = visualize_grid(grid, data=vod_data, cmap='viridis') plt.savefig(\"vod_plot.png\", dpi=300)</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_2d.py</code> <pre><code>def visualize_grid(\n    grid: HemiGrid,\n    data: np.ndarray | None = None,\n    style: PolarPlotStyle | None = None,\n    **kwargs: Any,\n) -&gt; tuple[Figure, Axes]:\n    \"\"\"Visualize hemispherical grid in 2D polar projection.\n\n    Convenience function providing simple interface to 2D visualization.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Grid to visualize\n    data : np.ndarray, optional\n        Data values per cell. If None, plots uniform grid.\n    style : PolarPlotStyle, optional\n        Styling configuration. If None, uses defaults.\n    **kwargs\n        Additional style parameter overrides\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Figure object\n    ax : matplotlib.axes.Axes\n        Polar axes object\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import create_hemigrid\n    &gt;&gt;&gt; from canvod.viz import visualize_grid\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\n    &gt;&gt;&gt; fig, ax = visualize_grid(grid, data=vod_data, cmap='viridis')\n    &gt;&gt;&gt; plt.savefig(\"vod_plot.png\", dpi=300)\n\n    \"\"\"\n    viz = HemisphereVisualizer2D(grid)\n    return viz.plot_grid_patches(data=data, style=style, **kwargs)\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.add_tissot_indicatrix","level":2,"title":"<code>add_tissot_indicatrix(ax, grid, radius_deg=None, n_sample=None, facecolor='gold', alpha=0.6, edgecolor='black', linewidth=0.5)</code>","text":"<p>Add Tissot's indicatrix circles to existing polar plot.</p> <p>Adds equal-sized circles to visualize grid distortion. In equal-area grids, circles should appear roughly equal-sized. Variation indicates distortion.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.add_tissot_indicatrix--parameters","level":4,"title":"Parameters","text":"<p>ax : matplotlib.axes.Axes     Existing polar axis to add circles to grid : HemiGrid     Grid instance radius_deg : float, optional     Angular radius of circles in degrees. If None, auto-calculated     as angular_resolution / 8. n_sample : int, optional     Subsample cells (use every nth cell) for performance.     If None, shows all cells. facecolor : str, default 'gold'     Fill color for circles alpha : float, default 0.6     Transparency (0=transparent, 1=opaque) edgecolor : str, default 'black'     Edge color for circles linewidth : float, default 0.5     Edge line width</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.add_tissot_indicatrix--returns","level":4,"title":"Returns","text":"<p>ax : matplotlib.axes.Axes     Modified axis with Tissot circles added</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_2d.add_tissot_indicatrix--examples","level":4,"title":"Examples","text":"<p>fig, ax = visualize_grid(grid, data=vod_data) add_tissot_indicatrix(ax, grid, radius_deg=3, n_sample=5) plt.savefig(\"vod_with_tissot.png\")</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_2d.py</code> <pre><code>def add_tissot_indicatrix(\n    ax: Axes,\n    grid: HemiGrid,\n    radius_deg: float | None = None,\n    n_sample: int | None = None,\n    facecolor: str = \"gold\",\n    alpha: float = 0.6,\n    edgecolor: str = \"black\",\n    linewidth: float = 0.5,\n) -&gt; Axes:\n    \"\"\"Add Tissot's indicatrix circles to existing polar plot.\n\n    Adds equal-sized circles to visualize grid distortion. In equal-area grids,\n    circles should appear roughly equal-sized. Variation indicates distortion.\n\n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes\n        Existing polar axis to add circles to\n    grid : HemiGrid\n        Grid instance\n    radius_deg : float, optional\n        Angular radius of circles in degrees. If None, auto-calculated\n        as angular_resolution / 8.\n    n_sample : int, optional\n        Subsample cells (use every nth cell) for performance.\n        If None, shows all cells.\n    facecolor : str, default 'gold'\n        Fill color for circles\n    alpha : float, default 0.6\n        Transparency (0=transparent, 1=opaque)\n    edgecolor : str, default 'black'\n        Edge color for circles\n    linewidth : float, default 0.5\n        Edge line width\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        Modified axis with Tissot circles added\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = visualize_grid(grid, data=vod_data)\n    &gt;&gt;&gt; add_tissot_indicatrix(ax, grid, radius_deg=3, n_sample=5)\n    &gt;&gt;&gt; plt.savefig(\"vod_with_tissot.png\")\n\n    \"\"\"\n    from matplotlib.patches import Ellipse\n\n    # Auto-calculate radius if not provided\n    if radius_deg is None:\n        if hasattr(grid, \"angular_resolution\"):\n            radius_deg = grid.angular_resolution / 8\n        else:\n            theta_vals = grid.grid[\"theta\"].to_numpy()\n            theta_spacing = np.median(np.diff(np.sort(np.unique(theta_vals))))\n            radius_deg = np.rad2deg(theta_spacing) / 8\n\n    radius_rad = np.deg2rad(radius_deg)\n\n    # Generate circle points on sphere\n    n_circle_points = 32\n    circle_angles = np.linspace(0, 2 * np.pi, n_circle_points, endpoint=False)\n\n    cell_count = 0\n    grid_df = grid.grid\n\n    # Different handling for triangular vs rectangular grids\n    if grid.grid_type in [\"htm\", \"geodesic\"]:\n        # For triangular grids: create circles on sphere surface and project\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            if n_sample is not None and i % n_sample != 0:\n                continue\n\n            phi_center = row[\"phi\"]\n            theta_center = row[\"theta\"]\n\n            if theta_center &gt; np.pi / 2:\n                continue\n\n            # Convert cell center to 3D Cartesian\n            x_c = np.sin(theta_center) * np.cos(phi_center)\n            y_c = np.sin(theta_center) * np.sin(phi_center)\n            z_c = np.cos(theta_center)\n            center_3d = np.array([x_c, y_c, z_c])\n\n            # Create tangent vectors\n            if theta_center &lt; 0.01:\n                tangent_1 = np.array([1, 0, 0])\n                tangent_2 = np.array([0, 1, 0])\n            else:\n                tangent_phi = np.array([-np.sin(phi_center), np.cos(phi_center), 0])\n                tangent_phi = tangent_phi / np.linalg.norm(tangent_phi)\n\n                tangent_theta = np.array(\n                    [\n                        np.cos(theta_center) * np.cos(phi_center),\n                        np.cos(theta_center) * np.sin(phi_center),\n                        -np.sin(theta_center),\n                    ]\n                )\n                tangent_theta = tangent_theta / np.linalg.norm(tangent_theta)\n\n                tangent_1 = tangent_phi\n                tangent_2 = tangent_theta\n\n            # Create circle on sphere surface\n            circle_3d = []\n            for angle in circle_angles:\n                offset = radius_rad * (\n                    np.cos(angle) * tangent_1 + np.sin(angle) * tangent_2\n                )\n                point_3d = center_3d + offset\n                norm = np.linalg.norm(point_3d)\n                if norm &gt; 1e-10:\n                    point_3d = point_3d / norm\n                circle_3d.append(point_3d)\n\n            circle_3d = np.array(circle_3d)\n\n            # Project to 2D polar coordinates\n            x_2d, y_2d, z_2d = circle_3d[:, 0], circle_3d[:, 1], circle_3d[:, 2]\n            theta_2d = np.arccos(np.clip(z_2d, -1, 1))\n            phi_2d = np.arctan2(y_2d, x_2d)\n\n            # Convert to polar plot coordinates (rho = sin(theta))\n            rho_2d = np.sin(theta_2d)\n            angle_2d = phi_2d\n\n            vertices_2d = np.column_stack([angle_2d, rho_2d])\n\n            poly = Polygon(\n                vertices_2d,\n                facecolor=facecolor,\n                alpha=alpha,\n                edgecolor=edgecolor,\n                linewidth=linewidth,\n            )\n            ax.add_patch(poly)\n            cell_count += 1\n\n    else:\n        # Rectangular grids: use simple ellipses at grid centers\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            if n_sample is not None and i % n_sample != 0:\n                continue\n\n            phi_center = row[\"phi\"]\n            theta_center = row[\"theta\"]\n\n            if theta_center &lt;= np.pi / 2:\n                # Convert to polar plot coordinates\n                rho_center = np.sin(theta_center)\n\n                ell = Ellipse(\n                    (phi_center, rho_center),\n                    width=2 * radius_rad,\n                    height=2 * radius_rad * np.sin(theta_center),  # Scale by projection\n                    facecolor=facecolor,\n                    alpha=alpha,\n                    edgecolor=edgecolor,\n                    linewidth=linewidth,\n                )\n                ax.add_patch(ell)\n                cell_count += 1\n\n    # Update title\n    current_title = ax.get_title()\n    if current_title:\n        ax.set_title(f\"{current_title} + Tissot ({cell_count} circles)\")\n    else:\n        ax.set_title(f\"Tissot's Indicatrix - {grid.grid_type} ({cell_count} circles)\")\n\n    return ax\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#3d-visualization","level":2,"title":"3D Visualization","text":"<p>3D hemisphere visualization using plotly for interactive exploration.</p> <p>Provides interactive 3D sphere surface plots with zoom, pan, and rotation capabilities.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D","level":2,"title":"<code>HemisphereVisualizer3D</code>","text":"<p>3D hemisphere visualization using plotly.</p> <p>Creates interactive 3D plots with rotation, zoom, and hover capabilities. Designed for exploratory data analysis and presentations.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D--parameters","level":4,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D--examples","level":4,"title":"Examples","text":"<p>from canvod.grids import create_hemigrid from canvod.viz import HemisphereVisualizer3D</p> <p>grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0) viz = HemisphereVisualizer3D(grid) fig = viz.plot_hemisphere_surface(data=vod_data, title=\"Interactive VOD\") fig.show()</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>class HemisphereVisualizer3D:\n    \"\"\"3D hemisphere visualization using plotly.\n\n    Creates interactive 3D plots with rotation, zoom, and hover capabilities.\n    Designed for exploratory data analysis and presentations.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import create_hemigrid\n    &gt;&gt;&gt; from canvod.viz import HemisphereVisualizer3D\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\n    &gt;&gt;&gt; viz = HemisphereVisualizer3D(grid)\n    &gt;&gt;&gt; fig = viz.plot_hemisphere_surface(data=vod_data, title=\"Interactive VOD\")\n    &gt;&gt;&gt; fig.show()\n\n    \"\"\"\n\n    def __init__(self, grid: HemiGrid) -&gt; None:\n        \"\"\"Initialize 3D hemisphere visualizer.\n\n        Parameters\n        ----------\n        grid : HemiGrid\n            Hemisphere grid to visualize\n\n        \"\"\"\n        self.grid = grid\n\n    def plot_hemisphere_surface(\n        self,\n        data: np.ndarray | None = None,\n        style: PlotStyle | None = None,\n        title: str | None = None,\n        colorscale: str = \"Viridis\",\n        opacity: float = 0.8,\n        show_wireframe: bool = True,\n        show_colorbar: bool = True,\n        width: int = 800,\n        height: int = 600,\n        **kwargs: Any,\n    ) -&gt; go.Figure:\n        \"\"\"Create 3D surface plot on hemisphere with actual cell patches.\n\n        Renders grid cells as colored 3D patches (not just points).\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell. If None, shows grid structure.\n        style : PlotStyle, optional\n            Styling configuration. If None, uses defaults.\n        title : str, optional\n            Plot title\n        colorscale : str, default 'Viridis'\n            Plotly colorscale name\n        opacity : float, default 0.8\n            Surface opacity (0=transparent, 1=opaque)\n        show_wireframe : bool, default True\n            Show grid lines on surface\n        show_colorbar : bool, default True\n            Display colorbar\n        width : int, default 800\n            Figure width in pixels\n        height : int, default 600\n            Figure height in pixels\n        **kwargs\n            Additional plotly trace parameters\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive 3D figure with cell patches\n\n        Examples\n        --------\n        &gt;&gt;&gt; fig = viz.plot_hemisphere_surface(\n        ...     data=vod_data,\n        ...     title=\"VOD Distribution 3D\",\n        ...     colorscale='Plasma',\n        ...     opacity=0.9\n        ... )\n        &gt;&gt;&gt; fig.write_html(\"vod_3d.html\")\n\n        \"\"\"\n        # Initialize style\n        if style is None:\n            style = PlotStyle()\n            colorscale = colorscale  # Use parameter\n        else:\n            colorscale = style.colorscale\n\n        # Render grid based on type\n        grid_type = self.grid.grid_type.lower()\n\n        if grid_type in [\"equal_area\", \"equal_angle\", \"equirectangular\"]:\n            trace = self._render_rectangular_cells(\n                data,\n                colorscale,\n                opacity,\n                show_colorbar,\n            )\n        elif grid_type == \"htm\":\n            trace = self._render_htm_cells(data, colorscale, opacity, show_colorbar)\n        elif grid_type == \"geodesic\":\n            trace = self._render_geodesic_cells(\n                data, colorscale, opacity, show_colorbar\n            )\n        elif grid_type == \"healpix\":\n            trace = self._render_healpix_cells(data, colorscale, opacity, show_colorbar)\n        elif grid_type == \"fibonacci\":\n            trace = self._render_fibonacci_cells(\n                data, colorscale, opacity, show_colorbar\n            )\n        else:\n            # Fallback to scatter for unknown types\n            trace = self._render_scatter_fallback(\n                data,\n                colorscale,\n                opacity,\n                show_colorbar,\n            )\n\n        fig = go.Figure(data=[trace])\n\n        # Apply layout\n        layout_config = style.to_plotly_layout() if style else {}\n        layout_config.update(\n            {\n                \"title\": title or \"Hemisphere 3D\",\n                \"scene\": dict(\n                    aspectmode=\"data\",\n                    xaxis=dict(title=\"East\", showbackground=False),\n                    yaxis=dict(title=\"North\", showbackground=False),\n                    zaxis=dict(title=\"Up\", showbackground=False),\n                    bgcolor=layout_config.get(\"plot_bgcolor\", \"white\"),\n                ),\n                \"width\": width,\n                \"height\": height,\n                \"margin\": dict(l=0, r=0, b=0, t=40),\n            }\n        )\n\n        fig.update_layout(**layout_config)\n\n        return fig\n\n    def _render_rectangular_cells(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Mesh3d:\n        \"\"\"Render rectangular grid cells as 3D mesh patches.\"\"\"\n        grid_df = self.grid.grid\n\n        all_x, all_y, all_z = [], [], []\n        all_i, all_j, all_k = [], [], []\n        all_colors = []\n\n        vertex_count = 0\n\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                phi_min, phi_max = row[\"phi_min\"], row[\"phi_max\"]\n                theta_min, theta_max = row[\"theta_min\"], row[\"theta_max\"]\n\n                # Skip if beyond hemisphere\n                if theta_min &gt; np.pi / 2:\n                    continue\n\n                # Create 4 corners of rectangular cell\n                phi_corners = [phi_min, phi_max, phi_max, phi_min]\n                theta_corners = [theta_min, theta_min, theta_max, theta_max]\n\n                patch_x, patch_y, patch_z = [], [], []\n                for phi, theta in zip(phi_corners, theta_corners):\n                    # Convert to 3D Cartesian\n                    x = np.sin(theta) * np.sin(phi)\n                    y = np.sin(theta) * np.cos(phi)\n                    z = np.cos(theta)\n                    patch_x.append(x)\n                    patch_y.append(y)\n                    patch_z.append(z)\n\n                all_x.extend(patch_x)\n                all_y.extend(patch_y)\n                all_z.extend(patch_z)\n\n                # Color value for this cell\n                color_val = data[i] if data is not None else 0.5\n                all_colors.extend([color_val] * 4)\n\n                # Two triangles per rectangle\n                all_i.extend([vertex_count, vertex_count])\n                all_j.extend([vertex_count + 1, vertex_count + 2])\n                all_k.extend([vertex_count + 2, vertex_count + 3])\n\n                vertex_count += 4\n\n            except (KeyError, IndexError):\n                continue\n\n        return go.Mesh3d(\n            x=all_x,\n            y=all_y,\n            z=all_z,\n            i=all_i,\n            j=all_j,\n            k=all_k,\n            intensity=all_colors,\n            colorscale=colorscale,\n            showscale=show_colorbar,\n            colorbar=dict(title=\"Value\") if show_colorbar else None,\n            opacity=opacity,\n            flatshading=True,\n            name=f\"{self.grid.grid_type.title()} Grid\",\n        )\n\n    def _render_htm_cells(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Mesh3d:\n        \"\"\"Render HTM triangular cells as 3D mesh.\"\"\"\n        grid_df = self.grid.grid\n\n        all_x, all_y, all_z = [], [], []\n        all_i, all_j, all_k = [], [], []\n        all_colors = []\n\n        vertex_count = 0\n\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                v0 = np.array(row[\"htm_vertex_0\"], dtype=float)\n                v1 = np.array(row[\"htm_vertex_1\"], dtype=float)\n                v2 = np.array(row[\"htm_vertex_2\"], dtype=float)\n\n                # Skip if beyond hemisphere\n                if np.all([v[2] &lt; 0 for v in [v0, v1, v2]]):\n                    continue\n\n                all_x.extend([v0[1], v1[1], v2[1]])\n                all_y.extend([v0[0], v1[0], v2[0]])\n                all_z.extend([v0[2], v1[2], v2[2]])\n\n                color_val = data[i] if data is not None else 0.5\n                all_colors.extend([color_val] * 3)\n\n                # One triangle per cell\n                all_i.append(vertex_count)\n                all_j.append(vertex_count + 1)\n                all_k.append(vertex_count + 2)\n\n                vertex_count += 3\n\n            except (KeyError, TypeError, ValueError):\n                continue\n\n        return go.Mesh3d(\n            x=all_x,\n            y=all_y,\n            z=all_z,\n            i=all_i,\n            j=all_j,\n            k=all_k,\n            intensity=all_colors,\n            colorscale=colorscale,\n            showscale=show_colorbar,\n            colorbar=dict(title=\"Value\") if show_colorbar else None,\n            opacity=opacity,\n            flatshading=True,\n            name=f\"{self.grid.grid_type.title()} Grid\",\n        )\n\n    def _render_geodesic_cells(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Mesh3d:\n        \"\"\"Render geodesic triangular cells as 3D mesh.\n\n        Reads ``geodesic_vertices`` (3 vertex indices per cell) and looks up\n        3D Cartesian coordinates from the shared ``grid.vertices`` array.\n        \"\"\"\n        grid_df = self.grid.grid\n        shared_vertices = self.grid.vertices\n\n        if shared_vertices is None or \"geodesic_vertices\" not in grid_df.columns:\n            return self._render_scatter_fallback(\n                data, colorscale, opacity, show_colorbar\n            )\n\n        all_x, all_y, all_z = [], [], []\n        all_i, all_j, all_k = [], [], []\n        all_colors = []\n        vertex_count = 0\n\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                v_indices = np.array(row[\"geodesic_vertices\"], dtype=int)\n                if len(v_indices) &lt; 3:\n                    continue\n\n                verts = shared_vertices[v_indices]  # (3, 3)\n\n                if np.all(verts[:, 2] &lt; 0):\n                    continue\n\n                all_x.extend(verts[:, 1].tolist())\n                all_y.extend(verts[:, 0].tolist())\n                all_z.extend(verts[:, 2].tolist())\n\n                color_val = data[i] if data is not None else 0.5\n                all_colors.extend([color_val] * 3)\n\n                all_i.append(vertex_count)\n                all_j.append(vertex_count + 1)\n                all_k.append(vertex_count + 2)\n                vertex_count += 3\n\n            except (KeyError, IndexError, TypeError, ValueError):\n                continue\n\n        return go.Mesh3d(\n            x=all_x,\n            y=all_y,\n            z=all_z,\n            i=all_i,\n            j=all_j,\n            k=all_k,\n            intensity=all_colors,\n            colorscale=colorscale,\n            showscale=show_colorbar,\n            colorbar=dict(title=\"Value\") if show_colorbar else None,\n            opacity=opacity,\n            flatshading=True,\n            name=\"Geodesic Grid\",\n        )\n\n    def _render_healpix_cells(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Mesh3d:\n        \"\"\"Render HEALPix curvilinear cells as 3D mesh.\n\n        Uses ``healpy.boundaries()`` to obtain true pixel boundaries,\n        then fan-triangulates each quadrilateral pixel.\n        \"\"\"\n        try:\n            import healpy as hp\n        except ImportError:\n            return self._render_scatter_fallback(\n                data, colorscale, opacity, show_colorbar\n            )\n\n        grid_df = self.grid.grid\n        if \"healpix_ipix\" not in grid_df.columns:\n            return self._render_scatter_fallback(\n                data, colorscale, opacity, show_colorbar\n            )\n\n        nside = int(grid_df[\"healpix_nside\"][0])\n        step = 4  # 4 sub-points per edge → 16 boundary vertices\n\n        all_x, all_y, all_z = [], [], []\n        all_i, all_j, all_k = [], [], []\n        all_colors = []\n        vertex_count = 0\n\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                ipix = int(row[\"healpix_ipix\"])\n                boundary = hp.boundaries(nside, ipix, step=step)\n                x, y, z = boundary[1], boundary[0], boundary[2]\n\n                if np.all(z &lt; -0.01):\n                    continue\n\n                n_verts = len(x)\n                all_x.extend(x.tolist())\n                all_y.extend(y.tolist())\n                all_z.extend(z.tolist())\n\n                color_val = data[i] if data is not None else 0.5\n                all_colors.extend([color_val] * n_verts)\n\n                # Fan triangulation from first vertex\n                for j in range(1, n_verts - 1):\n                    all_i.append(vertex_count)\n                    all_j.append(vertex_count + j)\n                    all_k.append(vertex_count + j + 1)\n\n                vertex_count += n_verts\n\n            except (KeyError, IndexError, TypeError, ValueError):\n                continue\n\n        return go.Mesh3d(\n            x=all_x,\n            y=all_y,\n            z=all_z,\n            i=all_i,\n            j=all_j,\n            k=all_k,\n            intensity=all_colors,\n            colorscale=colorscale,\n            showscale=show_colorbar,\n            colorbar=dict(title=\"Value\") if show_colorbar else None,\n            opacity=opacity,\n            flatshading=True,\n            name=\"HEALPix Grid\",\n        )\n\n    def _render_fibonacci_cells(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Mesh3d:\n        \"\"\"Render Fibonacci Voronoi cells as 3D mesh.\n\n        Reads ``voronoi_region`` (variable-length vertex index list) and\n        looks up 3D coordinates from ``grid.voronoi.vertices``.  The\n        vertex indices are already in correct polygon winding order\n        (``SphericalVoronoi.sort_vertices_of_regions()`` was called\n        during grid construction), so no re-sorting is needed.\n        Fan-triangulates each polygon for ``go.Mesh3d``.\n        \"\"\"\n        grid_df = self.grid.grid\n        voronoi = self.grid.voronoi\n\n        if voronoi is None or \"voronoi_region\" not in grid_df.columns:\n            return self._render_scatter_fallback(\n                data, colorscale, opacity, show_colorbar\n            )\n\n        all_x, all_y, all_z = [], [], []\n        all_i, all_j, all_k = [], [], []\n        all_colors = []\n        vertex_count = 0\n\n        for i, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                region_indices = row[\"voronoi_region\"]\n                if region_indices is None or len(region_indices) &lt; 3:\n                    continue\n\n                verts = voronoi.vertices[region_indices]\n\n                if np.all(verts[:, 2] &lt; -0.01):\n                    continue\n\n                # Vertices are already in polygon winding order from\n                # sort_vertices_of_regions() — use directly.\n                n_verts = len(verts)\n\n                all_x.extend(verts[:, 1].tolist())\n                all_y.extend(verts[:, 0].tolist())\n                all_z.extend(verts[:, 2].tolist())\n\n                color_val = data[i] if data is not None else 0.5\n                all_colors.extend([color_val] * n_verts)\n\n                # Fan triangulation from first vertex\n                for j in range(1, n_verts - 1):\n                    all_i.append(vertex_count)\n                    all_j.append(vertex_count + j)\n                    all_k.append(vertex_count + j + 1)\n\n                vertex_count += n_verts\n\n            except (KeyError, IndexError, TypeError, ValueError):\n                continue\n\n        return go.Mesh3d(\n            x=all_x,\n            y=all_y,\n            z=all_z,\n            i=all_i,\n            j=all_j,\n            k=all_k,\n            intensity=all_colors,\n            colorscale=colorscale,\n            showscale=show_colorbar,\n            colorbar=dict(title=\"Value\") if show_colorbar else None,\n            opacity=opacity,\n            flatshading=True,\n            name=\"Fibonacci Grid\",\n        )\n\n    def _render_scatter_fallback(\n        self,\n        data: np.ndarray | None,\n        colorscale: str,\n        opacity: float,\n        show_colorbar: bool,\n    ) -&gt; go.Scatter3d:\n        \"\"\"Fallback scatter plot for unsupported grid types.\"\"\"\n        grid_df = self.grid.grid\n        theta = grid_df[\"theta\"].to_numpy()\n        phi = grid_df[\"phi\"].to_numpy()\n\n        # Convert to 3D Cartesian coordinates\n        x = np.sin(theta) * np.sin(phi)\n        y = np.sin(theta) * np.cos(phi)\n        z = np.cos(theta)\n\n        # Prepare data values\n        if data is None:\n            values = np.ones(self.grid.ncells) * 0.5\n        else:\n            values = data\n\n        # Filter hemisphere only\n        hemisphere_mask = theta &lt;= np.pi / 2\n        x = x[hemisphere_mask]\n        y = y[hemisphere_mask]\n        z = z[hemisphere_mask]\n        values = values[hemisphere_mask]\n\n        return go.Scatter3d(\n            x=x,\n            y=y,\n            z=z,\n            mode=\"markers\",\n            marker=dict(\n                size=6,\n                color=values,\n                colorscale=colorscale,\n                opacity=opacity,\n                colorbar=dict(title=\"Value\") if show_colorbar else None,\n                cmin=np.nanmin(values),\n                cmax=np.nanmax(values),\n            ),\n            text=[f\"Cell {i}&lt;br&gt;Value: {v:.3f}\" for i, v in enumerate(values)],\n            hoverinfo=\"text\",\n        )\n\n    def plot_hemisphere_scatter(\n        self,\n        data: np.ndarray | None = None,\n        title: str | None = None,\n        colorscale: str = \"Viridis\",\n        marker_size: int | np.ndarray = 6,\n        opacity: float = 0.8,\n        width: int = 800,\n        height: int = 600,\n    ) -&gt; go.Figure:\n        \"\"\"Create 3D scatter plot of cell centers.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, optional\n            Plot title\n        colorscale : str, default 'Viridis'\n            Plotly colorscale name\n        marker_size : int or np.ndarray, default 6\n            Marker size (constant or per-point array)\n        opacity : float, default 0.8\n            Marker opacity\n        width : int, default 800\n            Figure width\n        height : int, default 600\n            Figure height\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive scatter plot\n\n        \"\"\"\n        # Note: Now renders as mesh, not scatter points\n        # Marker size parameter is ignored\n        fig = self.plot_hemisphere_surface(\n            data=data,\n            title=title,\n            colorscale=colorscale,\n            opacity=opacity,\n            width=width,\n            height=height,\n        )\n\n        return fig\n\n    def plot_cell_mesh(\n        self,\n        data: np.ndarray | None = None,\n        title: str | None = None,\n        colorscale: str = \"Viridis\",\n        opacity: float = 0.7,\n        show_edges: bool = True,\n        width: int = 800,\n        height: int = 600,\n    ) -&gt; go.Figure:\n        \"\"\"Create 3D mesh plot showing cell boundaries.\n\n        Parameters\n        ----------\n        data : np.ndarray, optional\n            Data values per cell\n        title : str, optional\n            Plot title\n        colorscale : str, default 'Viridis'\n            Plotly colorscale name\n        opacity : float, default 0.7\n            Mesh opacity\n        show_edges : bool, default True\n            Show cell edges\n        width : int, default 800\n            Figure width\n        height : int, default 600\n            Figure height\n\n        Returns\n        -------\n        plotly.graph_objects.Figure\n            Interactive mesh plot\n\n        Notes\n        -----\n        This method requires grid cells with vertex information.\n        Currently supports HTM and geodesic grids.\n\n        \"\"\"\n        traces = []\n\n        # Prepare data\n        if data is None:\n            values = np.ones(self.grid.ncells) * 0.5\n        else:\n            values = data\n\n        grid_df = self.grid.grid\n        grid_type = self.grid.grid_type.lower()\n\n        # Check if grid supports mesh rendering\n        if grid_type == \"htm\" and \"htm_vertex_0\" in grid_df.columns:\n            # HTM triangular mesh\n            for idx, row in enumerate(grid_df.iter_rows(named=True)):\n                try:\n                    v0 = np.array(row[\"htm_vertex_0\"], dtype=float)\n                    v1 = np.array(row[\"htm_vertex_1\"], dtype=float)\n                    v2 = np.array(row[\"htm_vertex_2\"], dtype=float)\n\n                    vertices = np.array([v0, v1, v2])\n\n                    # Check hemisphere\n                    z_coords = vertices[:, 2]\n                    if np.all(z_coords &lt; 0):\n                        continue\n\n                    # Normalize color value\n                    color_val = (values[idx] - np.nanmin(values)) / (\n                        np.nanmax(values) - np.nanmin(values)\n                    )\n                    color_rgb = sample_colorscale(colorscale, [color_val])[0]\n\n                    # Create triangle mesh\n                    trace = go.Mesh3d(\n                        x=vertices[:, 0],\n                        y=vertices[:, 1],\n                        z=vertices[:, 2],\n                        i=[0],\n                        j=[1],\n                        k=[2],\n                        color=color_rgb,\n                        opacity=opacity,\n                        flatshading=True,\n                        showscale=False,\n                        hoverinfo=\"skip\",\n                    )\n                    traces.append(trace)\n                except (KeyError, TypeError, ValueError):\n                    continue\n        else:\n            # Not supported for this grid type\n            raise NotImplementedError(\n                f\"Cell mesh rendering not implemented for {grid_type} grids\"\n            )\n\n        # Sample colorscale (no longer needed above, but kept for compatibility)\n\n        # Add colorbar trace\n        if values is not None and len(traces) &gt; 0:\n            dummy_trace = go.Scatter3d(\n                x=[None],\n                y=[None],\n                z=[None],\n                mode=\"markers\",\n                marker=dict(\n                    size=0.1,\n                    color=[np.nanmin(values), np.nanmax(values)],\n                    colorscale=colorscale,\n                    colorbar=dict(title=\"Value\"),\n                ),\n                showlegend=False,\n                hoverinfo=\"skip\",\n            )\n            traces.append(dummy_trace)\n\n        fig = go.Figure(data=traces)\n\n        # Update layout\n        fig.update_layout(\n            title=title or \"Hemisphere Mesh 3D\",\n            scene=dict(\n                aspectmode=\"data\",\n                xaxis=dict(title=\"East\", showbackground=False),\n                yaxis=dict(title=\"North\", showbackground=False),\n                zaxis=dict(title=\"Up\", showbackground=False),\n            ),\n            width=width,\n            height=height,\n            margin=dict(l=0, r=0, b=0, t=40),\n        )\n\n        return fig\n\n    def add_spherical_overlays(\n        self,\n        fig: go.Figure,\n        elevation_rings: list[int] | None = None,\n        meridians_deg: list[int] | None = None,\n        overlay_color: str = \"lightgray\",\n        line_width: float = 1,\n    ) -&gt; go.Figure:\n        \"\"\"Add elevation rings and meridians to 3D plot.\n\n        Parameters\n        ----------\n        fig : plotly.graph_objects.Figure\n            Existing 3D figure to add overlays to\n        elevation_rings : list of int, optional\n            Elevation angles in degrees. Default: [15, 30, 45, 60, 75, 90]\n        meridians_deg : list of int, optional\n            Meridian angles in degrees. Default: [0, 45, 90, 135, 180, 225, 270, 315]\n        overlay_color : str, default 'lightgray'\n            Color for overlay lines\n        line_width : float, default 1\n            Width of overlay lines\n\n        Returns\n        -------\n        fig : plotly.graph_objects.Figure\n            Modified figure with overlays\n\n        \"\"\"\n        if elevation_rings is None:\n            elevation_rings = [15, 30, 45, 60, 75, 90]\n        if meridians_deg is None:\n            meridians_deg = list(range(0, 360, 45))\n\n        # Elevation rings\n        for theta_deg in elevation_rings:\n            theta = np.radians(theta_deg)\n            phi = np.linspace(0, 2 * np.pi, 200)\n            x = np.sin(theta) * np.sin(phi)\n            y = np.sin(theta) * np.cos(phi)\n            z = np.full_like(phi, np.cos(theta))\n            fig.add_trace(\n                go.Scatter3d(\n                    x=x,\n                    y=y,\n                    z=z,\n                    mode=\"lines\",\n                    line=dict(color=overlay_color, width=line_width),\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                )\n            )\n\n        # Meridians\n        for phi_deg in meridians_deg:\n            phi = np.radians(phi_deg)\n            theta = np.linspace(0, np.pi / 2, 100)\n            x = np.sin(theta) * np.sin(phi)\n            y = np.sin(theta) * np.cos(phi)\n            z = np.cos(theta)\n            fig.add_trace(\n                go.Scatter3d(\n                    x=x,\n                    y=y,\n                    z=z,\n                    mode=\"lines\",\n                    line=dict(color=overlay_color, width=line_width),\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                )\n            )\n\n        return fig\n\n    def add_custom_axes(\n        self,\n        fig: go.Figure,\n        axis_length: float = 1.2,\n        axis_color: str = \"black\",\n        show_labels: bool = True,\n    ) -&gt; go.Figure:\n        \"\"\"Add custom coordinate axes with labels.\n\n        Parameters\n        ----------\n        fig : plotly.graph_objects.Figure\n            Existing 3D figure\n        axis_length : float, default 1.2\n            Length of axis lines\n        axis_color : str, default 'black'\n            Color for axes\n        show_labels : bool, default True\n            Show axis labels (E, N, Z)\n\n        Returns\n        -------\n        fig : plotly.graph_objects.Figure\n            Modified figure with custom axes\n\n        \"\"\"\n        # Axis lines\n        axes_lines = [\n            dict(x=[0, axis_length], y=[0, 0], z=[0, 0]),  # East\n            dict(x=[0, 0], y=[0, axis_length], z=[0, 0]),  # North\n            dict(x=[0, 0], y=[0, 0], z=[0, axis_length]),  # Up\n        ]\n\n        for axis in axes_lines:\n            fig.add_trace(\n                go.Scatter3d(\n                    x=axis[\"x\"],\n                    y=axis[\"y\"],\n                    z=axis[\"z\"],\n                    mode=\"lines\",\n                    line=dict(color=axis_color, width=6),\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                )\n            )\n\n        # Arrowheads\n        arrow_tip = axis_length + 0.05\n        arrow_size = 0.1\n        for pos, direction in zip(\n            [[arrow_tip, 0, 0], [0, arrow_tip, 0], [0, 0, arrow_tip]],\n            [[arrow_size, 0, 0], [0, arrow_size, 0], [0, 0, arrow_size]],\n        ):\n            fig.add_trace(\n                go.Cone(\n                    x=[pos[0]],\n                    y=[pos[1]],\n                    z=[pos[2]],\n                    u=[direction[0]],\n                    v=[direction[1]],\n                    w=[direction[2]],\n                    sizemode=\"absolute\",\n                    sizeref=arrow_size,\n                    anchor=\"tip\",\n                    showscale=False,\n                    colorscale=[[0, axis_color], [1, axis_color]],\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                )\n            )\n\n        # Labels\n        if show_labels:\n            label_offset = axis_length + 0.15\n            for label in [\n                dict(x=label_offset, y=0, z=0, text=\"E\"),\n                dict(x=0, y=label_offset, z=0, text=\"N\"),\n                dict(x=0, y=0, z=label_offset, text=\"Z\"),\n            ]:\n                fig.add_trace(\n                    go.Scatter3d(\n                        x=[label[\"x\"]],\n                        y=[label[\"y\"]],\n                        z=[label[\"z\"]],\n                        mode=\"text\",\n                        text=[label[\"text\"]],\n                        textfont=dict(size=16, color=axis_color),\n                        hoverinfo=\"skip\",\n                        showlegend=False,\n                    )\n                )\n\n        return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.__init__","level":3,"title":"<code>__init__(grid)</code>","text":"<p>Initialize 3D hemisphere visualizer.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.__init__--parameters","level":5,"title":"Parameters","text":"<p>grid : HemiGrid     Hemisphere grid to visualize</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def __init__(self, grid: HemiGrid) -&gt; None:\n    \"\"\"Initialize 3D hemisphere visualizer.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Hemisphere grid to visualize\n\n    \"\"\"\n    self.grid = grid\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.plot_hemisphere_surface","level":3,"title":"<code>plot_hemisphere_surface(data=None, style=None, title=None, colorscale='Viridis', opacity=0.8, show_wireframe=True, show_colorbar=True, width=800, height=600, **kwargs)</code>","text":"<p>Create 3D surface plot on hemisphere with actual cell patches.</p> <p>Renders grid cells as colored 3D patches (not just points).</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.plot_hemisphere_surface--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell. If None, shows grid structure. style : PlotStyle, optional     Styling configuration. If None, uses defaults. title : str, optional     Plot title colorscale : str, default 'Viridis'     Plotly colorscale name opacity : float, default 0.8     Surface opacity (0=transparent, 1=opaque) show_wireframe : bool, default True     Show grid lines on surface show_colorbar : bool, default True     Display colorbar width : int, default 800     Figure width in pixels height : int, default 600     Figure height in pixels **kwargs     Additional plotly trace parameters</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.plot_hemisphere_surface--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive 3D figure with cell patches</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.plot_hemisphere_surface--examples","level":5,"title":"Examples","text":"<p>fig = viz.plot_hemisphere_surface( ...     data=vod_data, ...     title=\"VOD Distribution 3D\", ...     colorscale='Plasma', ...     opacity=0.9 ... ) fig.write_html(\"vod_3d.html\")</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def plot_hemisphere_surface(\n    self,\n    data: np.ndarray | None = None,\n    style: PlotStyle | None = None,\n    title: str | None = None,\n    colorscale: str = \"Viridis\",\n    opacity: float = 0.8,\n    show_wireframe: bool = True,\n    show_colorbar: bool = True,\n    width: int = 800,\n    height: int = 600,\n    **kwargs: Any,\n) -&gt; go.Figure:\n    \"\"\"Create 3D surface plot on hemisphere with actual cell patches.\n\n    Renders grid cells as colored 3D patches (not just points).\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell. If None, shows grid structure.\n    style : PlotStyle, optional\n        Styling configuration. If None, uses defaults.\n    title : str, optional\n        Plot title\n    colorscale : str, default 'Viridis'\n        Plotly colorscale name\n    opacity : float, default 0.8\n        Surface opacity (0=transparent, 1=opaque)\n    show_wireframe : bool, default True\n        Show grid lines on surface\n    show_colorbar : bool, default True\n        Display colorbar\n    width : int, default 800\n        Figure width in pixels\n    height : int, default 600\n        Figure height in pixels\n    **kwargs\n        Additional plotly trace parameters\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive 3D figure with cell patches\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = viz.plot_hemisphere_surface(\n    ...     data=vod_data,\n    ...     title=\"VOD Distribution 3D\",\n    ...     colorscale='Plasma',\n    ...     opacity=0.9\n    ... )\n    &gt;&gt;&gt; fig.write_html(\"vod_3d.html\")\n\n    \"\"\"\n    # Initialize style\n    if style is None:\n        style = PlotStyle()\n        colorscale = colorscale  # Use parameter\n    else:\n        colorscale = style.colorscale\n\n    # Render grid based on type\n    grid_type = self.grid.grid_type.lower()\n\n    if grid_type in [\"equal_area\", \"equal_angle\", \"equirectangular\"]:\n        trace = self._render_rectangular_cells(\n            data,\n            colorscale,\n            opacity,\n            show_colorbar,\n        )\n    elif grid_type == \"htm\":\n        trace = self._render_htm_cells(data, colorscale, opacity, show_colorbar)\n    elif grid_type == \"geodesic\":\n        trace = self._render_geodesic_cells(\n            data, colorscale, opacity, show_colorbar\n        )\n    elif grid_type == \"healpix\":\n        trace = self._render_healpix_cells(data, colorscale, opacity, show_colorbar)\n    elif grid_type == \"fibonacci\":\n        trace = self._render_fibonacci_cells(\n            data, colorscale, opacity, show_colorbar\n        )\n    else:\n        # Fallback to scatter for unknown types\n        trace = self._render_scatter_fallback(\n            data,\n            colorscale,\n            opacity,\n            show_colorbar,\n        )\n\n    fig = go.Figure(data=[trace])\n\n    # Apply layout\n    layout_config = style.to_plotly_layout() if style else {}\n    layout_config.update(\n        {\n            \"title\": title or \"Hemisphere 3D\",\n            \"scene\": dict(\n                aspectmode=\"data\",\n                xaxis=dict(title=\"East\", showbackground=False),\n                yaxis=dict(title=\"North\", showbackground=False),\n                zaxis=dict(title=\"Up\", showbackground=False),\n                bgcolor=layout_config.get(\"plot_bgcolor\", \"white\"),\n            ),\n            \"width\": width,\n            \"height\": height,\n            \"margin\": dict(l=0, r=0, b=0, t=40),\n        }\n    )\n\n    fig.update_layout(**layout_config)\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.plot_hemisphere_scatter","level":3,"title":"<code>plot_hemisphere_scatter(data=None, title=None, colorscale='Viridis', marker_size=6, opacity=0.8, width=800, height=600)</code>","text":"<p>Create 3D scatter plot of cell centers.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.plot_hemisphere_scatter--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, optional     Plot title colorscale : str, default 'Viridis'     Plotly colorscale name marker_size : int or np.ndarray, default 6     Marker size (constant or per-point array) opacity : float, default 0.8     Marker opacity width : int, default 800     Figure width height : int, default 600     Figure height</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.plot_hemisphere_scatter--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive scatter plot</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def plot_hemisphere_scatter(\n    self,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    colorscale: str = \"Viridis\",\n    marker_size: int | np.ndarray = 6,\n    opacity: float = 0.8,\n    width: int = 800,\n    height: int = 600,\n) -&gt; go.Figure:\n    \"\"\"Create 3D scatter plot of cell centers.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    colorscale : str, default 'Viridis'\n        Plotly colorscale name\n    marker_size : int or np.ndarray, default 6\n        Marker size (constant or per-point array)\n    opacity : float, default 0.8\n        Marker opacity\n    width : int, default 800\n        Figure width\n    height : int, default 600\n        Figure height\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive scatter plot\n\n    \"\"\"\n    # Note: Now renders as mesh, not scatter points\n    # Marker size parameter is ignored\n    fig = self.plot_hemisphere_surface(\n        data=data,\n        title=title,\n        colorscale=colorscale,\n        opacity=opacity,\n        width=width,\n        height=height,\n    )\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.plot_cell_mesh","level":3,"title":"<code>plot_cell_mesh(data=None, title=None, colorscale='Viridis', opacity=0.7, show_edges=True, width=800, height=600)</code>","text":"<p>Create 3D mesh plot showing cell boundaries.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.plot_cell_mesh--parameters","level":5,"title":"Parameters","text":"<p>data : np.ndarray, optional     Data values per cell title : str, optional     Plot title colorscale : str, default 'Viridis'     Plotly colorscale name opacity : float, default 0.7     Mesh opacity show_edges : bool, default True     Show cell edges width : int, default 800     Figure width height : int, default 600     Figure height</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.plot_cell_mesh--returns","level":5,"title":"Returns","text":"<p>plotly.graph_objects.Figure     Interactive mesh plot</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.plot_cell_mesh--notes","level":5,"title":"Notes","text":"<p>This method requires grid cells with vertex information. Currently supports HTM and geodesic grids.</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def plot_cell_mesh(\n    self,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    colorscale: str = \"Viridis\",\n    opacity: float = 0.7,\n    show_edges: bool = True,\n    width: int = 800,\n    height: int = 600,\n) -&gt; go.Figure:\n    \"\"\"Create 3D mesh plot showing cell boundaries.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    colorscale : str, default 'Viridis'\n        Plotly colorscale name\n    opacity : float, default 0.7\n        Mesh opacity\n    show_edges : bool, default True\n        Show cell edges\n    width : int, default 800\n        Figure width\n    height : int, default 600\n        Figure height\n\n    Returns\n    -------\n    plotly.graph_objects.Figure\n        Interactive mesh plot\n\n    Notes\n    -----\n    This method requires grid cells with vertex information.\n    Currently supports HTM and geodesic grids.\n\n    \"\"\"\n    traces = []\n\n    # Prepare data\n    if data is None:\n        values = np.ones(self.grid.ncells) * 0.5\n    else:\n        values = data\n\n    grid_df = self.grid.grid\n    grid_type = self.grid.grid_type.lower()\n\n    # Check if grid supports mesh rendering\n    if grid_type == \"htm\" and \"htm_vertex_0\" in grid_df.columns:\n        # HTM triangular mesh\n        for idx, row in enumerate(grid_df.iter_rows(named=True)):\n            try:\n                v0 = np.array(row[\"htm_vertex_0\"], dtype=float)\n                v1 = np.array(row[\"htm_vertex_1\"], dtype=float)\n                v2 = np.array(row[\"htm_vertex_2\"], dtype=float)\n\n                vertices = np.array([v0, v1, v2])\n\n                # Check hemisphere\n                z_coords = vertices[:, 2]\n                if np.all(z_coords &lt; 0):\n                    continue\n\n                # Normalize color value\n                color_val = (values[idx] - np.nanmin(values)) / (\n                    np.nanmax(values) - np.nanmin(values)\n                )\n                color_rgb = sample_colorscale(colorscale, [color_val])[0]\n\n                # Create triangle mesh\n                trace = go.Mesh3d(\n                    x=vertices[:, 0],\n                    y=vertices[:, 1],\n                    z=vertices[:, 2],\n                    i=[0],\n                    j=[1],\n                    k=[2],\n                    color=color_rgb,\n                    opacity=opacity,\n                    flatshading=True,\n                    showscale=False,\n                    hoverinfo=\"skip\",\n                )\n                traces.append(trace)\n            except (KeyError, TypeError, ValueError):\n                continue\n    else:\n        # Not supported for this grid type\n        raise NotImplementedError(\n            f\"Cell mesh rendering not implemented for {grid_type} grids\"\n        )\n\n    # Sample colorscale (no longer needed above, but kept for compatibility)\n\n    # Add colorbar trace\n    if values is not None and len(traces) &gt; 0:\n        dummy_trace = go.Scatter3d(\n            x=[None],\n            y=[None],\n            z=[None],\n            mode=\"markers\",\n            marker=dict(\n                size=0.1,\n                color=[np.nanmin(values), np.nanmax(values)],\n                colorscale=colorscale,\n                colorbar=dict(title=\"Value\"),\n            ),\n            showlegend=False,\n            hoverinfo=\"skip\",\n        )\n        traces.append(dummy_trace)\n\n    fig = go.Figure(data=traces)\n\n    # Update layout\n    fig.update_layout(\n        title=title or \"Hemisphere Mesh 3D\",\n        scene=dict(\n            aspectmode=\"data\",\n            xaxis=dict(title=\"East\", showbackground=False),\n            yaxis=dict(title=\"North\", showbackground=False),\n            zaxis=dict(title=\"Up\", showbackground=False),\n        ),\n        width=width,\n        height=height,\n        margin=dict(l=0, r=0, b=0, t=40),\n    )\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.add_spherical_overlays","level":3,"title":"<code>add_spherical_overlays(fig, elevation_rings=None, meridians_deg=None, overlay_color='lightgray', line_width=1)</code>","text":"<p>Add elevation rings and meridians to 3D plot.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.add_spherical_overlays--parameters","level":5,"title":"Parameters","text":"<p>fig : plotly.graph_objects.Figure     Existing 3D figure to add overlays to elevation_rings : list of int, optional     Elevation angles in degrees. Default: [15, 30, 45, 60, 75, 90] meridians_deg : list of int, optional     Meridian angles in degrees. Default: [0, 45, 90, 135, 180, 225, 270, 315] overlay_color : str, default 'lightgray'     Color for overlay lines line_width : float, default 1     Width of overlay lines</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.add_spherical_overlays--returns","level":5,"title":"Returns","text":"<p>fig : plotly.graph_objects.Figure     Modified figure with overlays</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def add_spherical_overlays(\n    self,\n    fig: go.Figure,\n    elevation_rings: list[int] | None = None,\n    meridians_deg: list[int] | None = None,\n    overlay_color: str = \"lightgray\",\n    line_width: float = 1,\n) -&gt; go.Figure:\n    \"\"\"Add elevation rings and meridians to 3D plot.\n\n    Parameters\n    ----------\n    fig : plotly.graph_objects.Figure\n        Existing 3D figure to add overlays to\n    elevation_rings : list of int, optional\n        Elevation angles in degrees. Default: [15, 30, 45, 60, 75, 90]\n    meridians_deg : list of int, optional\n        Meridian angles in degrees. Default: [0, 45, 90, 135, 180, 225, 270, 315]\n    overlay_color : str, default 'lightgray'\n        Color for overlay lines\n    line_width : float, default 1\n        Width of overlay lines\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n        Modified figure with overlays\n\n    \"\"\"\n    if elevation_rings is None:\n        elevation_rings = [15, 30, 45, 60, 75, 90]\n    if meridians_deg is None:\n        meridians_deg = list(range(0, 360, 45))\n\n    # Elevation rings\n    for theta_deg in elevation_rings:\n        theta = np.radians(theta_deg)\n        phi = np.linspace(0, 2 * np.pi, 200)\n        x = np.sin(theta) * np.sin(phi)\n        y = np.sin(theta) * np.cos(phi)\n        z = np.full_like(phi, np.cos(theta))\n        fig.add_trace(\n            go.Scatter3d(\n                x=x,\n                y=y,\n                z=z,\n                mode=\"lines\",\n                line=dict(color=overlay_color, width=line_width),\n                hoverinfo=\"skip\",\n                showlegend=False,\n            )\n        )\n\n    # Meridians\n    for phi_deg in meridians_deg:\n        phi = np.radians(phi_deg)\n        theta = np.linspace(0, np.pi / 2, 100)\n        x = np.sin(theta) * np.sin(phi)\n        y = np.sin(theta) * np.cos(phi)\n        z = np.cos(theta)\n        fig.add_trace(\n            go.Scatter3d(\n                x=x,\n                y=y,\n                z=z,\n                mode=\"lines\",\n                line=dict(color=overlay_color, width=line_width),\n                hoverinfo=\"skip\",\n                showlegend=False,\n            )\n        )\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.add_custom_axes","level":3,"title":"<code>add_custom_axes(fig, axis_length=1.2, axis_color='black', show_labels=True)</code>","text":"<p>Add custom coordinate axes with labels.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.add_custom_axes--parameters","level":5,"title":"Parameters","text":"<p>fig : plotly.graph_objects.Figure     Existing 3D figure axis_length : float, default 1.2     Length of axis lines axis_color : str, default 'black'     Color for axes show_labels : bool, default True     Show axis labels (E, N, Z)</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.HemisphereVisualizer3D.add_custom_axes--returns","level":5,"title":"Returns","text":"<p>fig : plotly.graph_objects.Figure     Modified figure with custom axes</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def add_custom_axes(\n    self,\n    fig: go.Figure,\n    axis_length: float = 1.2,\n    axis_color: str = \"black\",\n    show_labels: bool = True,\n) -&gt; go.Figure:\n    \"\"\"Add custom coordinate axes with labels.\n\n    Parameters\n    ----------\n    fig : plotly.graph_objects.Figure\n        Existing 3D figure\n    axis_length : float, default 1.2\n        Length of axis lines\n    axis_color : str, default 'black'\n        Color for axes\n    show_labels : bool, default True\n        Show axis labels (E, N, Z)\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n        Modified figure with custom axes\n\n    \"\"\"\n    # Axis lines\n    axes_lines = [\n        dict(x=[0, axis_length], y=[0, 0], z=[0, 0]),  # East\n        dict(x=[0, 0], y=[0, axis_length], z=[0, 0]),  # North\n        dict(x=[0, 0], y=[0, 0], z=[0, axis_length]),  # Up\n    ]\n\n    for axis in axes_lines:\n        fig.add_trace(\n            go.Scatter3d(\n                x=axis[\"x\"],\n                y=axis[\"y\"],\n                z=axis[\"z\"],\n                mode=\"lines\",\n                line=dict(color=axis_color, width=6),\n                hoverinfo=\"skip\",\n                showlegend=False,\n            )\n        )\n\n    # Arrowheads\n    arrow_tip = axis_length + 0.05\n    arrow_size = 0.1\n    for pos, direction in zip(\n        [[arrow_tip, 0, 0], [0, arrow_tip, 0], [0, 0, arrow_tip]],\n        [[arrow_size, 0, 0], [0, arrow_size, 0], [0, 0, arrow_size]],\n    ):\n        fig.add_trace(\n            go.Cone(\n                x=[pos[0]],\n                y=[pos[1]],\n                z=[pos[2]],\n                u=[direction[0]],\n                v=[direction[1]],\n                w=[direction[2]],\n                sizemode=\"absolute\",\n                sizeref=arrow_size,\n                anchor=\"tip\",\n                showscale=False,\n                colorscale=[[0, axis_color], [1, axis_color]],\n                hoverinfo=\"skip\",\n                showlegend=False,\n            )\n        )\n\n    # Labels\n    if show_labels:\n        label_offset = axis_length + 0.15\n        for label in [\n            dict(x=label_offset, y=0, z=0, text=\"E\"),\n            dict(x=0, y=label_offset, z=0, text=\"N\"),\n            dict(x=0, y=0, z=label_offset, text=\"Z\"),\n        ]:\n            fig.add_trace(\n                go.Scatter3d(\n                    x=[label[\"x\"]],\n                    y=[label[\"y\"]],\n                    z=[label[\"z\"]],\n                    mode=\"text\",\n                    text=[label[\"text\"]],\n                    textfont=dict(size=16, color=axis_color),\n                    hoverinfo=\"skip\",\n                    showlegend=False,\n                )\n            )\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.visualize_grid_3d","level":2,"title":"<code>visualize_grid_3d(grid, data=None, title=None, colorscale='Viridis', add_overlays=False, add_axes=False, **kwargs)</code>","text":"<p>Visualize hemispherical grid in 3D interactive plot.</p> <p>Convenience function providing simple interface to 3D visualization.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.visualize_grid_3d--parameters","level":4,"title":"Parameters","text":"<p>grid : HemiGrid     Grid to visualize data : np.ndarray, optional     Data values per cell title : str, optional     Plot title colorscale : str, default 'Viridis'     Plotly colorscale name add_overlays : bool, default False     Add elevation rings and meridians add_axes : bool, default False     Add custom coordinate axes **kwargs     Additional parameters passed to plot_hemisphere_surface</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.visualize_grid_3d--returns","level":4,"title":"Returns","text":"<p>fig : plotly.graph_objects.Figure     Interactive 3D figure</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.hemisphere_3d.visualize_grid_3d--examples","level":4,"title":"Examples","text":"<p>from canvod.grids import create_hemigrid from canvod.viz import visualize_grid_3d</p> <p>grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0) fig = visualize_grid_3d( ...     grid, ...     data=vod_data, ...     title=\"VOD 3D\", ...     add_overlays=True ... ) fig.show()</p> Source code in <code>packages/canvod-viz/src/canvod/viz/hemisphere_3d.py</code> <pre><code>def visualize_grid_3d(\n    grid: HemiGrid,\n    data: np.ndarray | None = None,\n    title: str | None = None,\n    colorscale: str = \"Viridis\",\n    add_overlays: bool = False,\n    add_axes: bool = False,\n    **kwargs: Any,\n) -&gt; go.Figure:\n    \"\"\"Visualize hemispherical grid in 3D interactive plot.\n\n    Convenience function providing simple interface to 3D visualization.\n\n    Parameters\n    ----------\n    grid : HemiGrid\n        Grid to visualize\n    data : np.ndarray, optional\n        Data values per cell\n    title : str, optional\n        Plot title\n    colorscale : str, default 'Viridis'\n        Plotly colorscale name\n    add_overlays : bool, default False\n        Add elevation rings and meridians\n    add_axes : bool, default False\n        Add custom coordinate axes\n    **kwargs\n        Additional parameters passed to plot_hemisphere_surface\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n        Interactive 3D figure\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import create_hemigrid\n    &gt;&gt;&gt; from canvod.viz import visualize_grid_3d\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; grid = create_hemigrid(grid_type='equal_area', angular_resolution=10.0)\n    &gt;&gt;&gt; fig = visualize_grid_3d(\n    ...     grid,\n    ...     data=vod_data,\n    ...     title=\"VOD 3D\",\n    ...     add_overlays=True\n    ... )\n    &gt;&gt;&gt; fig.show()\n\n    \"\"\"\n    viz = HemisphereVisualizer3D(grid)\n    fig = viz.plot_hemisphere_surface(\n        data=data, title=title, colorscale=colorscale, **kwargs\n    )\n\n    if add_overlays:\n        viz.add_spherical_overlays(fig)\n\n    if add_axes:\n        viz.add_custom_axes(fig)\n\n    return fig\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#styles","level":2,"title":"Styles","text":"<p>Styling configuration for visualizations.</p> <p>Provides consistent styling across 2D matplotlib and 3D plotly visualizations.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.PolarPlotStyle","level":2,"title":"<code>PolarPlotStyle</code>  <code>dataclass</code>","text":"<p>Configuration for 2D polar plot styling (matplotlib).</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.PolarPlotStyle--parameters","level":4,"title":"Parameters","text":"<p>cmap : str, default 'viridis'     Matplotlib colormap name edgecolor : str, default 'black'     Edge color for grid cells linewidth : float, default 0.5     Line width for cell edges alpha : float, default 1.0     Transparency (0=transparent, 1=opaque) vmin : float or None, optional     Minimum value for colormap vmax : float or None, optional     Maximum value for colormap title : str or None, optional     Plot title figsize : tuple of float, default (10, 10)     Figure size in inches (width, height) dpi : int, default 100     Dots per inch for figure colorbar_label : str, default 'Value'     Label for colorbar colorbar_shrink : float, default 0.8     Colorbar size relative to axis colorbar_pad : float, default 0.1     Space between axis and colorbar colorbar_fontsize : int, default 11     Font size for colorbar label show_grid : bool, default True     Show polar grid lines grid_alpha : float, default 0.3     Grid line transparency grid_linestyle : str, default '--'     Grid line style show_degree_labels : bool, default True     Show degree labels on radial axis theta_labels : list of int, default [0, 30, 60, 90]     Elevation angles for labels (degrees)</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>@dataclass\nclass PolarPlotStyle:\n    \"\"\"Configuration for 2D polar plot styling (matplotlib).\n\n    Parameters\n    ----------\n    cmap : str, default 'viridis'\n        Matplotlib colormap name\n    edgecolor : str, default 'black'\n        Edge color for grid cells\n    linewidth : float, default 0.5\n        Line width for cell edges\n    alpha : float, default 1.0\n        Transparency (0=transparent, 1=opaque)\n    vmin : float or None, optional\n        Minimum value for colormap\n    vmax : float or None, optional\n        Maximum value for colormap\n    title : str or None, optional\n        Plot title\n    figsize : tuple of float, default (10, 10)\n        Figure size in inches (width, height)\n    dpi : int, default 100\n        Dots per inch for figure\n    colorbar_label : str, default 'Value'\n        Label for colorbar\n    colorbar_shrink : float, default 0.8\n        Colorbar size relative to axis\n    colorbar_pad : float, default 0.1\n        Space between axis and colorbar\n    colorbar_fontsize : int, default 11\n        Font size for colorbar label\n    show_grid : bool, default True\n        Show polar grid lines\n    grid_alpha : float, default 0.3\n        Grid line transparency\n    grid_linestyle : str, default '--'\n        Grid line style\n    show_degree_labels : bool, default True\n        Show degree labels on radial axis\n    theta_labels : list of int, default [0, 30, 60, 90]\n        Elevation angles for labels (degrees)\n\n    \"\"\"\n\n    cmap: str = \"viridis\"\n    edgecolor: str = \"black\"\n    linewidth: float = 0.5\n    alpha: float = 1.0\n    vmin: float | None = None\n    vmax: float | None = None\n    title: str | None = None\n    figsize: tuple[float, float] = (10, 10)\n    dpi: int = 100\n    colorbar_label: str = \"Value\"\n    colorbar_shrink: float = 0.8\n    colorbar_pad: float = 0.1\n    colorbar_fontsize: int = 11\n    show_grid: bool = True\n    grid_alpha: float = 0.3\n    grid_linestyle: str = \"--\"\n    show_degree_labels: bool = True\n    theta_labels: list[int] = field(default_factory=lambda: [0, 30, 60, 90])\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.PlotStyle","level":2,"title":"<code>PlotStyle</code>  <code>dataclass</code>","text":"<p>Unified styling configuration for both 2D and 3D plots.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.PlotStyle--parameters","level":4,"title":"Parameters","text":"<p>colormap : str, default 'viridis'     Colormap name (matplotlib or plotly) colorscale : str, default 'Viridis'     Plotly colorscale name background_color : str, default 'white'     Background color text_color : str, default 'black'     Text color grid_color : str, default 'lightgray'     Grid line color font_family : str, default 'sans-serif'     Font family font_size : int, default 11     Base font size title_size : int, default 14     Title font size label_size : int, default 12     Axis label font size edge_linewidth : float, default 0.5     Edge line width for cells opacity : float, default 0.8     3D surface opacity marker_size : int, default 8     3D marker size line_width : int, default 1     3D line width wireframe_opacity : float, default 0.2     3D wireframe transparency dark_mode : bool, default False     Use dark theme</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>@dataclass\nclass PlotStyle:\n    \"\"\"Unified styling configuration for both 2D and 3D plots.\n\n    Parameters\n    ----------\n    colormap : str, default 'viridis'\n        Colormap name (matplotlib or plotly)\n    colorscale : str, default 'Viridis'\n        Plotly colorscale name\n    background_color : str, default 'white'\n        Background color\n    text_color : str, default 'black'\n        Text color\n    grid_color : str, default 'lightgray'\n        Grid line color\n    font_family : str, default 'sans-serif'\n        Font family\n    font_size : int, default 11\n        Base font size\n    title_size : int, default 14\n        Title font size\n    label_size : int, default 12\n        Axis label font size\n    edge_linewidth : float, default 0.5\n        Edge line width for cells\n    opacity : float, default 0.8\n        3D surface opacity\n    marker_size : int, default 8\n        3D marker size\n    line_width : int, default 1\n        3D line width\n    wireframe_opacity : float, default 0.2\n        3D wireframe transparency\n    dark_mode : bool, default False\n        Use dark theme\n\n    \"\"\"\n\n    colormap: str = \"viridis\"\n    colorscale: str = \"Viridis\"\n    background_color: str = \"white\"\n    text_color: str = \"black\"\n    grid_color: str = \"lightgray\"\n    font_family: str = \"sans-serif\"\n    font_size: int = 11\n    title_size: int = 14\n    label_size: int = 12\n    edge_linewidth: float = 0.5\n    opacity: float = 0.8\n    marker_size: int = 8\n    line_width: int = 1\n    wireframe_opacity: float = 0.2\n    dark_mode: bool = False\n\n    def to_polar_style(self) -&gt; PolarPlotStyle:\n        \"\"\"Convert to PolarPlotStyle for 2D matplotlib plots.\n\n        Returns\n        -------\n        PolarPlotStyle\n            Equivalent 2D styling configuration\n\n        \"\"\"\n        return PolarPlotStyle(\n            cmap=self.colormap,\n            edgecolor=\"white\" if self.dark_mode else self.text_color,\n            linewidth=self.edge_linewidth,\n            alpha=1.0,\n            colorbar_fontsize=self.font_size,\n        )\n\n    def to_plotly_layout(self) -&gt; dict[str, Any]:\n        \"\"\"Convert to plotly layout configuration.\n\n        Returns\n        -------\n        dict\n            Plotly layout settings\n\n        \"\"\"\n        if self.dark_mode:\n            return {\n                \"template\": \"plotly_dark\",\n                \"paper_bgcolor\": \"#111111\",\n                \"plot_bgcolor\": \"#111111\",\n                \"font\": {\n                    \"family\": self.font_family,\n                    \"size\": self.font_size,\n                    \"color\": \"white\",\n                },\n            }\n        return {\n            \"template\": \"plotly\",\n            \"paper_bgcolor\": self.background_color,\n            \"plot_bgcolor\": self.background_color,\n            \"font\": {\n                \"family\": self.font_family,\n                \"size\": self.font_size,\n                \"color\": self.text_color,\n            },\n        }\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.PlotStyle.to_polar_style","level":3,"title":"<code>to_polar_style()</code>","text":"<p>Convert to PolarPlotStyle for 2D matplotlib plots.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.PlotStyle.to_polar_style--returns","level":5,"title":"Returns","text":"<p>PolarPlotStyle     Equivalent 2D styling configuration</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>def to_polar_style(self) -&gt; PolarPlotStyle:\n    \"\"\"Convert to PolarPlotStyle for 2D matplotlib plots.\n\n    Returns\n    -------\n    PolarPlotStyle\n        Equivalent 2D styling configuration\n\n    \"\"\"\n    return PolarPlotStyle(\n        cmap=self.colormap,\n        edgecolor=\"white\" if self.dark_mode else self.text_color,\n        linewidth=self.edge_linewidth,\n        alpha=1.0,\n        colorbar_fontsize=self.font_size,\n    )\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.PlotStyle.to_plotly_layout","level":3,"title":"<code>to_plotly_layout()</code>","text":"<p>Convert to plotly layout configuration.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.PlotStyle.to_plotly_layout--returns","level":5,"title":"Returns","text":"<p>dict     Plotly layout settings</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>def to_plotly_layout(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to plotly layout configuration.\n\n    Returns\n    -------\n    dict\n        Plotly layout settings\n\n    \"\"\"\n    if self.dark_mode:\n        return {\n            \"template\": \"plotly_dark\",\n            \"paper_bgcolor\": \"#111111\",\n            \"plot_bgcolor\": \"#111111\",\n            \"font\": {\n                \"family\": self.font_family,\n                \"size\": self.font_size,\n                \"color\": \"white\",\n            },\n        }\n    return {\n        \"template\": \"plotly\",\n        \"paper_bgcolor\": self.background_color,\n        \"plot_bgcolor\": self.background_color,\n        \"font\": {\n            \"family\": self.font_family,\n            \"size\": self.font_size,\n            \"color\": self.text_color,\n        },\n    }\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.create_publication_style","level":2,"title":"<code>create_publication_style()</code>","text":"<p>Create styling optimized for publication-quality figures.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.create_publication_style--returns","level":4,"title":"Returns","text":"<p>PlotStyle     Publication-optimized styling configuration</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.create_publication_style--examples","level":4,"title":"Examples","text":"<p>style = create_publication_style() viz.plot_2d(data=vod_data, style=style)</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>def create_publication_style() -&gt; PlotStyle:\n    \"\"\"Create styling optimized for publication-quality figures.\n\n    Returns\n    -------\n    PlotStyle\n        Publication-optimized styling configuration\n\n    Examples\n    --------\n    &gt;&gt;&gt; style = create_publication_style()\n    &gt;&gt;&gt; viz.plot_2d(data=vod_data, style=style)\n\n    \"\"\"\n    return PlotStyle(\n        colormap=\"viridis\",\n        colorscale=\"Viridis\",\n        background_color=\"white\",\n        text_color=\"black\",\n        font_family=\"sans-serif\",\n        font_size=12,\n        title_size=16,\n        label_size=14,\n        edge_linewidth=0.3,\n        opacity=0.9,\n        dark_mode=False,\n    )\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.create_interactive_style","level":2,"title":"<code>create_interactive_style(dark_mode=True)</code>","text":"<p>Create styling optimized for interactive exploration.</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.create_interactive_style--parameters","level":4,"title":"Parameters","text":"<p>dark_mode : bool, default True     Use dark theme for better screen viewing</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.create_interactive_style--returns","level":4,"title":"Returns","text":"<p>PlotStyle     Interactive-optimized styling configuration</p>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-viz/#canvod.viz.styles.create_interactive_style--examples","level":4,"title":"Examples","text":"<p>style = create_interactive_style(dark_mode=True) viz.plot_3d(data=vod_data, style=style)</p> Source code in <code>packages/canvod-viz/src/canvod/viz/styles.py</code> <pre><code>def create_interactive_style(dark_mode: bool = True) -&gt; PlotStyle:\n    \"\"\"Create styling optimized for interactive exploration.\n\n    Parameters\n    ----------\n    dark_mode : bool, default True\n        Use dark theme for better screen viewing\n\n    Returns\n    -------\n    PlotStyle\n        Interactive-optimized styling configuration\n\n    Examples\n    --------\n    &gt;&gt;&gt; style = create_interactive_style(dark_mode=True)\n    &gt;&gt;&gt; viz.plot_3d(data=vod_data, style=style)\n\n    \"\"\"\n    return PlotStyle(\n        colormap=\"plasma\" if dark_mode else \"viridis\",\n        colorscale=\"Plasma\" if dark_mode else \"Viridis\",\n        background_color=\"#111111\" if dark_mode else \"white\",\n        text_color=\"white\" if dark_mode else \"black\",\n        font_family=\"Open Sans, sans-serif\",\n        font_size=11,\n        title_size=14,\n        label_size=12,\n        edge_linewidth=0.5,\n        opacity=0.85,\n        marker_size=6,\n        wireframe_opacity=0.15,\n        dark_mode=dark_mode,\n    )\n</code></pre>","path":["API Reference","canvod.viz API Reference"],"tags":[]},{"location":"api/canvod-vod/","level":1,"title":"canvod.vod API Reference","text":"<p>Vegetation optical depth estimation using the tau-omega radiative transfer model.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#package","level":2,"title":"Package","text":"<p>canvod-vod: VOD calculation for GNSS vegetation analysis.</p> <p>This package provides VOD calculation algorithms based on the Tau-Omega model.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.TauOmegaZerothOrder","level":2,"title":"<code>TauOmegaZerothOrder</code>","text":"<p>               Bases: <code>VODCalculator</code></p> <p>Calculate VOD using the zeroth-order Tau-Omega approximation.</p> <p>Based on Humphrey, V., &amp; Frankenberg, C. (2022).</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>class TauOmegaZerothOrder(VODCalculator):\n    \"\"\"Calculate VOD using the zeroth-order Tau-Omega approximation.\n\n    Based on Humphrey, V., &amp; Frankenberg, C. (2022).\n    \"\"\"\n\n    def get_delta_snr(self) -&gt; xr.DataArray:\n        \"\"\"Calculate delta SNR = SNR_canopy - SNR_sky.\n\n        Returns\n        -------\n        xr.DataArray\n            Delta SNR in decibels.\n        \"\"\"\n        return self.canopy_ds[\"SNR\"] - self.sky_ds[\"SNR\"]\n\n    def decibel2linear(self, delta_snr_db: xr.DataArray) -&gt; xr.DataArray:\n        \"\"\"Convert decibel values to linear values.\n\n        Parameters\n        ----------\n        delta_snr_db : xr.DataArray\n            Delta SNR in decibels.\n\n        Returns\n        -------\n        xr.DataArray\n            Linear-scale values.\n        \"\"\"\n        return np.power(10, delta_snr_db / 10)\n\n    def calculate_vod(self) -&gt; xr.Dataset:\n        \"\"\"Calculate VOD using the zeroth-order approximation.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing VOD and angular coordinates.\n\n        Raises\n        ------\n        ValueError\n            If all delta SNR values are NaN.\n        \"\"\"\n        start_time = time.time()\n        log.info(\n            \"vod_calculation_started\",\n            canopy_epochs=len(self.canopy_ds.epoch),\n            sky_epochs=len(self.sky_ds.epoch),\n            sids=len(self.canopy_ds.sid),\n        )\n\n        delta_snr = self.get_delta_snr()\n\n        if delta_snr.isnull().all():\n            log.error(\n                \"vod_calculation_failed\",\n                reason=\"all_delta_snr_nan\",\n            )\n            raise ValueError(\n                \"All delta_snr values are NaN - check data alignment\",\n            )\n\n        canopy_transmissivity = self.decibel2linear(delta_snr)\n\n        if (canopy_transmissivity &lt;= 0).any():\n            n_invalid = (canopy_transmissivity &lt;= 0).sum().item()\n            total = canopy_transmissivity.size\n            print(\n                f\"Warning: {n_invalid}/{total} transmissivity values &lt;= 0 \"\n                \"(will produce NaN)\"\n            )\n            log.warning(\n                \"invalid_transmissivity\",\n                invalid_count=n_invalid,\n                total_count=total,\n                percent=round(100 * n_invalid / total, 2),\n            )\n\n        theta = self.canopy_ds[\"theta\"]\n        vod = -np.log(canopy_transmissivity) * np.cos(theta)\n\n        vod_ds = xr.Dataset(\n            {\n                \"VOD\": vod,\n                \"phi\": self.canopy_ds[\"phi\"],\n                \"theta\": self.canopy_ds[\"theta\"],\n            },\n            coords=self.canopy_ds.coords,\n        )\n\n        duration = time.time() - start_time\n        n_valid = (~vod.isnull()).sum().item()\n\n        log.info(\n            \"vod_calculation_complete\",\n            duration_seconds=round(duration, 2),\n            vod_values=vod.size,\n            valid_values=n_valid,\n            valid_percent=round(100 * n_valid / vod.size, 2),\n        )\n\n        return vod_ds\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.TauOmegaZerothOrder.get_delta_snr","level":3,"title":"<code>get_delta_snr()</code>","text":"<p>Calculate delta SNR = SNR_canopy - SNR_sky.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.TauOmegaZerothOrder.get_delta_snr--returns","level":5,"title":"Returns","text":"<p>xr.DataArray     Delta SNR in decibels.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>def get_delta_snr(self) -&gt; xr.DataArray:\n    \"\"\"Calculate delta SNR = SNR_canopy - SNR_sky.\n\n    Returns\n    -------\n    xr.DataArray\n        Delta SNR in decibels.\n    \"\"\"\n    return self.canopy_ds[\"SNR\"] - self.sky_ds[\"SNR\"]\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.TauOmegaZerothOrder.decibel2linear","level":3,"title":"<code>decibel2linear(delta_snr_db)</code>","text":"<p>Convert decibel values to linear values.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.TauOmegaZerothOrder.decibel2linear--parameters","level":5,"title":"Parameters","text":"<p>delta_snr_db : xr.DataArray     Delta SNR in decibels.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.TauOmegaZerothOrder.decibel2linear--returns","level":5,"title":"Returns","text":"<p>xr.DataArray     Linear-scale values.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>def decibel2linear(self, delta_snr_db: xr.DataArray) -&gt; xr.DataArray:\n    \"\"\"Convert decibel values to linear values.\n\n    Parameters\n    ----------\n    delta_snr_db : xr.DataArray\n        Delta SNR in decibels.\n\n    Returns\n    -------\n    xr.DataArray\n        Linear-scale values.\n    \"\"\"\n    return np.power(10, delta_snr_db / 10)\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.TauOmegaZerothOrder.calculate_vod","level":3,"title":"<code>calculate_vod()</code>","text":"<p>Calculate VOD using the zeroth-order approximation.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.TauOmegaZerothOrder.calculate_vod--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset containing VOD and angular coordinates.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.TauOmegaZerothOrder.calculate_vod--raises","level":5,"title":"Raises","text":"<p>ValueError     If all delta SNR values are NaN.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>def calculate_vod(self) -&gt; xr.Dataset:\n    \"\"\"Calculate VOD using the zeroth-order approximation.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing VOD and angular coordinates.\n\n    Raises\n    ------\n    ValueError\n        If all delta SNR values are NaN.\n    \"\"\"\n    start_time = time.time()\n    log.info(\n        \"vod_calculation_started\",\n        canopy_epochs=len(self.canopy_ds.epoch),\n        sky_epochs=len(self.sky_ds.epoch),\n        sids=len(self.canopy_ds.sid),\n    )\n\n    delta_snr = self.get_delta_snr()\n\n    if delta_snr.isnull().all():\n        log.error(\n            \"vod_calculation_failed\",\n            reason=\"all_delta_snr_nan\",\n        )\n        raise ValueError(\n            \"All delta_snr values are NaN - check data alignment\",\n        )\n\n    canopy_transmissivity = self.decibel2linear(delta_snr)\n\n    if (canopy_transmissivity &lt;= 0).any():\n        n_invalid = (canopy_transmissivity &lt;= 0).sum().item()\n        total = canopy_transmissivity.size\n        print(\n            f\"Warning: {n_invalid}/{total} transmissivity values &lt;= 0 \"\n            \"(will produce NaN)\"\n        )\n        log.warning(\n            \"invalid_transmissivity\",\n            invalid_count=n_invalid,\n            total_count=total,\n            percent=round(100 * n_invalid / total, 2),\n        )\n\n    theta = self.canopy_ds[\"theta\"]\n    vod = -np.log(canopy_transmissivity) * np.cos(theta)\n\n    vod_ds = xr.Dataset(\n        {\n            \"VOD\": vod,\n            \"phi\": self.canopy_ds[\"phi\"],\n            \"theta\": self.canopy_ds[\"theta\"],\n        },\n        coords=self.canopy_ds.coords,\n    )\n\n    duration = time.time() - start_time\n    n_valid = (~vod.isnull()).sum().item()\n\n    log.info(\n        \"vod_calculation_complete\",\n        duration_seconds=round(duration, 2),\n        vod_values=vod.size,\n        valid_values=n_valid,\n        valid_percent=round(100 * n_valid / vod.size, 2),\n    )\n\n    return vod_ds\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator","level":2,"title":"<code>VODCalculator</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract base class for VOD calculation from RINEX store data.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator--notes","level":4,"title":"Notes","text":"<p>This is an abstract base class (ABC) and a Pydantic model.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>class VODCalculator(ABC, BaseModel):\n    \"\"\"Abstract base class for VOD calculation from RINEX store data.\n\n    Notes\n    -----\n    This is an abstract base class (ABC) and a Pydantic model.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    canopy_ds: xr.Dataset\n    sky_ds: xr.Dataset\n\n    @field_validator(\"canopy_ds\", \"sky_ds\")\n    @classmethod\n    def validate_datasets(cls, v: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"Validate dataset inputs.\n\n        Parameters\n        ----------\n        v : xr.Dataset\n            Dataset to validate.\n\n        Returns\n        -------\n        xr.Dataset\n            Validated dataset.\n\n        Raises\n        ------\n        ValueError\n            If the input is not an xarray Dataset or lacks SNR.\n        \"\"\"\n        if not isinstance(v, xr.Dataset):\n            raise ValueError(\"Must be xr.Dataset\")\n        if \"SNR\" not in v.data_vars:\n            raise ValueError(\"Dataset must contain 'SNR' variable\")\n        return v\n\n    @abstractmethod\n    def calculate_vod(self) -&gt; xr.Dataset:\n        \"\"\"Calculate VOD and return a dataset with VOD, phi, theta.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing VOD and angular coordinates.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def from_icechunkstore(\n        cls,\n        icechunk_store_pth: Path,\n        canopy_group: str = \"canopy_01\",\n        sky_group: str = \"reference_01\",\n        **open_kwargs: Any,\n    ) -&gt; xr.Dataset:\n        \"\"\"Convenience method to calculate VOD directly from an IcechunkStore.\n\n        Parameters\n        ----------\n        icechunk_store_pth : Path\n            Path to Icechunk store.\n        canopy_group : str\n            Canopy receiver group name.\n        sky_group : str\n            Sky/reference receiver group name.\n        open_kwargs : dict[str, Any]\n            Additional keyword arguments for IcechunkStore.open().\n            Currently unused.\n\n        Returns\n        -------\n        xr.Dataset\n            VOD dataset.\n\n        Notes\n        -----\n        Requires canvod-store to be installed.\n        \"\"\"\n        try:\n            from canvod.store import MyIcechunkStore\n        except ImportError as e:\n            raise ImportError(\n                \"canvod-store package required for from_icechunkstore(). \"\n                \"Install with: pip install canvod-store\"\n            ) from e\n\n        store = MyIcechunkStore(icechunk_store_pth)\n\n        with store.readonly_session() as session:\n            canopy_ds = xr.open_zarr(store=session.store, group=canopy_group)\n            sky_ds = xr.open_zarr(store=session.store, group=sky_group)\n\n        return cls.from_datasets(\n            canopy_ds=canopy_ds,\n            sky_ds=sky_ds,\n            align=True,\n        )\n\n    @classmethod\n    def from_datasets(\n        cls,\n        canopy_ds: xr.Dataset,\n        sky_ds: xr.Dataset,\n        align: bool = True,\n    ) -&gt; xr.Dataset:\n        \"\"\"Convenience method to calculate VOD directly from datasets.\n\n        Parameters\n        ----------\n        canopy_ds : xr.Dataset\n            Canopy receiver dataset.\n        sky_ds : xr.Dataset\n            Sky/reference receiver dataset.\n        align : bool\n            Whether to align datasets on common coordinates.\n\n        Returns\n        -------\n        xr.Dataset\n            VOD dataset.\n        \"\"\"\n        if align:\n            canopy_ds, sky_ds = xr.align(canopy_ds, sky_ds, join=\"inner\")\n\n        calculator = cls(canopy_ds=canopy_ds, sky_ds=sky_ds)\n        return calculator.calculate_vod()\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.validate_datasets","level":3,"title":"<code>validate_datasets(v)</code>  <code>classmethod</code>","text":"<p>Validate dataset inputs.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.validate_datasets--parameters","level":5,"title":"Parameters","text":"<p>v : xr.Dataset     Dataset to validate.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.validate_datasets--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Validated dataset.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.validate_datasets--raises","level":5,"title":"Raises","text":"<p>ValueError     If the input is not an xarray Dataset or lacks SNR.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>@field_validator(\"canopy_ds\", \"sky_ds\")\n@classmethod\ndef validate_datasets(cls, v: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Validate dataset inputs.\n\n    Parameters\n    ----------\n    v : xr.Dataset\n        Dataset to validate.\n\n    Returns\n    -------\n    xr.Dataset\n        Validated dataset.\n\n    Raises\n    ------\n    ValueError\n        If the input is not an xarray Dataset or lacks SNR.\n    \"\"\"\n    if not isinstance(v, xr.Dataset):\n        raise ValueError(\"Must be xr.Dataset\")\n    if \"SNR\" not in v.data_vars:\n        raise ValueError(\"Dataset must contain 'SNR' variable\")\n    return v\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.calculate_vod","level":3,"title":"<code>calculate_vod()</code>  <code>abstractmethod</code>","text":"<p>Calculate VOD and return a dataset with VOD, phi, theta.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.calculate_vod--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset containing VOD and angular coordinates.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>@abstractmethod\ndef calculate_vod(self) -&gt; xr.Dataset:\n    \"\"\"Calculate VOD and return a dataset with VOD, phi, theta.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing VOD and angular coordinates.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.from_icechunkstore","level":3,"title":"<code>from_icechunkstore(icechunk_store_pth, canopy_group='canopy_01', sky_group='reference_01', **open_kwargs)</code>  <code>classmethod</code>","text":"<p>Convenience method to calculate VOD directly from an IcechunkStore.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.from_icechunkstore--parameters","level":5,"title":"Parameters","text":"<p>icechunk_store_pth : Path     Path to Icechunk store. canopy_group : str     Canopy receiver group name. sky_group : str     Sky/reference receiver group name. open_kwargs : dict[str, Any]     Additional keyword arguments for IcechunkStore.open().     Currently unused.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.from_icechunkstore--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     VOD dataset.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.from_icechunkstore--notes","level":5,"title":"Notes","text":"<p>Requires canvod-store to be installed.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>@classmethod\ndef from_icechunkstore(\n    cls,\n    icechunk_store_pth: Path,\n    canopy_group: str = \"canopy_01\",\n    sky_group: str = \"reference_01\",\n    **open_kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"Convenience method to calculate VOD directly from an IcechunkStore.\n\n    Parameters\n    ----------\n    icechunk_store_pth : Path\n        Path to Icechunk store.\n    canopy_group : str\n        Canopy receiver group name.\n    sky_group : str\n        Sky/reference receiver group name.\n    open_kwargs : dict[str, Any]\n        Additional keyword arguments for IcechunkStore.open().\n        Currently unused.\n\n    Returns\n    -------\n    xr.Dataset\n        VOD dataset.\n\n    Notes\n    -----\n    Requires canvod-store to be installed.\n    \"\"\"\n    try:\n        from canvod.store import MyIcechunkStore\n    except ImportError as e:\n        raise ImportError(\n            \"canvod-store package required for from_icechunkstore(). \"\n            \"Install with: pip install canvod-store\"\n        ) from e\n\n    store = MyIcechunkStore(icechunk_store_pth)\n\n    with store.readonly_session() as session:\n        canopy_ds = xr.open_zarr(store=session.store, group=canopy_group)\n        sky_ds = xr.open_zarr(store=session.store, group=sky_group)\n\n    return cls.from_datasets(\n        canopy_ds=canopy_ds,\n        sky_ds=sky_ds,\n        align=True,\n    )\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.from_datasets","level":3,"title":"<code>from_datasets(canopy_ds, sky_ds, align=True)</code>  <code>classmethod</code>","text":"<p>Convenience method to calculate VOD directly from datasets.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.from_datasets--parameters","level":5,"title":"Parameters","text":"<p>canopy_ds : xr.Dataset     Canopy receiver dataset. sky_ds : xr.Dataset     Sky/reference receiver dataset. align : bool     Whether to align datasets on common coordinates.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.VODCalculator.from_datasets--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     VOD dataset.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>@classmethod\ndef from_datasets(\n    cls,\n    canopy_ds: xr.Dataset,\n    sky_ds: xr.Dataset,\n    align: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Convenience method to calculate VOD directly from datasets.\n\n    Parameters\n    ----------\n    canopy_ds : xr.Dataset\n        Canopy receiver dataset.\n    sky_ds : xr.Dataset\n        Sky/reference receiver dataset.\n    align : bool\n        Whether to align datasets on common coordinates.\n\n    Returns\n    -------\n    xr.Dataset\n        VOD dataset.\n    \"\"\"\n    if align:\n        canopy_ds, sky_ds = xr.align(canopy_ds, sky_ds, join=\"inner\")\n\n    calculator = cls(canopy_ds=canopy_ds, sky_ds=sky_ds)\n    return calculator.calculate_vod()\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#calculator","level":2,"title":"Calculator","text":"<p>VOD calculators based on Tau-Omega model variants.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator","level":2,"title":"<code>VODCalculator</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract base class for VOD calculation from RINEX store data.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator--notes","level":4,"title":"Notes","text":"<p>This is an abstract base class (ABC) and a Pydantic model.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>class VODCalculator(ABC, BaseModel):\n    \"\"\"Abstract base class for VOD calculation from RINEX store data.\n\n    Notes\n    -----\n    This is an abstract base class (ABC) and a Pydantic model.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    canopy_ds: xr.Dataset\n    sky_ds: xr.Dataset\n\n    @field_validator(\"canopy_ds\", \"sky_ds\")\n    @classmethod\n    def validate_datasets(cls, v: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"Validate dataset inputs.\n\n        Parameters\n        ----------\n        v : xr.Dataset\n            Dataset to validate.\n\n        Returns\n        -------\n        xr.Dataset\n            Validated dataset.\n\n        Raises\n        ------\n        ValueError\n            If the input is not an xarray Dataset or lacks SNR.\n        \"\"\"\n        if not isinstance(v, xr.Dataset):\n            raise ValueError(\"Must be xr.Dataset\")\n        if \"SNR\" not in v.data_vars:\n            raise ValueError(\"Dataset must contain 'SNR' variable\")\n        return v\n\n    @abstractmethod\n    def calculate_vod(self) -&gt; xr.Dataset:\n        \"\"\"Calculate VOD and return a dataset with VOD, phi, theta.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing VOD and angular coordinates.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def from_icechunkstore(\n        cls,\n        icechunk_store_pth: Path,\n        canopy_group: str = \"canopy_01\",\n        sky_group: str = \"reference_01\",\n        **open_kwargs: Any,\n    ) -&gt; xr.Dataset:\n        \"\"\"Convenience method to calculate VOD directly from an IcechunkStore.\n\n        Parameters\n        ----------\n        icechunk_store_pth : Path\n            Path to Icechunk store.\n        canopy_group : str\n            Canopy receiver group name.\n        sky_group : str\n            Sky/reference receiver group name.\n        open_kwargs : dict[str, Any]\n            Additional keyword arguments for IcechunkStore.open().\n            Currently unused.\n\n        Returns\n        -------\n        xr.Dataset\n            VOD dataset.\n\n        Notes\n        -----\n        Requires canvod-store to be installed.\n        \"\"\"\n        try:\n            from canvod.store import MyIcechunkStore\n        except ImportError as e:\n            raise ImportError(\n                \"canvod-store package required for from_icechunkstore(). \"\n                \"Install with: pip install canvod-store\"\n            ) from e\n\n        store = MyIcechunkStore(icechunk_store_pth)\n\n        with store.readonly_session() as session:\n            canopy_ds = xr.open_zarr(store=session.store, group=canopy_group)\n            sky_ds = xr.open_zarr(store=session.store, group=sky_group)\n\n        return cls.from_datasets(\n            canopy_ds=canopy_ds,\n            sky_ds=sky_ds,\n            align=True,\n        )\n\n    @classmethod\n    def from_datasets(\n        cls,\n        canopy_ds: xr.Dataset,\n        sky_ds: xr.Dataset,\n        align: bool = True,\n    ) -&gt; xr.Dataset:\n        \"\"\"Convenience method to calculate VOD directly from datasets.\n\n        Parameters\n        ----------\n        canopy_ds : xr.Dataset\n            Canopy receiver dataset.\n        sky_ds : xr.Dataset\n            Sky/reference receiver dataset.\n        align : bool\n            Whether to align datasets on common coordinates.\n\n        Returns\n        -------\n        xr.Dataset\n            VOD dataset.\n        \"\"\"\n        if align:\n            canopy_ds, sky_ds = xr.align(canopy_ds, sky_ds, join=\"inner\")\n\n        calculator = cls(canopy_ds=canopy_ds, sky_ds=sky_ds)\n        return calculator.calculate_vod()\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.validate_datasets","level":3,"title":"<code>validate_datasets(v)</code>  <code>classmethod</code>","text":"<p>Validate dataset inputs.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.validate_datasets--parameters","level":5,"title":"Parameters","text":"<p>v : xr.Dataset     Dataset to validate.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.validate_datasets--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Validated dataset.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.validate_datasets--raises","level":5,"title":"Raises","text":"<p>ValueError     If the input is not an xarray Dataset or lacks SNR.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>@field_validator(\"canopy_ds\", \"sky_ds\")\n@classmethod\ndef validate_datasets(cls, v: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Validate dataset inputs.\n\n    Parameters\n    ----------\n    v : xr.Dataset\n        Dataset to validate.\n\n    Returns\n    -------\n    xr.Dataset\n        Validated dataset.\n\n    Raises\n    ------\n    ValueError\n        If the input is not an xarray Dataset or lacks SNR.\n    \"\"\"\n    if not isinstance(v, xr.Dataset):\n        raise ValueError(\"Must be xr.Dataset\")\n    if \"SNR\" not in v.data_vars:\n        raise ValueError(\"Dataset must contain 'SNR' variable\")\n    return v\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.calculate_vod","level":3,"title":"<code>calculate_vod()</code>  <code>abstractmethod</code>","text":"<p>Calculate VOD and return a dataset with VOD, phi, theta.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.calculate_vod--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset containing VOD and angular coordinates.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>@abstractmethod\ndef calculate_vod(self) -&gt; xr.Dataset:\n    \"\"\"Calculate VOD and return a dataset with VOD, phi, theta.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing VOD and angular coordinates.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.from_icechunkstore","level":3,"title":"<code>from_icechunkstore(icechunk_store_pth, canopy_group='canopy_01', sky_group='reference_01', **open_kwargs)</code>  <code>classmethod</code>","text":"<p>Convenience method to calculate VOD directly from an IcechunkStore.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.from_icechunkstore--parameters","level":5,"title":"Parameters","text":"<p>icechunk_store_pth : Path     Path to Icechunk store. canopy_group : str     Canopy receiver group name. sky_group : str     Sky/reference receiver group name. open_kwargs : dict[str, Any]     Additional keyword arguments for IcechunkStore.open().     Currently unused.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.from_icechunkstore--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     VOD dataset.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.from_icechunkstore--notes","level":5,"title":"Notes","text":"<p>Requires canvod-store to be installed.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>@classmethod\ndef from_icechunkstore(\n    cls,\n    icechunk_store_pth: Path,\n    canopy_group: str = \"canopy_01\",\n    sky_group: str = \"reference_01\",\n    **open_kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"Convenience method to calculate VOD directly from an IcechunkStore.\n\n    Parameters\n    ----------\n    icechunk_store_pth : Path\n        Path to Icechunk store.\n    canopy_group : str\n        Canopy receiver group name.\n    sky_group : str\n        Sky/reference receiver group name.\n    open_kwargs : dict[str, Any]\n        Additional keyword arguments for IcechunkStore.open().\n        Currently unused.\n\n    Returns\n    -------\n    xr.Dataset\n        VOD dataset.\n\n    Notes\n    -----\n    Requires canvod-store to be installed.\n    \"\"\"\n    try:\n        from canvod.store import MyIcechunkStore\n    except ImportError as e:\n        raise ImportError(\n            \"canvod-store package required for from_icechunkstore(). \"\n            \"Install with: pip install canvod-store\"\n        ) from e\n\n    store = MyIcechunkStore(icechunk_store_pth)\n\n    with store.readonly_session() as session:\n        canopy_ds = xr.open_zarr(store=session.store, group=canopy_group)\n        sky_ds = xr.open_zarr(store=session.store, group=sky_group)\n\n    return cls.from_datasets(\n        canopy_ds=canopy_ds,\n        sky_ds=sky_ds,\n        align=True,\n    )\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.from_datasets","level":3,"title":"<code>from_datasets(canopy_ds, sky_ds, align=True)</code>  <code>classmethod</code>","text":"<p>Convenience method to calculate VOD directly from datasets.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.from_datasets--parameters","level":5,"title":"Parameters","text":"<p>canopy_ds : xr.Dataset     Canopy receiver dataset. sky_ds : xr.Dataset     Sky/reference receiver dataset. align : bool     Whether to align datasets on common coordinates.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.VODCalculator.from_datasets--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     VOD dataset.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>@classmethod\ndef from_datasets(\n    cls,\n    canopy_ds: xr.Dataset,\n    sky_ds: xr.Dataset,\n    align: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Convenience method to calculate VOD directly from datasets.\n\n    Parameters\n    ----------\n    canopy_ds : xr.Dataset\n        Canopy receiver dataset.\n    sky_ds : xr.Dataset\n        Sky/reference receiver dataset.\n    align : bool\n        Whether to align datasets on common coordinates.\n\n    Returns\n    -------\n    xr.Dataset\n        VOD dataset.\n    \"\"\"\n    if align:\n        canopy_ds, sky_ds = xr.align(canopy_ds, sky_ds, join=\"inner\")\n\n    calculator = cls(canopy_ds=canopy_ds, sky_ds=sky_ds)\n    return calculator.calculate_vod()\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.TauOmegaZerothOrder","level":2,"title":"<code>TauOmegaZerothOrder</code>","text":"<p>               Bases: <code>VODCalculator</code></p> <p>Calculate VOD using the zeroth-order Tau-Omega approximation.</p> <p>Based on Humphrey, V., &amp; Frankenberg, C. (2022).</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>class TauOmegaZerothOrder(VODCalculator):\n    \"\"\"Calculate VOD using the zeroth-order Tau-Omega approximation.\n\n    Based on Humphrey, V., &amp; Frankenberg, C. (2022).\n    \"\"\"\n\n    def get_delta_snr(self) -&gt; xr.DataArray:\n        \"\"\"Calculate delta SNR = SNR_canopy - SNR_sky.\n\n        Returns\n        -------\n        xr.DataArray\n            Delta SNR in decibels.\n        \"\"\"\n        return self.canopy_ds[\"SNR\"] - self.sky_ds[\"SNR\"]\n\n    def decibel2linear(self, delta_snr_db: xr.DataArray) -&gt; xr.DataArray:\n        \"\"\"Convert decibel values to linear values.\n\n        Parameters\n        ----------\n        delta_snr_db : xr.DataArray\n            Delta SNR in decibels.\n\n        Returns\n        -------\n        xr.DataArray\n            Linear-scale values.\n        \"\"\"\n        return np.power(10, delta_snr_db / 10)\n\n    def calculate_vod(self) -&gt; xr.Dataset:\n        \"\"\"Calculate VOD using the zeroth-order approximation.\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset containing VOD and angular coordinates.\n\n        Raises\n        ------\n        ValueError\n            If all delta SNR values are NaN.\n        \"\"\"\n        start_time = time.time()\n        log.info(\n            \"vod_calculation_started\",\n            canopy_epochs=len(self.canopy_ds.epoch),\n            sky_epochs=len(self.sky_ds.epoch),\n            sids=len(self.canopy_ds.sid),\n        )\n\n        delta_snr = self.get_delta_snr()\n\n        if delta_snr.isnull().all():\n            log.error(\n                \"vod_calculation_failed\",\n                reason=\"all_delta_snr_nan\",\n            )\n            raise ValueError(\n                \"All delta_snr values are NaN - check data alignment\",\n            )\n\n        canopy_transmissivity = self.decibel2linear(delta_snr)\n\n        if (canopy_transmissivity &lt;= 0).any():\n            n_invalid = (canopy_transmissivity &lt;= 0).sum().item()\n            total = canopy_transmissivity.size\n            print(\n                f\"Warning: {n_invalid}/{total} transmissivity values &lt;= 0 \"\n                \"(will produce NaN)\"\n            )\n            log.warning(\n                \"invalid_transmissivity\",\n                invalid_count=n_invalid,\n                total_count=total,\n                percent=round(100 * n_invalid / total, 2),\n            )\n\n        theta = self.canopy_ds[\"theta\"]\n        vod = -np.log(canopy_transmissivity) * np.cos(theta)\n\n        vod_ds = xr.Dataset(\n            {\n                \"VOD\": vod,\n                \"phi\": self.canopy_ds[\"phi\"],\n                \"theta\": self.canopy_ds[\"theta\"],\n            },\n            coords=self.canopy_ds.coords,\n        )\n\n        duration = time.time() - start_time\n        n_valid = (~vod.isnull()).sum().item()\n\n        log.info(\n            \"vod_calculation_complete\",\n            duration_seconds=round(duration, 2),\n            vod_values=vod.size,\n            valid_values=n_valid,\n            valid_percent=round(100 * n_valid / vod.size, 2),\n        )\n\n        return vod_ds\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.TauOmegaZerothOrder.get_delta_snr","level":3,"title":"<code>get_delta_snr()</code>","text":"<p>Calculate delta SNR = SNR_canopy - SNR_sky.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.TauOmegaZerothOrder.get_delta_snr--returns","level":5,"title":"Returns","text":"<p>xr.DataArray     Delta SNR in decibels.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>def get_delta_snr(self) -&gt; xr.DataArray:\n    \"\"\"Calculate delta SNR = SNR_canopy - SNR_sky.\n\n    Returns\n    -------\n    xr.DataArray\n        Delta SNR in decibels.\n    \"\"\"\n    return self.canopy_ds[\"SNR\"] - self.sky_ds[\"SNR\"]\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.TauOmegaZerothOrder.decibel2linear","level":3,"title":"<code>decibel2linear(delta_snr_db)</code>","text":"<p>Convert decibel values to linear values.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.TauOmegaZerothOrder.decibel2linear--parameters","level":5,"title":"Parameters","text":"<p>delta_snr_db : xr.DataArray     Delta SNR in decibels.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.TauOmegaZerothOrder.decibel2linear--returns","level":5,"title":"Returns","text":"<p>xr.DataArray     Linear-scale values.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>def decibel2linear(self, delta_snr_db: xr.DataArray) -&gt; xr.DataArray:\n    \"\"\"Convert decibel values to linear values.\n\n    Parameters\n    ----------\n    delta_snr_db : xr.DataArray\n        Delta SNR in decibels.\n\n    Returns\n    -------\n    xr.DataArray\n        Linear-scale values.\n    \"\"\"\n    return np.power(10, delta_snr_db / 10)\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.TauOmegaZerothOrder.calculate_vod","level":3,"title":"<code>calculate_vod()</code>","text":"<p>Calculate VOD using the zeroth-order approximation.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.TauOmegaZerothOrder.calculate_vod--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     Dataset containing VOD and angular coordinates.</p>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvod-vod/#canvod.vod.calculator.TauOmegaZerothOrder.calculate_vod--raises","level":5,"title":"Raises","text":"<p>ValueError     If all delta SNR values are NaN.</p> Source code in <code>packages/canvod-vod/src/canvod/vod/calculator.py</code> <pre><code>def calculate_vod(self) -&gt; xr.Dataset:\n    \"\"\"Calculate VOD using the zeroth-order approximation.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing VOD and angular coordinates.\n\n    Raises\n    ------\n    ValueError\n        If all delta SNR values are NaN.\n    \"\"\"\n    start_time = time.time()\n    log.info(\n        \"vod_calculation_started\",\n        canopy_epochs=len(self.canopy_ds.epoch),\n        sky_epochs=len(self.sky_ds.epoch),\n        sids=len(self.canopy_ds.sid),\n    )\n\n    delta_snr = self.get_delta_snr()\n\n    if delta_snr.isnull().all():\n        log.error(\n            \"vod_calculation_failed\",\n            reason=\"all_delta_snr_nan\",\n        )\n        raise ValueError(\n            \"All delta_snr values are NaN - check data alignment\",\n        )\n\n    canopy_transmissivity = self.decibel2linear(delta_snr)\n\n    if (canopy_transmissivity &lt;= 0).any():\n        n_invalid = (canopy_transmissivity &lt;= 0).sum().item()\n        total = canopy_transmissivity.size\n        print(\n            f\"Warning: {n_invalid}/{total} transmissivity values &lt;= 0 \"\n            \"(will produce NaN)\"\n        )\n        log.warning(\n            \"invalid_transmissivity\",\n            invalid_count=n_invalid,\n            total_count=total,\n            percent=round(100 * n_invalid / total, 2),\n        )\n\n    theta = self.canopy_ds[\"theta\"]\n    vod = -np.log(canopy_transmissivity) * np.cos(theta)\n\n    vod_ds = xr.Dataset(\n        {\n            \"VOD\": vod,\n            \"phi\": self.canopy_ds[\"phi\"],\n            \"theta\": self.canopy_ds[\"theta\"],\n        },\n        coords=self.canopy_ds.coords,\n    )\n\n    duration = time.time() - start_time\n    n_valid = (~vod.isnull()).sum().item()\n\n    log.info(\n        \"vod_calculation_complete\",\n        duration_seconds=round(duration, 2),\n        vod_values=vod.size,\n        valid_values=n_valid,\n        valid_percent=round(100 * n_valid / vod.size, 2),\n    )\n\n    return vod_ds\n</code></pre>","path":["API Reference","canvod.vod API Reference"],"tags":[]},{"location":"api/canvodpy/","level":1,"title":"canvodpy API Reference","text":"<p>Umbrella package providing a unified, four-level API for the canVODpy framework.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#level-1-convenience-functions","level":2,"title":"Level 1 — Convenience Functions","text":"<p>One-liner functions for quick exploration.</p> <p>Process RINEX data for one date (convenience function).</p> <p>This is the simplest way to process GNSS data - just provide the site name and date.</p> <p>Calculate VOD for a receiver pair (convenience function).</p> <p>This is the simplest way to calculate VOD - just provide site, receivers, and date.</p> <p>Preview processing plan for a site (convenience function).</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.process_date--parameters","level":3,"title":"Parameters","text":"<p>site : str     Site name (e.g., \"Rosalia\") date : str     Date in YYYYDOY format (e.g., \"2025001\") keep_vars : list[str], optional     RINEX variables to keep (default: KEEP_RNX_VARS) aux_agency : str, default \"COD\"     Analysis center for auxiliary data n_workers : int, default 12     Number of parallel workers</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.process_date--returns","level":3,"title":"Returns","text":"<p>dict[str, xr.Dataset]     Processed datasets for each receiver</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.process_date--examples","level":3,"title":"Examples","text":"<p>from canvodpy import process_date data = process_date(\"Rosalia\", \"2025001\") print(data.keys()) dict_keys(['canopy_01', 'canopy_02', 'reference_01'])</p> Source code in <code>canvodpy/src/canvodpy/api.py</code> <pre><code>def process_date(\n    site: str,\n    date: str,\n    keep_vars: list[str] | None = None,\n    aux_agency: str = \"COD\",\n    n_workers: int = 12,\n) -&gt; dict[str, xr.Dataset]:\n    \"\"\"Process RINEX data for one date (convenience function).\n\n    This is the simplest way to process GNSS data - just provide\n    the site name and date.\n\n    Parameters\n    ----------\n    site : str\n        Site name (e.g., \"Rosalia\")\n    date : str\n        Date in YYYYDOY format (e.g., \"2025001\")\n    keep_vars : list[str], optional\n        RINEX variables to keep (default: KEEP_RNX_VARS)\n    aux_agency : str, default \"COD\"\n        Analysis center for auxiliary data\n    n_workers : int, default 12\n        Number of parallel workers\n\n    Returns\n    -------\n    dict[str, xr.Dataset]\n        Processed datasets for each receiver\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvodpy import process_date\n    &gt;&gt;&gt; data = process_date(\"Rosalia\", \"2025001\")\n    &gt;&gt;&gt; print(data.keys())\n    dict_keys(['canopy_01', 'canopy_02', 'reference_01'])\n\n    &gt;&gt;&gt; # With custom settings\n    &gt;&gt;&gt; data = process_date(\n    ...     \"Rosalia\",\n    ...     \"2025001\",\n    ...     keep_vars=[\"C1C\", \"L1C\"],\n    ...     aux_agency=\"ESA\"\n    ... )\n\n    \"\"\"\n    pipeline = Pipeline(\n        site=site,\n        keep_vars=keep_vars,\n        aux_agency=aux_agency,\n        n_workers=n_workers,\n    )\n    return pipeline.process_date(date)\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.process_date--with-custom-settings","level":2,"title":"With custom settings","text":"<p>data = process_date( ...     \"Rosalia\", ...     \"2025001\", ...     keep_vars=[\"C1C\", \"L1C\"], ...     aux_agency=\"ESA\" ... )</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.calculate_vod--parameters","level":3,"title":"Parameters","text":"<p>site : str     Site name canopy : str     Canopy receiver name reference : str     Reference receiver name date : str     Date in YYYYDOY format keep_vars : list[str], optional     RINEX variables to keep aux_agency : str, default \"COD\"     Analysis center</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.calculate_vod--returns","level":3,"title":"Returns","text":"<p>xr.Dataset     VOD analysis results</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.calculate_vod--examples","level":3,"title":"Examples","text":"<p>from canvodpy import calculate_vod vod = calculate_vod( ...     site=\"Rosalia\", ...     canopy=\"canopy_01\", ...     reference=\"reference_01\", ...     date=\"2025001\" ... ) print(vod.vod.mean().values) 0.42</p> Source code in <code>canvodpy/src/canvodpy/api.py</code> <pre><code>def calculate_vod(\n    site: str,\n    canopy: str,\n    reference: str,\n    date: str,\n    keep_vars: list[str] | None = None,\n    aux_agency: str = \"COD\",\n) -&gt; xr.Dataset:\n    \"\"\"Calculate VOD for a receiver pair (convenience function).\n\n    This is the simplest way to calculate VOD - just provide\n    site, receivers, and date.\n\n    Parameters\n    ----------\n    site : str\n        Site name\n    canopy : str\n        Canopy receiver name\n    reference : str\n        Reference receiver name\n    date : str\n        Date in YYYYDOY format\n    keep_vars : list[str], optional\n        RINEX variables to keep\n    aux_agency : str, default \"COD\"\n        Analysis center\n\n    Returns\n    -------\n    xr.Dataset\n        VOD analysis results\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvodpy import calculate_vod\n    &gt;&gt;&gt; vod = calculate_vod(\n    ...     site=\"Rosalia\",\n    ...     canopy=\"canopy_01\",\n    ...     reference=\"reference_01\",\n    ...     date=\"2025001\"\n    ... )\n    &gt;&gt;&gt; print(vod.vod.mean().values)\n    0.42\n\n    \"\"\"\n    pipeline = Pipeline(\n        site=site,\n        keep_vars=keep_vars,\n        aux_agency=aux_agency,\n    )\n    return pipeline.calculate_vod(canopy, reference, date)\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.preview_processing--parameters","level":3,"title":"Parameters","text":"<p>site : str     Site name</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.preview_processing--returns","level":3,"title":"Returns","text":"<p>dict     Processing plan summary</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.preview_processing--examples","level":3,"title":"Examples","text":"<p>from canvodpy import preview_processing plan = preview_processing(\"Rosalia\") print(f\"Total files: {plan['total_files']}\") Total files: 8640</p> Source code in <code>canvodpy/src/canvodpy/api.py</code> <pre><code>def preview_processing(site: str) -&gt; dict:\n    \"\"\"Preview processing plan for a site (convenience function).\n\n    Parameters\n    ----------\n    site : str\n        Site name\n\n    Returns\n    -------\n    dict\n        Processing plan summary\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvodpy import preview_processing\n    &gt;&gt;&gt; plan = preview_processing(\"Rosalia\")\n    &gt;&gt;&gt; print(f\"Total files: {plan['total_files']}\")\n    Total files: 8640\n\n    \"\"\"\n    pipeline = Pipeline(site, dry_run=True)\n    return pipeline.preview()\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#level-2-fluent-workflow-deferred-execution","level":2,"title":"Level 2 — Fluent Workflow (Deferred Execution)","text":"<p>Chainable pipeline where steps are recorded and executed only when a terminal method (<code>.result()</code>, <code>.to_store()</code>, <code>.plot()</code>) is called.</p> <p>VOD workflow orchestration using component factories.</p> <p>Provides high-level workflow coordination with structured logging and extensibility through factory pattern.</p> <p>Chainable, deferred-execution workflow for VOD analysis.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow--examples","level":3,"title":"Examples","text":"<p>Basic workflow:</p> <pre><code>&gt;&gt;&gt; from canvodpy import VODWorkflow\n&gt;&gt;&gt; workflow = VODWorkflow(site=\"Rosalia\")\n&gt;&gt;&gt; result = workflow.process_date(\"2025001\")\n</code></pre> <p>With custom components:</p> <pre><code>&gt;&gt;&gt; workflow = VODWorkflow(\n...     site=\"Rosalia\",\n...     grid=\"equal_area\",\n...     grid_params={\"angular_resolution\": 5.0},\n... )\n&gt;&gt;&gt; vod = workflow.calculate_vod(\"canopy_01\", \"reference_01\", \"2025001\")\n</code></pre> <p>Debug logging:</p> <pre><code>&gt;&gt;&gt; workflow = VODWorkflow(site=\"Rosalia\", log_level=\"DEBUG\")\n&gt;&gt;&gt; # All logs include site=\"Rosalia\" context\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow","level":2,"title":"<code>VODWorkflow</code>","text":"<p>Orchestrate complete VOD analysis workflow.</p> <p>Uses component factories for extensibility and structured logging for LLM-assisted debugging. Replaces legacy Site + Pipeline pattern.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow--parameters","level":4,"title":"Parameters","text":"<p>site : str or Site     Site name or Site object reader : str, default=\"rinex3\"     Registered reader name grid : str, default=\"equal_area\"     Registered grid name vod_calculator : str, default=\"tau_omega\"     Registered VOD calculator name grid_params : dict, optional     Parameters for grid creation (angular_resolution, cutoff_theta) keep_vars : list[str], optional     RINEX variables to keep. Defaults to KEEP_RNX_VARS. log_level : str, default=\"INFO\"     Logging level (DEBUG, INFO, WARNING, ERROR)</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow--attributes","level":4,"title":"Attributes","text":"<p>site : Site     Site configuration grid : GridData     Hemisphere grid structure log : BoundLogger     Structured logger with site context</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow--examples","level":4,"title":"Examples","text":"<p>Process one date:</p> <pre><code>&gt;&gt;&gt; workflow = VODWorkflow(site=\"Rosalia\")\n&gt;&gt;&gt; data = workflow.process_date(\"2025001\")\n&gt;&gt;&gt; print(data.keys())\ndict_keys(['canopy_01', 'reference_01'])\n</code></pre> <p>Calculate VOD:</p> <pre><code>&gt;&gt;&gt; vod = workflow.calculate_vod(\n...     \"canopy_01\", \"reference_01\", \"2025001\"\n... )\n&gt;&gt;&gt; print(vod.VOD.mean().item())\n0.42\n</code></pre> <p>Custom grid:</p> <pre><code>&gt;&gt;&gt; workflow = VODWorkflow(\n...     site=\"Rosalia\",\n...     grid=\"equal_area\",\n...     grid_params={\"angular_resolution\": 5.0},\n... )\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow--notes","level":4,"title":"Notes","text":"<p>Preserves logical flow from legacy API: Site → Pipeline → process_date → load → augment → grid → calculate</p> Source code in <code>canvodpy/src/canvodpy/workflow.py</code> <pre><code>class VODWorkflow:\n    \"\"\"\n    Orchestrate complete VOD analysis workflow.\n\n    Uses component factories for extensibility and structured logging\n    for LLM-assisted debugging. Replaces legacy Site + Pipeline pattern.\n\n    Parameters\n    ----------\n    site : str or Site\n        Site name or Site object\n    reader : str, default=\"rinex3\"\n        Registered reader name\n    grid : str, default=\"equal_area\"\n        Registered grid name\n    vod_calculator : str, default=\"tau_omega\"\n        Registered VOD calculator name\n    grid_params : dict, optional\n        Parameters for grid creation (angular_resolution, cutoff_theta)\n    keep_vars : list[str], optional\n        RINEX variables to keep. Defaults to KEEP_RNX_VARS.\n    log_level : str, default=\"INFO\"\n        Logging level (DEBUG, INFO, WARNING, ERROR)\n\n    Attributes\n    ----------\n    site : Site\n        Site configuration\n    grid : GridData\n        Hemisphere grid structure\n    log : BoundLogger\n        Structured logger with site context\n\n    Examples\n    --------\n    Process one date:\n\n        &gt;&gt;&gt; workflow = VODWorkflow(site=\"Rosalia\")\n        &gt;&gt;&gt; data = workflow.process_date(\"2025001\")\n        &gt;&gt;&gt; print(data.keys())\n        dict_keys(['canopy_01', 'reference_01'])\n\n    Calculate VOD:\n\n        &gt;&gt;&gt; vod = workflow.calculate_vod(\n        ...     \"canopy_01\", \"reference_01\", \"2025001\"\n        ... )\n        &gt;&gt;&gt; print(vod.VOD.mean().item())\n        0.42\n\n    Custom grid:\n\n        &gt;&gt;&gt; workflow = VODWorkflow(\n        ...     site=\"Rosalia\",\n        ...     grid=\"equal_area\",\n        ...     grid_params={\"angular_resolution\": 5.0},\n        ... )\n\n    Notes\n    -----\n    Preserves logical flow from legacy API:\n    Site → Pipeline → process_date → load → augment → grid → calculate\n    \"\"\"\n\n    def __init__(\n        self,\n        site: str | Site,\n        reader: str = \"rinex3\",\n        grid: str = \"equal_area\",\n        vod_calculator: str = \"tau_omega\",\n        grid_params: dict[str, Any] | None = None,\n        keep_vars: list[str] | None = None,\n        log_level: str = \"INFO\",\n    ) -&gt; None:\n        \"\"\"Initialize workflow with component factories.\"\"\"\n        # Handle site input\n        site_name = site if isinstance(site, str) else site.name\n        self.site = Site(site) if isinstance(site, str) else site\n\n        # Setup logging with site context\n        self.log: BoundLogger = get_logger(__name__).bind(site=site_name)\n\n        # Store configuration\n        self.reader_name = reader\n        self.grid_name = grid\n        self.vod_calculator_name = vod_calculator\n        if keep_vars is None:\n            from canvod.utils.config import load_config\n\n            keep_vars = load_config().processing.processing.keep_rnx_vars\n        self.keep_vars = keep_vars\n\n        # Create grid using factory (cached for workflow)\n        grid_params = grid_params or {}\n        builder = GridFactory.create(grid, **grid_params)\n        self.grid = builder.build()\n\n        self.log.info(\n            \"workflow_initialized\",\n            grid=grid,\n            reader=reader,\n            calculator=vod_calculator,\n            ncells=self.grid.ncells,\n        )\n\n    def process_date(\n        self,\n        date: str,\n        receivers: list[str] | None = None,\n    ) -&gt; dict[str, xr.Dataset]:\n        \"\"\"\n        Process RINEX data for one date.\n\n        Workflow: load_rinex → augment → assign_grid → store\n\n        Parameters\n        ----------\n        date : str\n            Date in YYYYDOY format (e.g., \"2025001\" = Jan 1, 2025)\n        receivers : list[str], optional\n            Receiver names to process. If None, use all active receivers.\n\n        Returns\n        -------\n        dict[str, xr.Dataset]\n            Processed datasets keyed by receiver name. Each dataset has\n            cell assignments and filtered data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; workflow = VODWorkflow(\"Rosalia\")\n        &gt;&gt;&gt; data = workflow.process_date(\"2025001\")\n        &gt;&gt;&gt; canopy = data[\"canopy_01\"]\n        &gt;&gt;&gt; print(canopy.sizes)\n        {'epoch': 2880, 'sv': 32, 'cell': 324}\n\n        Process specific receivers:\n\n        &gt;&gt;&gt; data = workflow.process_date(\n        ...     \"2025001\",\n        ...     receivers=[\"canopy_01\"]\n        ... )\n\n        Notes\n        -----\n        This is the main entry point for data processing. Orchestrates\n        the full pipeline from raw RINEX to grid-assigned observations.\n        \"\"\"\n        log = self.log.bind(date=date)\n        log.info(\"process_date_started\")\n\n        # Get receivers to process\n        receiver_list = receivers or list(self.site.active_receivers.keys())\n        results = {}\n\n        for recv_name in receiver_list:\n            recv_log = log.bind(receiver=recv_name)\n            recv_log.info(\"processing_receiver\")\n\n            try:\n                # Step 1: Load RINEX\n                ds = self._load_rinex(recv_name, date, recv_log)\n\n                # Step 2: Augment (preprocessing)\n                ds = self._augment_data(ds, recv_log)\n\n                # Step 3: Assign grid cells\n                ds = self._assign_grid_cells(ds, recv_log)\n\n                results[recv_name] = ds\n                recv_log.info(\n                    \"processing_complete\",\n                    variables=list(ds.data_vars),\n                    cells=ds.sizes.get(\"cell\", 0),\n                )\n\n            except Exception as e:\n                recv_log.error(\"processing_failed\", error=str(e), exc_info=True)\n                raise\n\n        log.info(\"process_date_complete\", receivers=len(results))\n        return results\n\n    def calculate_vod(\n        self,\n        canopy_receiver: str,\n        sky_receiver: str,\n        date: str,\n        use_cached: bool = True,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Calculate VOD from canopy and sky receivers.\n\n        Parameters\n        ----------\n        canopy_receiver : str\n            Receiver under canopy (e.g., \"canopy_01\")\n        sky_receiver : str\n            Sky reference receiver (e.g., \"reference_01\")\n        date : str\n            Date in YYYYDOY format\n        use_cached : bool, default=True\n            If True, reuse datasets from process_date cache\n\n        Returns\n        -------\n        xr.Dataset\n            VOD dataset with variables:\n            - VOD : Vegetation Optical Depth\n            - phi : Azimuth angles\n            - theta : Elevation angles\n\n        Examples\n        --------\n        &gt;&gt;&gt; workflow = VODWorkflow(\"Rosalia\")\n        &gt;&gt;&gt; vod = workflow.calculate_vod(\n        ...     \"canopy_01\", \"reference_01\", \"2025001\"\n        ... )\n        &gt;&gt;&gt; print(vod.VOD.mean().item())\n        0.42\n\n        Without caching:\n\n        &gt;&gt;&gt; vod = workflow.calculate_vod(\n        ...     \"canopy_01\",\n        ...     \"reference_01\",\n        ...     \"2025001\",\n        ...     use_cached=False,\n        ... )\n\n        Notes\n        -----\n        Uses VODFactory to create calculator. Supports community\n        extensions via factory registration.\n        \"\"\"\n        log = self.log.bind(date=date, canopy=canopy_receiver, sky=sky_receiver)\n        log.info(\"calculate_vod_started\")\n\n        # Load or retrieve datasets\n        if use_cached and hasattr(self, \"_dataset_cache\"):\n            canopy_ds = self._dataset_cache.get(canopy_receiver)\n            sky_ds = self._dataset_cache.get(sky_receiver)\n        else:\n            canopy_ds = None\n            sky_ds = None\n\n        # Process if not cached\n        if canopy_ds is None:\n            log.debug(\"loading_canopy\")\n            data = self.process_date(date, receivers=[canopy_receiver])\n            canopy_ds = data[canopy_receiver]\n\n        if sky_ds is None:\n            log.debug(\"loading_sky\")\n            data = self.process_date(date, receivers=[sky_receiver])\n            sky_ds = data[sky_receiver]\n\n        # Use factory to create calculator\n        calculator = VODFactory.create(\n            self.vod_calculator_name,\n            canopy_ds=canopy_ds,\n            sky_ds=sky_ds,\n        )\n\n        # Calculate VOD\n        vod_ds = calculator.calculate_vod()\n\n        log.info(\n            \"calculate_vod_complete\",\n            cells=vod_ds.sizes.get(\"cell\", 0) if \"cell\" in vod_ds.sizes else 0,\n            vod_mean=float(vod_ds.VOD.mean().item()) if \"VOD\" in vod_ds else None,\n        )\n        return vod_ds\n\n    def _load_rinex(self, receiver: str, date: str, log: BoundLogger) -&gt; xr.Dataset:\n        \"\"\"\n        Load RINEX using factory.\n\n        Parameters\n        ----------\n        receiver : str\n            Receiver name\n        date : str\n            Date in YYYYDOY format\n        log : BoundLogger\n            Logger with receiver context\n\n        Returns\n        -------\n        xr.Dataset\n            RINEX data\n\n        Notes\n        -----\n        Uses ReaderFactory to create reader instance.\n        \"\"\"\n        log.debug(\"load_rinex_started\")\n\n        # Get file path from site config\n        rinex_path = self._get_rinex_path(receiver, date)\n\n        # Create reader using factory\n        reader = ReaderFactory.create(\n            self.reader_name,\n            path=rinex_path,\n        )\n\n        # Read data\n        ds = reader.read()\n\n        # Filter variables\n        if self.keep_vars:\n            keep_vars_set = set(self.keep_vars)\n            drop_vars = [v for v in ds.data_vars if v not in keep_vars_set]\n            if drop_vars:\n                ds = ds.drop_vars(drop_vars)\n\n        log.debug(\"load_rinex_complete\", variables=list(ds.data_vars))\n        return ds\n\n    def _augment_data(self, ds: xr.Dataset, log: BoundLogger) -&gt; xr.Dataset:\n        \"\"\"\n        Apply augmentation steps.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Input dataset\n        log : BoundLogger\n            Logger with context\n\n        Returns\n        -------\n        xr.Dataset\n            Augmented dataset\n\n        Notes\n        -----\n        Currently passes through. Will use AugmentationFactory\n        for preprocessing steps (filtering, interpolation, etc.)\n        \"\"\"\n        log.debug(\"augment_data_started\")\n\n        # TODO: Use AugmentationFactory for preprocessing\n        # For now, pass through\n        # augmentations = [\n        #     AugmentationFactory.create(\"hampel\", window=5),\n        #     AugmentationFactory.create(\"interpolate\", method=\"linear\"),\n        # ]\n        # for aug in augmentations:\n        #     ds = aug.apply(ds)\n\n        log.debug(\"augment_data_complete\")\n        return ds\n\n    def _assign_grid_cells(self, ds: xr.Dataset, log: BoundLogger) -&gt; xr.Dataset:\n        \"\"\"\n        Assign grid cells to observations.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset with phi, theta coordinates\n        log : BoundLogger\n            Logger with context\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with added 'cell' coordinate\n\n        Notes\n        -----\n        Uses grid operations from canvod.grids package.\n        \"\"\"\n        log.debug(\"assign_grid_cells_started\")\n\n        # Use grid operations\n        from canvod.grids import add_cell_ids_to_ds_fast\n\n        ds_with_cells = add_cell_ids_to_ds_fast(ds, self.grid)\n\n        log.debug(\n            \"assign_grid_cells_complete\",\n            cells=ds_with_cells.sizes.get(\"cell\", 0),\n        )\n        return ds_with_cells\n\n    def _get_rinex_path(self, receiver: str, date: str) -&gt; Path:\n        \"\"\"\n        Get RINEX file path from site configuration.\n\n        Parameters\n        ----------\n        receiver : str\n            Receiver name\n        date : str\n            Date in YYYYDOY format\n\n        Returns\n        -------\n        Path\n            Path to RINEX file\n\n        Raises\n        ------\n        ValueError\n            If receiver not found in site configuration\n        \"\"\"\n        if receiver not in self.site.receivers:\n            available = list(self.site.receivers.keys())\n            msg = f\"Receiver '{receiver}' not in site. Available: {available}\"\n            raise ValueError(msg)\n\n        # Use internal site object to get path\n        return self.site._site.get_rinex_path(receiver, date)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation.\"\"\"\n        return (\n            f\"VODWorkflow(site='{self.site.name}', \"\n            f\"grid='{self.grid_name}', reader='{self.reader_name}')\"\n        )\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.__init__","level":3,"title":"<code>__init__(site, reader='rinex3', grid='equal_area', vod_calculator='tau_omega', grid_params=None, keep_vars=None, log_level='INFO')</code>","text":"<p>Initialize workflow with component factories.</p> Source code in <code>canvodpy/src/canvodpy/workflow.py</code> <pre><code>def __init__(\n    self,\n    site: str | Site,\n    reader: str = \"rinex3\",\n    grid: str = \"equal_area\",\n    vod_calculator: str = \"tau_omega\",\n    grid_params: dict[str, Any] | None = None,\n    keep_vars: list[str] | None = None,\n    log_level: str = \"INFO\",\n) -&gt; None:\n    \"\"\"Initialize workflow with component factories.\"\"\"\n    # Handle site input\n    site_name = site if isinstance(site, str) else site.name\n    self.site = Site(site) if isinstance(site, str) else site\n\n    # Setup logging with site context\n    self.log: BoundLogger = get_logger(__name__).bind(site=site_name)\n\n    # Store configuration\n    self.reader_name = reader\n    self.grid_name = grid\n    self.vod_calculator_name = vod_calculator\n    if keep_vars is None:\n        from canvod.utils.config import load_config\n\n        keep_vars = load_config().processing.processing.keep_rnx_vars\n    self.keep_vars = keep_vars\n\n    # Create grid using factory (cached for workflow)\n    grid_params = grid_params or {}\n    builder = GridFactory.create(grid, **grid_params)\n    self.grid = builder.build()\n\n    self.log.info(\n        \"workflow_initialized\",\n        grid=grid,\n        reader=reader,\n        calculator=vod_calculator,\n        ncells=self.grid.ncells,\n    )\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.process_date","level":3,"title":"<code>process_date(date, receivers=None)</code>","text":"<p>Process RINEX data for one date.</p> <p>Workflow: load_rinex → augment → assign_grid → store</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.process_date--parameters","level":5,"title":"Parameters","text":"<p>date : str     Date in YYYYDOY format (e.g., \"2025001\" = Jan 1, 2025) receivers : list[str], optional     Receiver names to process. If None, use all active receivers.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.process_date--returns","level":5,"title":"Returns","text":"<p>dict[str, xr.Dataset]     Processed datasets keyed by receiver name. Each dataset has     cell assignments and filtered data.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.process_date--examples","level":5,"title":"Examples","text":"<p>workflow = VODWorkflow(\"Rosalia\") data = workflow.process_date(\"2025001\") canopy = data[\"canopy_01\"] print(canopy.sizes)</p> <p>Process specific receivers:</p> <p>data = workflow.process_date( ...     \"2025001\", ...     receivers=[\"canopy_01\"] ... )</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.process_date--notes","level":5,"title":"Notes","text":"<p>This is the main entry point for data processing. Orchestrates the full pipeline from raw RINEX to grid-assigned observations.</p> Source code in <code>canvodpy/src/canvodpy/workflow.py</code> <pre><code>def process_date(\n    self,\n    date: str,\n    receivers: list[str] | None = None,\n) -&gt; dict[str, xr.Dataset]:\n    \"\"\"\n    Process RINEX data for one date.\n\n    Workflow: load_rinex → augment → assign_grid → store\n\n    Parameters\n    ----------\n    date : str\n        Date in YYYYDOY format (e.g., \"2025001\" = Jan 1, 2025)\n    receivers : list[str], optional\n        Receiver names to process. If None, use all active receivers.\n\n    Returns\n    -------\n    dict[str, xr.Dataset]\n        Processed datasets keyed by receiver name. Each dataset has\n        cell assignments and filtered data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; workflow = VODWorkflow(\"Rosalia\")\n    &gt;&gt;&gt; data = workflow.process_date(\"2025001\")\n    &gt;&gt;&gt; canopy = data[\"canopy_01\"]\n    &gt;&gt;&gt; print(canopy.sizes)\n    {'epoch': 2880, 'sv': 32, 'cell': 324}\n\n    Process specific receivers:\n\n    &gt;&gt;&gt; data = workflow.process_date(\n    ...     \"2025001\",\n    ...     receivers=[\"canopy_01\"]\n    ... )\n\n    Notes\n    -----\n    This is the main entry point for data processing. Orchestrates\n    the full pipeline from raw RINEX to grid-assigned observations.\n    \"\"\"\n    log = self.log.bind(date=date)\n    log.info(\"process_date_started\")\n\n    # Get receivers to process\n    receiver_list = receivers or list(self.site.active_receivers.keys())\n    results = {}\n\n    for recv_name in receiver_list:\n        recv_log = log.bind(receiver=recv_name)\n        recv_log.info(\"processing_receiver\")\n\n        try:\n            # Step 1: Load RINEX\n            ds = self._load_rinex(recv_name, date, recv_log)\n\n            # Step 2: Augment (preprocessing)\n            ds = self._augment_data(ds, recv_log)\n\n            # Step 3: Assign grid cells\n            ds = self._assign_grid_cells(ds, recv_log)\n\n            results[recv_name] = ds\n            recv_log.info(\n                \"processing_complete\",\n                variables=list(ds.data_vars),\n                cells=ds.sizes.get(\"cell\", 0),\n            )\n\n        except Exception as e:\n            recv_log.error(\"processing_failed\", error=str(e), exc_info=True)\n            raise\n\n    log.info(\"process_date_complete\", receivers=len(results))\n    return results\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.calculate_vod","level":3,"title":"<code>calculate_vod(canopy_receiver, sky_receiver, date, use_cached=True)</code>","text":"<p>Calculate VOD from canopy and sky receivers.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.calculate_vod--parameters","level":5,"title":"Parameters","text":"<p>canopy_receiver : str     Receiver under canopy (e.g., \"canopy_01\") sky_receiver : str     Sky reference receiver (e.g., \"reference_01\") date : str     Date in YYYYDOY format use_cached : bool, default=True     If True, reuse datasets from process_date cache</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.calculate_vod--returns","level":5,"title":"Returns","text":"<p>xr.Dataset     VOD dataset with variables:     - VOD : Vegetation Optical Depth     - phi : Azimuth angles     - theta : Elevation angles</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.calculate_vod--examples","level":5,"title":"Examples","text":"<p>workflow = VODWorkflow(\"Rosalia\") vod = workflow.calculate_vod( ...     \"canopy_01\", \"reference_01\", \"2025001\" ... ) print(vod.VOD.mean().item()) 0.42</p> <p>Without caching:</p> <p>vod = workflow.calculate_vod( ...     \"canopy_01\", ...     \"reference_01\", ...     \"2025001\", ...     use_cached=False, ... )</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.calculate_vod--notes","level":5,"title":"Notes","text":"<p>Uses VODFactory to create calculator. Supports community extensions via factory registration.</p> Source code in <code>canvodpy/src/canvodpy/workflow.py</code> <pre><code>def calculate_vod(\n    self,\n    canopy_receiver: str,\n    sky_receiver: str,\n    date: str,\n    use_cached: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Calculate VOD from canopy and sky receivers.\n\n    Parameters\n    ----------\n    canopy_receiver : str\n        Receiver under canopy (e.g., \"canopy_01\")\n    sky_receiver : str\n        Sky reference receiver (e.g., \"reference_01\")\n    date : str\n        Date in YYYYDOY format\n    use_cached : bool, default=True\n        If True, reuse datasets from process_date cache\n\n    Returns\n    -------\n    xr.Dataset\n        VOD dataset with variables:\n        - VOD : Vegetation Optical Depth\n        - phi : Azimuth angles\n        - theta : Elevation angles\n\n    Examples\n    --------\n    &gt;&gt;&gt; workflow = VODWorkflow(\"Rosalia\")\n    &gt;&gt;&gt; vod = workflow.calculate_vod(\n    ...     \"canopy_01\", \"reference_01\", \"2025001\"\n    ... )\n    &gt;&gt;&gt; print(vod.VOD.mean().item())\n    0.42\n\n    Without caching:\n\n    &gt;&gt;&gt; vod = workflow.calculate_vod(\n    ...     \"canopy_01\",\n    ...     \"reference_01\",\n    ...     \"2025001\",\n    ...     use_cached=False,\n    ... )\n\n    Notes\n    -----\n    Uses VODFactory to create calculator. Supports community\n    extensions via factory registration.\n    \"\"\"\n    log = self.log.bind(date=date, canopy=canopy_receiver, sky=sky_receiver)\n    log.info(\"calculate_vod_started\")\n\n    # Load or retrieve datasets\n    if use_cached and hasattr(self, \"_dataset_cache\"):\n        canopy_ds = self._dataset_cache.get(canopy_receiver)\n        sky_ds = self._dataset_cache.get(sky_receiver)\n    else:\n        canopy_ds = None\n        sky_ds = None\n\n    # Process if not cached\n    if canopy_ds is None:\n        log.debug(\"loading_canopy\")\n        data = self.process_date(date, receivers=[canopy_receiver])\n        canopy_ds = data[canopy_receiver]\n\n    if sky_ds is None:\n        log.debug(\"loading_sky\")\n        data = self.process_date(date, receivers=[sky_receiver])\n        sky_ds = data[sky_receiver]\n\n    # Use factory to create calculator\n    calculator = VODFactory.create(\n        self.vod_calculator_name,\n        canopy_ds=canopy_ds,\n        sky_ds=sky_ds,\n    )\n\n    # Calculate VOD\n    vod_ds = calculator.calculate_vod()\n\n    log.info(\n        \"calculate_vod_complete\",\n        cells=vod_ds.sizes.get(\"cell\", 0) if \"cell\" in vod_ds.sizes else 0,\n        vod_mean=float(vod_ds.VOD.mean().item()) if \"VOD\" in vod_ds else None,\n    )\n    return vod_ds\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.workflow.VODWorkflow.__repr__","level":3,"title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>canvodpy/src/canvodpy/workflow.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    return (\n        f\"VODWorkflow(site='{self.site.name}', \"\n        f\"grid='{self.grid_name}', reader='{self.reader_name}')\"\n    )\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow--parameters","level":3,"title":"Parameters","text":"<p>site : str or Site     Site name or :class:<code>~canvodpy.api.Site</code> object. reader : str     Registered reader name (default <code>\"rinex3\"</code>). grid_type : str     Registered grid type (default <code>\"equal_area\"</code>). vod_calculator : str     Registered VOD calculator (default <code>\"tau_omega\"</code>). keep_vars : list[str], optional     RINEX variables to retain.  Defaults to :data:<code>KEEP_RNX_VARS</code>.</p> Source code in <code>canvodpy/src/canvodpy/fluent.py</code> <pre><code>class FluentWorkflow:\n    \"\"\"Chainable, deferred-execution workflow for VOD analysis.\n\n    Parameters\n    ----------\n    site : str or Site\n        Site name or :class:`~canvodpy.api.Site` object.\n    reader : str\n        Registered reader name (default ``\"rinex3\"``).\n    grid_type : str\n        Registered grid type (default ``\"equal_area\"``).\n    vod_calculator : str\n        Registered VOD calculator (default ``\"tau_omega\"``).\n    keep_vars : list[str], optional\n        RINEX variables to retain.  Defaults to :data:`KEEP_RNX_VARS`.\n    \"\"\"\n\n    def __init__(\n        self,\n        site: str | Site,\n        reader: str = \"rinex3\",\n        grid_type: str = \"equal_area\",\n        vod_calculator: str = \"tau_omega\",\n        keep_vars: list[str] | None = None,\n    ) -&gt; None:\n        self._plan: list[tuple] = []\n\n        # State populated by steps\n        self._datasets: dict[str, xr.Dataset] = {}\n        self._vod_result: xr.Dataset | None = None\n        self._grid: Any = None\n\n        # Configuration\n        self._site = Site(site) if isinstance(site, str) else site\n        self._reader_name = reader\n        self._grid_type = grid_type\n        self._vod_calculator_name = vod_calculator\n        if keep_vars is None:\n            from canvod.utils.config import load_config\n\n            keep_vars = load_config().processing.processing.keep_rnx_vars\n        self._keep_vars = keep_vars\n\n        self.log = get_logger(__name__).bind(site=self._site.name)\n\n    # ------------------------------------------------------------------\n    # Steps (deferred)\n    # ------------------------------------------------------------------\n\n    @step\n    def read(self, date: str, receivers: list[str] | None = None) -&gt; FluentWorkflow:\n        \"\"\"Load RINEX observations for *date*.\n\n        Parameters\n        ----------\n        date : str\n            Date in ``YYYYDOY`` format (e.g. ``\"2025001\"``).\n        receivers : list[str], optional\n            Receiver names to load.  If ``None``, all active receivers\n            for the site are loaded.\n        \"\"\"\n        receiver_list = receivers or list(self._site.active_receivers.keys())\n        log = self.log.bind(date=date)\n\n        for name in receiver_list:\n            path = self._site._site.get_rinex_path(name, date)\n            reader_obj = ReaderFactory.create(self._reader_name, path=path)\n            ds = reader_obj.read()\n\n            # Filter variables\n            if self._keep_vars:\n                drop = [v for v in ds.data_vars if v not in set(self._keep_vars)]\n                if drop:\n                    ds = ds.drop_vars(drop)\n\n            self._datasets[name] = ds\n            log.info(\"read_complete\", receiver=name)\n\n        return self  # never reached (decorator returns self), but aids type checkers\n\n    @step\n    def preprocess(self, agency: str = \"COD\") -&gt; FluentWorkflow:\n        \"\"\"Apply auxiliary preprocessing to loaded datasets.\n\n        Parameters\n        ----------\n        agency : str\n            Analysis centre for auxiliary products (default ``\"COD\"``).\n        \"\"\"\n        log = self.log.bind(agency=agency)\n\n        for name, ds in self._datasets.items():\n            try:\n                from canvod.auxiliary import preprocess_aux_for_interpolation\n\n                ds = preprocess_aux_for_interpolation(ds)\n                self._datasets[name] = ds\n                log.info(\"preprocess_complete\", receiver=name)\n            except ImportError:\n                log.debug(\"canvod.auxiliary not available, skipping preprocessing\")\n\n        return self\n\n    @step\n    def grid(self, kind: str | None = None, **params: Any) -&gt; FluentWorkflow:\n        \"\"\"Build a hemisphere grid and assign cell IDs to all datasets.\n\n        Parameters\n        ----------\n        kind : str, optional\n            Grid type override.  Defaults to the value set at init.\n        **params\n            Passed to :meth:`GridFactory.create` (e.g.\n            ``angular_resolution=5.0``).\n        \"\"\"\n        from canvod.grids import add_cell_ids_to_ds_fast\n\n        grid_type = kind or self._grid_type\n        builder = GridFactory.create(grid_type, **params)\n        self._grid = builder.build()\n\n        for name, ds in self._datasets.items():\n            self._datasets[name] = add_cell_ids_to_ds_fast(ds, self._grid)\n\n        self.log.info(\"grid_complete\", grid=grid_type, ncells=self._grid.ncells)\n        return self\n\n    @step\n    def vod(self, canopy: str, reference: str) -&gt; FluentWorkflow:\n        \"\"\"Compute vegetation optical depth for a receiver pair.\n\n        Parameters\n        ----------\n        canopy : str\n            Canopy receiver name (e.g. ``\"canopy_01\"``).\n        reference : str\n            Sky/reference receiver name (e.g. ``\"reference_01\"``).\n        \"\"\"\n        canopy_ds = self._datasets[canopy]\n        ref_ds = self._datasets[reference]\n\n        calculator = VODFactory.create(\n            self._vod_calculator_name,\n            canopy_ds=canopy_ds,\n            sky_ds=ref_ds,\n        )\n        self._vod_result = calculator.calculate_vod()\n\n        self.log.info(\"vod_complete\", canopy=canopy, reference=reference)\n        return self\n\n    # ------------------------------------------------------------------\n    # Terminals (trigger execution)\n    # ------------------------------------------------------------------\n\n    @terminal\n    def result(self) -&gt; xr.Dataset | dict[str, xr.Dataset]:\n        \"\"\"Execute the plan and return the final data.\n\n        Returns the VOD dataset if a ``.vod()`` step was included,\n        otherwise returns the dict of per-receiver datasets.\n        \"\"\"\n        if self._vod_result is not None:\n            return self._vod_result\n        return dict(self._datasets)\n\n    @terminal\n    def to_store(self) -&gt; FluentWorkflow:\n        \"\"\"Execute the plan and write results to Icechunk storage.\"\"\"\n        if self._vod_result is not None:\n            # Store VOD result — requires a store name convention\n            self.log.info(\"to_store_vod\")\n            self._site.vod_store.write_group(\"vod_result\", self._vod_result)\n        else:\n            for name, ds in self._datasets.items():\n                self.log.info(\"to_store_dataset\", receiver=name)\n                self._site.rinex_store.write_group(name, ds)\n        return self\n\n    @terminal\n    def plot(self) -&gt; Any:\n        \"\"\"Execute the plan and visualise the result.\"\"\"\n        from canvod.viz import HemisphereVisualizer\n\n        data = self._vod_result if self._vod_result is not None else self._datasets\n        viz = HemisphereVisualizer()\n        return viz.plot_2d(data)\n\n    # ------------------------------------------------------------------\n    # Plan inspection (does NOT execute)\n    # ------------------------------------------------------------------\n\n    def explain(self) -&gt; list[dict[str, Any]]:\n        \"\"\"Return a description of the recorded plan without executing it.\n\n        Returns\n        -------\n        list[dict]\n            One entry per step with keys ``\"step\"``, ``\"args\"``, and\n            ``\"kwargs\"``.\n        \"\"\"\n        return [\n            {\"step\": fn.__name__, \"args\": args, \"kwargs\": kwargs}\n            for fn, args, kwargs in self._plan\n        ]\n\n    # ------------------------------------------------------------------\n    # Dunder\n    # ------------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        n = len(self._plan)\n        return f\"FluentWorkflow(site={self._site.name!r}, pending_steps={n})\"\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.read","level":2,"title":"<code>read(date, receivers=None)</code>","text":"<p>Load RINEX observations for date.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.read--parameters","level":4,"title":"Parameters","text":"<p>date : str     Date in <code>YYYYDOY</code> format (e.g. <code>\"2025001\"</code>). receivers : list[str], optional     Receiver names to load.  If <code>None</code>, all active receivers     for the site are loaded.</p> Source code in <code>canvodpy/src/canvodpy/fluent.py</code> <pre><code>@step\ndef read(self, date: str, receivers: list[str] | None = None) -&gt; FluentWorkflow:\n    \"\"\"Load RINEX observations for *date*.\n\n    Parameters\n    ----------\n    date : str\n        Date in ``YYYYDOY`` format (e.g. ``\"2025001\"``).\n    receivers : list[str], optional\n        Receiver names to load.  If ``None``, all active receivers\n        for the site are loaded.\n    \"\"\"\n    receiver_list = receivers or list(self._site.active_receivers.keys())\n    log = self.log.bind(date=date)\n\n    for name in receiver_list:\n        path = self._site._site.get_rinex_path(name, date)\n        reader_obj = ReaderFactory.create(self._reader_name, path=path)\n        ds = reader_obj.read()\n\n        # Filter variables\n        if self._keep_vars:\n            drop = [v for v in ds.data_vars if v not in set(self._keep_vars)]\n            if drop:\n                ds = ds.drop_vars(drop)\n\n        self._datasets[name] = ds\n        log.info(\"read_complete\", receiver=name)\n\n    return self  # never reached (decorator returns self), but aids type checkers\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.preprocess","level":2,"title":"<code>preprocess(agency='COD')</code>","text":"<p>Apply auxiliary preprocessing to loaded datasets.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.preprocess--parameters","level":4,"title":"Parameters","text":"<p>agency : str     Analysis centre for auxiliary products (default <code>\"COD\"</code>).</p> Source code in <code>canvodpy/src/canvodpy/fluent.py</code> <pre><code>@step\ndef preprocess(self, agency: str = \"COD\") -&gt; FluentWorkflow:\n    \"\"\"Apply auxiliary preprocessing to loaded datasets.\n\n    Parameters\n    ----------\n    agency : str\n        Analysis centre for auxiliary products (default ``\"COD\"``).\n    \"\"\"\n    log = self.log.bind(agency=agency)\n\n    for name, ds in self._datasets.items():\n        try:\n            from canvod.auxiliary import preprocess_aux_for_interpolation\n\n            ds = preprocess_aux_for_interpolation(ds)\n            self._datasets[name] = ds\n            log.info(\"preprocess_complete\", receiver=name)\n        except ImportError:\n            log.debug(\"canvod.auxiliary not available, skipping preprocessing\")\n\n    return self\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.grid","level":2,"title":"<code>grid(kind=None, **params)</code>","text":"<p>Build a hemisphere grid and assign cell IDs to all datasets.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.grid--parameters","level":4,"title":"Parameters","text":"<p>kind : str, optional     Grid type override.  Defaults to the value set at init. **params     Passed to :meth:<code>GridFactory.create</code> (e.g.     <code>angular_resolution=5.0</code>).</p> Source code in <code>canvodpy/src/canvodpy/fluent.py</code> <pre><code>@step\ndef grid(self, kind: str | None = None, **params: Any) -&gt; FluentWorkflow:\n    \"\"\"Build a hemisphere grid and assign cell IDs to all datasets.\n\n    Parameters\n    ----------\n    kind : str, optional\n        Grid type override.  Defaults to the value set at init.\n    **params\n        Passed to :meth:`GridFactory.create` (e.g.\n        ``angular_resolution=5.0``).\n    \"\"\"\n    from canvod.grids import add_cell_ids_to_ds_fast\n\n    grid_type = kind or self._grid_type\n    builder = GridFactory.create(grid_type, **params)\n    self._grid = builder.build()\n\n    for name, ds in self._datasets.items():\n        self._datasets[name] = add_cell_ids_to_ds_fast(ds, self._grid)\n\n    self.log.info(\"grid_complete\", grid=grid_type, ncells=self._grid.ncells)\n    return self\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.vod","level":2,"title":"<code>vod(canopy, reference)</code>","text":"<p>Compute vegetation optical depth for a receiver pair.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.vod--parameters","level":4,"title":"Parameters","text":"<p>canopy : str     Canopy receiver name (e.g. <code>\"canopy_01\"</code>). reference : str     Sky/reference receiver name (e.g. <code>\"reference_01\"</code>).</p> Source code in <code>canvodpy/src/canvodpy/fluent.py</code> <pre><code>@step\ndef vod(self, canopy: str, reference: str) -&gt; FluentWorkflow:\n    \"\"\"Compute vegetation optical depth for a receiver pair.\n\n    Parameters\n    ----------\n    canopy : str\n        Canopy receiver name (e.g. ``\"canopy_01\"``).\n    reference : str\n        Sky/reference receiver name (e.g. ``\"reference_01\"``).\n    \"\"\"\n    canopy_ds = self._datasets[canopy]\n    ref_ds = self._datasets[reference]\n\n    calculator = VODFactory.create(\n        self._vod_calculator_name,\n        canopy_ds=canopy_ds,\n        sky_ds=ref_ds,\n    )\n    self._vod_result = calculator.calculate_vod()\n\n    self.log.info(\"vod_complete\", canopy=canopy, reference=reference)\n    return self\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.result","level":2,"title":"<code>result()</code>","text":"<p>Execute the plan and return the final data.</p> <p>Returns the VOD dataset if a <code>.vod()</code> step was included, otherwise returns the dict of per-receiver datasets.</p> Source code in <code>canvodpy/src/canvodpy/fluent.py</code> <pre><code>@terminal\ndef result(self) -&gt; xr.Dataset | dict[str, xr.Dataset]:\n    \"\"\"Execute the plan and return the final data.\n\n    Returns the VOD dataset if a ``.vod()`` step was included,\n    otherwise returns the dict of per-receiver datasets.\n    \"\"\"\n    if self._vod_result is not None:\n        return self._vod_result\n    return dict(self._datasets)\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.to_store","level":2,"title":"<code>to_store()</code>","text":"<p>Execute the plan and write results to Icechunk storage.</p> Source code in <code>canvodpy/src/canvodpy/fluent.py</code> <pre><code>@terminal\ndef to_store(self) -&gt; FluentWorkflow:\n    \"\"\"Execute the plan and write results to Icechunk storage.\"\"\"\n    if self._vod_result is not None:\n        # Store VOD result — requires a store name convention\n        self.log.info(\"to_store_vod\")\n        self._site.vod_store.write_group(\"vod_result\", self._vod_result)\n    else:\n        for name, ds in self._datasets.items():\n            self.log.info(\"to_store_dataset\", receiver=name)\n            self._site.rinex_store.write_group(name, ds)\n    return self\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.plot","level":2,"title":"<code>plot()</code>","text":"<p>Execute the plan and visualise the result.</p> Source code in <code>canvodpy/src/canvodpy/fluent.py</code> <pre><code>@terminal\ndef plot(self) -&gt; Any:\n    \"\"\"Execute the plan and visualise the result.\"\"\"\n    from canvod.viz import HemisphereVisualizer\n\n    data = self._vod_result if self._vod_result is not None else self._datasets\n    viz = HemisphereVisualizer()\n    return viz.plot_2d(data)\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.explain","level":2,"title":"<code>explain()</code>","text":"<p>Return a description of the recorded plan without executing it.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.FluentWorkflow.explain--returns","level":4,"title":"Returns","text":"<p>list[dict]     One entry per step with keys <code>\"step\"</code>, <code>\"args\"</code>, and     <code>\"kwargs\"</code>.</p> Source code in <code>canvodpy/src/canvodpy/fluent.py</code> <pre><code>def explain(self) -&gt; list[dict[str, Any]]:\n    \"\"\"Return a description of the recorded plan without executing it.\n\n    Returns\n    -------\n    list[dict]\n        One entry per step with keys ``\"step\"``, ``\"args\"``, and\n        ``\"kwargs\"``.\n    \"\"\"\n    return [\n        {\"step\": fn.__name__, \"args\": args, \"kwargs\": kwargs}\n        for fn, args, kwargs in self._plan\n    ]\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#level-3-vodworkflow-eager-execution","level":2,"title":"Level 3 — VODWorkflow (Eager Execution)","text":"<p>Stateful orchestrator with factory integration and structured logging.</p> <p>User-friendly wrapper around GnssResearchSite.</p> <p>Provides a clean, modern API for site management while using the proven GnssResearchSite implementation internally.</p> <p>User-friendly wrapper around PipelineOrchestrator.</p> <p>Provides a clean API for processing workflows while using proven orchestrator logic internally.</p> <p>Orchestrate complete VOD analysis workflow.</p> <p>Uses component factories for extensibility and structured logging for LLM-assisted debugging. Replaces legacy Site + Pipeline pattern.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site--parameters","level":3,"title":"Parameters","text":"<p>name : str     Site name from configuration (e.g., \"Rosalia\")</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site--attributes","level":3,"title":"Attributes","text":"<p>name : str     Site name receivers : dict     All configured receivers active_receivers : dict     Only active receivers vod_analyses : dict     Configured VOD analysis pairs rinex_store     Access to RINEX data store vod_store     Access to VOD results store</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site--examples","level":3,"title":"Examples","text":"<p>site = Site(\"Rosalia\") print(site.receivers) {'canopy_01': {...}, 'reference_01': {...}}</p> Source code in <code>canvodpy/src/canvodpy/api.py</code> <pre><code>class Site:\n    \"\"\"User-friendly wrapper around GnssResearchSite.\n\n    Provides a clean, modern API for site management while using\n    the proven GnssResearchSite implementation internally.\n\n    Parameters\n    ----------\n    name : str\n        Site name from configuration (e.g., \"Rosalia\")\n\n    Attributes\n    ----------\n    name : str\n        Site name\n    receivers : dict\n        All configured receivers\n    active_receivers : dict\n        Only active receivers\n    vod_analyses : dict\n        Configured VOD analysis pairs\n    rinex_store\n        Access to RINEX data store\n    vod_store\n        Access to VOD results store\n\n    Examples\n    --------\n    &gt;&gt;&gt; site = Site(\"Rosalia\")\n    &gt;&gt;&gt; print(site.receivers)\n    {'canopy_01': {...}, 'reference_01': {...}}\n\n    &gt;&gt;&gt; # Create pipeline\n    &gt;&gt;&gt; pipeline = site.pipeline()\n\n    &gt;&gt;&gt; # Access stores\n    &gt;&gt;&gt; site.rinex_store.list_groups()\n\n    \"\"\"\n\n    def __init__(self, name: str) -&gt; None:\n        # Lazy import to avoid circular dependency\n        from canvod.store import GnssResearchSite\n\n        # Use proven implementation\n        self._site = GnssResearchSite(name)\n        self.name = name\n\n    @property\n    def receivers(self) -&gt; dict:\n        \"\"\"Get all configured receivers.\"\"\"\n        return self._site.receivers\n\n    @property\n    def active_receivers(self) -&gt; dict:\n        \"\"\"Get only active receivers.\"\"\"\n        return self._site.active_receivers\n\n    @property\n    def vod_analyses(self) -&gt; dict:\n        \"\"\"Get configured VOD analysis pairs.\"\"\"\n        return self._site.active_vod_analyses\n\n    @property\n    def rinex_store(self) -&gt; MyIcechunkStore:\n        \"\"\"Access RINEX data store.\"\"\"\n        return self._site.rinex_store\n\n    @property\n    def vod_store(self) -&gt; MyIcechunkStore:\n        \"\"\"Access VOD results store.\"\"\"\n        return self._site.vod_store\n\n    def pipeline(\n        self,\n        keep_vars: list[str] | None = None,\n        aux_agency: str = \"COD\",\n        n_workers: int = 12,\n        dry_run: bool = False,\n    ) -&gt; Pipeline:\n        \"\"\"Create a processing pipeline for this site.\n\n        Parameters\n        ----------\n        keep_vars : list[str], optional\n            RINEX variables to keep (default: KEEP_RNX_VARS from globals)\n        aux_agency : str, default \"COD\"\n            Analysis center for auxiliary data (COD, ESA, GFZ, JPL)\n        n_workers : int, default 12\n            Number of parallel workers\n        dry_run : bool, default False\n            If True, simulate processing without execution\n\n        Returns\n        -------\n        Pipeline\n            Configured pipeline for this site\n\n        Examples\n        --------\n        &gt;&gt;&gt; site = Site(\"Rosalia\")\n        &gt;&gt;&gt; pipeline = site.pipeline(aux_agency=\"ESA\", n_workers=8)\n        &gt;&gt;&gt; data = pipeline.process_date(\"2025001\")\n\n        \"\"\"\n        return Pipeline(\n            site=self,\n            keep_vars=keep_vars,\n            aux_agency=aux_agency,\n            n_workers=n_workers,\n            dry_run=dry_run,\n        )\n\n    def __repr__(self) -&gt; str:\n        n_receivers = len(self.active_receivers)\n        n_analyses = len(self.vod_analyses)\n        return f\"Site('{self.name}', receivers={n_receivers}, analyses={n_analyses})\"\n\n    def __str__(self) -&gt; str:\n        return f\"GNSS Site: {self.name}\"\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site--create-pipeline","level":2,"title":"Create pipeline","text":"<p>pipeline = site.pipeline()</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site--access-stores","level":2,"title":"Access stores","text":"<p>site.rinex_store.list_groups()</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site.receivers","level":2,"title":"<code>receivers</code>  <code>property</code>","text":"<p>Get all configured receivers.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site.active_receivers","level":2,"title":"<code>active_receivers</code>  <code>property</code>","text":"<p>Get only active receivers.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site.vod_analyses","level":2,"title":"<code>vod_analyses</code>  <code>property</code>","text":"<p>Get configured VOD analysis pairs.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site.rinex_store","level":2,"title":"<code>rinex_store</code>  <code>property</code>","text":"<p>Access RINEX data store.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site.vod_store","level":2,"title":"<code>vod_store</code>  <code>property</code>","text":"<p>Access VOD results store.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site.pipeline","level":2,"title":"<code>pipeline(keep_vars=None, aux_agency='COD', n_workers=12, dry_run=False)</code>","text":"<p>Create a processing pipeline for this site.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site.pipeline--parameters","level":4,"title":"Parameters","text":"<p>keep_vars : list[str], optional     RINEX variables to keep (default: KEEP_RNX_VARS from globals) aux_agency : str, default \"COD\"     Analysis center for auxiliary data (COD, ESA, GFZ, JPL) n_workers : int, default 12     Number of parallel workers dry_run : bool, default False     If True, simulate processing without execution</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site.pipeline--returns","level":4,"title":"Returns","text":"<p>Pipeline     Configured pipeline for this site</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Site.pipeline--examples","level":4,"title":"Examples","text":"<p>site = Site(\"Rosalia\") pipeline = site.pipeline(aux_agency=\"ESA\", n_workers=8) data = pipeline.process_date(\"2025001\")</p> Source code in <code>canvodpy/src/canvodpy/api.py</code> <pre><code>def pipeline(\n    self,\n    keep_vars: list[str] | None = None,\n    aux_agency: str = \"COD\",\n    n_workers: int = 12,\n    dry_run: bool = False,\n) -&gt; Pipeline:\n    \"\"\"Create a processing pipeline for this site.\n\n    Parameters\n    ----------\n    keep_vars : list[str], optional\n        RINEX variables to keep (default: KEEP_RNX_VARS from globals)\n    aux_agency : str, default \"COD\"\n        Analysis center for auxiliary data (COD, ESA, GFZ, JPL)\n    n_workers : int, default 12\n        Number of parallel workers\n    dry_run : bool, default False\n        If True, simulate processing without execution\n\n    Returns\n    -------\n    Pipeline\n        Configured pipeline for this site\n\n    Examples\n    --------\n    &gt;&gt;&gt; site = Site(\"Rosalia\")\n    &gt;&gt;&gt; pipeline = site.pipeline(aux_agency=\"ESA\", n_workers=8)\n    &gt;&gt;&gt; data = pipeline.process_date(\"2025001\")\n\n    \"\"\"\n    return Pipeline(\n        site=self,\n        keep_vars=keep_vars,\n        aux_agency=aux_agency,\n        n_workers=n_workers,\n        dry_run=dry_run,\n    )\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline--parameters","level":3,"title":"Parameters","text":"<p>site : Site or str     Site object or site name keep_vars : list[str], optional     RINEX variables to keep aux_agency : str, default \"COD\"     Analysis center for auxiliary data n_workers : int, default 12     Number of parallel workers dry_run : bool, default False     If True, simulate without execution</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline--examples","level":3,"title":"Examples","text":"Source code in <code>canvodpy/src/canvodpy/api.py</code> <pre><code>class Pipeline:\n    \"\"\"User-friendly wrapper around PipelineOrchestrator.\n\n    Provides a clean API for processing workflows while using\n    proven orchestrator logic internally.\n\n    Parameters\n    ----------\n    site : Site or str\n        Site object or site name\n    keep_vars : list[str], optional\n        RINEX variables to keep\n    aux_agency : str, default \"COD\"\n        Analysis center for auxiliary data\n    n_workers : int, default 12\n        Number of parallel workers\n    dry_run : bool, default False\n        If True, simulate without execution\n\n    Examples\n    --------\n    &gt;&gt;&gt; # From site object\n    &gt;&gt;&gt; site = Site(\"Rosalia\")\n    &gt;&gt;&gt; pipeline = Pipeline(site)\n\n    &gt;&gt;&gt; # Or directly from name\n    &gt;&gt;&gt; pipeline = Pipeline(\"Rosalia\")\n\n    &gt;&gt;&gt; # Process single date\n    &gt;&gt;&gt; data = pipeline.process_date(\"2025001\")\n\n    &gt;&gt;&gt; # Process range\n    &gt;&gt;&gt; for date, datasets in pipeline.process_range(\"2025001\", \"2025007\"):\n    ...     print(f\"Processed {date}\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        site: Site | str,\n        keep_vars: list[str] | None = None,\n        aux_agency: str = \"COD\",\n        n_workers: int = 12,\n        dry_run: bool = False,\n    ) -&gt; None:\n        # Handle both Site object and string\n        if isinstance(site, str):\n            site = Site(site)\n\n        self.site = site\n        if keep_vars is None:\n            from canvod.utils.config import load_config\n\n            keep_vars = load_config().processing.processing.keep_rnx_vars\n        self.keep_vars = keep_vars\n        self.aux_agency = aux_agency\n        self.n_workers = n_workers\n        self.dry_run = dry_run\n\n        # Setup logging\n        from canvodpy.logging import get_logger\n\n        self.log = get_logger(__name__).bind(\n            site=site.name,\n            component=\"pipeline\",\n        )\n\n        # Lazy import to avoid circular dependency\n        from canvodpy.orchestrator import PipelineOrchestrator\n\n        # Use proven orchestrator implementation\n        self._orchestrator = PipelineOrchestrator(\n            site=site._site,\n            n_max_workers=n_workers,\n            dry_run=dry_run,\n        )\n\n        self.log.info(\n            \"pipeline_initialized\",\n            aux_agency=aux_agency,\n            n_workers=n_workers,\n            keep_vars=len(self.keep_vars),\n            dry_run=dry_run,\n        )\n\n    def process_date(self, date: str) -&gt; dict[str, xr.Dataset]:\n        \"\"\"Process RINEX data for one date.\n\n        Parameters\n        ----------\n        date : str\n            Date in YYYYDOY format (e.g., \"2025001\" for Jan 1, 2025)\n\n        Returns\n        -------\n        dict[str, xr.Dataset]\n            Processed datasets for each receiver\n\n        Examples\n        --------\n        &gt;&gt;&gt; pipeline = Pipeline(\"Rosalia\")\n        &gt;&gt;&gt; data = pipeline.process_date(\"2025001\")\n        &gt;&gt;&gt; print(data.keys())\n        dict_keys(['canopy_01', 'canopy_02', 'reference_01'])\n\n        &gt;&gt;&gt; # Access individual receiver\n        &gt;&gt;&gt; canopy_data = data['canopy_01']\n        &gt;&gt;&gt; print(canopy_data.dims)\n        Dimensions: (epoch: 2880, sv: 32, ...)\n\n        \"\"\"\n        log = self.log.bind(date=date)\n        log.info(\"date_processing_started\")\n\n        # Use proven orchestrator logic\n        for _date_key, datasets, _timing in self._orchestrator.process_by_date(\n            keep_vars=self.keep_vars,\n            start_from=date,\n            end_at=date,\n        ):\n            log.info(\n                \"date_processing_complete\",\n                receivers=len(datasets),\n                receiver_names=list(datasets.keys()),\n            )\n            return datasets  # Return first (only) date\n\n        log.warning(\"no_data_processed\", date=date)\n        return {}  # No data processed\n\n    def process_range(\n        self,\n        start: str,\n        end: str,\n    ) -&gt; Generator[tuple[str, dict[str, xr.Dataset]], None, None]:\n        \"\"\"Process RINEX data for a date range.\n\n        Parameters\n        ----------\n        start : str\n            Start date (YYYYDOY)\n        end : str\n            End date (YYYYDOY)\n\n        Yields\n        ------\n        tuple[str, dict[str, xr.Dataset]]\n            (date_key, datasets) for each processed date\n\n        Examples\n        --------\n        &gt;&gt;&gt; pipeline = Pipeline(\"Rosalia\")\n        &gt;&gt;&gt; for date, datasets in pipeline.process_range(\"2025001\", \"2025007\"):\n        ...     print(f\"Processed {date}: {len(datasets)} receivers\")\n        Processed 2025001: 3 receivers\n        Processed 2025002: 3 receivers\n        ...\n\n        \"\"\"\n        # Use proven orchestrator logic\n        for date_key, datasets, _timing in self._orchestrator.process_by_date(\n            keep_vars=self.keep_vars,\n            start_from=start,\n            end_at=end,\n        ):\n            yield date_key, datasets\n\n    def calculate_vod(\n        self,\n        canopy: str,\n        reference: str,\n        date: str,\n    ) -&gt; xr.Dataset:\n        \"\"\"Calculate VOD for a receiver pair.\n\n        Parameters\n        ----------\n        canopy : str\n            Canopy receiver name (e.g., \"canopy_01\")\n        reference : str\n            Reference receiver name (e.g., \"reference_01\")\n        date : str\n            Date in YYYYDOY format\n\n        Returns\n        -------\n        xr.Dataset\n            VOD analysis results\n\n        Examples\n        --------\n        &gt;&gt;&gt; pipeline = Pipeline(\"Rosalia\")\n        &gt;&gt;&gt; vod = pipeline.calculate_vod(\"canopy_01\", \"reference_01\", \"2025001\")\n        &gt;&gt;&gt; print(vod.vod.mean().values)\n        0.42\n\n        \"\"\"\n        log = self.log.bind(date=date, canopy=canopy, reference=reference)\n        log.info(\"vod_calculation_started\")\n\n        try:\n            # Load processed data from stores\n            canopy_data = self.site.rinex_store.read_group(canopy, date=date)\n            ref_data = self.site.rinex_store.read_group(reference, date=date)\n\n            # Lazy import to avoid circular dependency\n            from canvod.vod import VODCalculator\n\n            # Use proven VOD calculator\n            calculator = VODCalculator()\n            vod_results = calculator.compute(canopy_data, ref_data)\n\n            # Store results\n            analysis_name = f\"{canopy}_vs_{reference}\"\n            self.site.vod_store.write_group(analysis_name, vod_results)\n\n            log.info(\n                \"vod_calculation_complete\",\n                analysis=analysis_name,\n                vod_mean=float(vod_results.vod.mean().values)\n                if \"vod\" in vod_results\n                else None,\n            )\n            return vod_results\n        except Exception as e:\n            log.error(\n                \"vod_calculation_failed\",\n                error=str(e),\n                exception=type(e).__name__,\n            )\n            raise\n\n    def preview(self) -&gt; dict:\n        \"\"\"Preview processing plan without execution.\n\n        Returns\n        -------\n        dict\n            Summary of dates, receivers, and files\n\n        Examples\n        --------\n        &gt;&gt;&gt; pipeline = Pipeline(\"Rosalia\")\n        &gt;&gt;&gt; plan = pipeline.preview()\n        &gt;&gt;&gt; print(f\"Total files: {plan['total_files']}\")\n\n        \"\"\"\n        return self._orchestrator.preview_processing_plan()\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"Pipeline(site='{self.site.name}', \"\n            f\"keep_vars={len(self.keep_vars)} vars, \"\n            f\"workers={self.n_workers})\"\n        )\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline--from-site-object","level":2,"title":"From site object","text":"<p>site = Site(\"Rosalia\") pipeline = Pipeline(site)</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline--or-directly-from-name","level":2,"title":"Or directly from name","text":"<p>pipeline = Pipeline(\"Rosalia\")</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline--process-single-date","level":2,"title":"Process single date","text":"<p>data = pipeline.process_date(\"2025001\")</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline--process-range","level":2,"title":"Process range","text":"<p>for date, datasets in pipeline.process_range(\"2025001\", \"2025007\"): ...     print(f\"Processed {date}\")</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.process_date","level":2,"title":"<code>process_date(date)</code>","text":"<p>Process RINEX data for one date.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.process_date--parameters","level":4,"title":"Parameters","text":"<p>date : str     Date in YYYYDOY format (e.g., \"2025001\" for Jan 1, 2025)</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.process_date--returns","level":4,"title":"Returns","text":"<p>dict[str, xr.Dataset]     Processed datasets for each receiver</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.process_date--examples","level":4,"title":"Examples","text":"<p>pipeline = Pipeline(\"Rosalia\") data = pipeline.process_date(\"2025001\") print(data.keys()) dict_keys(['canopy_01', 'canopy_02', 'reference_01'])</p> Source code in <code>canvodpy/src/canvodpy/api.py</code> <pre><code>def process_date(self, date: str) -&gt; dict[str, xr.Dataset]:\n    \"\"\"Process RINEX data for one date.\n\n    Parameters\n    ----------\n    date : str\n        Date in YYYYDOY format (e.g., \"2025001\" for Jan 1, 2025)\n\n    Returns\n    -------\n    dict[str, xr.Dataset]\n        Processed datasets for each receiver\n\n    Examples\n    --------\n    &gt;&gt;&gt; pipeline = Pipeline(\"Rosalia\")\n    &gt;&gt;&gt; data = pipeline.process_date(\"2025001\")\n    &gt;&gt;&gt; print(data.keys())\n    dict_keys(['canopy_01', 'canopy_02', 'reference_01'])\n\n    &gt;&gt;&gt; # Access individual receiver\n    &gt;&gt;&gt; canopy_data = data['canopy_01']\n    &gt;&gt;&gt; print(canopy_data.dims)\n    Dimensions: (epoch: 2880, sv: 32, ...)\n\n    \"\"\"\n    log = self.log.bind(date=date)\n    log.info(\"date_processing_started\")\n\n    # Use proven orchestrator logic\n    for _date_key, datasets, _timing in self._orchestrator.process_by_date(\n        keep_vars=self.keep_vars,\n        start_from=date,\n        end_at=date,\n    ):\n        log.info(\n            \"date_processing_complete\",\n            receivers=len(datasets),\n            receiver_names=list(datasets.keys()),\n        )\n        return datasets  # Return first (only) date\n\n    log.warning(\"no_data_processed\", date=date)\n    return {}  # No data processed\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.process_date--access-individual-receiver","level":3,"title":"Access individual receiver","text":"<p>canopy_data = data['canopy_01'] print(canopy_data.dims) Dimensions: (epoch: 2880, sv: 32, ...)</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.process_range","level":2,"title":"<code>process_range(start, end)</code>","text":"<p>Process RINEX data for a date range.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.process_range--parameters","level":4,"title":"Parameters","text":"<p>start : str     Start date (YYYYDOY) end : str     End date (YYYYDOY)</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.process_range--yields","level":4,"title":"Yields","text":"<p>tuple[str, dict[str, xr.Dataset]]     (date_key, datasets) for each processed date</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.process_range--examples","level":4,"title":"Examples","text":"<p>pipeline = Pipeline(\"Rosalia\") for date, datasets in pipeline.process_range(\"2025001\", \"2025007\"): ...     print(f\"Processed {date}: {len(datasets)} receivers\") Processed 2025001: 3 receivers Processed 2025002: 3 receivers ...</p> Source code in <code>canvodpy/src/canvodpy/api.py</code> <pre><code>def process_range(\n    self,\n    start: str,\n    end: str,\n) -&gt; Generator[tuple[str, dict[str, xr.Dataset]], None, None]:\n    \"\"\"Process RINEX data for a date range.\n\n    Parameters\n    ----------\n    start : str\n        Start date (YYYYDOY)\n    end : str\n        End date (YYYYDOY)\n\n    Yields\n    ------\n    tuple[str, dict[str, xr.Dataset]]\n        (date_key, datasets) for each processed date\n\n    Examples\n    --------\n    &gt;&gt;&gt; pipeline = Pipeline(\"Rosalia\")\n    &gt;&gt;&gt; for date, datasets in pipeline.process_range(\"2025001\", \"2025007\"):\n    ...     print(f\"Processed {date}: {len(datasets)} receivers\")\n    Processed 2025001: 3 receivers\n    Processed 2025002: 3 receivers\n    ...\n\n    \"\"\"\n    # Use proven orchestrator logic\n    for date_key, datasets, _timing in self._orchestrator.process_by_date(\n        keep_vars=self.keep_vars,\n        start_from=start,\n        end_at=end,\n    ):\n        yield date_key, datasets\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.calculate_vod","level":2,"title":"<code>calculate_vod(canopy, reference, date)</code>","text":"<p>Calculate VOD for a receiver pair.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.calculate_vod--parameters","level":4,"title":"Parameters","text":"<p>canopy : str     Canopy receiver name (e.g., \"canopy_01\") reference : str     Reference receiver name (e.g., \"reference_01\") date : str     Date in YYYYDOY format</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.calculate_vod--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     VOD analysis results</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.calculate_vod--examples","level":4,"title":"Examples","text":"<p>pipeline = Pipeline(\"Rosalia\") vod = pipeline.calculate_vod(\"canopy_01\", \"reference_01\", \"2025001\") print(vod.vod.mean().values) 0.42</p> Source code in <code>canvodpy/src/canvodpy/api.py</code> <pre><code>def calculate_vod(\n    self,\n    canopy: str,\n    reference: str,\n    date: str,\n) -&gt; xr.Dataset:\n    \"\"\"Calculate VOD for a receiver pair.\n\n    Parameters\n    ----------\n    canopy : str\n        Canopy receiver name (e.g., \"canopy_01\")\n    reference : str\n        Reference receiver name (e.g., \"reference_01\")\n    date : str\n        Date in YYYYDOY format\n\n    Returns\n    -------\n    xr.Dataset\n        VOD analysis results\n\n    Examples\n    --------\n    &gt;&gt;&gt; pipeline = Pipeline(\"Rosalia\")\n    &gt;&gt;&gt; vod = pipeline.calculate_vod(\"canopy_01\", \"reference_01\", \"2025001\")\n    &gt;&gt;&gt; print(vod.vod.mean().values)\n    0.42\n\n    \"\"\"\n    log = self.log.bind(date=date, canopy=canopy, reference=reference)\n    log.info(\"vod_calculation_started\")\n\n    try:\n        # Load processed data from stores\n        canopy_data = self.site.rinex_store.read_group(canopy, date=date)\n        ref_data = self.site.rinex_store.read_group(reference, date=date)\n\n        # Lazy import to avoid circular dependency\n        from canvod.vod import VODCalculator\n\n        # Use proven VOD calculator\n        calculator = VODCalculator()\n        vod_results = calculator.compute(canopy_data, ref_data)\n\n        # Store results\n        analysis_name = f\"{canopy}_vs_{reference}\"\n        self.site.vod_store.write_group(analysis_name, vod_results)\n\n        log.info(\n            \"vod_calculation_complete\",\n            analysis=analysis_name,\n            vod_mean=float(vod_results.vod.mean().values)\n            if \"vod\" in vod_results\n            else None,\n        )\n        return vod_results\n    except Exception as e:\n        log.error(\n            \"vod_calculation_failed\",\n            error=str(e),\n            exception=type(e).__name__,\n        )\n        raise\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.preview","level":2,"title":"<code>preview()</code>","text":"<p>Preview processing plan without execution.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.preview--returns","level":4,"title":"Returns","text":"<p>dict     Summary of dates, receivers, and files</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.Pipeline.preview--examples","level":4,"title":"Examples","text":"<p>pipeline = Pipeline(\"Rosalia\") plan = pipeline.preview() print(f\"Total files: {plan['total_files']}\")</p> Source code in <code>canvodpy/src/canvodpy/api.py</code> <pre><code>def preview(self) -&gt; dict:\n    \"\"\"Preview processing plan without execution.\n\n    Returns\n    -------\n    dict\n        Summary of dates, receivers, and files\n\n    Examples\n    --------\n    &gt;&gt;&gt; pipeline = Pipeline(\"Rosalia\")\n    &gt;&gt;&gt; plan = pipeline.preview()\n    &gt;&gt;&gt; print(f\"Total files: {plan['total_files']}\")\n\n    \"\"\"\n    return self._orchestrator.preview_processing_plan()\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow--parameters","level":3,"title":"Parameters","text":"<p>site : str or Site     Site name or Site object reader : str, default=\"rinex3\"     Registered reader name grid : str, default=\"equal_area\"     Registered grid name vod_calculator : str, default=\"tau_omega\"     Registered VOD calculator name grid_params : dict, optional     Parameters for grid creation (angular_resolution, cutoff_theta) keep_vars : list[str], optional     RINEX variables to keep. Defaults to KEEP_RNX_VARS. log_level : str, default=\"INFO\"     Logging level (DEBUG, INFO, WARNING, ERROR)</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow--attributes","level":3,"title":"Attributes","text":"<p>site : Site     Site configuration grid : GridData     Hemisphere grid structure log : BoundLogger     Structured logger with site context</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow--examples","level":3,"title":"Examples","text":"<p>Process one date:</p> <pre><code>&gt;&gt;&gt; workflow = VODWorkflow(site=\"Rosalia\")\n&gt;&gt;&gt; data = workflow.process_date(\"2025001\")\n&gt;&gt;&gt; print(data.keys())\ndict_keys(['canopy_01', 'reference_01'])\n</code></pre> <p>Calculate VOD:</p> <pre><code>&gt;&gt;&gt; vod = workflow.calculate_vod(\n...     \"canopy_01\", \"reference_01\", \"2025001\"\n... )\n&gt;&gt;&gt; print(vod.VOD.mean().item())\n0.42\n</code></pre> <p>Custom grid:</p> <pre><code>&gt;&gt;&gt; workflow = VODWorkflow(\n...     site=\"Rosalia\",\n...     grid=\"equal_area\",\n...     grid_params={\"angular_resolution\": 5.0},\n... )\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow--notes","level":3,"title":"Notes","text":"<p>Preserves logical flow from legacy API: Site → Pipeline → process_date → load → augment → grid → calculate</p> Source code in <code>canvodpy/src/canvodpy/workflow.py</code> <pre><code>class VODWorkflow:\n    \"\"\"\n    Orchestrate complete VOD analysis workflow.\n\n    Uses component factories for extensibility and structured logging\n    for LLM-assisted debugging. Replaces legacy Site + Pipeline pattern.\n\n    Parameters\n    ----------\n    site : str or Site\n        Site name or Site object\n    reader : str, default=\"rinex3\"\n        Registered reader name\n    grid : str, default=\"equal_area\"\n        Registered grid name\n    vod_calculator : str, default=\"tau_omega\"\n        Registered VOD calculator name\n    grid_params : dict, optional\n        Parameters for grid creation (angular_resolution, cutoff_theta)\n    keep_vars : list[str], optional\n        RINEX variables to keep. Defaults to KEEP_RNX_VARS.\n    log_level : str, default=\"INFO\"\n        Logging level (DEBUG, INFO, WARNING, ERROR)\n\n    Attributes\n    ----------\n    site : Site\n        Site configuration\n    grid : GridData\n        Hemisphere grid structure\n    log : BoundLogger\n        Structured logger with site context\n\n    Examples\n    --------\n    Process one date:\n\n        &gt;&gt;&gt; workflow = VODWorkflow(site=\"Rosalia\")\n        &gt;&gt;&gt; data = workflow.process_date(\"2025001\")\n        &gt;&gt;&gt; print(data.keys())\n        dict_keys(['canopy_01', 'reference_01'])\n\n    Calculate VOD:\n\n        &gt;&gt;&gt; vod = workflow.calculate_vod(\n        ...     \"canopy_01\", \"reference_01\", \"2025001\"\n        ... )\n        &gt;&gt;&gt; print(vod.VOD.mean().item())\n        0.42\n\n    Custom grid:\n\n        &gt;&gt;&gt; workflow = VODWorkflow(\n        ...     site=\"Rosalia\",\n        ...     grid=\"equal_area\",\n        ...     grid_params={\"angular_resolution\": 5.0},\n        ... )\n\n    Notes\n    -----\n    Preserves logical flow from legacy API:\n    Site → Pipeline → process_date → load → augment → grid → calculate\n    \"\"\"\n\n    def __init__(\n        self,\n        site: str | Site,\n        reader: str = \"rinex3\",\n        grid: str = \"equal_area\",\n        vod_calculator: str = \"tau_omega\",\n        grid_params: dict[str, Any] | None = None,\n        keep_vars: list[str] | None = None,\n        log_level: str = \"INFO\",\n    ) -&gt; None:\n        \"\"\"Initialize workflow with component factories.\"\"\"\n        # Handle site input\n        site_name = site if isinstance(site, str) else site.name\n        self.site = Site(site) if isinstance(site, str) else site\n\n        # Setup logging with site context\n        self.log: BoundLogger = get_logger(__name__).bind(site=site_name)\n\n        # Store configuration\n        self.reader_name = reader\n        self.grid_name = grid\n        self.vod_calculator_name = vod_calculator\n        if keep_vars is None:\n            from canvod.utils.config import load_config\n\n            keep_vars = load_config().processing.processing.keep_rnx_vars\n        self.keep_vars = keep_vars\n\n        # Create grid using factory (cached for workflow)\n        grid_params = grid_params or {}\n        builder = GridFactory.create(grid, **grid_params)\n        self.grid = builder.build()\n\n        self.log.info(\n            \"workflow_initialized\",\n            grid=grid,\n            reader=reader,\n            calculator=vod_calculator,\n            ncells=self.grid.ncells,\n        )\n\n    def process_date(\n        self,\n        date: str,\n        receivers: list[str] | None = None,\n    ) -&gt; dict[str, xr.Dataset]:\n        \"\"\"\n        Process RINEX data for one date.\n\n        Workflow: load_rinex → augment → assign_grid → store\n\n        Parameters\n        ----------\n        date : str\n            Date in YYYYDOY format (e.g., \"2025001\" = Jan 1, 2025)\n        receivers : list[str], optional\n            Receiver names to process. If None, use all active receivers.\n\n        Returns\n        -------\n        dict[str, xr.Dataset]\n            Processed datasets keyed by receiver name. Each dataset has\n            cell assignments and filtered data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; workflow = VODWorkflow(\"Rosalia\")\n        &gt;&gt;&gt; data = workflow.process_date(\"2025001\")\n        &gt;&gt;&gt; canopy = data[\"canopy_01\"]\n        &gt;&gt;&gt; print(canopy.sizes)\n        {'epoch': 2880, 'sv': 32, 'cell': 324}\n\n        Process specific receivers:\n\n        &gt;&gt;&gt; data = workflow.process_date(\n        ...     \"2025001\",\n        ...     receivers=[\"canopy_01\"]\n        ... )\n\n        Notes\n        -----\n        This is the main entry point for data processing. Orchestrates\n        the full pipeline from raw RINEX to grid-assigned observations.\n        \"\"\"\n        log = self.log.bind(date=date)\n        log.info(\"process_date_started\")\n\n        # Get receivers to process\n        receiver_list = receivers or list(self.site.active_receivers.keys())\n        results = {}\n\n        for recv_name in receiver_list:\n            recv_log = log.bind(receiver=recv_name)\n            recv_log.info(\"processing_receiver\")\n\n            try:\n                # Step 1: Load RINEX\n                ds = self._load_rinex(recv_name, date, recv_log)\n\n                # Step 2: Augment (preprocessing)\n                ds = self._augment_data(ds, recv_log)\n\n                # Step 3: Assign grid cells\n                ds = self._assign_grid_cells(ds, recv_log)\n\n                results[recv_name] = ds\n                recv_log.info(\n                    \"processing_complete\",\n                    variables=list(ds.data_vars),\n                    cells=ds.sizes.get(\"cell\", 0),\n                )\n\n            except Exception as e:\n                recv_log.error(\"processing_failed\", error=str(e), exc_info=True)\n                raise\n\n        log.info(\"process_date_complete\", receivers=len(results))\n        return results\n\n    def calculate_vod(\n        self,\n        canopy_receiver: str,\n        sky_receiver: str,\n        date: str,\n        use_cached: bool = True,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Calculate VOD from canopy and sky receivers.\n\n        Parameters\n        ----------\n        canopy_receiver : str\n            Receiver under canopy (e.g., \"canopy_01\")\n        sky_receiver : str\n            Sky reference receiver (e.g., \"reference_01\")\n        date : str\n            Date in YYYYDOY format\n        use_cached : bool, default=True\n            If True, reuse datasets from process_date cache\n\n        Returns\n        -------\n        xr.Dataset\n            VOD dataset with variables:\n            - VOD : Vegetation Optical Depth\n            - phi : Azimuth angles\n            - theta : Elevation angles\n\n        Examples\n        --------\n        &gt;&gt;&gt; workflow = VODWorkflow(\"Rosalia\")\n        &gt;&gt;&gt; vod = workflow.calculate_vod(\n        ...     \"canopy_01\", \"reference_01\", \"2025001\"\n        ... )\n        &gt;&gt;&gt; print(vod.VOD.mean().item())\n        0.42\n\n        Without caching:\n\n        &gt;&gt;&gt; vod = workflow.calculate_vod(\n        ...     \"canopy_01\",\n        ...     \"reference_01\",\n        ...     \"2025001\",\n        ...     use_cached=False,\n        ... )\n\n        Notes\n        -----\n        Uses VODFactory to create calculator. Supports community\n        extensions via factory registration.\n        \"\"\"\n        log = self.log.bind(date=date, canopy=canopy_receiver, sky=sky_receiver)\n        log.info(\"calculate_vod_started\")\n\n        # Load or retrieve datasets\n        if use_cached and hasattr(self, \"_dataset_cache\"):\n            canopy_ds = self._dataset_cache.get(canopy_receiver)\n            sky_ds = self._dataset_cache.get(sky_receiver)\n        else:\n            canopy_ds = None\n            sky_ds = None\n\n        # Process if not cached\n        if canopy_ds is None:\n            log.debug(\"loading_canopy\")\n            data = self.process_date(date, receivers=[canopy_receiver])\n            canopy_ds = data[canopy_receiver]\n\n        if sky_ds is None:\n            log.debug(\"loading_sky\")\n            data = self.process_date(date, receivers=[sky_receiver])\n            sky_ds = data[sky_receiver]\n\n        # Use factory to create calculator\n        calculator = VODFactory.create(\n            self.vod_calculator_name,\n            canopy_ds=canopy_ds,\n            sky_ds=sky_ds,\n        )\n\n        # Calculate VOD\n        vod_ds = calculator.calculate_vod()\n\n        log.info(\n            \"calculate_vod_complete\",\n            cells=vod_ds.sizes.get(\"cell\", 0) if \"cell\" in vod_ds.sizes else 0,\n            vod_mean=float(vod_ds.VOD.mean().item()) if \"VOD\" in vod_ds else None,\n        )\n        return vod_ds\n\n    def _load_rinex(self, receiver: str, date: str, log: BoundLogger) -&gt; xr.Dataset:\n        \"\"\"\n        Load RINEX using factory.\n\n        Parameters\n        ----------\n        receiver : str\n            Receiver name\n        date : str\n            Date in YYYYDOY format\n        log : BoundLogger\n            Logger with receiver context\n\n        Returns\n        -------\n        xr.Dataset\n            RINEX data\n\n        Notes\n        -----\n        Uses ReaderFactory to create reader instance.\n        \"\"\"\n        log.debug(\"load_rinex_started\")\n\n        # Get file path from site config\n        rinex_path = self._get_rinex_path(receiver, date)\n\n        # Create reader using factory\n        reader = ReaderFactory.create(\n            self.reader_name,\n            path=rinex_path,\n        )\n\n        # Read data\n        ds = reader.read()\n\n        # Filter variables\n        if self.keep_vars:\n            keep_vars_set = set(self.keep_vars)\n            drop_vars = [v for v in ds.data_vars if v not in keep_vars_set]\n            if drop_vars:\n                ds = ds.drop_vars(drop_vars)\n\n        log.debug(\"load_rinex_complete\", variables=list(ds.data_vars))\n        return ds\n\n    def _augment_data(self, ds: xr.Dataset, log: BoundLogger) -&gt; xr.Dataset:\n        \"\"\"\n        Apply augmentation steps.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Input dataset\n        log : BoundLogger\n            Logger with context\n\n        Returns\n        -------\n        xr.Dataset\n            Augmented dataset\n\n        Notes\n        -----\n        Currently passes through. Will use AugmentationFactory\n        for preprocessing steps (filtering, interpolation, etc.)\n        \"\"\"\n        log.debug(\"augment_data_started\")\n\n        # TODO: Use AugmentationFactory for preprocessing\n        # For now, pass through\n        # augmentations = [\n        #     AugmentationFactory.create(\"hampel\", window=5),\n        #     AugmentationFactory.create(\"interpolate\", method=\"linear\"),\n        # ]\n        # for aug in augmentations:\n        #     ds = aug.apply(ds)\n\n        log.debug(\"augment_data_complete\")\n        return ds\n\n    def _assign_grid_cells(self, ds: xr.Dataset, log: BoundLogger) -&gt; xr.Dataset:\n        \"\"\"\n        Assign grid cells to observations.\n\n        Parameters\n        ----------\n        ds : xr.Dataset\n            Dataset with phi, theta coordinates\n        log : BoundLogger\n            Logger with context\n\n        Returns\n        -------\n        xr.Dataset\n            Dataset with added 'cell' coordinate\n\n        Notes\n        -----\n        Uses grid operations from canvod.grids package.\n        \"\"\"\n        log.debug(\"assign_grid_cells_started\")\n\n        # Use grid operations\n        from canvod.grids import add_cell_ids_to_ds_fast\n\n        ds_with_cells = add_cell_ids_to_ds_fast(ds, self.grid)\n\n        log.debug(\n            \"assign_grid_cells_complete\",\n            cells=ds_with_cells.sizes.get(\"cell\", 0),\n        )\n        return ds_with_cells\n\n    def _get_rinex_path(self, receiver: str, date: str) -&gt; Path:\n        \"\"\"\n        Get RINEX file path from site configuration.\n\n        Parameters\n        ----------\n        receiver : str\n            Receiver name\n        date : str\n            Date in YYYYDOY format\n\n        Returns\n        -------\n        Path\n            Path to RINEX file\n\n        Raises\n        ------\n        ValueError\n            If receiver not found in site configuration\n        \"\"\"\n        if receiver not in self.site.receivers:\n            available = list(self.site.receivers.keys())\n            msg = f\"Receiver '{receiver}' not in site. Available: {available}\"\n            raise ValueError(msg)\n\n        # Use internal site object to get path\n        return self.site._site.get_rinex_path(receiver, date)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation.\"\"\"\n        return (\n            f\"VODWorkflow(site='{self.site.name}', \"\n            f\"grid='{self.grid_name}', reader='{self.reader_name}')\"\n        )\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.__init__","level":2,"title":"<code>__init__(site, reader='rinex3', grid='equal_area', vod_calculator='tau_omega', grid_params=None, keep_vars=None, log_level='INFO')</code>","text":"<p>Initialize workflow with component factories.</p> Source code in <code>canvodpy/src/canvodpy/workflow.py</code> <pre><code>def __init__(\n    self,\n    site: str | Site,\n    reader: str = \"rinex3\",\n    grid: str = \"equal_area\",\n    vod_calculator: str = \"tau_omega\",\n    grid_params: dict[str, Any] | None = None,\n    keep_vars: list[str] | None = None,\n    log_level: str = \"INFO\",\n) -&gt; None:\n    \"\"\"Initialize workflow with component factories.\"\"\"\n    # Handle site input\n    site_name = site if isinstance(site, str) else site.name\n    self.site = Site(site) if isinstance(site, str) else site\n\n    # Setup logging with site context\n    self.log: BoundLogger = get_logger(__name__).bind(site=site_name)\n\n    # Store configuration\n    self.reader_name = reader\n    self.grid_name = grid\n    self.vod_calculator_name = vod_calculator\n    if keep_vars is None:\n        from canvod.utils.config import load_config\n\n        keep_vars = load_config().processing.processing.keep_rnx_vars\n    self.keep_vars = keep_vars\n\n    # Create grid using factory (cached for workflow)\n    grid_params = grid_params or {}\n    builder = GridFactory.create(grid, **grid_params)\n    self.grid = builder.build()\n\n    self.log.info(\n        \"workflow_initialized\",\n        grid=grid,\n        reader=reader,\n        calculator=vod_calculator,\n        ncells=self.grid.ncells,\n    )\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.process_date","level":2,"title":"<code>process_date(date, receivers=None)</code>","text":"<p>Process RINEX data for one date.</p> <p>Workflow: load_rinex → augment → assign_grid → store</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.process_date--parameters","level":4,"title":"Parameters","text":"<p>date : str     Date in YYYYDOY format (e.g., \"2025001\" = Jan 1, 2025) receivers : list[str], optional     Receiver names to process. If None, use all active receivers.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.process_date--returns","level":4,"title":"Returns","text":"<p>dict[str, xr.Dataset]     Processed datasets keyed by receiver name. Each dataset has     cell assignments and filtered data.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.process_date--examples","level":4,"title":"Examples","text":"<p>workflow = VODWorkflow(\"Rosalia\") data = workflow.process_date(\"2025001\") canopy = data[\"canopy_01\"] print(canopy.sizes)</p> <p>Process specific receivers:</p> <p>data = workflow.process_date( ...     \"2025001\", ...     receivers=[\"canopy_01\"] ... )</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.process_date--notes","level":4,"title":"Notes","text":"<p>This is the main entry point for data processing. Orchestrates the full pipeline from raw RINEX to grid-assigned observations.</p> Source code in <code>canvodpy/src/canvodpy/workflow.py</code> <pre><code>def process_date(\n    self,\n    date: str,\n    receivers: list[str] | None = None,\n) -&gt; dict[str, xr.Dataset]:\n    \"\"\"\n    Process RINEX data for one date.\n\n    Workflow: load_rinex → augment → assign_grid → store\n\n    Parameters\n    ----------\n    date : str\n        Date in YYYYDOY format (e.g., \"2025001\" = Jan 1, 2025)\n    receivers : list[str], optional\n        Receiver names to process. If None, use all active receivers.\n\n    Returns\n    -------\n    dict[str, xr.Dataset]\n        Processed datasets keyed by receiver name. Each dataset has\n        cell assignments and filtered data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; workflow = VODWorkflow(\"Rosalia\")\n    &gt;&gt;&gt; data = workflow.process_date(\"2025001\")\n    &gt;&gt;&gt; canopy = data[\"canopy_01\"]\n    &gt;&gt;&gt; print(canopy.sizes)\n    {'epoch': 2880, 'sv': 32, 'cell': 324}\n\n    Process specific receivers:\n\n    &gt;&gt;&gt; data = workflow.process_date(\n    ...     \"2025001\",\n    ...     receivers=[\"canopy_01\"]\n    ... )\n\n    Notes\n    -----\n    This is the main entry point for data processing. Orchestrates\n    the full pipeline from raw RINEX to grid-assigned observations.\n    \"\"\"\n    log = self.log.bind(date=date)\n    log.info(\"process_date_started\")\n\n    # Get receivers to process\n    receiver_list = receivers or list(self.site.active_receivers.keys())\n    results = {}\n\n    for recv_name in receiver_list:\n        recv_log = log.bind(receiver=recv_name)\n        recv_log.info(\"processing_receiver\")\n\n        try:\n            # Step 1: Load RINEX\n            ds = self._load_rinex(recv_name, date, recv_log)\n\n            # Step 2: Augment (preprocessing)\n            ds = self._augment_data(ds, recv_log)\n\n            # Step 3: Assign grid cells\n            ds = self._assign_grid_cells(ds, recv_log)\n\n            results[recv_name] = ds\n            recv_log.info(\n                \"processing_complete\",\n                variables=list(ds.data_vars),\n                cells=ds.sizes.get(\"cell\", 0),\n            )\n\n        except Exception as e:\n            recv_log.error(\"processing_failed\", error=str(e), exc_info=True)\n            raise\n\n    log.info(\"process_date_complete\", receivers=len(results))\n    return results\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.calculate_vod","level":2,"title":"<code>calculate_vod(canopy_receiver, sky_receiver, date, use_cached=True)</code>","text":"<p>Calculate VOD from canopy and sky receivers.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.calculate_vod--parameters","level":4,"title":"Parameters","text":"<p>canopy_receiver : str     Receiver under canopy (e.g., \"canopy_01\") sky_receiver : str     Sky reference receiver (e.g., \"reference_01\") date : str     Date in YYYYDOY format use_cached : bool, default=True     If True, reuse datasets from process_date cache</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.calculate_vod--returns","level":4,"title":"Returns","text":"<p>xr.Dataset     VOD dataset with variables:     - VOD : Vegetation Optical Depth     - phi : Azimuth angles     - theta : Elevation angles</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.calculate_vod--examples","level":4,"title":"Examples","text":"<p>workflow = VODWorkflow(\"Rosalia\") vod = workflow.calculate_vod( ...     \"canopy_01\", \"reference_01\", \"2025001\" ... ) print(vod.VOD.mean().item()) 0.42</p> <p>Without caching:</p> <p>vod = workflow.calculate_vod( ...     \"canopy_01\", ...     \"reference_01\", ...     \"2025001\", ...     use_cached=False, ... )</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.calculate_vod--notes","level":4,"title":"Notes","text":"<p>Uses VODFactory to create calculator. Supports community extensions via factory registration.</p> Source code in <code>canvodpy/src/canvodpy/workflow.py</code> <pre><code>def calculate_vod(\n    self,\n    canopy_receiver: str,\n    sky_receiver: str,\n    date: str,\n    use_cached: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Calculate VOD from canopy and sky receivers.\n\n    Parameters\n    ----------\n    canopy_receiver : str\n        Receiver under canopy (e.g., \"canopy_01\")\n    sky_receiver : str\n        Sky reference receiver (e.g., \"reference_01\")\n    date : str\n        Date in YYYYDOY format\n    use_cached : bool, default=True\n        If True, reuse datasets from process_date cache\n\n    Returns\n    -------\n    xr.Dataset\n        VOD dataset with variables:\n        - VOD : Vegetation Optical Depth\n        - phi : Azimuth angles\n        - theta : Elevation angles\n\n    Examples\n    --------\n    &gt;&gt;&gt; workflow = VODWorkflow(\"Rosalia\")\n    &gt;&gt;&gt; vod = workflow.calculate_vod(\n    ...     \"canopy_01\", \"reference_01\", \"2025001\"\n    ... )\n    &gt;&gt;&gt; print(vod.VOD.mean().item())\n    0.42\n\n    Without caching:\n\n    &gt;&gt;&gt; vod = workflow.calculate_vod(\n    ...     \"canopy_01\",\n    ...     \"reference_01\",\n    ...     \"2025001\",\n    ...     use_cached=False,\n    ... )\n\n    Notes\n    -----\n    Uses VODFactory to create calculator. Supports community\n    extensions via factory registration.\n    \"\"\"\n    log = self.log.bind(date=date, canopy=canopy_receiver, sky=sky_receiver)\n    log.info(\"calculate_vod_started\")\n\n    # Load or retrieve datasets\n    if use_cached and hasattr(self, \"_dataset_cache\"):\n        canopy_ds = self._dataset_cache.get(canopy_receiver)\n        sky_ds = self._dataset_cache.get(sky_receiver)\n    else:\n        canopy_ds = None\n        sky_ds = None\n\n    # Process if not cached\n    if canopy_ds is None:\n        log.debug(\"loading_canopy\")\n        data = self.process_date(date, receivers=[canopy_receiver])\n        canopy_ds = data[canopy_receiver]\n\n    if sky_ds is None:\n        log.debug(\"loading_sky\")\n        data = self.process_date(date, receivers=[sky_receiver])\n        sky_ds = data[sky_receiver]\n\n    # Use factory to create calculator\n    calculator = VODFactory.create(\n        self.vod_calculator_name,\n        canopy_ds=canopy_ds,\n        sky_ds=sky_ds,\n    )\n\n    # Calculate VOD\n    vod_ds = calculator.calculate_vod()\n\n    log.info(\n        \"calculate_vod_complete\",\n        cells=vod_ds.sizes.get(\"cell\", 0) if \"cell\" in vod_ds.sizes else 0,\n        vod_mean=float(vod_ds.VOD.mean().item()) if \"VOD\" in vod_ds else None,\n    )\n    return vod_ds\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODWorkflow.__repr__","level":2,"title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>canvodpy/src/canvodpy/workflow.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    return (\n        f\"VODWorkflow(site='{self.site.name}', \"\n        f\"grid='{self.grid_name}', reader='{self.reader_name}')\"\n    )\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#level-4-functional-api","level":2,"title":"Level 4 — Functional API","text":"<p>Pure, stateless functions for composable pipelines and Airflow DAGs.</p> <p>Read RINEX file using registered reader.</p> <p>Create hemisphere grid using registered builder.</p> <p>Assign grid cells to observations.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.read_rinex--parameters","level":3,"title":"Parameters","text":"<p>path : str or Path     Path to RINEX file reader : str, default=\"rinex3\"     Registered reader name from ReaderFactory **reader_kwargs : Any     Additional reader-specific parameters</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.read_rinex--returns","level":3,"title":"Returns","text":"<p>xr.Dataset     RINEX data with variables (SNR, azimuth, elevation, etc.)</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.read_rinex--examples","level":3,"title":"Examples","text":"<p>data = read_rinex(\"ROSA0010.25o\") print(data.data_vars) Data variables: SNR, azimuth, elevation, ...</p> <p>With custom reader:</p> <p>data = read_rinex(\"data.rnx\", reader=\"custom_reader\")</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.read_rinex--notes","level":3,"title":"Notes","text":"<p>Pure function - stateless, no side effects. Uses ReaderFactory for extensibility.</p> Source code in <code>canvodpy/src/canvodpy/functional.py</code> <pre><code>def read_rinex(\n    path: str | Path,\n    reader: str = \"rinex3\",\n    **reader_kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read RINEX file using registered reader.\n\n    Parameters\n    ----------\n    path : str or Path\n        Path to RINEX file\n    reader : str, default=\"rinex3\"\n        Registered reader name from ReaderFactory\n    **reader_kwargs : Any\n        Additional reader-specific parameters\n\n    Returns\n    -------\n    xr.Dataset\n        RINEX data with variables (SNR, azimuth, elevation, etc.)\n\n    Examples\n    --------\n    &gt;&gt;&gt; data = read_rinex(\"ROSA0010.25o\")\n    &gt;&gt;&gt; print(data.data_vars)\n    Data variables: SNR, azimuth, elevation, ...\n\n    With custom reader:\n\n    &gt;&gt;&gt; data = read_rinex(\"data.rnx\", reader=\"custom_reader\")\n\n    Notes\n    -----\n    Pure function - stateless, no side effects.\n    Uses ReaderFactory for extensibility.\n    \"\"\"\n    log.info(\"read_rinex\", path=str(path), reader=reader)\n\n    reader_obj = ReaderFactory.create(\n        reader,\n        path=path,\n        **reader_kwargs,\n    )\n\n    ds = reader_obj.read()\n    log.info(\"read_rinex_complete\", variables=len(ds.data_vars))\n    return ds\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.create_grid--parameters","level":3,"title":"Parameters","text":"<p>grid_type : str, default=\"equal_area\"     Registered grid type from GridFactory **grid_params : Any     Grid parameters:     - angular_resolution : float     - cutoff_theta : float     - phi_rotation : float</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.create_grid--returns","level":3,"title":"Returns","text":"<p>GridData     Grid structure with cells, vertices, metadata</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.create_grid--examples","level":3,"title":"Examples","text":"<p>grid = create_grid(\"equal_area\", angular_resolution=5.0) print(grid.ncells) 1296</p> <p>With cutoff:</p> <p>grid = create_grid( ...     \"equal_area\", ...     angular_resolution=10.0, ...     cutoff_theta=75.0, ... )</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.create_grid--notes","level":3,"title":"Notes","text":"<p>Pure function - stateless, deterministic. Uses GridFactory for extensibility.</p> Source code in <code>canvodpy/src/canvodpy/functional.py</code> <pre><code>def create_grid(\n    grid_type: str = \"equal_area\",\n    **grid_params: Any,\n) -&gt; Any:  # Returns GridData\n    \"\"\"\n    Create hemisphere grid using registered builder.\n\n    Parameters\n    ----------\n    grid_type : str, default=\"equal_area\"\n        Registered grid type from GridFactory\n    **grid_params : Any\n        Grid parameters:\n        - angular_resolution : float\n        - cutoff_theta : float\n        - phi_rotation : float\n\n    Returns\n    -------\n    GridData\n        Grid structure with cells, vertices, metadata\n\n    Examples\n    --------\n    &gt;&gt;&gt; grid = create_grid(\"equal_area\", angular_resolution=5.0)\n    &gt;&gt;&gt; print(grid.ncells)\n    1296\n\n    With cutoff:\n\n    &gt;&gt;&gt; grid = create_grid(\n    ...     \"equal_area\",\n    ...     angular_resolution=10.0,\n    ...     cutoff_theta=75.0,\n    ... )\n\n    Notes\n    -----\n    Pure function - stateless, deterministic.\n    Uses GridFactory for extensibility.\n    \"\"\"\n    log.info(\"create_grid\", grid_type=grid_type, params=grid_params)\n\n    builder = GridFactory.create(grid_type, **grid_params)\n    grid = builder.build()\n\n    log.info(\"create_grid_complete\", ncells=grid.ncells)\n    return grid\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.assign_grid_cells--parameters","level":3,"title":"Parameters","text":"<p>ds : xr.Dataset     Dataset with phi, theta coordinates grid : GridData     Grid structure from create_grid()</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.assign_grid_cells--returns","level":3,"title":"Returns","text":"<p>xr.Dataset     Dataset with added 'cell' coordinate</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.assign_grid_cells--examples","level":3,"title":"Examples","text":"<p>grid = create_grid(\"equal_area\") ds_with_cells = assign_grid_cells(data, grid) print(ds_with_cells.sizes)</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.assign_grid_cells--notes","level":3,"title":"Notes","text":"<p>Pure function - no side effects on input. Uses KDTree-based assignment for performance.</p> Source code in <code>canvodpy/src/canvodpy/functional.py</code> <pre><code>def assign_grid_cells(\n    ds: xr.Dataset,\n    grid: Any,  # GridData\n) -&gt; xr.Dataset:\n    \"\"\"\n    Assign grid cells to observations.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n        Dataset with phi, theta coordinates\n    grid : GridData\n        Grid structure from create_grid()\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset with added 'cell' coordinate\n\n    Examples\n    --------\n    &gt;&gt;&gt; grid = create_grid(\"equal_area\")\n    &gt;&gt;&gt; ds_with_cells = assign_grid_cells(data, grid)\n    &gt;&gt;&gt; print(ds_with_cells.sizes)\n    {'epoch': 2880, 'sv': 32, 'cell': 324}\n\n    Notes\n    -----\n    Pure function - no side effects on input.\n    Uses KDTree-based assignment for performance.\n    \"\"\"\n    log.info(\"assign_grid_cells\")\n\n    from canvod.grids import add_cell_ids_to_ds_fast\n\n    ds_with_cells = add_cell_ids_to_ds_fast(ds, grid)\n\n    log.info(\n        \"assign_grid_cells_complete\",\n        cells=ds_with_cells.sizes.get(\"cell\", 0),\n    )\n    return ds_with_cells\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#factories","level":2,"title":"Factories","text":"<p>               Bases: <code>ComponentFactory</code></p> <p>Factory for GNSS data readers.</p> <p>All registered readers must inherit from <code>GNSSDataReader</code> ABC.</p> <p>               Bases: <code>ComponentFactory</code></p> <p>Factory for grid builders.</p> <p>All registered builders must inherit from <code>BaseGridBuilder</code> ABC.</p> <p>               Bases: <code>ComponentFactory</code></p> <p>Factory for VOD calculators.</p> <p>All registered calculators must inherit from <code>VODCalculator</code> ABC.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.ReaderFactory--examples","level":3,"title":"Examples","text":"<p>from canvod.readers import Rnxv3Obs ReaderFactory.register(\"rinex3\", Rnxv3Obs) reader = ReaderFactory.create(\"rinex3\", path=\"data.rnx\") data = reader.read()</p> Source code in <code>canvodpy/src/canvodpy/factories.py</code> <pre><code>class ReaderFactory(ComponentFactory):\n    \"\"\"\n    Factory for GNSS data readers.\n\n    All registered readers must inherit from `GNSSDataReader` ABC.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.readers import Rnxv3Obs\n    &gt;&gt;&gt; ReaderFactory.register(\"rinex3\", Rnxv3Obs)\n    &gt;&gt;&gt; reader = ReaderFactory.create(\"rinex3\", path=\"data.rnx\")\n    &gt;&gt;&gt; data = reader.read()\n    \"\"\"\n\n    _registry: ClassVar[dict[str, type]] = {}\n\n    @classmethod\n    def _set_abc_class(cls) -&gt; None:\n        \"\"\"Lazy import to avoid circular dependencies.\"\"\"\n        if cls._abc_class is None:\n            from canvod.readers.base import GNSSDataReader\n\n            cls._abc_class = GNSSDataReader\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.GridFactory--examples","level":3,"title":"Examples","text":"<p>from canvod.grids import EqualAreaBuilder GridFactory.register(\"equal_area\", EqualAreaBuilder) grid = GridFactory.create( ...     \"equal_area\", ...     angular_resolution=5.0, ...     cutoff_theta=75.0, ... )</p> Source code in <code>canvodpy/src/canvodpy/factories.py</code> <pre><code>class GridFactory(ComponentFactory):\n    \"\"\"\n    Factory for grid builders.\n\n    All registered builders must inherit from `BaseGridBuilder` ABC.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.grids import EqualAreaBuilder\n    &gt;&gt;&gt; GridFactory.register(\"equal_area\", EqualAreaBuilder)\n    &gt;&gt;&gt; grid = GridFactory.create(\n    ...     \"equal_area\",\n    ...     angular_resolution=5.0,\n    ...     cutoff_theta=75.0,\n    ... )\n    \"\"\"\n\n    _registry: ClassVar[dict[str, type]] = {}\n\n    @classmethod\n    def _set_abc_class(cls) -&gt; None:\n        \"\"\"Lazy import to avoid circular dependencies.\"\"\"\n        if cls._abc_class is None:\n            from canvod.grids.core.grid_builder import BaseGridBuilder\n\n            cls._abc_class = BaseGridBuilder\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.VODFactory--examples","level":3,"title":"Examples","text":"<p>from canvod.vod import TauOmegaZerothOrder VODFactory.register(\"tau_omega\", TauOmegaZerothOrder) calc = VODFactory.create( ...     \"tau_omega\", ...     canopy_ds=canopy, ...     sky_ds=sky, ... ) vod = calc.calculate_vod()</p> Source code in <code>canvodpy/src/canvodpy/factories.py</code> <pre><code>class VODFactory(ComponentFactory):\n    \"\"\"\n    Factory for VOD calculators.\n\n    All registered calculators must inherit from `VODCalculator` ABC.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from canvod.vod import TauOmegaZerothOrder\n    &gt;&gt;&gt; VODFactory.register(\"tau_omega\", TauOmegaZerothOrder)\n    &gt;&gt;&gt; calc = VODFactory.create(\n    ...     \"tau_omega\",\n    ...     canopy_ds=canopy,\n    ...     sky_ds=sky,\n    ... )\n    &gt;&gt;&gt; vod = calc.calculate_vod()\n    \"\"\"\n\n    _registry: ClassVar[dict[str, type]] = {}\n\n    @classmethod\n    def _set_abc_class(cls) -&gt; None:\n        \"\"\"Lazy import to avoid circular dependencies.\"\"\"\n        if cls._abc_class is None:\n            from canvod.vod.calculator import VODCalculator\n\n            cls._abc_class = VODCalculator\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#configuration","level":2,"title":"Configuration","text":"<p>Get a logger instance for a module.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.get_logger--parameters","level":3,"title":"Parameters","text":"<p>name : str     Logger name, typically <code>__name__</code> of the module.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.get_logger--returns","level":3,"title":"Returns","text":"<p>structlog.stdlib.BoundLogger     Configured logger instance.</p>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"api/canvodpy/#canvodpy.get_logger--examples","level":3,"title":"Examples","text":"<p>log = get_logger(name) log.info(\"event_name\", key=\"value\", count=42)</p> Source code in <code>canvodpy/src/canvodpy/logging/__init__.py</code> <pre><code>def get_logger(name: str) -&gt; structlog.stdlib.BoundLogger:\n    \"\"\"\n    Get a logger instance for a module.\n\n    Parameters\n    ----------\n    name : str\n        Logger name, typically `__name__` of the module.\n\n    Returns\n    -------\n    structlog.stdlib.BoundLogger\n        Configured logger instance.\n\n    Examples\n    --------\n    &gt;&gt;&gt; log = get_logger(__name__)\n    &gt;&gt;&gt; log.info(\"event_name\", key=\"value\", count=42)\n    \"\"\"\n    return structlog.get_logger(name)\n</code></pre>","path":["API Reference","canvodpy API Reference"],"tags":[]},{"location":"guides/DEVELOPMENT/","level":1,"title":"Development Guide","text":"","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#initial-setup","level":2,"title":"Initial Setup","text":"<pre><code>git clone https://github.com/nfb2021/canvodpy.git\ncd canvodpy\ngit submodule update --init --recursive\nuv sync\njust test\n</code></pre> <p>The submodule step pulls two external data repositories:</p> <ul> <li><code>packages/canvod-readers/tests/test_data</code> — validation test data (falsified/corrupted RINEX files)</li> <li><code>demo</code> — clean real-world data for demos and documentation</li> </ul> <p>Tests that depend on these datasets are automatically skipped if the submodules are not initialized.</p>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#prerequisites","level":2,"title":"Prerequisites","text":"<p>Two external tools are required:</p> <ol> <li>uv -- Python package manager (installation)</li> <li>just -- Command runner (installation)</li> </ol> <p>Verify installation:</p> <pre><code>just check-dev-tools\n</code></pre>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#configuration-management","level":2,"title":"Configuration Management","text":"<p>canVODpy uses three YAML files in the <code>config/</code> directory:</p> File Purpose <code>sites.yaml</code> Research sites: data root paths, receiver definitions (name, type, directory), and VOD analysis pairs. Each receiver's <code>directory</code> is the full relative path from the site data root to the raw RINEX date folders (e.g. <code>01_reference/01_GNSS/01_raw</code>). <code>processing.yaml</code> Processing parameters: metadata, credentials (NASA Earthdata), auxiliary data settings, time aggregation, compression, Icechunk storage, and store strategies. <code>sids.yaml</code> Signal ID filtering: choose <code>all</code>, a named <code>preset</code> (e.g. <code>gps_galileo</code>), or list <code>custom</code> SIDs to keep. <p>Each file has a <code>.example</code> template in the same directory.</p>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#just-commands","level":3,"title":"Just Commands","text":"<pre><code>just config-init             # Initialize config files from templates\njust config-show             # View resolved settings\njust config-validate         # Validate configuration\n</code></pre>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#cli-configuration-tool","level":3,"title":"CLI Configuration Tool","text":"<pre><code>uv run canvodpy config init         # Initialize configuration files\nuv run canvodpy config show         # View current settings\nuv run canvodpy config validate     # Validate configuration\nuv run canvodpy config edit processing  # Edit processing config\n</code></pre>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#first-time-setup","level":3,"title":"First-Time Setup","text":"<ol> <li>Initialize config files: <code>just config-init</code></li> <li>Edit research sites: <code>uv run canvodpy config edit sites</code></li> <li>Edit processing configuration: <code>uv run canvodpy config edit processing</code></li> <li>Validate: <code>just config-validate</code></li> </ol>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#testing","level":2,"title":"Testing","text":"<pre><code>just test                    # All tests\njust test-package canvod-readers  # Specific package\njust test-coverage           # With coverage report\n</code></pre> <p>Tests are located in each package's <code>tests/</code> directory.</p>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#code-quality","level":2,"title":"Code Quality","text":"<pre><code>just check                   # Lint + format + type-check\njust check-lint              # Linting only\njust check-format            # Formatting only\n</code></pre> <p>Tools used: - ruff: Linting and formatting - ty: Type checking - pytest: Testing with coverage</p>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#documentation","level":2,"title":"Documentation","text":"<pre><code>just docs                    # Build and serve locally\n</code></pre> <p>Documentation is built with MyST/Zensical and served at http://localhost:3000.</p>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#dependency-analysis","level":2,"title":"Dependency Analysis","text":"<pre><code>just deps-report             # Full metrics report\njust deps-graph              # Mermaid dependency graph\n</code></pre> <p>Architecture summary: <pre><code>Foundation (0 deps):          Consumers (1 dep):\n  canvod-readers              canvod-auxiliary -&gt; canvod-readers\n  canvod-grids                canvod-viz -&gt; canvod-grids\n  canvod-vod                  canvod-store -&gt; canvod-grids\n  canvod-utils\n</code></pre></p>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#contributing-workflow","level":2,"title":"Contributing Workflow","text":"<ol> <li>Create a feature branch: <code>git checkout -b feature/my-feature</code></li> <li>Make changes in <code>packages/&lt;package&gt;/src/canvod/&lt;package&gt;/</code></li> <li>Add tests in <code>packages/&lt;package&gt;/tests/</code></li> <li>Run quality checks: <code>just check &amp;&amp; just test</code></li> <li>Commit with conventional commits:    <pre><code>git commit -m \"feat(readers): add RINEX 4.0 support\"\n</code></pre></li> <li>Push and create PR: <code>git push origin feature/my-feature</code></li> </ol>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#conventional-commit-scopes","level":3,"title":"Conventional Commit Scopes","text":"<p><code>readers</code>, <code>aux</code>, <code>grids</code>, <code>vod</code>, <code>store</code>, <code>viz</code>, <code>utils</code>, <code>docs</code>, <code>ci</code>, <code>deps</code></p>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#common-just-commands","level":2,"title":"Common Just Commands","text":"<pre><code>just                         # List all commands\njust check                   # Lint + format + type-check\njust test                    # Run all tests\njust sync                    # Install/update dependencies\njust clean                   # Remove build artifacts\njust hooks                   # Install pre-commit hooks\njust config-init             # Initialize config files from templates\njust config-validate         # Validate configuration\njust config-show             # View resolved configuration\njust docs                    # Preview documentation\njust build-all               # Build all packages\njust release &lt;VERSION&gt;       # Full release workflow\n</code></pre>","path":["Development Guide"],"tags":[]},{"location":"guides/DEVELOPMENT/#troubleshooting","level":2,"title":"Troubleshooting","text":"<p>\"No module named 'canvod.X'\": Run <code>uv sync</code> to install packages.</p> <p>\"Command not found: canvodpy\": Use <code>uv run canvodpy config init</code>.</p> <p>Tests fail after dependency changes: Run <code>uv sync --all-extras</code>.</p>","path":["Development Guide"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/","level":1,"title":"How the Release Infrastructure Works","text":"<p>Created: 2026-02-04 Audience: Maintainers &amp; Contributors</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#table-of-contents","level":2,"title":"Table of Contents","text":"<ol> <li>Conventional Commits System</li> <li>Git Changelog Generator</li> <li>Version Management with Commitizen</li> <li>GitHub Releases Automation</li> <li>PyPI Publishing Setup</li> <li>Trusted Publishing with OIDC</li> </ol>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#1-conventional-commits-system","level":2,"title":"1. Conventional Commits System","text":"","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#what-it-is","level":3,"title":"What It Is","text":"<p>A specification for writing commit messages with a structured format that machines can parse.</p> <p>Format: <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre></p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#how-it-works-in-canvodpy","level":3,"title":"How It Works in canvodpy","text":"<p>Step 1: Developer writes a commit <pre><code>git add .\ngit commit -m \"feat(readers): add RINEX 4.0 support\"\n</code></pre></p> <p>Step 2: Pre-commit hook intercepts - File: <code>.git/hooks/commit-msg</code> (installed by <code>just hooks</code>) - Runs: <code>commitizen check</code> on the message - Validates format matches conventional commits</p> <p>Step 3: Hook validates or rejects</p> <p>Valid commit - allowed: <pre><code>feat(readers): add RINEX 4.0 support\nfix(vod): correct tau calculation\ndocs: update installation guide\n</code></pre></p> <p>Invalid commit - rejected: <pre><code>Added new feature\nWIP\nfixed bug\n</code></pre></p> <p>Configuration Files:</p> <ol> <li> <p><code>pyproject.toml</code> - Commitizen config    <pre><code>[tool.commitizen]\nname = \"cz_conventional_commits\"\nversion = \"0.1.0\"\n\n[tool.commitizen.customize]\nscopes = [\n    \"readers\", \"aux\", \"grids\", \"vod\",\n    \"store\", \"viz\", \"utils\",\n]\n</code></pre></p> </li> <li> <p><code>.pre-commit-config.yaml</code> - Hook registration    <pre><code>- repo: https://github.com/commitizen-tools/commitizen\n  rev: v4.13.3\n  hooks:\n    - id: commitizen\n      stages: [commit-msg]\n</code></pre></p> </li> </ol>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#why-it-matters","level":3,"title":"Why It Matters","text":"<ul> <li>Automated changelog: Tools can parse commits to generate CHANGELOG.md</li> <li>Semantic versioning: Type determines version bump (feat=minor, fix=patch)</li> <li>Clear history: Easy to see what changed without reading code</li> <li>Community standard: Expected in 2026 for open-source projects</li> </ul>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#2-git-changelog-generator","level":2,"title":"2. Git Changelog Generator","text":"","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#what-it-is_1","level":3,"title":"What It Is","text":"<p>A tool (git-changelog) that reads your git history and generates a beautiful CHANGELOG.md file.</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#how-it-works","level":3,"title":"How It Works","text":"<p>Input: Git Commits <pre><code>git log --oneline\nabc1234 feat(vod): add tau-omega calculator\ndef5678 fix(readers): handle empty files\nghi9012 docs: update API reference\n</code></pre></p> <p>Processing: 1. Read all commits since last tag (or all commits) 2. Parse conventional commit format 3. Group by type (Features, Bug Fixes, Docs) 4. Extract issue numbers and link to GitHub 5. Generate markdown sections</p> <p>Output: CHANGELOG.md <pre><code>## [0.2.0] - 2026-02-04\n\n### Features\n- **vod:** add tau-omega calculator ([abc1234](link))\n\n### Bug Fixes\n- **readers:** handle empty files ([def5678](link))\n\n### Documentation\n- update API reference ([ghi9012](link))\n</code></pre></p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#configuration-git-changelogtoml","level":3,"title":"Configuration: <code>.git-changelog.toml</code>","text":"<pre><code>[changelog]\nconvention = \"angular\"  # Use Angular/conventional commits style\n\nsections = [\n    { name = \"Features\", types = [\"feat\"], order = 1 },\n    { name = \"Bug Fixes\", types = [\"fix\"], order = 2 },\n    { name = \"Performance\", types = [\"perf\"], order = 3 },\n]\n\ntemplate = \"keepachangelog\"  # Use Keep a Changelog format\nprovider = \"github\"\nrepository = \"nfb2021/canvodpy\"\n</code></pre>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#usage","level":3,"title":"Usage","text":"<p>Manual generation: <pre><code>just changelog        # Generate for current version\njust changelog v0.2.0 # Generate for specific version\n</code></pre></p> <p>What happens: 1. Runs: <code>uvx git-changelog -Tio CHANGELOG.md -B=\"v0.2.0\" -c angular</code> 2. Reads commits from git history 3. Parses with Angular convention 4. Inserts at <code>&lt;!-- insertion marker --&gt;</code> in CHANGELOG.md 5. Preserves existing content</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#the-magic","level":3,"title":"The Magic","text":"<ul> <li>No manual updates: CHANGELOG writes itself from commits!</li> <li>Links everywhere: Auto-links to GitHub commits, issues, PRs</li> <li>Consistent format: Always follows Keep a Changelog style</li> <li>Version tracking: Each release gets its own section</li> </ul>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#3-version-management-with-commitizen","level":2,"title":"3. Version Management with Commitizen","text":"","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#what-it-is_2","level":3,"title":"What It Is","text":"<p>Commitizen can bump versions across multiple files in a coordinated way.</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#how-it-works-for-monorepo","level":3,"title":"How It Works for Monorepo","text":"<p>Configuration: <code>pyproject.toml</code> <pre><code>[tool.commitizen]\nversion = \"0.1.0\"  # Current version\n\n# All files to update when bumping\nversion_files = [\n    \"canvodpy/pyproject.toml:version\",\n    \"packages/canvod-readers/pyproject.toml:version\",\n    \"packages/canvod-auxiliary/pyproject.toml:version\",\n    # ... all 8 packages\n]\n\ntag_format = \"v$version\"  # Creates tags like v0.2.0\n</code></pre></p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#the-bump-process","level":3,"title":"The Bump Process","text":"<p>Command: <pre><code>just bump 0.2.0\n</code></pre></p> <p>What happens:</p> <ol> <li>Read current version from <code>pyproject.toml</code></li> <li> <p>Current: <code>0.1.0</code></p> </li> <li> <p>Calculate new version</p> </li> <li>Target: <code>0.2.0</code></li> <li> <p>Can also use: <code>minor</code>, <code>patch</code>, <code>major</code></p> </li> <li> <p>Update all version_files (8 packages!)    <pre><code>canvodpy/pyproject.toml:     version = \"0.2.0\"\npackages/canvod-readers/...: version = \"0.2.0\"\npackages/canvod-auxiliary/...:     version = \"0.2.0\"\n# ... all packages updated\n</code></pre></p> </li> <li> <p>Update uv.lock <pre><code>uv lock  # Sync lockfile with new versions\n</code></pre></p> </li> <li> <p>Commit changes <pre><code>git add .\ngit commit -m \"chore: bump version to 0.2.0\"\n</code></pre></p> </li> <li> <p>Create git tag <pre><code>git tag -a \"v0.2.0\" -m \"Release v0.2.0\"\n</code></pre></p> </li> </ol>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#why-unified-versioning","level":3,"title":"Why Unified Versioning","text":"<p>Problem without it: - canvod-readers: 1.2.0 - canvod-vod: 0.5.3 - canvod-store: 2.1.0 - User installs... which versions work together?</p> <p>Solution with unified versioning: - All packages: 0.2.0 - User installs canvodpy 0.2.0 - All components guaranteed compatible</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#4-github-releases-automation","level":2,"title":"4. GitHub Releases Automation","text":"","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#what-it-is_3","level":3,"title":"What It Is","text":"<p>A GitHub Actions workflow that automatically creates releases when you push version tags.</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#the-workflow-githubworkflowsreleaseyml","level":3,"title":"The Workflow: <code>.github/workflows/release.yml</code>","text":"<p>Trigger: <pre><code>on:\n  push:\n    tags:\n      - \"v*.*.*\"  # Matches v0.1.0, v1.0.0, etc.\n</code></pre></p> <p>Steps:</p> <ol> <li>Detect tag push</li> <li>You run: <code>git push --tags</code></li> <li>GitHub sees: new tag <code>v0.2.0</code></li> <li> <p>Workflow starts</p> </li> <li> <p>Checkout code with history <pre><code>- uses: actions/checkout@v6\n  with:\n    fetch-depth: 0  # Need full history for changelog\n</code></pre></p> </li> <li> <p>Install tools <pre><code>- uses: astral-sh/setup-uv@v7\n- run: uv python install 3.13\n</code></pre></p> </li> <li> <p>Generate release notes <pre><code>- run: uvx git-changelog --release-notes &gt; release-notes.md\n</code></pre></p> </li> </ol> <p>Extracts commits for THIS release only (since last tag)</p> <ol> <li>Create GitHub release <pre><code>- run: |\n    gh release create v0.2.0 \\\n      --title \"canvodpy v0.2.0\" \\\n      --notes-file release-notes.md \\\n      --draft \\\n      --verify-tag\n</code></pre></li> </ol> <p>Creates a draft (not published yet!)</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#the-complete-flow","level":3,"title":"The Complete Flow","text":"<pre><code>Developer                    GitHub Actions                 GitHub\n    |                               |                          |\n    | just release 0.2.0           |                          |\n    |-----------------------------&gt;|                          |\n    |                               |                          |\n    | git push --tags              |                          |\n    |------------------------------------------------------------&gt;\n    |                               |                          |\n    |                               | Detect v0.2.0 tag        |\n    |                               |&lt;-------------------------|\n    |                               |                          |\n    |                               | Checkout code            |\n    |                               | Generate release notes   |\n    |                               | Create draft release     |\n    |                               |-------------------------&gt;|\n    |                               |                          |\n    | Review draft release          |                          |\n    |&lt;----------------------------------------------------------\n    |                               |                          |\n    | Click \"Publish Release\"       |                          |\n    |----------------------------------------------------------&gt;|\n    |                               |                          |\n</code></pre>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#why-draft-releases","level":3,"title":"Why Draft Releases?","text":"<ul> <li>Safety: Review before making public</li> <li>Flexibility: Add migration notes, binaries, etc.</li> <li>Control: You decide when to announce</li> </ul>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#5-pypi-publishing-setup","level":2,"title":"5. PyPI Publishing Setup","text":"","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#pypi","level":3,"title":"PyPI","text":"<p>PyPI (Python Package Index) is where Python packages live.</p> <ul> <li>Users install: <code>pip install canvodpy</code></li> <li>Searches: https://pypi.org/project/canvodpy/</li> <li>Hosts: Wheels (.whl) and source distributions (.tar.gz)</li> </ul> <p>TestPyPI is the sandbox for testing before real PyPI.</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#setup-process","level":3,"title":"Setup Process","text":"","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#step-1-register-account","level":4,"title":"Step 1: Register Account","text":"<p>TestPyPI (do this first!): 1. Go to: https://test.pypi.org/account/register/ 2. Create account with your email 3. Verify email</p> <p>Real PyPI (after testing): 1. Go to: https://pypi.org/account/register/ 2. Create account (can use same email) 3. Verify email</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#step-2-reserve-package-name","level":4,"title":"Step 2: Reserve Package Name","text":"<p>On TestPyPI: 1. Build package locally:    <pre><code>uv build\n</code></pre>    Creates: <code>dist/canvodpy-0.1.0-py3-none-any.whl</code></p> <ol> <li> <p>Upload manually (first time only):    <pre><code>uvx twine upload --repository testpypi dist/*\n</code></pre></p> </li> <li> <p>Enter credentials when prompted</p> </li> <li>Package name now reserved!</li> </ol> <p>On real PyPI: - Same process, but omit <code>--repository testpypi</code> - CAUTION: Package names are permanent!</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#step-3-verify-manual-upload","level":4,"title":"Step 3: Verify Manual Upload","text":"<pre><code># Install from TestPyPI\npip install --index-url https://test.pypi.org/simple/ canvodpy\n\n# Test it works\npython -c \"import canvodpy; print(canvodpy.__version__)\"\n</code></pre>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#6-trusted-publishing-with-oidc","level":2,"title":"6. Trusted Publishing with OIDC","text":"","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#oidc","level":3,"title":"OIDC","text":"<p>OIDC (OpenID Connect) provides authentication without passwords or API tokens.</p> <p>Old way (API tokens): <pre><code>GitHub Actions → API Token → PyPI\nToken in GitHub Secrets (can leak)\nToken expires\nManual rotation needed\n</code></pre></p> <p>New way (Trusted Publishing with OIDC): <pre><code>GitHub Actions → OIDC JWT → PyPI\nNo tokens to store\nNever expires\nCryptographically secure\nGitHub identity proves it's really you\n</code></pre></p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#how-oidc-works-simplified","level":3,"title":"How OIDC Works (Simplified)","text":"<ol> <li>GitHub Actions runs your workflow</li> <li>Workflow has identity: <code>repo: nfb2021/canvodpy</code></li> <li> <p>GitHub issues a JWT (JSON Web Token)</p> </li> <li> <p>Workflow requests upload to PyPI</p> </li> <li>Sends JWT instead of password/token</li> <li> <p>JWT says: \"I'm an official GitHub Actions run from nfb2021/canvodpy\"</p> </li> <li> <p>PyPI verifies JWT</p> </li> <li>Checks signature (is it really from GitHub?)</li> <li>Checks claims (is it the right repo?)</li> <li>Allows upload</li> </ol>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#setup-process-detailed","level":3,"title":"Setup Process (Detailed)","text":"","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#part-a-configure-on-pypi","level":4,"title":"Part A: Configure on PyPI","text":"<ol> <li>Go to your project on PyPI:</li> <li>https://test.pypi.org/manage/project/canvodpy/settings/publishing/</li> <li> <p>(or real PyPI after testing)</p> </li> <li> <p>Click \"Add a new publisher\"</p> </li> <li> <p>Fill in the form: <pre><code>PyPI Project Name:     canvodpy\nOwner:                 nfb2021\nRepository name:       canvodpy\nWorkflow name:         publish_pypi.yml\nEnvironment name:      release\n</code></pre></p> </li> <li> <p>Click \"Add\"</p> </li> <li> <p>PyPI now trusts GitHub Actions from that repo/workflow.</p> </li> </ol>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#part-b-create-github-workflow","level":4,"title":"Part B: Create GitHub Workflow","text":"<p>Create: <code>.github/workflows/publish_pypi.yml</code></p> <pre><code>name: Publish to PyPI\n\non:\n  release:\n    types: [published]  # Trigger when you publish a release\n\npermissions:\n  id-token: write  # REQUIRED for OIDC\n  contents: read\n\njobs:\n  publish:\n    name: Upload to PyPI\n    runs-on: ubuntu-latest\n    environment: release  # MUST match PyPI config!\n\n    steps:\n      - uses: actions/checkout@v6\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v7\n\n      - name: Build package\n        run: uv build\n\n      - name: Publish to PyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          # NO PASSWORD OR TOKEN NEEDED!\n          # OIDC handles authentication\n</code></pre>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#part-c-create-github-environment","level":4,"title":"Part C: Create GitHub Environment","text":"<ol> <li>Go to your GitHub repo:</li> <li> <p>Settings → Environments → New environment</p> </li> <li> <p>Name it: <code>release</code> (must match workflow!)</p> </li> <li> <p>Add protection rules (optional but recommended):</p> </li> <li>Required reviewers: Add yourself</li> <li> <p>Wait timer: 5 minutes (think before publishing)</p> </li> <li> <p>Save</p> </li> </ol>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#the-complete-publishing-flow","level":3,"title":"The Complete Publishing Flow","text":"<pre><code>1. Developer creates release\n   ├─&gt; just release 0.2.0\n   ├─&gt; git push --tags\n   └─&gt; GitHub Actions creates draft release\n\n2. Developer publishes release on GitHub\n   └─&gt; Click \"Publish Release\" button\n\n3. Publish workflow triggers\n   ├─&gt; GitHub generates OIDC JWT\n   ├─&gt; Workflow builds package\n   └─&gt; Sends package + JWT to PyPI\n\n4. PyPI verifies and publishes\n   ├─&gt; Verifies JWT is from nfb2021/canvodpy\n   ├─&gt; Accepts upload\n   └─&gt; Package now live!\n\n5. Users can install\n   └─&gt; pip install canvodpy==0.2.0\n</code></pre>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#security-benefits","level":3,"title":"Security Benefits","text":"<p>Traditional API Tokens: - Must be stored in GitHub Secrets - Can be leaked if workflow compromised - Have broad permissions - Need manual rotation</p> <p>OIDC Trusted Publishing: - No secrets to store - JWT valid for ~10 minutes only - Scoped to specific repo + workflow - Automatic, no maintenance</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#testing-with-testpypi","level":3,"title":"Testing with TestPyPI","text":"<p>Why test first? - Real PyPI uploads are permanent (can't delete!) - Package names are reserved forever - Better to catch mistakes on TestPyPI</p> <p>Setup for TestPyPI:</p> <ol> <li>Configure on TestPyPI:</li> <li>https://test.pypi.org/manage/project/canvodpy/settings/publishing/</li> <li> <p>Add publisher (same form as above)</p> </li> <li> <p>Modify workflow for testing: <pre><code>- name: Publish to TestPyPI\n  uses: pypa/gh-action-pypi-publish@release/v1\n  with:\n    repository-url: https://test.pypi.org/legacy/\n</code></pre></p> </li> <li> <p>Test the whole flow: <pre><code># Create test release\njust release 0.1.0-beta.1\ngit push --tags\n\n# Publish draft release\n# Workflow uploads to TestPyPI\n\n# Verify\npip install --index-url https://test.pypi.org/simple/ canvodpy\n</code></pre></p> </li> </ol>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#complete-end-to-end-example","level":2,"title":"Complete End-to-End Example","text":"","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#scenario-youre-creating-version-020","level":3,"title":"Scenario: You're creating version 0.2.0","text":"<p>Step 1: Development <pre><code># Make your changes\ngit add .\ngit commit -m \"feat(readers): add RINEX 4.0 support\"\ngit commit -m \"fix(vod): correct tau calculation\"\n\n# Push to main\ngit push origin main\n</code></pre></p> <p>Step 2: Create Release <pre><code># Run release command\njust release 0.2.0\n\n# Output:\nTests passed\nCHANGELOG.md updated\nVersion bumped to 0.2.0\nTag v0.2.0 created\nRelease ready\n\nNext: git push &amp;&amp; git push --tags\n</code></pre></p> <p>Step 3: Push to GitHub <pre><code>git push origin main\ngit push origin --tags\n</code></pre></p> <p>Step 4: GitHub Actions (automatic) <pre><code>→ release.yml workflow triggers\n→ Generates release notes\n→ Creates draft release on GitHub\n  (Takes ~1-2 minutes)\n</code></pre></p> <p>Step 5: Review &amp; Publish <pre><code>→ Go to: https://github.com/nfb2021/canvodpy/releases\n→ See draft release v0.2.0\n→ Review release notes\n→ Click \"Publish Release\"\n</code></pre></p> <p>Step 6: PyPI Publishing (automatic) <pre><code>→ publish_pypi.yml workflow triggers\n→ Builds package with uv\n→ Authenticates with OIDC (no password!)\n→ Uploads to PyPI\n  (Takes ~2-3 minutes)\n</code></pre></p> <p>Step 7: Users Install <pre><code>pip install canvodpy==0.2.0\n# Works\n</code></pre></p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#conventional-commits-hook-failing","level":3,"title":"Conventional Commits Hook Failing","text":"<p>Error: <code>[ERROR] Commit message does not follow conventional commits format</code></p> <p>Fix: <pre><code># Check format\ngit log -1 --oneline\n\n# Should be: type(scope): description\n# Bad:  \"fixed the bug\"\n# Good: \"fix(vod): correct tau calculation\"\n\n# Amend if needed\ngit commit --amend -m \"fix(vod): correct tau calculation\"\n</code></pre></p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#changelog-not-updating","level":3,"title":"Changelog Not Updating","text":"<p>Problem: Running <code>just changelog</code> but CHANGELOG.md unchanged</p> <p>Causes &amp; Fixes: 1. No conventional commits: Write proper commit messages 2. No <code>&lt;!-- insertion marker --&gt;</code> in CHANGELOG.md: Add it 3. Wrong version range: Check with <code>git tag -l</code></p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#oidc-upload-failing","level":3,"title":"OIDC Upload Failing","text":"<p>Error: <code>Error: The workflow is not configured for publishing</code></p> <p>Fix: 1. Check environment name matches: <code>release</code> 2. Verify PyPI publisher settings (repo, workflow name) 3. Ensure workflow has <code>id-token: write</code> permission</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#version-bump-not-working","level":3,"title":"Version Bump Not Working","text":"<p>Error: <code>cz bump</code> fails</p> <p>Causes: 1. Uncommitted changes: <code>git status</code> → commit first 2. No conventional commits: Can't determine bump type 3. Version format wrong: Must be <code>0.2.0</code> not <code>v0.2.0</code></p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/HOW_RELEASE_WORKS/#summary","level":2,"title":"Summary","text":"<p>You now have a production-grade release system:</p> <ol> <li>Conventional commits enforce standard format</li> <li>Git-changelog auto-generates CHANGELOG.md</li> <li>Commitizen manages unified versioning</li> <li>GitHub Actions automates releases</li> <li>OIDC enables secure PyPI publishing</li> </ol> <p>Result: One command creates a professional release!</p> <pre><code>just release 0.2.0\n</code></pre> <p>That's it.</p>","path":["Release & Publishing","How the Release Infrastructure Works"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/","level":1,"title":"OIDC Trusted Publishing: How It Works","text":"<p>TL;DR: OIDC lets GitHub Actions publish to PyPI without passwords or tokens. It uses cryptographic JWT tokens that expire in 10 minutes.</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#the-traditional-way-api-tokens","level":2,"title":"The Traditional Way (API Tokens)","text":"<pre><code>You → Generate API token on PyPI → Store in GitHub Secrets → Workflow uses token\n</code></pre> <p>Problems: - Long-lived tokens (months/years) - If GitHub is compromised, token leaks - Token has broad permissions (all packages) - Need manual rotation - Can be accidentally committed to git</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#the-oidc-way-2026-standard","level":2,"title":"The OIDC Way (2026 Standard)","text":"","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#overview","level":3,"title":"Overview","text":"<p>OIDC (OpenID Connect) is an authentication protocol built on OAuth 2.0. Instead of storing long-lived tokens, it uses short-lived cryptographic proofs (JWT tokens).</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#the-flow","level":3,"title":"The Flow","text":"<pre><code>┌─────────────┐\n│ Developer   │\n│ git push    │\n│ --tags      │\n└──────┬──────┘\n       │\n       ▼\n┌─────────────────────────────────────────────────────────┐\n│ GitHub Actions Workflow                                 │\n│                                                         │\n│ 1. Builds packages                                      │\n│ 2. Requests JWT from GitHub OIDC provider              │\n│ 3. GitHub generates signed JWT with workflow identity  │\n│ 4. Sends JWT + packages to PyPI                        │\n└──────┬──────────────────────────────────────────────────┘\n       │\n       ▼\n┌─────────────────────────────────────────────────────────┐\n│ PyPI Validation                                         │\n│                                                         │\n│ For each package:                                       │\n│   ✓ Verify JWT signature (GitHub's public key)         │\n│   ✓ Check JWT not expired (~10 min TTL)                │\n│   ✓ Validate claims:                                    │\n│     • repository: nfb2021/canvodpy ✓                    │\n│     • workflow: publish_testpypi.yml ✓                  │\n│     • environment: testpypi ✓                          │\n│     • package name: canvod-readers ✓                    │\n│                                                         │\n│ All match? → PUBLISH                                    │\n│ Mismatch?   → REJECT                                    │\n└─────────────────────────────────────────────────────────┘\n</code></pre>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#step-by-step-breakdown","level":2,"title":"Step-by-Step Breakdown","text":"","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#1-registration-phase-one-time-setup","level":3,"title":"1. Registration Phase (One-Time Setup)","text":"<p>You register a trust relationship on PyPI:</p> <p>\"I trust GitHub Actions from repository <code>nfb2021/canvodpy</code>, specifically the workflow <code>publish_testpypi.yml</code> running in environment <code>testpypi</code>, to publish package <code>canvod-readers</code>\"</p> <p>No secrets exchanged! This just tells PyPI what to expect.</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#2-workflow-execution","level":3,"title":"2. Workflow Execution","text":"<p>When you push a tag (<code>v0.1.0-beta.1</code>), the workflow runs:</p> <pre><code>permissions:\n  id-token: write  # ← Request OIDC JWT capability\n</code></pre>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#3-jwt-token-generation","level":3,"title":"3. JWT Token Generation","text":"<p>GitHub's OIDC provider generates a JWT containing:</p> <pre><code>{\n  \"iss\": \"https://token.actions.githubusercontent.com\",\n  \"sub\": \"repo:nfb2021/canvodpy:environment:testpypi\",\n  \"aud\": \"https://test.pypi.org\",\n  \"repository\": \"nfb2021/canvodpy\",\n  \"repository_owner\": \"nfb2021\",\n  \"repository_owner_id\": \"93122788\",\n  \"workflow\": \"publish_testpypi.yml\",\n  \"workflow_ref\": \"nfb2021/canvodpy/.github/workflows/publish_testpypi.yml@refs/tags/v0.1.0-beta.1\",\n  \"environment\": \"testpypi\",\n  \"ref\": \"refs/tags/v0.1.0-beta.1\",\n  \"iat\": 1738677600,  // Issued at\n  \"exp\": 1738678200   // Expires at (10 minutes later)\n}\n</code></pre> <p>Cryptographically signed with GitHub's private key.</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#4-publish-action","level":3,"title":"4. Publish Action","text":"<p>The <code>pypa/gh-action-pypi-publish</code> action: 1. Retrieves the JWT from GitHub's OIDC provider 2. Bundles it with the package files 3. Sends to PyPI/TestPyPI upload endpoint</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#5-pypi-validation","level":3,"title":"5. PyPI Validation","text":"<p>PyPI receives the request and validates:</p> <p>1. Signature Verification <pre><code>✓ JWT signed by GitHub? (verify with GitHub's public key)\n</code></pre></p> <p>2. Expiration Check <pre><code>✓ Current time &lt; exp claim?\n✓ iat claim reasonable?\n</code></pre></p> <p>3. Claims Validation <pre><code>✓ repository == \"nfb2021/canvodpy\"?\n✓ workflow == \"publish_testpypi.yml\"?\n✓ environment == \"testpypi\"?\n✓ Package name == \"canvod-readers\"? (matches upload)\n</code></pre></p> <p>4. Trust Relationship Lookup <pre><code>✓ Is there a registered publisher matching these claims?\n✓ Does the publisher have permission for this package?\n</code></pre></p> <p>All pass? → Package published. Any fail? → <code>403 Forbidden</code> with detailed error</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#security-properties","level":2,"title":"Security Properties","text":"","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#no-long-lived-secrets","level":3,"title":"No Long-Lived Secrets","text":"<p>Traditional tokens: - Valid for months/years - If leaked, usable until manually revoked - Broad permissions</p> <p>OIDC JWTs: - Valid for ~10 minutes - Automatically expire - Scoped to specific workflow + environment + package</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#cryptographic-verification","level":3,"title":"Cryptographic Verification","text":"<p>How JWT signatures work:</p> <ol> <li>GitHub has a private key (secret, never shared)</li> <li>GitHub publishes a public key at <code>https://token.actions.githubusercontent.com/.well-known/jwks</code></li> <li>JWT is signed with private key</li> <li>PyPI verifies signature with public key</li> </ol> <p>Why it's secure: - Only GitHub can create valid JWTs (has private key) - Anyone can verify JWTs (public key is public) - Forging a JWT requires private key (mathematically infeasible)</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#scoped-permissions","level":3,"title":"Scoped Permissions","text":"<p>Each trust relationship is scoped to:</p> Scope Example Purpose Repository <code>nfb2021/canvodpy</code> Only this repo can publish Workflow <code>publish_testpypi.yml</code> Only this workflow file Environment <code>testpypi</code> Only this environment Package <code>canvod-readers</code> Only this PyPI package <p>Example attack prevention:</p> <ul> <li>Can't use JWT from different repo</li> <li>Can't use JWT from different workflow</li> <li>Can't use JWT from different environment</li> <li>Can't publish to different package</li> <li>Can't replay expired JWT</li> <li>Can't modify JWT claims (signature breaks)</li> </ul>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#audit-trail","level":3,"title":"Audit Trail","text":"<p>GitHub records: - Who triggered the workflow - What tag/branch/commit - When it ran - What environment was used</p> <p>PyPI records: - Which JWT claims were used - When publish occurred - Which files were uploaded</p> <p>Full traceability: \"v0.1.0 was published by workflow X triggered by user Y at time Z\"</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#why-multiple-publishers","level":2,"title":"Why Multiple Publishers?","text":"<p>Each package needs its own trust relationship:</p> <pre><code>PyPI Trust Registry:\n┌──────────────────┬─────────────────┬──────────────────────┬─────────┐\n│ Package          │ Repository      │ Workflow             │ Env     │\n├──────────────────┼─────────────────┼──────────────────────┼─────────┤\n│ canvod-readers   │ nfb2021/canvodpy│ publish_testpypi.yml │ testpypi│\n│ canvod-auxiliary       │ nfb2021/canvodpy│ publish_testpypi.yml │ testpypi│\n│ canvod-grids     │ nfb2021/canvodpy│ publish_testpypi.yml │ testpypi│\n│ ...              │ ...             │ ...                  │ ...     │\n└──────────────────┴─────────────────┴──────────────────────┴─────────┘\n</code></pre> <p>Why? PyPI validates per-package:</p> <p>\"Does this JWT have permission to publish THIS specific package?\"</p> <p>If you only register <code>canvodpy</code>, the workflow can't publish <code>canvod-readers</code>.</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#github-environments","level":2,"title":"GitHub Environments","text":"<pre><code>environment: testpypi  # ← Why this matters\n</code></pre>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#security-benefits","level":3,"title":"Security Benefits","text":"<p>1. Protection Rules - Require manual approval before publish - Limit to specific branches - Add reviewers</p> <p>2. Secrets Scoping - Environment-specific secrets (if needed) - Separate test vs production</p> <p>3. Audit &amp; History - See all deployments to this environment - Track who approved what - Deployment logs</p> <p>4. OIDC Scoping - JWT includes environment name in claims - Different environments = different publishers - Example: <code>testpypi</code> environment → test.pypi.org            <code>pypi</code> environment → pypi.org</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#common-misconfigurations","level":2,"title":"Common Misconfigurations","text":"","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#1-mismatched-environment-name","level":3,"title":"1. Mismatched Environment Name","text":"<p>Workflow says: <pre><code>environment: testpypi\n</code></pre></p> <p>PyPI registered with: <pre><code>Environment: test-pypi  # ← Note the dash!\n</code></pre></p> <p>Result: JWT validation fails (environment claim doesn't match)</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#2-mismatched-workflow-filename","level":3,"title":"2. Mismatched Workflow Filename","text":"<p>Workflow filename: <pre><code>.github/workflows/publish-testpypi.yml  # ← Note the dash!\n</code></pre></p> <p>PyPI registered with: <pre><code>Workflow: publish_testpypi.yml  # ← Note the underscore!\n</code></pre></p> <p>Result: JWT validation fails</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#3-wrong-repository-owner","level":3,"title":"3. Wrong Repository Owner","text":"<p>Your GitHub username: <code>nfb2021</code> PyPI registered with: <code>NFB2021</code> (different case!)</p> <p>Result: JWT validation fails (case-sensitive!)</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#4-forgot-to-register-package","level":3,"title":"4. Forgot to Register Package","text":"<p>You set up OIDC for <code>canvodpy</code> but forgot <code>canvod-readers</code>.</p> <p>Result: <code>canvodpy</code> publishes successfully, <code>canvod-readers</code> fails</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#advantages-over-other-methods","level":2,"title":"Advantages Over Other Methods","text":"","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#vs-usernamepassword","level":3,"title":"vs. Username/Password","text":"Method OIDC Username/Password Security Cryptographic Password-based Lifetime 10 minutes Forever (until changed) Revocation Automatic expiry Manual change MFA Compatible Always Sometimes Audit Trail Built-in Manual logging","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#vs-api-tokens","level":3,"title":"vs. API Tokens","text":"Method OIDC API Token Storage None needed GitHub Secrets Rotation Not needed Manual (quarterly?) Scope Per-package Account-wide Leak Risk Low (expires fast) High (long-lived) Setup One-time registration Generate + store","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#real-world-analogy","level":2,"title":"Real-World Analogy","text":"<p>API Tokens = House Key - You give someone a key - They can use it anytime - If lost, anyone can use it - Must change locks to revoke</p> <p>OIDC = Hotel Key Card - Hotel validates your identity (JWT) - Issues temporary card (expires checkout time) - Card only works for your room (scoped) - Card automatically deactivates (expiration) - Can't duplicate card (cryptographic signature) - Full audit trail (who, when, what room)</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/OIDC_EXPLAINED/#further-reading","level":2,"title":"Further Reading","text":"<ul> <li>PyPI Trusted Publishers Documentation</li> <li>OIDC Specification</li> <li>GitHub OIDC Documentation</li> <li>JWT.io - Debug JWTs</li> <li>GitHub JWKS Endpoint</li> </ul> <p>Questions? Open an issue or discussion!</p>","path":["Release & Publishing","OIDC Trusted Publishing Explained"],"tags":[]},{"location":"guides/PYPI_SETUP/","level":1,"title":"PYPI SETUP","text":"","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#overview-multi-package-monorepo-publishing","level":2,"title":"Overview: Multi-Package Monorepo Publishing","text":"<p>This monorepo publishes 8 separate packages to PyPI:</p> <p>Individual packages: 1. <code>canvod-readers</code> - GNSS data readers 2. <code>canvod-auxiliary</code> - Auxiliary data handling 3. <code>canvod-grids</code> - Grid operations 4. <code>canvod-store</code> - Storage backends 5. <code>canvod-utils</code> - Utility functions 6. <code>canvod-viz</code> - Visualization tools 7. <code>canvod-vod</code> - VOD calculation</p> <p>Umbrella package: 8. <code>canvodpy</code> - Depends on all 7 above</p> <p>User experience: <pre><code>pip install canvodpy  # Gets all 8 packages automatically\n</code></pre></p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#quick-commands","level":2,"title":"Quick Commands","text":"<pre><code># Build all 8 packages locally\njust build-all\n\n# Manual publish to TestPyPI (requires credentials)\njust publish-testpypi\n\n# Manual publish to PyPI (requires credentials)\njust publish-pypi\n\n# Automated publish (beta) - triggers workflow\ngit tag v0.1.0-beta.1 &amp;&amp; git push --tags\n\n# Automated publish (production) - triggers workflow\ngit tag v0.1.0 &amp;&amp; git push --tags\n</code></pre>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#phase-1-testpypi-setup-completed","level":2,"title":"Phase 1: TestPyPI Setup (Completed)","text":"","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-1-create-testpypi-account-done","level":3,"title":"Step 1: Create TestPyPI Account (Done)","text":"<p>Already done! Account created at https://test.pypi.org</p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-2-manual-first-publish-done","level":3,"title":"Step 2: Manual First Publish (Done)","text":"<p>Already completed with <code>twine upload --repository testpypi dist/*</code></p> <p>All 8 packages published: - https://test.pypi.org/project/canvod-readers/ - https://test.pypi.org/project/canvod-auxiliary/ - https://test.pypi.org/project/canvod-grids/ - https://test.pypi.org/project/canvod-store/ - https://test.pypi.org/project/canvod-utils/ - https://test.pypi.org/project/canvod-viz/ - https://test.pypi.org/project/canvod-vod/ - https://test.pypi.org/project/canvodpy/</p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-3-set-up-oidc-automation-done","level":3,"title":"Step 3: Set Up OIDC Automation (Done)","text":"<p>3.1: Create GitHub Environment</p> <ol> <li>Go to: https://github.com/nfb2021/canvodpy/settings/environments</li> <li>Click \"New environment\"</li> <li>Name: <code>testpypi</code></li> <li>Save</li> </ol> <p>3.2: Register Trusted Publishers (FOR EACH OF 8 PACKAGES)</p> <p>For each package, go to its publishing settings: - https://test.pypi.org/manage/project/canvod-readers/settings/publishing/ - https://test.pypi.org/manage/project/canvod-auxiliary/settings/publishing/ - https://test.pypi.org/manage/project/canvod-grids/settings/publishing/ - https://test.pypi.org/manage/project/canvod-store/settings/publishing/ - https://test.pypi.org/manage/project/canvod-utils/settings/publishing/ - https://test.pypi.org/manage/project/canvod-viz/settings/publishing/ - https://test.pypi.org/manage/project/canvod-vod/settings/publishing/ - https://test.pypi.org/manage/project/canvodpy/settings/publishing/</p> <p>On each page: 1. Click \"Add a new publisher\" 2. Select \"GitHub\" 3. Fill in:    - Owner: <code>nfb2021</code>    - Repository: <code>canvodpy</code>    - Workflow: <code>publish_testpypi.yml</code>    - Environment: <code>testpypi</code> 4. Click \"Add\"</p> <p>3.3: Test OIDC Publishing</p> <pre><code>git tag v0.1.0-beta.1 -m \"Test OIDC\"\ngit push --tags\ncd /path/to/canvodpy\nuv build\n</code></pre> <p>Upload manually (first time only): <pre><code>uvx twine upload --repository testpypi dist/*\n</code></pre></p> <p>When prompted: <pre><code>username: your_testpypi_username\npassword: your_testpypi_password\n</code></pre></p> <p>Success looks like: <pre><code>Uploading canvodpy-0.1.0-py3-none-any.whl\n100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n</code></pre></p> <p>Verify it's there: - Go to: https://test.pypi.org/project/canvodpy/</p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-3-configure-oidc-on-testpypi","level":3,"title":"Step 3: Configure OIDC on TestPyPI","text":"<ol> <li>Go to project settings:</li> <li> <p>https://test.pypi.org/manage/project/canvodpy/settings/publishing/</p> </li> <li> <p>Scroll to \"Publishing\"</p> </li> <li> <p>Click \"Add a new publisher\"</p> </li> <li> <p>Fill in the form: <pre><code>PyPI project name:     canvodpy\nOwner:                 nfb2021\nRepository name:       canvodpy\nWorkflow name:         publish_testpypi.yml\nEnvironment name:      test-release\n</code></pre></p> </li> <li> <p>Click \"Add\"</p> </li> <li> <p>You should see: <pre><code>✓ Trusted publisher added\n\nPublisher:\n- Repository: nfb2021/canvodpy\n- Workflow: publish_testpypi.yml\n- Environment: test-release\n</code></pre></p> </li> </ol>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-4-create-github-environment","level":3,"title":"Step 4: Create GitHub Environment","text":"<ol> <li>Go to your repo:</li> <li> <p>https://github.com/nfb2021/canvodpy</p> </li> <li> <p>Settings → Environments</p> </li> <li> <p>New environment:</p> </li> <li>Name: <code>test-release</code></li> <li> <p>Click \"Configure environment\"</p> </li> <li> <p>Add protection rules (optional):</p> </li> <li>Required reviewers: Add yourself</li> <li> <p>Wait timer: 0 minutes (it's just testing)</p> </li> <li> <p>Save</p> </li> </ol>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-5-create-testpypi-workflow","level":3,"title":"Step 5: Create TestPyPI Workflow","text":"<p>Create: <code>.github/workflows/publish_testpypi.yml</code></p> <pre><code>name: Publish to TestPyPI\n\non:\n  release:\n    types: [published]\n\n  # Manual trigger for testing\n  workflow_dispatch:\n\npermissions:\n  id-token: write  # REQUIRED for OIDC\n  contents: read\n\njobs:\n  publish-testpypi:\n    name: Upload to TestPyPI\n    runs-on: ubuntu-latest\n    environment: test-release\n\n    # Only run if tag contains 'beta' or 'alpha' (for testing)\n    if: contains(github.ref, 'beta') || contains(github.ref, 'alpha')\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v6\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v7\n        with:\n          version: \"0.9.27\"\n\n      - name: Set up Python\n        run: uv python install 3.13\n\n      - name: Build package\n        run: uv build\n\n      - name: Publish to TestPyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          repository-url: https://test.pypi.org/legacy/\n          print-hash: true\n</code></pre>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-6-test-the-workflow","level":3,"title":"Step 6: Test the Workflow","text":"<p>Create a beta release: <pre><code>just release 0.1.0-beta.1\ngit push origin main\ngit push origin --tags\n</code></pre></p> <p>Publish the draft release: 1. Go to: https://github.com/nfb2021/canvodpy/releases 2. Find draft for <code>v0.1.0-beta.1</code> 3. Click \"Publish release\"</p> <p>Watch the workflow: 1. Go to: https://github.com/nfb2021/canvodpy/actions 2. See \"Publish to TestPyPI\" running 3. Wait ~2-3 minutes 4. Should say \"✓ Complete\"</p> <p>Verify upload: - https://test.pypi.org/project/canvodpy/ - Should see version 0.1.0b1</p> <p>Test installation: <pre><code># Create test environment\nuv venv test-env\nsource test-env/bin/activate  # or test-env\\Scripts\\activate on Windows\n\n# Install from TestPyPI\npip install --index-url https://test.pypi.org/simple/ canvodpy\n\n# Test it works\npython -c \"import canvodpy; print(canvodpy.__version__)\"\n# Should print: 0.1.0b1\n\n# Clean up\ndeactivate\nrm -rf test-env\n</code></pre></p> <p>If it works, you are ready for real PyPI.</p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#phase-2-real-pypi-setup","level":2,"title":"Phase 2: Real PyPI Setup","text":"","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-7-create-real-pypi-account","level":3,"title":"Step 7: Create Real PyPI Account","text":"<ol> <li>Go to: https://pypi.org/account/register/</li> <li>Fill in details (can use same email as TestPyPI)</li> <li>Verify email</li> <li>Enable 2FA (PyPI requires it for trusted publishers!)</li> </ol>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-8-reserve-package-name-on-pypi","level":3,"title":"Step 8: Reserve Package Name on PyPI","text":"<p>CAUTION: This is permanent! Double-check everything.</p> <pre><code># Build fresh\ncd /path/to/canvodpy\nrm -rf dist/\nuv build\n\n# Upload to REAL PyPI\nuvx twine upload dist/*\n</code></pre> <p>When prompted: <pre><code>username: your_pypi_username\npassword: your_pypi_password\n</code></pre></p> <p>Verify: - https://pypi.org/project/canvodpy/</p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-9-configure-oidc-on-real-pypi","level":3,"title":"Step 9: Configure OIDC on Real PyPI","text":"<p>Exactly same as TestPyPI:</p> <ol> <li>Go to:</li> <li> <p>https://pypi.org/manage/project/canvodpy/settings/publishing/</p> </li> <li> <p>Add publisher: <pre><code>PyPI project name:     canvodpy\nOwner:                 nfb2021\nRepository name:       canvodpy\nWorkflow name:         publish_pypi.yml\nEnvironment name:      release\n</code></pre></p> </li> <li> <p>Click \"Add\"</p> </li> </ol>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-10-create-production-github-environment","level":3,"title":"Step 10: Create Production GitHub Environment","text":"<ol> <li>Settings → Environments → New</li> <li>Name: <code>release</code></li> <li>Protection rules (IMPORTANT!):</li> <li>Required reviewers: Add all maintainers</li> <li>Wait timer: 5 minutes (think before publishing)</li> <li>Save</li> </ol>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-11-create-production-workflow","level":3,"title":"Step 11: Create Production Workflow","text":"<p>Create: <code>.github/workflows/publish_pypi.yml</code></p> <pre><code>name: Publish to PyPI\n\non:\n  release:\n    types: [published]\n\npermissions:\n  id-token: write\n  contents: read\n\njobs:\n  publish-pypi:\n    name: Upload to PyPI\n    runs-on: ubuntu-latest\n    environment: release\n\n    # Skip beta/alpha releases for production PyPI\n    if: \"!contains(github.ref, 'beta') &amp;&amp; !contains(github.ref, 'alpha')\"\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v6\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v7\n        with:\n          version: \"0.9.27\"\n\n      - name: Set up Python\n        run: uv python install 3.13\n\n      - name: Build package\n        run: uv build\n\n      - name: Check package\n        run: |\n          uvx twine check dist/*\n          ls -lh dist/\n\n      - name: Publish to PyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          print-hash: true\n</code></pre>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#step-12-first-production-release","level":3,"title":"Step 12: First Production Release","text":"<p>When ready for v0.1.0:</p> <pre><code># Create release\njust release 0.1.0\ngit push origin main\ngit push origin --tags\n</code></pre> <p>Publish: 1. Go to releases 2. Find draft v0.1.0 3. Review carefully! 4. Click \"Publish release\" 5. Approve the workflow when prompted 6. Wait 5 minutes (your wait timer) 7. Workflow runs 8. Package published to PyPI!</p> <p>Verify: <pre><code>pip install canvodpy\npython -c \"import canvodpy; print(canvodpy.__version__)\"\n# Should print: 0.1.0\n</code></pre></p> <p>Your package is now on PyPI.</p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#summary-what-you-get","level":2,"title":"Summary: What You Get","text":"","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#for-betatest-releases","level":3,"title":"For Beta/Test Releases:","text":"<pre><code>just release 0.2.0-beta.1\ngit push --tags\n# → Publishes to TestPyPI\n</code></pre>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#for-stable-releases","level":3,"title":"For Stable Releases:","text":"<pre><code>just release 0.2.0\ngit push --tags\n# → Publishes to real PyPI\n</code></pre>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#users-can-install","level":3,"title":"Users Can Install:","text":"<pre><code>pip install canvodpy       # Latest stable\npip install canvodpy==0.2.0   # Specific version\n</code></pre>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#publisher-not-configured","level":3,"title":"\"Publisher not configured\"","text":"<p>Problem: OIDC upload fails with authentication error</p> <p>Check: 1. Environment name in workflow matches PyPI config 2. Workflow name matches exactly 3. Repository owner/name correct 4. You've enabled 2FA on PyPI</p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#package-already-exists","level":3,"title":"\"Package already exists\"","text":"<p>Problem: Can't upload same version twice</p> <p>Solution: Bump version and try again <pre><code>just bump patch  # 0.1.0 → 0.1.1\n</code></pre></p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#build-failed","level":3,"title":"\"Build failed\"","text":"<p>Problem: <code>uv build</code> doesn't work</p> <p>Check: <pre><code># Test locally first\nuv build\nls dist/\n# Should see .whl and .tar.gz files\n</code></pre></p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#environment-approval-stuck","level":3,"title":"\"Environment approval stuck\"","text":"<p>Problem: Workflow waiting for approval forever</p> <p>Solution: Check Environment protection rules - Maybe you need to approve manually - Check who's configured as reviewer</p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#best-practices","level":2,"title":"Best Practices","text":"<ol> <li>Always test on TestPyPI first</li> <li>Use <code>-beta.X</code> or <code>-alpha.X</code> versions</li> <li> <p>Verify installation works</p> </li> <li> <p>Use environment protection</p> </li> <li>Required reviewers prevent accidents</li> <li> <p>Wait timer gives you time to think</p> </li> <li> <p>Version numbers are permanent</p> </li> <li>Can't delete from PyPI</li> <li>Can't reuse version numbers</li> <li> <p>Be careful!</p> </li> <li> <p>Keep pre-releases separate</p> </li> <li>TestPyPI: beta/alpha releases</li> <li> <p>Real PyPI: stable releases only</p> </li> <li> <p>Monitor your releases</p> </li> <li>Check PyPI after each upload</li> <li>Verify install works</li> <li>Check download stats</li> </ol>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/PYPI_SETUP/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li> Test with beta release on TestPyPI</li> <li> First production release v0.1.0</li> <li> Set up Zenodo for DOI (academic citations)</li> <li> Configure GitHub Sponsors (optional)</li> <li> Add download badges to README</li> </ul> <p>Questions? Check <code>/files/how_it_works.md</code> for detailed explanations!</p>","path":["Release & Publishing","PYPI SETUP"],"tags":[]},{"location":"guides/ZENODO_SETUP/","level":1,"title":"DOI and Citations Setup for FAIR Science","text":"<p>Goal: Make canvodpy properly citable with permanent DOI for academic papers.</p> <p>Standard Solution: Zenodo + CITATION.cff + DOI badge</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#zenodo","level":2,"title":"Zenodo","text":"<p>Zenodo is a research data repository operated by CERN:</p> <ul> <li>Free for open source</li> <li>Automatic DOI for each GitHub release</li> <li>Permanent archival (CERN guarantees 20+ years)</li> <li>FAIR compliant (Findable, Accessible, Interoperable, Reusable)</li> <li>Integrates with GitHub (fully automated)</li> <li>Searchable by researchers worldwide</li> <li>Version-specific DOIs (each release gets unique DOI)</li> </ul>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#how-it-works","level":2,"title":"How It Works","text":"","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#1-initial-setup-one-time","level":3,"title":"1. Initial Setup (One-Time)","text":"<p>You link your GitHub repo to Zenodo: - Zenodo watches for new releases - No manual uploads needed</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#2-every-release","level":3,"title":"2. Every Release","text":"<pre><code>just release 0.1.0\ngit push &amp;&amp; git push --tags\n</code></pre> <p>Automatic workflow: 1. GitHub creates release v0.1.0 2. Zenodo detects new release 3. Zenodo archives the release 4. Zenodo mints DOI: <code>10.5281/zenodo.XXXXX</code> 5. Zenodo creates citation metadata</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#3-users-cite-your-work","level":3,"title":"3. Users Cite Your Work","text":"<p>Researchers can cite: <pre><code>Bader, N. F. (2026). canvodpy: GNSS Vegetation Optical Depth Analysis (v0.1.0).\nZenodo. https://doi.org/10.5281/zenodo.XXXXX\n</code></pre></p> <p>Version-specific DOIs: - v0.1.0 → DOI: 10.5281/zenodo.12345 - v0.2.0 → DOI: 10.5281/zenodo.12346 - Concept DOI → DOI: 10.5281/zenodo.12344 (always latest)</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#step-1-create-citationcff-done","level":2,"title":"Step 1: Create CITATION.cff (Done)","text":"<p>File created: <code>CITATION.cff</code></p> <p>This enables: - \"Cite this repository\" button on GitHub - Machine-readable citation metadata - Export to BibTeX, APA, EndNote, etc. - Zenodo reads this for metadata</p> <p>TODO: Add your ORCID iD to <code>CITATION.cff</code> (line 14) - Get ORCID: https://orcid.org/register - Helps link your publications to your code</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#step-2-connect-github-to-zenodo","level":2,"title":"Step 2: Connect GitHub to Zenodo","text":"","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#21-create-zenodo-account","level":3,"title":"2.1: Create Zenodo Account","text":"<ol> <li>Go to: https://zenodo.org</li> <li>Click \"Sign up\"</li> <li>Important: Sign up with GitHub!</li> <li>Click \"Log in with GitHub\"</li> <li>This enables automatic integration</li> </ol>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#22-enable-repository-on-zenodo","level":3,"title":"2.2: Enable Repository on Zenodo","text":"<ol> <li>After logging in, go to: https://zenodo.org/account/settings/github/</li> <li>Find <code>nfb2021/canvodpy</code> in the list</li> <li>Toggle ON the switch next to it</li> <li>Zenodo is now watching for releases!</li> </ol>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#23-verify-connection","level":3,"title":"2.3: Verify Connection","text":"<ul> <li>Should see: \"✓ Enabled\" next to canvodpy</li> <li>Zenodo will archive next release</li> </ul>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#step-3-create-first-release","level":2,"title":"Step 3: Create First Release","text":"","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#option-a-test-with-beta-release","level":3,"title":"Option A: Test with Beta Release","text":"<pre><code>git tag v0.1.0-beta.2 -m \"Test Zenodo DOI creation\"\ngit push --tags\n</code></pre> <ul> <li>Creates TestPyPI release</li> <li>Zenodo creates DOI</li> <li>You can test the process safely</li> </ul>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#option-b-production-release","level":3,"title":"Option B: Production Release","text":"<pre><code>just release 0.1.0\ngit push &amp;&amp; git push --tags\n</code></pre> <ul> <li>Creates production PyPI release</li> <li>Zenodo creates DOI</li> <li>This is the \"real\" first citable version</li> </ul>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#after-release","level":3,"title":"After Release","text":"<ol> <li>Go to: https://zenodo.org/account/settings/github/repository/nfb2021/canvodpy</li> <li>You'll see your release listed</li> <li>Click to view the DOI</li> <li>Copy the DOI badge markdown</li> </ol> <p>Example DOI badge: <pre><code>[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12345.svg)](https://doi.org/10.5281/zenodo.12345)\n</code></pre></p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#step-4-add-doi-badge-to-readme","level":2,"title":"Step 4: Add DOI Badge to README","text":"<p>Add to <code>README.md</code> (after other badges):</p> <pre><code>[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.XXXXX.svg)](https://doi.org/10.5281/zenodo.XXXXX)\n</code></pre> <p>Replace <code>XXXXX</code> with your actual Zenodo ID.</p> <p>Tip: Use the \"concept DOI\" (always points to latest version)</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#step-5-update-documentation","level":2,"title":"Step 5: Update Documentation","text":"<p>Add citation section to <code>README.md</code>:</p> <pre><code>## Citing canvodpy\n\nIf you use canvodpy in your research, please cite:\n\nBader, N. F. (2026). canvodpy: GNSS Vegetation Optical Depth Analysis (v0.1.0).\nZenodo. https://doi.org/10.5281/zenodo.XXXXX\n\nBibTeX:\n\\`\\`\\`bibtex\n@software{bader2026canvodpy,\n  author       = {Bader, Nicolas François},\n  title        = {canvodpy: GNSS Vegetation Optical Depth Analysis},\n  year         = 2026,\n  publisher    = {Zenodo},\n  version      = {v0.1.0},\n  doi          = {10.5281/zenodo.XXXXX},\n  url          = {https://doi.org/10.5281/zenodo.XXXXX}\n}\n\\`\\`\\`\n</code></pre>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#how-citations-work","level":2,"title":"How Citations Work","text":"","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#for-researchers-using-your-package","level":3,"title":"For Researchers Using Your Package","text":"<p>From GitHub: 1. Click \"Cite this repository\" (top right) 2. Choose format (APA, BibTeX, etc.) 3. Copy citation</p> <p>From Zenodo: 1. Visit Zenodo DOI link 2. Click \"Export\" → Choose format 3. Copy citation</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#for-papers","level":3,"title":"For Papers","text":"<p>Researchers cite specific versions:</p> <p>Correct:</p> <p>\"We used canvodpy v0.1.0 (Bader, 2026) for VOD calculations.\"</p> <p>Reference:</p> <p>Bader, N. F. (2026). canvodpy v0.1.0. Zenodo. https://doi.org/10.5281/zenodo.12345</p> <p>Why version-specific? - Reproducibility: v0.1.0 code is archived forever - Transparency: Readers know exactly what was used - FAIR principles: Findable specific version</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#concept-doi-vs-version-doi","level":3,"title":"Concept DOI vs Version DOI","text":"<p>Concept DOI (10.5281/zenodo.12344): - Always points to latest version - Use for: General citations, badges</p> <p>Version DOI (10.5281/zenodo.12345): - Points to specific version (v0.1.0) - Use for: Research papers (reproducibility)</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#benefits-for-your-project","level":2,"title":"Benefits for Your Project","text":"","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#academic-recognition","level":3,"title":"Academic Recognition","text":"<ul> <li>Citable software = recognized academic output</li> <li>DOI = trackable impact (who cites your work)</li> <li>ORCID integration = links to your researcher profile</li> <li>Searchable = increases discoverability</li> </ul>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#fair-compliance","level":3,"title":"FAIR Compliance","text":"<ul> <li>Findable: DOI, Zenodo indexing, GitHub</li> <li>Accessible: Open access, permanent archival</li> <li>Interoperable: Standard metadata formats</li> <li>Reusable: Clear license, version tracking</li> </ul>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#professional-standards","level":3,"title":"Professional Standards","text":"<ul> <li>Permanent archival (CERN guarantees preservation)</li> <li>Version tracking (every release archived)</li> <li>Metadata standards (Dublin Core, DataCite)</li> <li>Trusted repository (used by CERN, NASA, ESA)</li> </ul>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#i-dont-see-my-repo-on-zenodo","level":3,"title":"\"I don't see my repo on Zenodo\"","text":"<p>Solutions: 1. Ensure you logged in with GitHub 2. Try logging out and back in 3. Check repo is public (Zenodo only indexes public repos) 4. Wait a few minutes (sync can take time)</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#no-doi-was-created-for-my-release","level":3,"title":"\"No DOI was created for my release\"","text":"<p>Solutions: 1. Check toggle is ON at https://zenodo.org/account/settings/github/ 2. Ensure release is published (not draft) 3. Check Zenodo email for errors 4. May need to create release again</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#how-do-i-update-citation-metadata","level":3,"title":"\"How do I update citation metadata?\"","text":"<p>Solution: 1. Update <code>CITATION.cff</code> in your repo 2. Create new release 3. Zenodo reads updated metadata for new DOI</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#can-i-get-doi-for-old-releases","level":3,"title":"\"Can I get DOI for old releases?\"","text":"<p>Solution: - No, Zenodo only archives releases created AFTER integration - But: You can create new tags/releases - Old code is still in GitHub history</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#example-how-it-looks","level":2,"title":"Example: How It Looks","text":"","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#on-github","level":3,"title":"On GitHub","text":"<p>Cite button: <pre><code>[Cite this repository ▼]\n├── APA\n├── BibTeX\n└── More formats...\n</code></pre></p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#on-zenodo","level":3,"title":"On Zenodo","text":"<p>Record page: <pre><code>DOI: 10.5281/zenodo.12345\nTitle: canvodpy: GNSS Vegetation Optical Depth Analysis\nAuthors: Nicolas François Bader\nVersion: v0.1.0\nPublication date: 2026-02-04\nResource type: Software\nLicense: Apache-2.0\n\n[Download] [Cite] [Share] [Export]\n</code></pre></p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#in-papers","level":3,"title":"In Papers","text":"<p>Methods section:</p> <p>\"VOD was calculated using canvodpy v0.1.0 (Bader, 2026), an open-source Python package for GNSS-based vegetation analysis.\"</p> <p>References:</p> <p>Bader, N. F. (2026). canvodpy: GNSS Vegetation Optical Depth Analysis (v0.1.0). Zenodo. https://doi.org/10.5281/zenodo.12345</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#additional-metadata-optional","level":2,"title":"Additional Metadata (Optional)","text":"","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#zenodojson","level":3,"title":".zenodo.json","text":"<p>For advanced metadata customization, create <code>.zenodo.json</code>:</p> <pre><code>{\n  \"title\": \"canvodpy: GNSS Vegetation Optical Depth Analysis\",\n  \"description\": \"Python package for VOD calculation from GNSS SNR data\",\n  \"creators\": [\n    {\n      \"name\": \"Bader, Nicolas François\",\n      \"affiliation\": \"TU Wien\",\n      \"orcid\": \"0000-0000-0000-0000\"\n    }\n  ],\n  \"keywords\": [\"GNSS\", \"VOD\", \"vegetation\", \"remote sensing\"],\n  \"license\": \"Apache-2.0\",\n  \"communities\": [\n    {\"identifier\": \"zenodo\"}\n  ]\n}\n</code></pre> <p>Note: CITATION.cff is usually sufficient!</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#resources","level":2,"title":"Resources","text":"<ul> <li>Zenodo Homepage</li> <li>Zenodo GitHub Integration Guide</li> <li>CITATION.cff Format</li> <li>ORCID Registration</li> <li>FAIR Principles</li> <li>Making Software Citable</li> </ul>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/ZENODO_SETUP/#quick-start-checklist","level":2,"title":"Quick Start Checklist","text":"<ul> <li> Get ORCID iD (if you don't have one)</li> <li> Update CITATION.cff with your ORCID</li> <li> Create Zenodo account (log in with GitHub)</li> <li> Enable canvodpy on Zenodo settings</li> <li> Create first release (test with beta or go production)</li> <li> Copy DOI badge to README</li> <li> Add citation section to README</li> <li> Update documentation with citation instructions</li> </ul> <p>Questions? Check Zenodo help or ask in discussions!</p>","path":["Release & Publishing","Setting Up DOI and Citations (Zenodo)"],"tags":[]},{"location":"guides/architecture-design/","level":1,"title":"Architecture and Design Patterns","text":"","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#design-principles","level":2,"title":"Design Principles","text":"<p>canvodpy follows these principles:</p> <ol> <li>Modularity: Independent, reusable packages with minimal coupling</li> <li>Extensibility: ABC + Factory pattern for user-defined implementations</li> <li>Type safety: Modern Python type hints with Pydantic validation</li> <li>Simplicity: Explicit over implicit; no framework dependencies</li> <li>Scientific focus: Optimized for research workflows and reproducibility</li> </ol>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#package-architecture","level":2,"title":"Package Architecture","text":"","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#the-sollbruchstellen-principle","level":3,"title":"The Sollbruchstellen Principle","text":"<p>canvodpy applies the engineering concept of Sollbruchstellen (predetermined breaking points): packages are designed to be independent so they can be used separately or replaced without affecting the rest of the system.</p>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#independence-metrics","level":3,"title":"Independence Metrics","text":"<ul> <li>No circular dependencies</li> <li>4 packages with zero inter-package dependencies (57%)</li> <li>Only 3 total internal dependency edges</li> <li>Maximum dependency depth: 1</li> </ul>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#dependency-layers","level":3,"title":"Dependency Layers","text":"<pre><code>Foundation (0 dependencies):\n  canvod-readers, canvod-grids, canvod-vod, canvod-utils\n\nConsumer (1 dependency):\n  canvod-auxiliary -&gt; canvod-readers\n  canvod-viz -&gt; canvod-grids\n  canvod-store -&gt; canvod-grids\n\nOrchestration:\n  canvodpy -&gt; all packages\n</code></pre>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#abc-factory-pattern","level":2,"title":"ABC + Factory Pattern","text":"<pre><code>flowchart TD\n    subgraph ABCS[\"Abstract Base Classes\"]\n        READER_ABC[\"GNSSDataReader\\n(read, to_ds, validate_output)\"]\n        GRID_ABC[\"BaseGridBuilder\\n(_build_grid, build)\"]\n        VOD_ABC[\"VODCalculator\\n(calculate_vod)\"]\n        AUG_ABC[\"AugmentationStep\\n(apply)\"]\n    end\n\n    subgraph FACTORIES[\"Factory Registry\"]\n        RF[\"ReaderFactory\"]\n        GF[\"GridFactory\"]\n        VF[\"VODFactory\"]\n        AF[\"AugmentationFactory\"]\n    end\n\n    subgraph BUILTIN[\"Built-in Implementations\"]\n        RINEX3[\"Rnxv3Obs\\n(RINEX v3.04)\"]\n        EA[\"EqualAreaBuilder\"]\n        HP[\"HEALPixBuilder\"]\n        GD[\"GeodesicBuilder\"]\n        FB[\"FibonacciBuilder\"]\n        TO[\"TauOmegaZerothOrder\"]\n    end\n\n    subgraph CUSTOM[\"User Extension\"]\n        IMPL[\"Custom Implementation\\n(inherits ABC)\"]\n        REG[\"Factory.register(\\nname, class)\"]\n        USE[\"Factory.create(\\nname, **params)\"]\n    end\n\n    subgraph VALIDATION[\"Validation Chain\"]\n        V1[\"1. ABC compliance check\\n(issubclass)\"]\n        V2[\"2. Pydantic model\\nvalidation (at creation)\"]\n        V3[\"3. Runtime type\\nchecking (Generic T)\"]\n    end\n\n    READER_ABC --&gt; RF\n    GRID_ABC --&gt; GF\n    VOD_ABC --&gt; VF\n    AUG_ABC --&gt; AF\n\n    RINEX3 --&gt; RF\n    EA --&gt; GF\n    HP --&gt; GF\n    GD --&gt; GF\n    FB --&gt; GF\n    TO --&gt; VF\n\n    IMPL --&gt; REG\n    REG --&gt; V1\n    V1 --&gt; V2\n    V2 --&gt; V3\n    V3 --&gt; USE</code></pre>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#abstract-base-class-contract","level":3,"title":"Abstract Base Class (Contract)","text":"<pre><code>from abc import ABC, abstractmethod\nimport xarray as xr\n\nclass GNSSDataReader(ABC):\n    \"\"\"Abstract base class for GNSS data format readers.\"\"\"\n\n    @abstractmethod\n    def to_ds(self, keep_rnx_data_vars=None) -&gt; xr.Dataset:\n        \"\"\"Convert data to xarray Dataset.\"\"\"\n\n    @property\n    @abstractmethod\n    def metadata(self) -&gt; dict:\n        \"\"\"Return file metadata.\"\"\"\n</code></pre>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#concrete-implementation","level":3,"title":"Concrete Implementation","text":"<pre><code>class Rnxv3Obs(GNSSDataReader):\n    \"\"\"RINEX 3.04 observation file reader.\"\"\"\n\n    def to_ds(self, keep_rnx_data_vars=None) -&gt; xr.Dataset:\n        return xr.Dataset(...)\n\n    @property\n    def metadata(self) -&gt; dict:\n        return {\"version\": \"3.04\", ...}\n</code></pre>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#type-safe-factory","level":3,"title":"Type-Safe Factory","text":"<pre><code>from typing import Generic, TypeVar\n\nT = TypeVar('T')\n\nclass ComponentFactory(Generic[T]):\n    \"\"\"Generic factory with type-safe registration and creation.\"\"\"\n\n    _registry: dict[str, type[T]] = {}\n\n    @classmethod\n    def register(cls, name: str, component_class: type[T]) -&gt; None:\n        cls._registry[name] = component_class\n\n    @classmethod\n    def create(cls, name: str, **kwargs) -&gt; T:\n        if name not in cls._registry:\n            raise ValueError(f\"Unknown component: {name}\")\n        return cls._registry[name](**kwargs)\n\n    @classmethod\n    def list_available(cls) -&gt; list[str]:\n        return list(cls._registry.keys())\n\nclass ReaderFactory(ComponentFactory[GNSSDataReader]):\n    pass\n\nclass GridFactory(ComponentFactory[BaseGridBuilder]):\n    pass\n</code></pre>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#user-extensibility","level":3,"title":"User Extensibility","text":"<pre><code>from canvodpy import ReaderFactory\nfrom canvod.readers import GNSSDataReader\n\nclass MyLabReader(GNSSDataReader):\n    def to_ds(self, keep_rnx_data_vars=None) -&gt; xr.Dataset:\n        return xr.Dataset(...)\n\n    @property\n    def metadata(self) -&gt; dict:\n        return {\"format\": \"mylab_v1\"}\n\nReaderFactory.register(\"mylab_v1\", MyLabReader)\n</code></pre>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#unified-api-surface","level":2,"title":"Unified API Surface","text":"<p>canvodpy provides four API levels, all accessing the same underlying packages:</p>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#level-1-convenience-functions","level":3,"title":"Level 1 — Convenience Functions","text":"<pre><code>from canvodpy import process_date, calculate_vod\n\ndata = process_date(\"Rosalia\", \"2025001\")\nvod  = calculate_vod(\"Rosalia\", \"canopy_01\", \"reference_01\", \"2025001\")\n</code></pre>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#level-2-fluent-workflow-deferred-execution","level":3,"title":"Level 2 — Fluent Workflow (Deferred Execution)","text":"<p>Steps are recorded but not executed until a terminal method is called. Powered by <code>@step</code> and <code>@terminal</code> decorators.</p> <pre><code>import canvodpy\n\nresult = (canvodpy.workflow(\"Rosalia\")\n    .read(\"2025001\")\n    .preprocess()\n    .grid(\"equal_area\", angular_resolution=5.0)\n    .vod(\"canopy_01\", \"reference_01\")\n    .result())\n\n# Preview without executing:\nplan = canvodpy.workflow(\"Rosalia\").read(\"2025001\").preprocess().explain()\n</code></pre> <p>The <code>@step</code> decorator appends <code>(method, args, kwargs)</code> to an internal plan list and returns <code>self</code> for chaining. The <code>@terminal</code> decorator replays all recorded steps, clears the plan, then runs the terminal method.</p>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#level-3-vodworkflow-eager-execution","level":3,"title":"Level 3 — VODWorkflow (Eager Execution)","text":"<pre><code>from canvodpy import VODWorkflow\n\nwf = VODWorkflow(site=\"Rosalia\", grid=\"equal_area\")\ndatasets = wf.process_date(\"2025001\")\nvod = wf.calculate_vod(\"canopy_01\", \"reference_01\", \"2025001\")\n</code></pre>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#level-4-functional-api","level":3,"title":"Level 4 — Functional API","text":"<pre><code>from canvodpy import read_rinex, create_grid, assign_grid_cells\n\nds   = read_rinex(path, reader=\"rinex3\")\ngrid = create_grid(grid_type=\"equal_area\", angular_resolution=5.0)\nds   = assign_grid_cells(ds, grid)\n</code></pre>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#direct-package-access","level":3,"title":"Direct Package Access","text":"<pre><code>from canvod.readers import Rnxv3Obs\nfrom canvod.grids import EqualAreaBuilder\nfrom canvod.vod import VODCalculator\n</code></pre>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#configuration-management","level":2,"title":"Configuration Management","text":"<pre><code>flowchart LR\n    subgraph CONFIG[\"YAML Configuration\"]\n        PROC[\"processing.yaml\\n\\nProcessing parameters\\nStorage paths\\nCompression settings\\nNASA credentials\"]\n        SITES[\"sites.yaml\\n\\nReceiver definitions\\nDirectory mappings\\nVOD analysis pairs\"]\n        SIDS[\"sids.yaml\\n\\nSignal ID filters\\n(system, band, code)\"]\n    end\n\n    subgraph LOADER[\"Configuration Loader\"]\n        DEFAULTS[\"Package Defaults\\n(built-in YAML)\"]\n        MERGE[\"Deep Merge\\n(user overrides defaults)\"]\n        PYDANTIC[\"Pydantic Validation\\n(type safety, constraints)\"]\n    end\n\n    subgraph VALIDATED[\"CanvodConfig (validated)\"]\n        PC[\"ProcessingConfig\\n  ProcessingParams\\n  StorageConfig\\n  IcechunkConfig\\n  MetadataConfig\"]\n        SC[\"SitesConfig\\n  ReceiverConfig\\n  VODAnalysisConfig\"]\n        SIC[\"SidsConfig\\n  Signal filters\"]\n    end\n\n    subgraph VERSIONED[\"Versioned Data Storage\"]\n        IC_WRITE[\"Icechunk Write\\n(atomic commits)\"]\n        SNAPSHOT[\"Snapshot ID\\n(content-addressable)\"]\n        BRANCH[\"Branch Management\\n(main, rechunk)\"]\n        METADATA[\"Metadata Parquet\\n(RINEX hash, epochs,\\nsnapshot, timestamp)\"]\n    end\n\n    subgraph PROVENANCE[\"Full Provenance\"]\n        TRACE[\"Every dataset traceable to:\\n  Config version\\n  RINEX file hash\\n  Snapshot ID\\n  Processing timestamp\"]\n    end\n\n    PROC --&gt; MERGE\n    SITES --&gt; MERGE\n    SIDS --&gt; MERGE\n    DEFAULTS --&gt; MERGE\n    MERGE --&gt; PYDANTIC\n    PYDANTIC --&gt; PC\n    PYDANTIC --&gt; SC\n    PYDANTIC --&gt; SIC\n\n    PC --&gt; IC_WRITE\n    IC_WRITE --&gt; SNAPSHOT\n    SNAPSHOT --&gt; METADATA\n    BRANCH --&gt; SNAPSHOT\n    METADATA --&gt; TRACE\n    SC --&gt; TRACE</code></pre> <p>All configuration is managed through YAML files validated by Pydantic models:</p> <ul> <li><code>config/processing.yaml</code> — Processing parameters, storage paths, metadata, NASA CDDIS credentials</li> <li><code>config/sites.yaml</code> — Site definitions and receiver mappings</li> <li><code>config/sids.yaml</code> — Signal ID filtering rules</li> </ul> <pre><code>from canvod.utils.config import load_config\n\ncfg = load_config()\ncfg.processing.aux_data.nasa_earthdata_acc_mail  # Optional NASA CDDIS email\ncfg.processing.storage.gnss_root_dir             # Data directory\n</code></pre> <p>Initialize configuration with <code>canvodpy config init</code>, inspect with <code>canvodpy config show</code>.</p>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/architecture-design/#airflow-compatibility","level":2,"title":"Airflow Compatibility","text":"<p>API functions are stateless, suitable for distributed execution:</p> <pre><code>from airflow.decorators import task\n\n@task\ndef process_rinex_task(file_path: str, date: str) -&gt; str:\n    from canvodpy import read_rinex\n    obs = read_rinex(file_path, date)\n    output = f\"/data/obs_{date}.zarr\"\n    obs.to_zarr(output)\n    return output\n</code></pre> <p>Factory registration occurs at module import time, ensuring each worker process has access to all registered implementations.</p>","path":["Development","Architecture and Design Patterns"],"tags":[]},{"location":"guides/getting-started/","level":1,"title":"Getting Started","text":"<p>This guide walks you through everything you need — from creating a GitHub account to running your first test — so you can start contributing to canVODpy even if you have never used Git, GitHub, or Python tooling before.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#1-create-a-github-account","level":2,"title":"1. Create a GitHub account","text":"<p>GitHub is a website that hosts code and lets teams collaborate on software projects. If you don't already have an account, sign up at github.com/signup.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#2-install-homebrew-macos-only","level":2,"title":"2. Install Homebrew (macOS only)","text":"<p>Homebrew is the standard package manager for macOS — it lets you install developer tools with a single command. Open Terminal (press Cmd+Space, type \"Terminal\", press Enter) and run:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Follow the on-screen instructions. When it finishes, it will tell you to run two commands to add Homebrew to your PATH — copy and run them. Then verify:</p> <pre><code>brew --version\n</code></pre> <p>Note</p> <p>If you already have Homebrew installed, run <code>brew update</code> to make sure it's current.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#3-install-git","level":2,"title":"3. Install Git","text":"<p>Git is the version-control system that tracks changes in the codebase. Install it for your operating system:</p> macOSLinux (Debian/Ubuntu)Linux (Fedora)Windows <pre><code>brew install git\n</code></pre> <pre><code>sudo apt update &amp;&amp; sudo apt install git\n</code></pre> <pre><code>sudo dnf install git\n</code></pre> <p>Download and run the installer from git-scm.com. Accept the default options during installation. Afterwards, open Git Bash (installed with Git) to run the commands in this guide.</p> <p>Verify the installation:</p> <pre><code>git --version\n</code></pre> <p>You should see something like <code>git version 2.x.x</code>.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#4-set-up-an-ssh-key-for-github","level":2,"title":"4. Set up an SSH key for GitHub","text":"<p>SSH keys let you securely connect to GitHub without typing your password every time.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#generate-a-key","level":3,"title":"Generate a key","text":"macOS / LinuxWindows (Git Bash) <pre><code>ssh-keygen -t ed25519 -C \"your_email@example.com\"\n</code></pre> <p>Press Enter three times to accept the defaults (default file location, no passphrase).</p> <pre><code>ssh-keygen -t ed25519 -C \"your_email@example.com\"\n</code></pre> <p>Press Enter three times to accept the defaults.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#add-the-key-to-the-ssh-agent","level":3,"title":"Add the key to the SSH agent","text":"macOSLinuxWindows (Git Bash) <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n</code></pre> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n</code></pre> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n</code></pre>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#copy-the-public-key","level":3,"title":"Copy the public key","text":"macOSLinuxWindows (Git Bash) <pre><code>pbcopy &lt; ~/.ssh/id_ed25519.pub\n</code></pre> <pre><code>cat ~/.ssh/id_ed25519.pub\n</code></pre> <p>Select and copy the output.</p> <pre><code>clip &lt; ~/.ssh/id_ed25519.pub\n</code></pre>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#add-the-key-to-github","level":3,"title":"Add the key to GitHub","text":"<ol> <li>Go to github.com/settings/keys.</li> <li>Click New SSH key.</li> <li>Give it a title (e.g. \"My Laptop\"), paste the key, and click Add SSH key.</li> </ol>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#test-the-connection","level":3,"title":"Test the connection","text":"<pre><code>ssh -T git@github.com\n</code></pre> <p>You should see: <code>Hi &lt;username&gt;! You've successfully authenticated...</code></p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#5-configure-your-git-identity","level":2,"title":"5. Configure your Git identity","text":"<p>Tell Git who you are (this information appears in your commits):</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your_email@example.com\"\n</code></pre> <p>Use the same email you registered on GitHub.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#6-install-development-tools","level":2,"title":"6. Install development tools","text":"<p>canVODpy uses two command-line tools to manage the project:</p> <ul> <li>uv — a fast Python package manager that handles dependencies and virtual environments.</li> <li>just — a command runner (like a simplified Makefile) that provides shortcuts for common tasks.</li> </ul>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#install-uv","level":3,"title":"Install uv","text":"macOSLinuxWindows <pre><code>brew install uv\n</code></pre> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#install-just","level":3,"title":"Install just","text":"macOSLinux (any distro)Linux (Ubuntu 23.04+ / Debian 13+)Windows <pre><code>brew install just\n</code></pre> <p>This works on all Linux distributions (Ubuntu, Mint, Fedora, etc.):</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://just.systems/install.sh | bash -s -- --to ~/.local/bin\n</code></pre> <p>Make sure <code>~/.local/bin</code> is on your PATH. Add this to your <code>~/.bashrc</code> or <code>~/.zshrc</code> if needed:</p> <pre><code>export PATH=\"$HOME/.local/bin:$PATH\"\n</code></pre> <pre><code>sudo apt install just\n</code></pre> <p>Warning</p> <p>This does not work on Linux Mint or older Ubuntu versions. Use the \"Linux (any distro)\" tab instead.</p> <pre><code>winget install Casey.Just\n</code></pre>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#verify-both-tools","level":3,"title":"Verify both tools","text":"<pre><code>uv --version\njust --version\n</code></pre>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#7-fork-and-clone-the-repository","level":2,"title":"7. Fork and clone the repository","text":"<p>A fork is your own copy of the project on GitHub. You make changes in your fork and then propose them back to the original project via a \"pull request.\"</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#fork-on-github","level":3,"title":"Fork on GitHub","text":"<ol> <li>Go to github.com/nfb2021/canvodpy.</li> <li>Click the Fork button in the top-right corner.</li> <li>GitHub will create a copy at <code>github.com/YOUR_USERNAME/canvodpy</code>.</li> </ol>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#clone-your-fork","level":3,"title":"Clone your fork","text":"<pre><code>git clone git@github.com:YOUR_USERNAME/canvodpy.git\ncd canvodpy\n</code></pre> <p>Replace <code>YOUR_USERNAME</code> with your actual GitHub username.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#add-the-upstream-remote","level":3,"title":"Add the upstream remote","text":"<p>This lets you pull in updates from the original repository later:</p> <pre><code>git remote add upstream git@github.com:nfb2021/canvodpy.git\n</code></pre> <p>Verify your remotes:</p> <pre><code>git remote -v\n</code></pre> <p>You should see <code>origin</code> (your fork) and <code>upstream</code> (the original).</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#8-initialize-submodules","level":2,"title":"8. Initialize submodules","text":"<p>The repository uses two Git submodules that contain test data and demo data. Initialize them after cloning:</p> <pre><code>git submodule update --init --recursive\n</code></pre> <p>This pulls:</p> <ul> <li><code>packages/canvod-readers/tests/test_data</code> — validation test data (falsified/corrupted RINEX files for testing)</li> <li><code>demo</code> — clean real-world data for demos and documentation</li> </ul> <p>If you skip this step, tests that require these datasets will be automatically skipped.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#9-set-up-the-development-environment","level":2,"title":"9. Set up the development environment","text":"<p>From inside the <code>canvodpy</code> directory, run:</p> <pre><code># Verify required tools are available\njust check-dev-tools\n\n# Install all Python dependencies into a virtual environment\nuv sync\n\n# Install pre-commit hooks (automatic code checks before each commit)\njust hooks\n</code></pre>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#10-configure-the-project","level":2,"title":"10. Configure the project","text":"<p>canVODpy uses three YAML configuration files in the <code>config/</code> directory:</p> File Purpose <code>sites.yaml</code> Defines research sites: data root paths, receiver definitions (name, type, directory), and VOD analysis pairs. Each receiver's <code>directory</code> is the full relative path from the site data root to the raw RINEX date folders (e.g. <code>01_reference/01_GNSS/01_raw</code>). <code>processing.yaml</code> Processing parameters: metadata, credentials (NASA Earthdata), auxiliary data settings (agency, product type), time aggregation, compression, Icechunk storage, and store strategies. <code>sids.yaml</code> Signal ID (SID) filtering: choose <code>all</code>, a named <code>preset</code> (e.g. <code>gps_galileo</code>), or list <code>custom</code> SIDs to keep. <p>Each file has a corresponding <code>.example</code> template. To initialize them:</p> <pre><code>just config-init\n</code></pre> <p>After editing, validate your configuration:</p> <pre><code>just config-validate\n</code></pre> <p>To view the resolved configuration:</p> <pre><code>just config-show\n</code></pre>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#11-verify-everything-works","level":2,"title":"11. Verify everything works","text":"<p>Run the test suite:</p> <pre><code>just test\n</code></pre> <p>Run code-quality checks (linting, formatting, type checking):</p> <pre><code>just check\n</code></pre> <p>If both commands complete without errors, your environment is ready.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#12-working-in-teams","level":2,"title":"12. Working in teams","text":"<p>During a hackathon or collaborative sprint, each team works on its own topic (often aligned with a package like <code>canvod-grids</code> or <code>canvod-readers</code>). A shared develop branch (e.g. <code>develop/hackathon2026</code>) acts as the integration point — no one commits to it directly. Instead, each team gets its own team branch.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#branch-structure","level":3,"title":"Branch structure","text":"<pre><code>main\n└── develop/hackathon2026              ← integration branch (shared by all teams)\n    ├── team-grids/                    ← Team A\n    │   ├── (direct commits)           ← Workflow A\n    │   └── team-grids/add-healpix     ← Workflow B feature branches\n    ├── team-readers/                  ← Team B\n    └── team-vod/                      ← Team C\n</code></pre>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#set-up-the-team-branch","level":3,"title":"Set up the team branch","text":"<p>The team lead creates the branch once:</p> <pre><code>git checkout develop/hackathon2026\ngit checkout -b team-grids\ngit push -u origin team-grids\n</code></pre> <p>All other team members fetch and switch to it:</p> <pre><code>git fetch origin\ngit checkout team-grids\n</code></pre> <p>From here, choose the workflow that fits your team:</p> Workflow A: Push to the team branch directlyWorkflow B: Individual feature branches with PRs <p>Best for small teams (2–3 people) or when members work on separate files.</p> <p>Everyone commits and pushes to the team branch:</p> <pre><code># Make your changes, then:\ngit add &lt;files you changed&gt;\ngit commit -m \"feat(grids): add new grid type\"\ngit push origin team-grids\n</code></pre> <p>If someone else pushed before you, pull first:</p> <pre><code>git pull --rebase origin team-grids\ngit push origin team-grids\n</code></pre> <p>Best for larger teams or when you want lightweight review within the team.</p> <p>Create a feature branch off the team branch:</p> <pre><code>git checkout team-grids\ngit checkout -b team-grids/add-healpix\n</code></pre> <p>Work, commit, and push your feature branch:</p> <pre><code>git add &lt;files you changed&gt;\ngit commit -m \"feat(grids): add HEALPix support\"\ngit push -u origin team-grids/add-healpix\n</code></pre> <p>Then open a pull request on GitHub targeting <code>team-grids</code>.</p> <p>To stay up to date with teammates' merged work:</p> <pre><code>git checkout team-grids\ngit pull origin team-grids\ngit checkout team-grids/add-healpix\ngit rebase team-grids\n</code></pre>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#merge-the-teams-work-into-the-develop-branch","level":3,"title":"Merge the team's work into the develop branch","text":"<p>When your team's feature is ready, open one pull request from <code>team-grids</code> into <code>develop/hackathon2026</code> on GitHub. This is where the maintainer reviews the team's combined work.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#13-your-first-contribution","level":2,"title":"13. Your first contribution","text":"","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#make-your-changes","level":3,"title":"Make your changes","text":"<p>Edit files with your favorite text editor or IDE.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#run-quality-checks","level":3,"title":"Run quality checks","text":"<pre><code>just test\njust check\n</code></pre>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#stage-and-commit","level":3,"title":"Stage and commit","text":"<pre><code>git add &lt;files you changed&gt;\ngit commit -m \"feat(grids): add new grid type\"\n</code></pre> <p>Commit messages follow the Conventional Commits format: <code>type(scope): description</code>. Common types: <code>feat</code>, <code>fix</code>, <code>docs</code>, <code>test</code>, <code>refactor</code>.</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#push","level":3,"title":"Push","text":"<p>Push to your team branch or feature branch (see Working in teams above).</p>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#open-a-pull-request","level":3,"title":"Open a pull request","text":"<ol> <li>Go to your fork on GitHub — you'll see a banner suggesting to open a pull request.</li> <li>Click Compare &amp; pull request.</li> <li>Set the base branch to your team branch (or the develop branch, depending on your workflow).</li> <li>Add a title and description, then click Create pull request.</li> </ol>","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#14-common-commands-cheat-sheet","level":2,"title":"14. Common commands cheat sheet","text":"Command What it does <code>just test</code> Run all tests <code>just check</code> Lint, format, and type-check all code <code>just hooks</code> Install pre-commit hooks <code>just check-dev-tools</code> Verify uv, just, and python3 are installed <code>just config-init</code> Initialize configuration files from templates <code>just config-validate</code> Validate the current configuration <code>just config-show</code> Show the resolved configuration <code>just docs</code> Preview documentation locally <code>just test-coverage</code> Run tests with coverage report <code>just clean</code> Remove build artifacts and caches <code>uv sync</code> Install/update Python dependencies","path":["Development","Getting Started"],"tags":[]},{"location":"guides/getting-started/#15-troubleshooting","level":2,"title":"15. Troubleshooting","text":"\"command not found\" for uv, just, or git The tool is not installed or not on your <code>PATH</code>. Re-run the installation step and, if needed, open a new terminal window so your shell picks up the updated <code>PATH</code>. \"Permission denied (publickey)\" when pushing or cloning Your SSH key is not set up correctly. Go back to step 4 and make sure the key is added to both the SSH agent and your GitHub account. <code>uv sync</code> fails with a Python version error canVODpy requires Python 3.13 or 3.14. Install a supported version with <code>uv python install 3.13</code> and try again. Pre-commit hook fails on commit Run <code>just check</code> — it will auto-fix most linting and formatting issues. Stage the fixed files and commit again. \"push rejected\" or \"failed to push\" <p>Your branch is behind the remote. Pull the latest changes first:</p> <pre><code>git pull --rebase origin my-feature\n</code></pre> Windows: <code>just</code> says \"could not find the shell\" or \"system cannot find the path\" <p>The Justfile expects Git Bash at <code>C:\\Program Files\\Git\\bin\\bash.exe</code> (the default Git for Windows location). If Git is installed elsewhere, find its location by running in PowerShell:</p> <pre><code>where.exe bash\n</code></pre> <p>Then update the path in the first lines of the Justfile:</p> <pre><code>set windows-shell := [\"C:/YOUR/ACTUAL/PATH/TO/bash.exe\", \"-c\"]\n</code></pre> <p>Warning</p> <p>Do not commit this change — it's specific to your machine. The default path works for most installations and for CI.</p> Windows: uv install fails with \"execution of scripts is disabled\" <p>PowerShell's default execution policy blocks scripts. Use the bypass flag:</p> <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> Windows: line ending warnings (<code>LF will be replaced by CRLF</code>) <p>Configure Git to keep Unix-style line endings:</p> <pre><code>git config --global core.autocrlf input\n</code></pre>","path":["Development","Getting Started"],"tags":[]},{"location":"packages/auxiliary/coordinates/","level":1,"title":"Coordinate Systems","text":"","path":["Packages","canvod-auxiliary","Coordinate Systems"],"tags":[]},{"location":"packages/auxiliary/coordinates/#overview","level":2,"title":"Overview","text":"<p>Three coordinate systems are used in GNSS positioning within canvod-auxiliary:</p> <ol> <li>ECEF (Earth-Centered, Earth-Fixed): Cartesian coordinates (X, Y, Z) in meters</li> <li>Geodetic (WGS84): Latitude, longitude (degrees), altitude (meters)</li> <li>Spherical: Slant range, polar angle, azimuth (r, theta, phi) relative to the receiver</li> </ol>","path":["Packages","canvod-auxiliary","Coordinate Systems"],"tags":[]},{"location":"packages/auxiliary/coordinates/#ecef-coordinates","level":2,"title":"ECEF Coordinates","text":"<p>Earth-Centered, Earth-Fixed Cartesian coordinates define positions relative to the Earth's center of mass.</p> <pre><code>from canvod.auxiliary import ECEFPosition\n\necef = ECEFPosition(x=4075539.8, y=931735.3, z=4801629.6)\necef = ECEFPosition.from_ds_metadata(rinex_ds)\n\nlat, lon, alt = ecef.to_geodetic()\n</code></pre>","path":["Packages","canvod-auxiliary","Coordinate Systems"],"tags":[]},{"location":"packages/auxiliary/coordinates/#geodetic-coordinates","level":2,"title":"Geodetic Coordinates","text":"<p>WGS84 geodetic coordinates (latitude, longitude, altitude above ellipsoid).</p> <pre><code>from canvod.auxiliary import GeodeticPosition\n\ngeo = GeodeticPosition(lat=48.2, lon=16.4, alt=200.0)\nx, y, z = geo.to_ecef()\n</code></pre>","path":["Packages","canvod-auxiliary","Coordinate Systems"],"tags":[]},{"location":"packages/auxiliary/coordinates/#spherical-coordinates","level":2,"title":"Spherical Coordinates","text":"<p>Spherical coordinates relative to the receiver position, following the navigation convention:</p> <ul> <li>r: Slant range from receiver to satellite (meters)</li> <li>theta: Polar angle from zenith [0, pi] radians (0 = zenith, pi/2 = horizon)</li> <li>phi: Azimuthal angle from North, clockwise [0, 2*pi) radians (0 = North, pi/2 = East)</li> </ul> <pre><code>from canvod.auxiliary import compute_spherical_coordinates\n\nr, theta, phi = compute_spherical_coordinates(\n    sat_x, sat_y, sat_z, receiver_position\n)\n</code></pre> <p>These coordinates are essential for VOD calculation: theta determines the reflection zone geometry, phi maps to the hemispheric grid azimuth, and r is used in geometric corrections.</p>","path":["Packages","canvod-auxiliary","Coordinate Systems"],"tags":[]},{"location":"packages/auxiliary/coordinates/#adding-coordinates-to-datasets","level":2,"title":"Adding Coordinates to Datasets","text":"<pre><code>from canvod.auxiliary import add_spherical_coords_to_dataset\n\naugmented_ds = add_spherical_coords_to_dataset(rinex_ds, r, theta, phi)\n# Adds r, theta, phi as data variables\n</code></pre>","path":["Packages","canvod-auxiliary","Coordinate Systems"],"tags":[]},{"location":"packages/auxiliary/interpolation/","level":1,"title":"Interpolation Methods","text":"","path":["Packages","canvod-auxiliary","Interpolation Methods"],"tags":[]},{"location":"packages/auxiliary/interpolation/#overview","level":2,"title":"Overview","text":"<p>Interpolation increases the temporal resolution of auxiliary data from SP3/CLK sampling rates (15 min, 5 min) to RINEX observation rates (typically 30s or 15s).</p>","path":["Packages","canvod-auxiliary","Interpolation Methods"],"tags":[]},{"location":"packages/auxiliary/interpolation/#hermite-cubic-splines-ephemerides","level":2,"title":"Hermite Cubic Splines (Ephemerides)","text":"<p>Hermite cubic splines are used for SP3 ephemeris interpolation because:</p> <ul> <li>Orbital motion is physically smooth, well-suited to polynomial interpolation</li> <li>SP3 files provide both positions and velocities, enabling Hermite (not just Lagrange) interpolation</li> <li>The resulting interpolant is C1 continuous (continuous first derivative)</li> <li>Sub-millimeter interpolation accuracy is achieved for IGS final products</li> </ul> <p>Configuration:</p> <pre><code>from canvod.auxiliary.interpolation import Sp3Config, Sp3InterpolationStrategy\n\nconfig = Sp3Config(\n    use_velocities=True,\n    fallback_method='linear',\n    extrapolation_method='nearest'\n)\ninterpolator = Sp3InterpolationStrategy(config=config)\nresult = interpolator.interpolate(sp3_data, target_epochs)\n</code></pre>","path":["Packages","canvod-auxiliary","Interpolation Methods"],"tags":[]},{"location":"packages/auxiliary/interpolation/#piecewise-linear-interpolation-clock-corrections","level":2,"title":"Piecewise Linear Interpolation (Clock Corrections)","text":"<p>Clock corrections are interpolated using piecewise linear segments because:</p> <ul> <li>Satellite clock behavior exhibits discontinuities at maneuvers and clock parameter uploads</li> <li>No derivative information is available in CLK files</li> <li>Linear interpolation provides sub-nanosecond accuracy between knot points</li> <li>Jump detection prevents interpolation across discontinuities</li> </ul> <p>Configuration:</p> <pre><code>from canvod.auxiliary.interpolation import ClockConfig, ClockInterpolationStrategy\n\nconfig = ClockConfig(\n    window_size=9,\n    jump_threshold=1e-6,\n    extrapolation='nearest'\n)\ninterpolator = ClockInterpolationStrategy(config=config)\nresult = interpolator.interpolate(clk_data, target_epochs)\n</code></pre>","path":["Packages","canvod-auxiliary","Interpolation Methods"],"tags":[]},{"location":"packages/auxiliary/interpolation/#custom-strategies","level":2,"title":"Custom Strategies","text":"<p>Custom interpolation strategies can be implemented by subclassing <code>InterpolationStrategy</code>:</p> <pre><code>from canvod.auxiliary.interpolation import InterpolationStrategy\n\nclass CustomStrategy(InterpolationStrategy):\n    def interpolate(self, aux_ds, target_epochs):\n        # Implementation\n        return interpolated_ds\n</code></pre>","path":["Packages","canvod-auxiliary","Interpolation Methods"],"tags":[]},{"location":"packages/auxiliary/overview/","level":1,"title":"canvod-auxiliary","text":"","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/overview/#purpose","level":2,"title":"Purpose","text":"<p>The <code>canvod-auxiliary</code> package provides auxiliary data management for GNSS Transmissometry (GNSS-T) analysis. It handles downloading, parsing, preprocessing, and interpolating SP3 ephemerides and CLK clock corrections to augment RINEX observation data with precise satellite positions and timing information.</p>","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/overview/#the-dimension-alignment-problem","level":2,"title":"The Dimension Alignment Problem","text":"<p>GNSS-T analysis requires combining two data sources with different indexing schemes:</p> <p>RINEX Observation Files: - High temporal resolution (30s, 15s, or sub-second) - Signal-level indexing by Signal ID (sid): <code>\"G01|L1|C\"</code> - Dimensions: <code>(epoch: 2880, sid: 384)</code></p> <p>Auxiliary Files (SP3/CLK): - Low temporal resolution (15 min for SP3, 5 min for CLK) - Satellite-level indexing by satellite vehicle (sv): <code>\"G01\"</code> - Dimensions: <code>(epoch: 96, sv: 32)</code></p> <p>Combining these requires: - Dimension conversion from sv (32 satellites) to sid (384 signal IDs) - Temporal interpolation from 15 min to 30s sampling - Coordinate system transformations (ECEF to geodetic to spherical) - Scientifically appropriate interpolation methods per data type</p>","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/overview/#preprocessing-first-architecture","level":2,"title":"Preprocessing-First Architecture","text":"<p>Preprocessing must occur before interpolation. Each satellite transmits on approximately 12 signal IDs; interpolation operates per-signal, and RINEX data is already signal-indexed.</p> <pre><code>graph LR\n    A[SP3 File&lt;br/&gt;sv dimension] --&gt;|Download| B[Raw Dataset&lt;br/&gt;96 epochs, 32 svs]\n    B --&gt;|Preprocess| C[Preprocessed&lt;br/&gt;96 epochs, 384 sids]\n    C --&gt;|Interpolate| D[Interpolated&lt;br/&gt;2880 epochs, 384 sids]\n    D --&gt;|Match| E[RINEX Data&lt;br/&gt;2880 epochs, 384 sids]\n</code></pre> <pre><code># Correct ordering: preprocess then interpolate\nsp3_data = sp3_file.to_dataset()  # {'epoch': 96, 'sv': 32}\nsp3_sid = preprocess_aux_for_interpolation(sp3_data)  # {'epoch': 96, 'sid': 384}\nsp3_interp = interpolator.interpolate(sp3_sid, target_epochs)  # {'epoch': 2880, 'sid': 384}\n</code></pre>","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/overview/#interpolation-strategies","level":2,"title":"Interpolation Strategies","text":"<p>Different data types require different interpolation methods based on the underlying physics:</p> <pre><code>graph TD\n    A[InterpolationStrategy&lt;br/&gt;Abstract Base] --&gt; B[Sp3InterpolationStrategy&lt;br/&gt;Hermite Splines]\n    A --&gt; C[ClockInterpolationStrategy&lt;br/&gt;Piecewise Linear]\n\n    B --&gt; D[Config:&lt;br/&gt;use_velocities=True&lt;br/&gt;fallback='linear']\n    C --&gt; E[Config:&lt;br/&gt;window_size=9&lt;br/&gt;jump_threshold=1e-6]\n</code></pre> <p>Ephemerides (SP3): Hermite cubic splines exploit the smooth orbital motion and available velocity data to achieve sub-millimeter accuracy with C1 continuity.</p> <p>Clock corrections (CLK): Piecewise linear interpolation accommodates discontinuities from satellite maneuvers and clock uploads, where no derivative information is available.</p>","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/overview/#key-components","level":2,"title":"Key Components","text":"","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/overview/#file-handlers","level":3,"title":"File Handlers","text":"<ul> <li>Sp3File: Reads SP3a/c/d formats; extracts positions (X, Y, Z) and velocities (VX, VY, VZ)</li> <li>ClkFile: Reads RINEX clock format; extracts satellite clock biases</li> <li>ProductSpec: Declarative configuration for product URLs, latency, and authentication</li> </ul>","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/overview/#preprocessing-pipeline","level":3,"title":"Preprocessing Pipeline","text":"<pre><code>from canvod.auxiliary.preprocessing import prep_aux_ds\n\n# Full 4-step pipeline: sv-&gt;sid, pad to global, normalize dtype, strip _FillValue\nsp3_prep = prep_aux_ds(sp3_data)\n</code></pre>","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/overview/#position-classes","level":3,"title":"Position Classes","text":"<ul> <li>ECEFPosition: Earth-Centered, Earth-Fixed Cartesian coordinates (X, Y, Z)</li> <li>GeodeticPosition: WGS84 latitude, longitude, altitude</li> <li>Spherical coordinates: (r, theta, phi) relative to receiver position</li> </ul>","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/overview/#usage","level":2,"title":"Usage","text":"","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/overview/#rinex-data-augmentation","level":3,"title":"RINEX Data Augmentation","text":"<pre><code>from canvod.auxiliary import (\n    Sp3File, preprocess_aux_for_interpolation,\n    Sp3InterpolationStrategy, Sp3Config,\n    compute_spherical_coordinates, ECEFPosition,\n    add_spherical_coords_to_dataset,\n)\n\nrinex_ds = Rnxv3Obs(\"station.24o\").to_ds()\ntarget_epochs = rinex_ds.epoch.values\n\nsp3_data = Sp3File.from_url(date, \"CODE\", \"final\").to_dataset()\nsp3_sid = preprocess_aux_for_interpolation(sp3_data)\n\nconfig = Sp3Config(use_velocities=True)\ninterpolator = Sp3InterpolationStrategy(config=config)\nsp3_interp = interpolator.interpolate(sp3_sid, target_epochs)\n\nreceiver_pos = ECEFPosition.from_ds_metadata(rinex_ds)\nr, theta, phi = compute_spherical_coordinates(\n    sp3_interp['X'], sp3_interp['Y'], sp3_interp['Z'], receiver_pos\n)\n\naugmented_ds = add_spherical_coords_to_dataset(rinex_ds, r, theta, phi)\n</code></pre>","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/overview/#data-flow","level":2,"title":"Data Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Sp3File\n    participant Preprocessor\n    participant Interpolator\n    participant Coordinates\n    participant RINEX\n\n    User-&gt;&gt;Sp3File: from_url(date, agency, product_type)\n    Sp3File-&gt;&gt;Sp3File: Download from FTP\n    Sp3File-&gt;&gt;Sp3File: Parse SP3 format\n    Sp3File--&gt;&gt;User: to_dataset()\n\n    User-&gt;&gt;Preprocessor: preprocess_aux_for_interpolation(sp3_data)\n    Preprocessor-&gt;&gt;Preprocessor: map_aux_sv_to_sid()\n    Preprocessor--&gt;&gt;User: Dataset with sid dimension\n\n    User-&gt;&gt;Interpolator: interpolate(sp3_sid, target_epochs)\n    Interpolator-&gt;&gt;Interpolator: Hermite splines\n    Interpolator--&gt;&gt;User: Interpolated dataset\n\n    User-&gt;&gt;Coordinates: compute_spherical_coordinates()\n    Coordinates--&gt;&gt;User: (r, theta, phi)\n\n    User-&gt;&gt;RINEX: add_spherical_coords_to_dataset()\n    RINEX--&gt;&gt;User: Augmented RINEX data</code></pre>","path":["Packages","canvod-auxiliary"],"tags":[]},{"location":"packages/auxiliary/preprocessing/","level":1,"title":"Preprocessing Pipeline","text":"","path":["Packages","canvod-auxiliary","Preprocessing Pipeline"],"tags":[]},{"location":"packages/auxiliary/preprocessing/#overview","level":2,"title":"Overview","text":"<p>Preprocessing converts auxiliary data from satellite vehicle (sv) indexing to signal ID (sid) indexing, aligning it with the structure of RINEX observation data. This step is required before interpolation.</p>","path":["Packages","canvod-auxiliary","Preprocessing Pipeline"],"tags":[]},{"location":"packages/auxiliary/preprocessing/#dimension-mismatch","level":2,"title":"Dimension Mismatch","text":"<p>SP3 and CLK files index data by satellite vehicle (sv), while RINEX files index by signal ID (sid). Each satellite transmits on multiple frequencies and codes, so GPS satellite G01 maps to approximately 20 signal IDs.</p> <pre><code>sp3_data.dims   # {'epoch': 96, 'sv': 32}\nrinex_data.dims # {'epoch': 2880, 'sid': 384}\n</code></pre>","path":["Packages","canvod-auxiliary","Preprocessing Pipeline"],"tags":[]},{"location":"packages/auxiliary/preprocessing/#pipeline-functions","level":2,"title":"Pipeline Functions","text":"Function Purpose Transformation <code>preprocess_aux_for_interpolation()</code> Minimal preprocessing for interpolation sv:32 -&gt; sid:384 <code>prep_aux_ds()</code> Full preprocessing for Icechunk storage sv:32 -&gt; sid:~2000 <code>map_aux_sv_to_sid()</code> Step 1: sv to sid conversion sv:32 -&gt; sid:384 <code>pad_to_global_sid()</code> Step 2: pad to all constellations sid:384 -&gt; sid:~2000 <code>normalize_sid_dtype()</code> Step 3: convert to object dtype dtype fix <code>strip_fillvalue()</code> Step 4: remove _FillValue attributes attribute cleanup","path":["Packages","canvod-auxiliary","Preprocessing Pipeline"],"tags":[]},{"location":"packages/auxiliary/preprocessing/#step-1-map_aux_sv_to_sid","level":2,"title":"Step 1: map_aux_sv_to_sid()","text":"<p>Each satellite position is replicated across all its signal IDs:</p> <pre><code>from canvod.auxiliary.preprocessing import map_aux_sv_to_sid\n\nsp3_sid = map_aux_sv_to_sid(sp3_data)\n\n# G01 position replicated across all signal IDs\nsp3_sid['X'].sel(sid='G01|L1|C')  # 12345678.9 m\nsp3_sid['X'].sel(sid='G01|L2|W')  # 12345678.9 m (identical)\nsp3_sid['X'].sel(sid='G01|L5|I')  # 12345678.9 m (identical)\n</code></pre> <p>Signal IDs generated for GPS G01: <pre><code>['G01|L1|C', 'G01|L1|L', 'G01|L1|P', 'G01|L1|S', 'G01|L1|W', 'G01|L1|X', 'G01|L1|Y',\n 'G01|L2|C', 'G01|L2|D', 'G01|L2|L', 'G01|L2|M', 'G01|L2|P', 'G01|L2|S', 'G01|L2|W',\n 'G01|L2|X', 'G01|L2|Y', 'G01|L5|I', 'G01|L5|Q', 'G01|L5|X', 'G01|X1|X']\n</code></pre></p> <p>Signal ID format: <code>\"{SV}|{BAND}|{CODE}\"</code> - SV: Satellite vehicle (G01, E02, R24) - BAND: Frequency band (L1, L2, L5, E1, E5a, G1, G2) - CODE: Tracking code (C, P, W, I, Q, X)</p>","path":["Packages","canvod-auxiliary","Preprocessing Pipeline"],"tags":[]},{"location":"packages/auxiliary/preprocessing/#step-2-pad_to_global_sid","level":2,"title":"Step 2: pad_to_global_sid()","text":"<p>Pads the dataset to include all possible signal IDs across all constellations (~1987 total). This ensures consistent dimensions for Icechunk storage, where appended datasets must share the same coordinate space.</p>","path":["Packages","canvod-auxiliary","Preprocessing Pipeline"],"tags":[]},{"location":"packages/auxiliary/preprocessing/#step-3-normalize_sid_dtype","level":2,"title":"Step 3: normalize_sid_dtype()","text":"<p>Converts the sid coordinate to object dtype for Zarr/Icechunk compatibility. Fixed-length Unicode string types cause dtype conflicts during sequential appends.</p>","path":["Packages","canvod-auxiliary","Preprocessing Pipeline"],"tags":[]},{"location":"packages/auxiliary/preprocessing/#step-4-strip_fillvalue","level":2,"title":"Step 4: strip_fillvalue()","text":"<p>Removes <code>_FillValue</code> attributes that conflict with Icechunk's internal missing-data handling. NaN serves as the standard missing value marker.</p>","path":["Packages","canvod-auxiliary","Preprocessing Pipeline"],"tags":[]},{"location":"packages/auxiliary/preprocessing/#scientific-correctness-of-data-replication","level":2,"title":"Scientific Correctness of Data Replication","text":"<p>Replicating satellite positions across signal IDs is scientifically valid: satellite position is independent of signal frequency. All signals originate from the same antenna. Position accuracy of IGS final products is approximately 1 cm, and antenna offset corrections at the millimeter level are already applied in SP3 files.</p>","path":["Packages","canvod-auxiliary","Preprocessing Pipeline"],"tags":[]},{"location":"packages/auxiliary/products/","level":1,"title":"Product Registry","text":"","path":["Packages","canvod-auxiliary","Product Registry"],"tags":[]},{"location":"packages/auxiliary/products/#overview","level":2,"title":"Overview","text":"<p>The product registry provides declarative configuration for 37 SP3 and CLK products from 17 analysis centers. Products are defined in TOML configuration rather than hardcoded URLs.</p>","path":["Packages","canvod-auxiliary","Product Registry"],"tags":[]},{"location":"packages/auxiliary/products/#available-agencies","level":2,"title":"Available Agencies","text":"<p>Primary sources: - CODE (Center for Orbit Determination in Europe): Final, Rapid - GFZ (GeoForschungsZentrum Potsdam): Final, Rapid - ESA (European Space Agency): Final, Rapid, Ultra-rapid - JPL (Jet Propulsion Laboratory): Final - IGS (International GNSS Service): Final, Rapid, Ultra-rapid</p>","path":["Packages","canvod-auxiliary","Product Registry"],"tags":[]},{"location":"packages/auxiliary/products/#product-types","level":2,"title":"Product Types","text":"Type Latency Accuracy Use Case Final 14-21 days Highest (cm-level) Scientific research, reprocessing Rapid 17-24 hours Near-final (cm-level) Near-real-time processing Ultra-rapid 3-9 hours Good (few cm) Real-time applications","path":["Packages","canvod-auxiliary","Product Registry"],"tags":[]},{"location":"packages/auxiliary/products/#usage","level":2,"title":"Usage","text":"<pre><code>from canvod.auxiliary import get_product_spec, Sp3File\nfrom datetime import date\n\nspec = get_product_spec(\"CODE\", \"final\")\nprint(spec.latency_hours)  # 336 (14 days)\n\nsp3 = Sp3File.from_url(date(2024, 1, 1), \"CODE\", \"final\")\nds = sp3.to_dataset()\n</code></pre>","path":["Packages","canvod-auxiliary","Product Registry"],"tags":[]},{"location":"packages/auxiliary/products/#configuration-format","level":2,"title":"Configuration Format","text":"<p>Products are defined in <code>products/registry.toml</code>:</p> <pre><code>[CODE.final]\nsp3_url_template = \"ftp://ftp.aiub.unibe.ch/CODE/{yyyy}/COD{gpsweek}{dow}.EPH.Z\"\nclk_url_template = \"ftp://ftp.aiub.unibe.ch/CODE/{yyyy}/COD{gpsweek}{dow}.CLK.Z\"\nlatency_hours = 336\nftp_server = \"ftp.aiub.unibe.ch\"\nrequires_auth = false\n</code></pre>","path":["Packages","canvod-auxiliary","Product Registry"],"tags":[]},{"location":"packages/grids/overview/","level":1,"title":"canvod-grids","text":"","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#purpose","level":2,"title":"Purpose","text":"<p>The <code>canvod-grids</code> package provides spatial grid implementations for hemispheric GNSS signal analysis. It discretizes the hemisphere visible from a ground-based receiver into cells, which is required for spatially resolved VOD estimation.</p>","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#grid-types","level":2,"title":"Grid Types","text":"<p>Seven grid implementations are available, all inheriting from <code>BaseGridBuilder</code>:</p> Builder Grid Type Notes <code>EqualAreaBuilder</code> Equal-area Cells of approximately equal solid angle (recommended) <code>EqualAngleBuilder</code> Equal-angle Regular angular spacing; cells near zenith are smaller <code>EquirectangularBuilder</code> Equirectangular Simple rectangular latitude/longitude grid <code>HEALPixBuilder</code> HEALPix Hierarchical Equal Area isoLatitude Pixelization (requires <code>healpy</code>) <code>GeodesicBuilder</code> Geodesic Icosahedron subdivision; near-uniform cell area <code>FibonacciBuilder</code> Fibonacci Fibonacci-sphere sampling (requires <code>scipy</code>) <code>HTMBuilder</code> HTM Hierarchical Triangular Mesh <p>All builders accept <code>angular_resolution</code> (degrees) and produce a <code>GridData</code> object.</p>","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#usage","level":2,"title":"Usage","text":"","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#factory-function","level":3,"title":"Factory function","text":"<pre><code>from canvod.grids import create_hemigrid\n\ngrid = create_hemigrid(\"equal_area\", angular_resolution=5.0)\n</code></pre>","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#builder-pattern","level":3,"title":"Builder pattern","text":"<pre><code>from canvod.grids import EqualAreaBuilder\n\nbuilder = EqualAreaBuilder(angular_resolution=5.0, cutoff_theta=10.0)\ngrid = builder.build()\n</code></pre>","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#via-canvodpy-factory","level":3,"title":"Via canvodpy factory","text":"<pre><code>from canvodpy import GridFactory\n\nbuilder = GridFactory.create(\"equal_area\", angular_resolution=5.0)\ngrid = builder.build()\n</code></pre>","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#griddata","level":2,"title":"GridData","text":"<p>The <code>GridData</code> object returned by all builders provides:</p> <ul> <li><code>df</code> — Polars DataFrame with cell geometry (boundaries, centers, solid angles)</li> <li><code>ncells</code> — Total number of grid cells</li> <li><code>nbands</code> — Number of elevation bands</li> <li><code>definition</code> — Human-readable grid description</li> </ul>","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#operations","level":2,"title":"Operations","text":"<p>Functions in <code>canvod.grids.operations</code> handle the interface between grids and xarray Datasets:</p> Function Purpose <code>add_cell_ids_to_ds_fast</code> Assign each observation to a grid cell <code>add_cell_ids_to_vod</code> Assign cell IDs to VOD datasets <code>add_cell_ids_to_vod_fast</code> Assign cell IDs to VOD datasets (KD-tree accelerated) <code>grid_to_dataset</code> Convert GridData to an xarray Dataset <code>extract_grid_vertices</code> Get cell boundary polygons <code>store_grid</code> / <code>load_grid</code> Persist and load grid definitions","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#aggregation","level":2,"title":"Aggregation","text":"<p><code>CellAggregator</code> and associated functions compute summary statistics over grid cells:</p> <ul> <li><code>aggregate_data_to_grid</code> — Assign data to grid cells and aggregate</li> <li><code>compute_hemisphere_percell</code> — Per-cell statistics across the full hemisphere</li> <li><code>compute_zenith_percell</code> — Zenith-weighted per-cell statistics</li> <li><code>compute_percell_timeseries</code> — Per-cell time series</li> <li><code>compute_global_average</code> / <code>compute_regional_average</code> — Spatial averages</li> <li><code>analyze_diurnal_patterns</code> / <code>analyze_spatial_patterns</code> — Pattern analysis</li> </ul>","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#analysis-subpackage","level":2,"title":"Analysis Subpackage","text":"<p><code>canvod.grids.analysis</code> provides per-cell and global analysis tools:</p> Module Purpose <code>filtering</code> Global statistical filters (IQR, Z-score) <code>per_cell_filtering</code> Per-cell variants of the above filters <code>hampel_filtering</code> Hampel (median-MAD) outlier filtering <code>sigma_clip_filter</code> Numba-accelerated sigma-clipping <code>masking</code> Spatial and temporal mask construction <code>weighting</code> Per-cell weight calculators <code>solar</code> Solar geometry (elevation, azimuth) <code>temporal</code> Weighted temporal aggregation and diurnal analysis <code>spatial</code> Per-cell spatial statistics <code>per_cell_analysis</code> Multi-dataset per-cell VOD analysis <code>analysis_storage</code> Persistent Icechunk storage for results (requires <code>canvod-store</code>)","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#workflows","level":2,"title":"Workflows","text":"<p><code>canvod.grids.workflows</code> contains the end-to-end pipeline classes that tie together filtering, grid operations, and Icechunk persistence. This module requires <code>canvod-store</code> at runtime.</p> <ul> <li><code>AdaptedVODWorkflow</code> — Full pipeline: filtering, grid operations, Icechunk persistence</li> <li><code>get_workflow_for_store</code> — Create a workflow from an existing Icechunk store path</li> <li><code>check_processed_data_status</code> — Check which dates have already been processed</li> </ul>","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#role-in-the-vod-pipeline","level":2,"title":"Role in the VOD Pipeline","text":"<p>Grids discretize the hemisphere above the receiver into cells. Each GNSS satellite observation is assigned to a grid cell based on its elevation and azimuth angles (computed by canvod-auxiliary). VOD is then estimated per cell or aggregated across the hemisphere.</p>","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/grids/overview/#coordinate-convention","level":2,"title":"Coordinate Convention","text":"<p>All grids use standard spherical coordinates:</p> <ul> <li>phi — Azimuth angle, 0 to 2pi (0 = North, pi/2 = East, clockwise)</li> <li>theta — Polar angle from zenith, 0 to pi/2 (0 = zenith, pi/2 = horizon)</li> </ul>","path":["Packages","canvod-grids"],"tags":[]},{"location":"packages/readers/architecture/","level":1,"title":"Reader Architecture","text":"<p>This page describes the architectural principles behind canvod-readers, with particular focus on the Abstract Base Class (ABC) pattern used to ensure extensibility and consistency across reader implementations.</p>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#the-gnssdatareader-abstract-base-class","level":2,"title":"The <code>GNSSDataReader</code> Abstract Base Class","text":"<p>The ABC pattern provides contract enforcement (all readers must implement required methods), type safety for static type checkers, explicit interface changes, mock implementations for testing downstream code, and self-documenting interface definitions.</p> <p>The <code>GNSSDataReader</code> class is located in <code>canvod/readers/base.py</code>:</p> <pre><code>from abc import ABC, abstractmethod\nfrom pathlib import Path\nimport xarray as xr\n\nclass GNSSDataReader(ABC):\n    \"\"\"Abstract base for all GNSS data format readers.\n\n    All readers must:\n    1. Inherit from this class\n    2. Implement all abstract methods\n    3. Return xarray.Dataset that passes DatasetStructureValidator\n    4. Provide file hash for deduplication\n    \"\"\"\n\n    fpath: Path  # Provided by Pydantic BaseModel in concrete classes\n\n    @property\n    @abstractmethod\n    def file_hash(self) -&gt; str:\n        \"\"\"SHA256 hash of file for deduplication.\"\"\"\n        pass\n\n    @abstractmethod\n    def to_ds(\n        self,\n        keep_rnx_data_vars: list[str] | None = None,\n        **kwargs\n    ) -&gt; xr.Dataset:\n        \"\"\"Convert data to xarray.Dataset.\"\"\"\n        pass\n\n    @abstractmethod\n    def iter_epochs(self):\n        \"\"\"Iterate over epochs in file.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def start_time(self) -&gt; datetime:\n        \"\"\"Start time of observations.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def end_time(self) -&gt; datetime:\n        \"\"\"End time of observations.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def systems(self) -&gt; list[str]:\n        \"\"\"GNSS systems in file.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def num_epochs(self) -&gt; int:\n        \"\"\"Number of epochs.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def num_satellites(self) -&gt; int:\n        \"\"\"Number of unique satellites.\"\"\"\n        pass\n\n    def validate_output(\n        self,\n        dataset: xr.Dataset,\n        required_vars: list[str] | None = None\n    ) -&gt; None:\n        \"\"\"Validate Dataset structure.\"\"\"\n        validator = DatasetStructureValidator(dataset=dataset)\n        validator.validate_all(required_vars=required_vars)\n</code></pre>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#contract-guarantees","level":3,"title":"Contract Guarantees","text":"<p>Any implementation of <code>GNSSDataReader</code> guarantees the following:</p> <ol> <li>File Access: The <code>fpath</code> attribute contains a Path to the data file.</li> <li>Hash Computation: <code>file_hash</code> returns a deterministic identifier.</li> <li>Dataset Conversion: <code>to_ds()</code> returns a validated xarray.Dataset.</li> <li>Iteration: <code>iter_epochs()</code> yields epoch-by-epoch data.</li> <li>Metadata: Properties provide time range, systems, and counts.</li> <li>Validation: Output passes <code>DatasetStructureValidator</code>.</li> </ol>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#layered-architecture","level":2,"title":"Layered Architecture","text":"<pre><code>graph TB\n    subgraph \"User Layer\"\n        A[User Code]\n    end\n\n    subgraph \"Interface Layer\"\n        B[GNSSDataReader ABC]\n        C[DatasetStructureValidator]\n    end\n\n    subgraph \"Implementation Layer\"\n        D[Rnxv3Obs]\n        E[Future: Rnxv2Obs]\n        F[Future: Rnxv4Obs]\n    end\n\n    subgraph \"Support Layer\"\n        G[gnss_specs&lt;br/&gt;Constellation definitions]\n        H[SignalIDMapper]\n        I[Metadata]\n    end\n\n    subgraph \"Model Layer\"\n        J[Pydantic Models&lt;br/&gt;Type-safe parsing]\n    end\n\n    A --&gt; B\n    D -.implements.-&gt; B\n    E -.implements.-&gt; B\n    F -.implements.-&gt; B\n\n    D --&gt; J\n    D --&gt; G\n    D --&gt; H\n    D --&gt; I\n    D --&gt; C\n</code></pre>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#layer-responsibilities","level":3,"title":"Layer Responsibilities","text":"<p>User Layer -- Instantiates readers, calls <code>to_ds()</code> and <code>iter_epochs()</code>, and operates on returned Datasets.</p> <p>Interface Layer (ABC) -- Defines required methods, enforces contracts, and validates output structure.</p> <p>Implementation Layer (Concrete Readers) -- Parses specific formats, implements abstract methods, and handles format-specific details.</p> <p>Support Layer -- Provides constellation specifications (GPS, Galileo, etc.), Signal ID mapping, and metadata templates.</p> <p>Model Layer (Pydantic) -- Supplies type-safe data models with automatic validation for parsing RINEX structure.</p>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#component-interactions","level":2,"title":"Component Interactions","text":"","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#parsing-flow","level":3,"title":"Parsing Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Reader as Rnxv3Obs&lt;br/&gt;(Pydantic)\n    participant Header as Rnxv3ObsHeader&lt;br/&gt;(Pydantic)\n    participant Epoch as Rnxv3ObsEpoch&lt;br/&gt;(Pydantic)\n    participant Mapper as SignalIDMapper\n    participant Dataset as xr.Dataset\n    participant Validator\n\n    User-&gt;&gt;Reader: Rnxv3Obs(fpath=path)\n    activate Reader\n    Reader-&gt;&gt;Header: Parse header section\n    Header--&gt;&gt;Reader: Validated header data\n    deactivate Reader\n\n    User-&gt;&gt;Reader: to_ds()\n    activate Reader\n\n    loop For each epoch\n        Reader-&gt;&gt;Epoch: Parse epoch line\n        Epoch--&gt;&gt;Reader: Validated epoch data\n\n        loop For each observation\n            Reader-&gt;&gt;Mapper: create_signal_id()\n            Mapper--&gt;&gt;Reader: Signal ID\n        end\n    end\n\n    Reader-&gt;&gt;Dataset: Build xr.Dataset\n    Reader-&gt;&gt;Validator: validate_output()\n    Validator--&gt;&gt;Reader: Passes validation\n\n    Reader--&gt;&gt;User: xr.Dataset\n    deactivate Reader</code></pre> <p>Key interactions in this flow:</p> <ol> <li>Pydantic Models parse and validate raw text.</li> <li>SignalIDMapper converts observation codes to Signal IDs.</li> <li>Dataset Builder constructs the xarray structure.</li> <li>Validator ensures output meets structural requirements.</li> </ol>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#design-principles","level":2,"title":"Design Principles","text":"","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#early-validation-with-pydantic","level":3,"title":"Early Validation with Pydantic","text":"<p>Errors discovered during analysis are expensive to diagnose and correct. Validation is therefore performed during parsing:</p> <pre><code>from pydantic import BaseModel, field_validator\n\nclass Rnxv3ObsHeader(BaseModel):\n    \"\"\"RINEX v3 header with automatic validation.\"\"\"\n\n    rinex_version: float\n    rinex_type: str\n\n    @field_validator('rinex_version')\n    def check_version(cls, v):\n        if not (3.0 &lt;= v &lt; 4.0):\n            raise ValueError(f\"Expected RINEX v3, got {v}\")\n        return v\n\n    @field_validator('rinex_type')\n    def check_type(cls, v):\n        if v != 'O':\n            raise ValueError(f\"Expected observation file, got {v}\")\n        return v\n</code></pre> <p>This approach catches errors at parse time with clear, structured error messages and provides type safety throughout the codebase.</p>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#immutability","level":3,"title":"Immutability","text":"<p>Once created, readers and their outputs are immutable:</p> <pre><code>class Rnxv3Obs(BaseModel):\n    \"\"\"Immutable after initialization.\"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    fpath: Path\n\n    # Attempting to modify raises FrozenInstanceError\n    # reader.fpath = new_path  # raises error\n</code></pre> <p>Immutability ensures predictable behavior, thread safety, and cacheable results.</p>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#separation-of-format-and-processing","level":3,"title":"Separation of Format and Processing","text":"<p>Format-specific code is contained within the reader:</p> <pre><code># In Rnxv3Obs\ndef _parse_epoch_line(self, line: str) -&gt; Rnxv3ObsEpochRecord:\n    \"\"\"RINEX v3 specific parsing.\"\"\"\n    ...\n</code></pre> <p>Generic processing is handled by shared helpers:</p> <pre><code># In gnss_specs\ndef create_signal_id(sv: str, obs_code: str) -&gt; str:\n    \"\"\"Works for any format.\"\"\"\n    ...\n</code></pre>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#explicit-configuration","level":3,"title":"Explicit Configuration","text":"<p>Configuration is always explicit:</p> <pre><code># Explicit parameter specifying which variables to retain\nds = reader.to_ds(keep_rnx_data_vars=[\"SNR\", \"Phase\"])\n</code></pre>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#mandatory-validation","level":3,"title":"Mandatory Validation","text":"<p>Every Dataset must be validated before it is returned:</p> <pre><code>def to_ds(self, **kwargs) -&gt; xr.Dataset:\n    \"\"\"Convert to Dataset.\"\"\"\n    ds = self._build_dataset(**kwargs)\n\n    # Validation is mandatory, not optional\n    self.validate_output(ds)\n\n    return ds\n</code></pre>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#datasetstructurevalidator","level":2,"title":"DatasetStructureValidator","text":"<p>The validator ensures all readers produce compatible output:</p> <pre><code>class DatasetStructureValidator(BaseModel):\n    \"\"\"Validates xarray.Dataset structure.\"\"\"\n\n    dataset: xr.Dataset\n\n    def validate_dimensions(self) -&gt; None:\n        \"\"\"Check (epoch, sid) dimensions exist.\"\"\"\n        required = {\"epoch\", \"sid\"}\n        missing = required - set(self.dataset.dims)\n        if missing:\n            raise ValueError(f\"Missing dimensions: {missing}\")\n\n    def validate_coordinates(self) -&gt; None:\n        \"\"\"Check all required coordinates with correct dtypes.\"\"\"\n        required_coords = {\n            \"epoch\": \"datetime64[ns]\",\n            \"sid\": \"object\",\n            \"sv\": \"object\",\n            \"system\": \"object\",\n            \"band\": \"object\",\n            \"code\": \"object\",\n            \"freq_center\": \"float32\",\n            \"freq_min\": \"float32\",\n            \"freq_max\": \"float32\",\n        }\n\n        for coord, expected_dtype in required_coords.items():\n            if coord not in self.dataset.coords:\n                raise ValueError(f\"Missing coordinate: {coord}\")\n\n            actual_dtype = str(self.dataset[coord].dtype)\n            if expected_dtype not in actual_dtype:\n                raise ValueError(\n                    f\"Wrong dtype for {coord}: \"\n                    f\"expected {expected_dtype}, got {actual_dtype}\"\n                )\n\n    def validate_data_variables(\n        self, required_vars: list[str] | None = None\n    ) -&gt; None:\n        \"\"\"Check required data variables exist with correct structure.\"\"\"\n        if required_vars is None:\n            required_vars = [\"SNR\", \"Phase\"]\n\n        missing = set(required_vars) - set(self.dataset.data_vars)\n        if missing:\n            raise ValueError(f\"Missing variables: {missing}\")\n\n        # All variables must be (epoch, sid)\n        for var in self.dataset.data_vars:\n            if self.dataset[var].dims != (\"epoch\", \"sid\"):\n                raise ValueError(\n                    f\"{var} has wrong dimensions: \"\n                    f\"expected (epoch, sid), got {self.dataset[var].dims}\"\n                )\n\n    def validate_attributes(self) -&gt; None:\n        \"\"\"Check required global attributes for storage.\"\"\"\n        required = {\n            \"Created\",\n            \"Software\",\n            \"Institution\",\n            \"RINEX File Hash\",  # For MyIcechunkStore deduplication\n        }\n\n        missing = required - set(self.dataset.attrs.keys())\n        if missing:\n            raise ValueError(f\"Missing attributes: {missing}\")\n</code></pre>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#rationale-for-structural-requirements","level":3,"title":"Rationale for Structural Requirements","text":"<p>Dimensions (epoch, sid) -- Standardizes time series structure, enables efficient indexing and slicing, and maintains compatibility with xarray operations.</p> <p>Coordinates -- <code>freq_*</code> coordinates are required for band overlap detection. <code>system</code>, <code>band</code>, and <code>code</code> enable constellation- and signal-level filtering. <code>sv</code> tracks individual satellites.</p> <p>Attributes -- <code>\"RINEX File Hash\"</code> prevents duplicate ingestion in storage. Other metadata attributes support provenance tracking and reproducibility.</p>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#readerfactory-pattern","level":2,"title":"ReaderFactory Pattern","text":"<p>The <code>ReaderFactory</code> provides automatic format detection:</p> <pre><code>class ReaderFactory:\n    \"\"\"Factory for creating appropriate reader.\"\"\"\n\n    _readers: dict[str, type] = {}\n\n    @classmethod\n    def register(cls, format_name: str, reader_class: type) -&gt; None:\n        \"\"\"Register a reader for a format.\"\"\"\n        if not issubclass(reader_class, GNSSDataReader):\n            raise TypeError(f\"{reader_class} must inherit GNSSDataReader\")\n        cls._readers[format_name] = reader_class\n\n    @classmethod\n    def create(cls, fpath: Path, **kwargs) -&gt; GNSSDataReader:\n        \"\"\"Create appropriate reader for file.\"\"\"\n        format_name = cls._detect_format(fpath)\n\n        if format_name not in cls._readers:\n            raise ValueError(f\"No reader for format: {format_name}\")\n\n        reader_class = cls._readers[format_name]\n        return reader_class(fpath=fpath, **kwargs)\n\n    @staticmethod\n    def _detect_format(fpath: Path) -&gt; str:\n        \"\"\"Detect format from file content.\"\"\"\n        with open(fpath, 'r') as f:\n            first_line = f.readline()\n\n        # RINEX version in columns 1-9\n        version_str = first_line[:9].strip()\n        version = float(version_str)\n\n        if 3.0 &lt;= version &lt; 4.0:\n            return 'rinex_v3'\n        elif 2.0 &lt;= version &lt; 3.0:\n            return 'rinex_v2'\n        else:\n            raise ValueError(f\"Unsupported RINEX version: {version}\")\n</code></pre> <p>Usage:</p> <pre><code># Register readers\nReaderFactory.register('rinex_v3', Rnxv3Obs)\nReaderFactory.register('rinex_v2', Rnxv2Obs)  # When implemented\n\n# Automatic detection\nreader = ReaderFactory.create(\"unknown_file.rnx\")\n# Returns Rnxv3Obs or Rnxv2Obs based on content\n</code></pre>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/architecture/#summary","level":2,"title":"Summary","text":"<p>The canvod-readers architecture is characterized by:</p> <ol> <li>Contract enforcement through the ABC, ensuring consistent behavior across all readers.</li> <li>Type safety via Pydantic, catching errors during parsing.</li> <li>Structural validation through <code>DatasetStructureValidator</code>, ensuring downstream compatibility.</li> <li>Extensibility through the ABC pattern, allowing new formats to be added without modifying existing code.</li> <li>Separation of concerns between format-specific parsing, generic processing, and validation.</li> </ol>","path":["Packages","canvod-readers","Reader Architecture"],"tags":[]},{"location":"packages/readers/extending/","level":1,"title":"Extending Readers","text":"<p>This guide describes how to add support for new GNSS data formats by implementing the <code>GNSSDataReader</code> abstract base class.</p>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#implementation-summary","level":2,"title":"Implementation Summary","text":"<p>Adding a new reader requires the following steps:</p> <ol> <li>Create a class inheriting from <code>GNSSDataReader</code> and <code>BaseModel</code>.</li> <li>Implement all abstract methods.</li> <li>Ensure output passes <code>DatasetStructureValidator</code>.</li> <li>Write tests.</li> <li>Register with <code>ReaderFactory</code> (optional).</li> </ol>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#step-by-step-implementation","level":2,"title":"Step-by-Step Implementation","text":"","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#step-1-create-the-reader-class","level":3,"title":"Step 1: Create the Reader Class","text":"<pre><code>from pathlib import Path\nfrom datetime import datetime\nfrom typing import Generator\nimport xarray as xr\nfrom pydantic import BaseModel, ConfigDict\n\nfrom canvod.readers.base import GNSSDataReader\n\nclass MyFormatReader(BaseModel, GNSSDataReader):\n    \"\"\"Reader for My Custom Format.\n\n    Implements GNSSDataReader ABC for custom GNSS data format.\n    \"\"\"\n\n    model_config = ConfigDict(\n        frozen=True,  # Immutable\n        arbitrary_types_allowed=True,  # Allow Path, etc.\n    )\n\n    # Required fields\n    fpath: Path\n\n    # Optional: Format-specific fields\n    # my_custom_header: MyCustomHeader | None = None\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#step-2-implement-file-hash","level":3,"title":"Step 2: Implement File Hash","text":"<pre><code>from canvod.readers.gnss_specs.utils import rinex_file_hash\n\nclass MyFormatReader(BaseModel, GNSSDataReader):\n    # ... previous code ...\n\n    @property\n    def file_hash(self) -&gt; str:\n        \"\"\"Compute file hash for deduplication.\n\n        Returns\n        -------\n        str\n            16-character SHA256 hash of file content.\n        \"\"\"\n        return rinex_file_hash(self.fpath)\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#step-3-implement-metadata-properties","level":3,"title":"Step 3: Implement Metadata Properties","text":"<pre><code>from datetime import datetime\n\nclass MyFormatReader(BaseModel, GNSSDataReader):\n    # ... previous code ...\n\n    @property\n    def start_time(self) -&gt; datetime:\n        \"\"\"Return start time of observations.\"\"\"\n        return self._parse_start_time()\n\n    @property\n    def end_time(self) -&gt; datetime:\n        \"\"\"Return end time of observations.\"\"\"\n        return self._parse_end_time()\n\n    @property\n    def systems(self) -&gt; list[str]:\n        \"\"\"Return list of GNSS systems in file.\"\"\"\n        return self._parse_systems()\n\n    @property\n    def num_epochs(self) -&gt; int:\n        \"\"\"Return number of epochs.\"\"\"\n        return self._count_epochs()\n\n    @property\n    def num_satellites(self) -&gt; int:\n        \"\"\"Return number of unique satellites.\"\"\"\n        return self._count_satellites()\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#step-4-implement-epoch-iteration","level":3,"title":"Step 4: Implement Epoch Iteration","text":"<pre><code>class MyFormatReader(BaseModel, GNSSDataReader):\n    # ... previous code ...\n\n    def iter_epochs(self) -&gt; Generator:\n        \"\"\"Iterate over epochs in file.\n\n        Yields\n        ------\n        EpochData\n            Format-specific epoch representation containing\n            timestamp and observations.\n        \"\"\"\n        with open(self.fpath, 'r') as f:\n            # Skip header if needed\n            self._skip_header(f)\n\n            # Parse data section\n            for line in f:\n                epoch_data = self._parse_epoch(line)\n                if epoch_data:\n                    yield epoch_data\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#step-5-implement-dataset-conversion","level":3,"title":"Step 5: Implement Dataset Conversion","text":"<p>This is the core method, converting the custom format to an xarray.Dataset:</p> <pre><code>import numpy as np\nfrom canvod.readers.gnss_specs.signals import SignalIDMapper\nfrom canvod.readers.gnss_specs.metadata import (\n    SNR_METADATA,\n    COORDS_METADATA,\n    get_global_attrs,\n)\n\nclass MyFormatReader(BaseModel, GNSSDataReader):\n    # ... previous code ...\n\n    def to_ds(\n        self,\n        keep_rnx_data_vars: list[str] | None = None,\n        **kwargs\n    ) -&gt; xr.Dataset:\n        \"\"\"Convert to xarray.Dataset.\n\n        Parameters\n        ----------\n        keep_rnx_data_vars : list of str, optional\n            Variables to include. If None, includes all available.\n        **kwargs\n            Format-specific parameters.\n\n        Returns\n        -------\n        xr.Dataset\n            Validated dataset with standardized structure.\n        \"\"\"\n        # 1. Collect all observations\n        all_epochs = list(self.iter_epochs())\n\n        # 2. Build Signal ID index\n        mapper = SignalIDMapper()\n        all_sids = set()\n\n        for epoch in all_epochs:\n            for obs in epoch.observations:\n                sid = mapper.create_signal_id(obs.sv, obs.code)\n                all_sids.add(sid)\n\n        sids = sorted(all_sids)\n\n        # 3. Create coordinate arrays\n        epochs = [e.timestamp for e in all_epochs]\n\n        # Extract metadata from Signal IDs\n        sv_arr = np.array([sid.split('|')[0] for sid in sids])\n        band_arr = np.array([sid.split('|')[1] for sid in sids])\n        code_arr = np.array([sid.split('|')[2] for sid in sids])\n        system_arr = np.array([sid[0] for sid in sids])\n\n        # Get frequencies\n        freq_center = np.array([\n            mapper.get_band_frequency(sid.split('|')[1])\n            for sid in sids\n        ], dtype=np.float64)\n\n        bandwidth = np.array([\n            mapper.get_band_bandwidth(sid.split('|')[1])\n            for sid in sids\n        ], dtype=np.float64)\n\n        freq_min = freq_center - (bandwidth / 2.0)\n        freq_max = freq_center + (bandwidth / 2.0)\n\n        # 4. Build data arrays\n        data_vars = {}\n\n        if keep_rnx_data_vars is None or \"SNR\" in keep_rnx_data_vars:\n            snr_data = np.full(\n                (len(epochs), len(sids)),\n                np.nan,\n                dtype=np.float32\n            )\n\n            # Fill with observations\n            sid_to_idx = {sid: i for i, sid in enumerate(sids)}\n            for epoch_idx, epoch in enumerate(all_epochs):\n                for obs in epoch.observations:\n                    sid = mapper.create_signal_id(obs.sv, obs.code)\n                    sid_idx = sid_to_idx[sid]\n                    snr_data[epoch_idx, sid_idx] = obs.snr\n\n            data_vars[\"SNR\"] = (\n                (\"epoch\", \"sid\"),\n                snr_data,\n                SNR_METADATA\n            )\n\n        # Similar for other variables (Phase, Pseudorange, Doppler)\n\n        # 5. Create Dataset\n        ds = xr.Dataset(\n            data_vars=data_vars,\n            coords={\n                \"epoch\": (\"epoch\", epochs, COORDS_METADATA[\"epoch\"]),\n                \"sid\": (\"sid\", sids, COORDS_METADATA[\"sid\"]),\n                \"sv\": (\"sid\", sv_arr, COORDS_METADATA[\"sv\"]),\n                \"system\": (\"sid\", system_arr, COORDS_METADATA[\"system\"]),\n                \"band\": (\"sid\", band_arr, COORDS_METADATA[\"band\"]),\n                \"code\": (\"sid\", code_arr, COORDS_METADATA[\"code\"]),\n                \"freq_center\": (\"sid\", freq_center, COORDS_METADATA[\"freq_center\"]),\n                \"freq_min\": (\"sid\", freq_min, COORDS_METADATA[\"freq_min\"]),\n                \"freq_max\": (\"sid\", freq_max, COORDS_METADATA[\"freq_max\"]),\n            },\n            attrs={\n                **get_global_attrs(),\n                \"Created\": datetime.now().isoformat(),\n                \"RINEX File Hash\": self.file_hash,\n                \"Source Format\": \"My Custom Format\",\n            }\n        )\n\n        # 6. CRITICAL: Validate before returning\n        self.validate_output(ds, required_vars=keep_rnx_data_vars)\n\n        return ds\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#complete-example-rinex-v2-reader-stub","level":2,"title":"Complete Example: RINEX v2 Reader Stub","text":"<p>The following skeleton illustrates a RINEX v2 reader implementation:</p> <pre><code>from pathlib import Path\nfrom datetime import datetime\nfrom typing import Generator\nimport xarray as xr\nimport numpy as np\nfrom pydantic import BaseModel, ConfigDict, model_validator\n\nfrom canvod.readers.base import GNSSDataReader\nfrom canvod.readers.gnss_specs.utils import rinex_file_hash\nfrom canvod.readers.gnss_specs.signals import SignalIDMapper\n\nclass Rnxv2ObsHeader(BaseModel):\n    \"\"\"RINEX v2 header parser.\"\"\"\n\n    rinex_version: float\n    rinex_type: str\n    obs_types: list[str]\n    interval: float | None = None\n    first_obs: datetime | None = None\n\n    @classmethod\n    def from_lines(cls, lines: list[str]) -&gt; 'Rnxv2ObsHeader':\n        \"\"\"Parse header from lines.\"\"\"\n        data = {}\n\n        for line in lines:\n            label = line[60:80].strip()\n\n            if label == \"RINEX VERSION / TYPE\":\n                data['rinex_version'] = float(line[0:9])\n                data['rinex_type'] = line[20]\n\n            elif label == \"# / TYPES OF OBSERV\":\n                num_obs = int(line[0:6])\n                obs_types = line[10:60].split()\n                data['obs_types'] = obs_types\n\n            # ... parse other fields\n\n        return cls(**data)\n\n\nclass Rnxv2ObsEpoch:\n    \"\"\"RINEX v2 epoch data.\"\"\"\n\n    def __init__(self, timestamp: datetime, satellites: list):\n        self.timestamp = timestamp\n        self.satellites = satellites\n\n\nclass Rnxv2Obs(BaseModel, GNSSDataReader):\n    \"\"\"RINEX v2 observation file reader.\n\n    Implements GNSSDataReader ABC for RINEX v2.xx format.\n\n    Examples\n    --------\n    &gt;&gt;&gt; reader = Rnxv2Obs(fpath=Path(\"station.10o\"))\n    &gt;&gt;&gt; ds = reader.to_ds(keep_rnx_data_vars=[\"SNR\"])\n    &gt;&gt;&gt; print(ds)\n    \"\"\"\n\n    model_config = ConfigDict(\n        frozen=True,\n        arbitrary_types_allowed=True,\n    )\n\n    fpath: Path\n    header: Rnxv2ObsHeader | None = None\n\n    @model_validator(mode='after')\n    def parse_header(self):\n        \"\"\"Parse header on initialization.\"\"\"\n        with open(self.fpath, 'r') as f:\n            header_lines = []\n            for line in f:\n                header_lines.append(line)\n                if \"END OF HEADER\" in line:\n                    break\n\n        self.header = Rnxv2ObsHeader.from_lines(header_lines)\n        return self\n\n    @property\n    def file_hash(self) -&gt; str:\n        \"\"\"Compute file hash.\"\"\"\n        return rinex_file_hash(self.fpath)\n\n    @property\n    def start_time(self) -&gt; datetime:\n        \"\"\"Return start time.\"\"\"\n        return self.header.first_obs\n\n    @property\n    def end_time(self) -&gt; datetime:\n        \"\"\"Return end time.\"\"\"\n        # Would need to parse or compute\n        raise NotImplementedError(\"End time parsing not yet implemented\")\n\n    @property\n    def systems(self) -&gt; list[str]:\n        \"\"\"Return GNSS systems.\"\"\"\n        # RINEX v2 typically only has GPS\n        return ['G']\n\n    @property\n    def num_epochs(self) -&gt; int:\n        \"\"\"Return number of epochs.\"\"\"\n        count = 0\n        for _ in self.iter_epochs():\n            count += 1\n        return count\n\n    @property\n    def num_satellites(self) -&gt; int:\n        \"\"\"Return number of unique satellites.\"\"\"\n        sats = set()\n        for epoch in self.iter_epochs():\n            for sat in epoch.satellites:\n                sats.add(sat.sv)\n        return len(sats)\n\n    def iter_epochs(self) -&gt; Generator[Rnxv2ObsEpoch, None, None]:\n        \"\"\"Iterate over epochs.\"\"\"\n        with open(self.fpath, 'r') as f:\n            # Skip to data section\n            for line in f:\n                if \"END OF HEADER\" in line:\n                    break\n\n            # Parse epoch records\n            current_epoch = None\n            for line in f:\n                # RINEX v2 epoch line format differs from v3\n                if len(line) &gt;= 29 and line[0] != ' ':\n                    # New epoch\n                    if current_epoch:\n                        yield current_epoch\n\n                    # Parse epoch line\n                    timestamp = self._parse_epoch_line(line)\n                    current_epoch = Rnxv2ObsEpoch(timestamp, [])\n                else:\n                    # Observation line\n                    self._parse_observation_line(line, current_epoch)\n\n            if current_epoch:\n                yield current_epoch\n\n    def to_ds(\n        self,\n        keep_rnx_data_vars: list[str] | None = None,\n        **kwargs\n    ) -&gt; xr.Dataset:\n        \"\"\"Convert to Dataset.\"\"\"\n        # Implementation following the pattern shown above\n        # ... (similar to RINEX v3)\n\n        # MUST call validation\n        self.validate_output(ds, required_vars=keep_rnx_data_vars)\n\n        return ds\n\n    def _parse_epoch_line(self, line: str) -&gt; datetime:\n        \"\"\"Parse RINEX v2 epoch line.\"\"\"\n        # RINEX v2 format: YY MM DD HH MM SS.SSSSSSS\n        year = int(line[1:3]) + 2000  # Y2K handling\n        month = int(line[4:6])\n        day = int(line[7:9])\n        hour = int(line[10:12])\n        minute = int(line[13:15])\n        second = float(line[16:26])\n\n        return datetime(\n            year, month, day, hour, minute,\n            int(second), int((second % 1) * 1e6)\n        )\n\n    def _parse_observation_line(self, line: str, epoch: Rnxv2ObsEpoch):\n        \"\"\"Parse observation line.\"\"\"\n        # Implementation based on RINEX v2 specification\n        pass\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#validation-requirements","level":2,"title":"Validation Requirements","text":"","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#required-dimensions","level":3,"title":"Required Dimensions","text":"<pre><code># The Dataset must have these dimensions\nassert \"epoch\" in ds.dims\nassert \"sid\" in ds.dims\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#required-coordinates","level":3,"title":"Required Coordinates","text":"<pre><code># The Dataset must have these coordinates with correct dtypes\nrequired_coords = {\n    \"epoch\": \"datetime64[ns]\",\n    \"sid\": \"object\",           # string\n    \"sv\": \"object\",\n    \"system\": \"object\",\n    \"band\": \"object\",\n    \"code\": \"object\",\n    \"freq_center\": \"float64\",  # NOT float32\n    \"freq_min\": \"float64\",\n    \"freq_max\": \"float64\",\n}\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#required-attributes","level":3,"title":"Required Attributes","text":"<pre><code># The Dataset must have these global attributes\nrequired_attrs = {\n    \"Created\",\n    \"Software\",\n    \"Institution\",\n    \"RINEX File Hash\",  # For storage deduplication\n}\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#data-variables","level":3,"title":"Data Variables","text":"<pre><code># At minimum, SNR and Phase must be present\n# All data variables must have dimensions (\"epoch\", \"sid\")\nassert \"SNR\" in ds.data_vars\nassert \"Phase\" in ds.data_vars\nassert ds.SNR.dims == (\"epoch\", \"sid\")\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#testing","level":2,"title":"Testing","text":"","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#unit-tests","level":3,"title":"Unit Tests","text":"<pre><code># tests/test_my_format.py\nfrom pathlib import Path\nimport pytest\nfrom my_package.readers import MyFormatReader\n\nclass TestMyFormatReader:\n    \"\"\"Test custom format reader.\"\"\"\n\n    def test_initialization(self):\n        \"\"\"Test reader initialization.\"\"\"\n        reader = MyFormatReader(fpath=Path(\"test.dat\"))\n        assert reader.fpath.name == \"test.dat\"\n\n    def test_file_hash(self, tmp_path):\n        \"\"\"Test file hash computation.\"\"\"\n        test_file = tmp_path / \"test.dat\"\n        test_file.write_text(\"test content\")\n\n        reader = MyFormatReader(fpath=test_file)\n        hash1 = reader.file_hash\n        hash2 = reader.file_hash  # Should be deterministic\n\n        assert hash1 == hash2\n        assert len(hash1) == 16\n\n    def test_to_ds_structure(self):\n        \"\"\"Test Dataset structure.\"\"\"\n        reader = MyFormatReader(fpath=Path(\"real_test_file.dat\"))\n        ds = reader.to_ds()\n\n        # Validate structure\n        assert \"epoch\" in ds.dims\n        assert \"sid\" in ds.dims\n        assert \"SNR\" in ds.data_vars\n        assert ds.SNR.dims == (\"epoch\", \"sid\")\n\n    def test_validation_passes(self):\n        \"\"\"Test validation passes for valid output.\"\"\"\n        reader = MyFormatReader(fpath=Path(\"real_test_file.dat\"))\n        ds = reader.to_ds()\n\n        # Should not raise\n        from canvod.readers.base import DatasetStructureValidator\n        validator = DatasetStructureValidator(dataset=ds)\n        validator.validate_all()\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#integration-tests","level":3,"title":"Integration Tests","text":"<pre><code>def test_full_pipeline():\n    \"\"\"Test complete pipeline from file to filtered Dataset.\"\"\"\n    reader = MyFormatReader(fpath=Path(\"real_file.dat\"))\n\n    # Convert to Dataset\n    ds = reader.to_ds(keep_rnx_data_vars=[\"SNR\"])\n\n    # Filter by system\n    gps = ds.where(ds.system == 'G', drop=True)\n    assert len(gps.sid) &gt; 0\n\n    # Compute statistics\n    mean_snr = gps.SNR.mean()\n    assert mean_snr &gt; 0\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#registering-with-readerfactory","level":2,"title":"Registering with ReaderFactory","text":"<p>Once the reader is verified, it can be registered with the factory for automatic format detection:</p> <pre><code>from canvod.readers.base import ReaderFactory\nfrom my_package.readers import MyFormatReader\n\n# Register\nReaderFactory.register('my_format', MyFormatReader)\n\n# Automatic format detection\nreader = ReaderFactory.create(\"file.dat\")\n# Returns MyFormatReader instance if format is detected\n</code></pre> <p>The detection logic should be updated to recognize the new format:</p> <pre><code># In ReaderFactory._detect_format()\n@staticmethod\ndef _detect_format(fpath: Path) -&gt; str:\n    \"\"\"Detect file format.\"\"\"\n    with open(fpath, 'r') as f:\n        first_line = f.readline()\n\n    # Check for RINEX\n    if first_line[60:73].strip() == \"RINEX VERSION\":\n        version = float(first_line[:9].strip())\n        if 3.0 &lt;= version &lt; 4.0:\n            return 'rinex_v3'\n        elif 2.0 &lt;= version &lt; 3.0:\n            return 'rinex_v2'\n\n    # Check for custom format\n    if first_line.startswith(\"MY_FORMAT\"):\n        return 'my_format'\n\n    raise ValueError(f\"Unknown format: {fpath}\")\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#common-pitfalls","level":2,"title":"Common Pitfalls","text":"","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#incorrect-dtype-for-frequencies","level":3,"title":"Incorrect dtype for frequencies","text":"<pre><code># Incorrect: float32\nfreq_center = np.array([...], dtype=np.float32)\n\n# Correct: float64\nfreq_center = np.array([...], dtype=np.float64)\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#missing-validation","level":3,"title":"Missing validation","text":"<pre><code># Incorrect: No validation before return\ndef to_ds(self, **kwargs) -&gt; xr.Dataset:\n    ds = self._build_dataset()\n    return ds  # Missing validation\n\n# Correct: Always validate\ndef to_ds(self, **kwargs) -&gt; xr.Dataset:\n    ds = self._build_dataset()\n    self.validate_output(ds)  # Required\n    return ds\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#incorrect-dimension-names","level":3,"title":"Incorrect dimension names","text":"<pre><code># Incorrect: Non-standard dimension names\ndata_vars={\"SNR\": ((\"time\", \"signal\"), data)}\n\n# Correct: Must be (epoch, sid)\ndata_vars={\"SNR\": ((\"epoch\", \"sid\"), data)}\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#missing-file-hash-attribute","level":3,"title":"Missing file hash attribute","text":"<pre><code># Incorrect: No file hash in attributes\nattrs={\"Created\": \"...\", \"Software\": \"...\"}\n\n# Correct: Include file hash\nattrs={\n    \"Created\": \"...\",\n    \"Software\": \"...\",\n    \"RINEX File Hash\": self.file_hash,  # Required\n}\n</code></pre>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/extending/#pre-submission-checklist","level":2,"title":"Pre-Submission Checklist","text":"<p>Before submitting a new reader, the following requirements should be verified:</p> <ul> <li>Inherits from <code>GNSSDataReader</code> and <code>BaseModel</code></li> <li>Implements all abstract methods</li> <li>Returns validated xarray.Dataset</li> <li>Uses <code>SignalIDMapper</code> for Signal IDs</li> <li>Includes proper metadata (COORDS_METADATA, etc.)</li> <li>Has comprehensive tests (&gt;90% coverage)</li> <li>Has docstrings (NumPy style)</li> <li>Type hints on all methods</li> <li>Validation passes (<code>validate_output()</code> called)</li> <li>File hash included in attributes</li> <li>Registered with <code>ReaderFactory</code> (if applicable)</li> </ul>","path":["Packages","canvod-readers","Extending Readers"],"tags":[]},{"location":"packages/readers/overview/","level":1,"title":"canvod-readers","text":"","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#purpose","level":2,"title":"Purpose","text":"<p>The <code>canvod-readers</code> package provides validated parsers for GNSS observation data. It transforms raw RINEX files into analysis-ready xarray Datasets, serving as the data ingestion layer for GNSS Transmissometry (GNSS-T) analysis.</p>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#supported-input-formats","level":2,"title":"Supported Input Formats","text":"<p>GNSS data is distributed in various formats with complex structures:</p> <ul> <li>Complex headers containing station metadata, observation types, and system information</li> <li>Multiple constellations: GPS, GLONASS, Galileo, BeiDou, QZSS, IRNSS, SBAS</li> <li>Variable frequency bands: L1, L2, L5, E1, E5a, E5b, B1, B2, B3, G1, G2</li> <li>Multiple observable types: SNR, Phase, Pseudorange, Doppler</li> <li>Irregular data: missing epochs, incomplete observations, varying sampling rates</li> </ul> <p>The package addresses these complexities by providing:</p> <ul> <li>Standardized output format (xarray.Dataset)</li> <li>Automatic validation (completeness checks, dtype verification)</li> <li>Memory-efficient parsing (lazy loading, epoch-by-epoch iteration)</li> <li>Signal disambiguation through unique Signal IDs</li> <li>Extensibility via the <code>GNSSDataReader</code> abstract base class</li> </ul>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#design","level":2,"title":"Design","text":"","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#separation-of-concerns","level":3,"title":"Separation of Concerns","text":"<pre><code>graph TD\n    A[Raw RINEX File] --&gt; B[Parser: Rnxv3Obs]\n    B --&gt; C[Validation: DatasetStructureValidator]\n    C --&gt; D[Standardized xarray.Dataset]\n    D --&gt; E[Downstream Analysis]\n</code></pre> <p>Each component has a single responsibility:</p> <ul> <li>Parser (<code>Rnxv3Obs</code>): Reads RINEX files and extracts observations</li> <li>Validator (<code>DatasetStructureValidator</code>): Ensures output meets structural requirements</li> <li>Dataset: Standard format consumed by downstream packages</li> </ul>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#contract-based-design","level":3,"title":"Contract-Based Design","text":"<p>All readers implement the <code>GNSSDataReader</code> abstract base class:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass GNSSDataReader(ABC):\n    \"\"\"Base class for all GNSS data format readers.\"\"\"\n\n    @abstractmethod\n    def to_ds(self, **kwargs) -&gt; xr.Dataset:\n        \"\"\"Convert to xarray.Dataset.\"\"\"\n\n    @abstractmethod\n    def iter_epochs(self):\n        \"\"\"Iterate through epochs.\"\"\"\n\n    @property\n    @abstractmethod\n    def file_hash(self) -&gt; str:\n        \"\"\"Compute file hash for deduplication.\"\"\"\n</code></pre> <p>This contract ensures all readers produce compatible output, allowing downstream code to operate independently of the specific reader implementation.</p>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#validation","level":3,"title":"Validation","text":"<p>Every Dataset produced by any reader must pass validation:</p> <pre><code>validator = DatasetStructureValidator(dataset=ds)\nvalidator.validate_all()  # Raises ValueError if invalid\n</code></pre> <p>Validation checks include:</p> <ul> <li>Required dimensions: <code>(epoch, sid)</code></li> <li>Required coordinates: <code>epoch</code>, <code>sid</code>, <code>sv</code>, <code>system</code>, <code>band</code>, <code>code</code>, <code>freq_*</code></li> <li>Correct dtypes: <code>float32</code> for frequencies, <code>datetime64[ns]</code> for epoch</li> <li>Required attributes: <code>\"Created\"</code>, <code>\"RINEX File Hash\"</code></li> <li>Minimum data variables: <code>SNR</code>, <code>Phase</code></li> </ul>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#usage-examples","level":2,"title":"Usage Examples","text":"","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#vod-analysis-pipeline","level":3,"title":"VOD Analysis Pipeline","text":"<pre><code>reader = Rnxv3Obs(fpath=\"station.24o\")\nds = reader.to_ds(keep_rnx_data_vars=[\"SNR\"])\n\n# Filter L-band signals\nl_band = ds.where(ds.band.isin(['L1', 'L2', 'L5']), drop=True)\n\n# Pass to VOD calculator\nfrom canvod.vod import VODCalculator\nvod = VODCalculator(l_band).compute()\n</code></pre>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#multi-constellation-analysis","level":3,"title":"Multi-Constellation Analysis","text":"<pre><code>ds = reader.to_ds()\n\nfor system in ['G', 'R', 'E', 'C']:\n    system_ds = ds.where(ds.system == system, drop=True)\n    mean_snr = system_ds.SNR.mean(dim=['epoch', 'sid'])\n    print(f\"{system}: {mean_snr:.2f} dB\")\n</code></pre>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#time-series-processing","level":3,"title":"Time Series Processing","text":"<pre><code>from pathlib import Path\n\nrinex_dir = Path(\"/data/station/2024/\")\ndatasets = []\n\nfor rinex_file in sorted(rinex_dir.glob(\"*.24o\")):\n    reader = Rnxv3Obs(fpath=rinex_file)\n    ds = reader.to_ds(keep_rnx_data_vars=[\"SNR\"])\n    datasets.append(ds)\n\ntime_series = xr.concat(datasets, dim='epoch')\n</code></pre>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#quality-control","level":3,"title":"Quality Control","text":"<pre><code>ds = reader.to_ds()\n\n# Find satellites with low SNR\nlow_snr_sats = ds.where(ds.SNR &lt; 30, drop=True).sv.values\nprint(f\"Low SNR satellites: {set(low_snr_sats)}\")\n\n# Check epoch completeness\nexpected_epochs = 2880  # 30s sampling, 24 hours\nactual_epochs = len(ds.epoch)\ncompleteness = (actual_epochs / expected_epochs) * 100\nprint(f\"Epoch completeness: {completeness:.1f}%\")\n</code></pre>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#key-components","level":2,"title":"Key Components","text":"","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#readers","level":3,"title":"Readers","text":"<p>Currently implemented:</p> <ul> <li><code>Rnxv3Obs</code>: RINEX v3.04 observation files</li> </ul> <p>Planned:</p> <ul> <li><code>Rnxv2Obs</code>: RINEX v2.11 observation files</li> <li><code>Rnxv4Obs</code>: RINEX v4.00 observation files</li> </ul>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#gnss-specifications","level":3,"title":"GNSS Specifications","text":"<p>The <code>gnss_specs</code> module provides constellation-specific information:</p> <pre><code>from canvod.readers.gnss_specs import GPS, GALILEO, SignalIDMapper\n\ngps = GPS()\nprint(gps.BANDS)  # {'1': 'L1', '2': 'L2', '5': 'L5'}\n\nmapper = SignalIDMapper()\nsid = mapper.create_signal_id(\"G01\", \"G01|S1C\")\nprint(sid)  # \"G01|L1|C\"\n</code></pre>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#validators","level":3,"title":"Validators","text":"<p>The <code>DatasetStructureValidator</code> ensures data quality and compatibility:</p> <pre><code>from canvod.readers.base import DatasetStructureValidator\n\nvalidator = DatasetStructureValidator(dataset=ds)\nvalidator.validate_dimensions()      # Check (epoch, sid)\nvalidator.validate_coordinates()     # Check required coords\nvalidator.validate_data_variables()  # Check SNR, Phase exist\nvalidator.validate_attributes()      # Check metadata\n</code></pre>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#data-flow","level":2,"title":"Data Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Reader as Rnxv3Obs\n    participant Parser as RinexParser\n    participant Mapper as SignalIDMapper\n    participant Validator as Validator\n    participant Dataset as xr.Dataset\n\n    User-&gt;&gt;Reader: Rnxv3Obs(fpath)\n    Reader-&gt;&gt;Parser: Parse header\n    Parser--&gt;&gt;Reader: Header info\n\n    User-&gt;&gt;Reader: to_ds()\n    Reader-&gt;&gt;Parser: iter_epochs()\n    loop Each epoch\n        Parser--&gt;&gt;Reader: Epoch data\n        Reader-&gt;&gt;Mapper: create_signal_id()\n        Mapper--&gt;&gt;Reader: Signal IDs\n    end\n\n    Reader-&gt;&gt;Dataset: Build xr.Dataset\n    Reader-&gt;&gt;Validator: validate_output()\n    Validator--&gt;&gt;Reader: Valid\n    Reader--&gt;&gt;User: xr.Dataset</code></pre>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/overview/#optimization-guidelines","level":2,"title":"Optimization Guidelines","text":"<ul> <li>Filter early: Use <code>keep_rnx_data_vars</code> to limit data variables loaded into memory.</li> <li>System filtering: Process one constellation at a time for large files.</li> <li>Batch processing: Use multiprocessing for multiple files.</li> <li>Storage: Save to Zarr/NetCDF for faster subsequent access.</li> </ul>","path":["Packages","canvod-readers"],"tags":[]},{"location":"packages/readers/rinex-format/","level":1,"title":"RINEX v3.04 Parsing","text":"<p>The <code>Rnxv3Obs</code> class implements a RINEX v3.04 observation file reader. This page documents the internal parsing pipeline, data models, and performance characteristics.</p>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#format-overview","level":2,"title":"Format Overview","text":"<p>RINEX (Receiver Independent Exchange Format) is the standard exchange format for GNSS observations. Version 3.04 supports:</p> <ul> <li>Multiple GNSS constellations: GPS, GLONASS, Galileo, BeiDou, QZSS, IRNSS, SBAS</li> <li>Observation types: SNR, Phase, Pseudorange, Doppler, LLI, SSI</li> <li>Hatanaka compression for compact representation</li> <li>Extended metadata: antenna, receiver, and station information</li> </ul>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#file-structure","level":3,"title":"File Structure","text":"<pre><code>RINEX v3.04 Observation File\n\n+----------------------------------+\n|         HEADER SECTION           |\n|  - Version, Type                 |\n|  - Antenna, Receiver info        |\n|  - Observation types             |\n|  - Time of first obs             |\n|  - Interval, Number of obs       |\n+----------------------------------+\n+----------------------------------+\n|         DATA SECTION             |\n|  Epoch 1: 2024-01-01 00:00:00   |\n|    G01  observations             |\n|    G02  observations             |\n|    ...                           |\n|  Epoch 2: 2024-01-01 00:00:30   |\n|    G01  observations             |\n|    ...                           |\n+----------------------------------+\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#implementation","level":2,"title":"Implementation","text":"","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#class-hierarchy","level":3,"title":"Class Hierarchy","text":"<pre><code>classDiagram\n    GNSSDataReader &lt;|-- Rnxv3Obs\n    BaseModel &lt;|-- Rnxv3Obs\n\n    Rnxv3Obs *-- Rnxv3ObsHeader\n    Rnxv3Obs *-- SignalIDMapper\n\n    Rnxv3ObsHeader *-- Rnxv3ObsHeaderFileVars\n\n    class GNSSDataReader{\n        &lt;&lt;abstract&gt;&gt;\n        +to_ds()*\n        +iter_epochs()*\n        +file_hash*\n    }\n\n    class Rnxv3Obs{\n        +Path fpath\n        +Rnxv3ObsHeader header\n        +to_ds()\n        +iter_epochs()\n        +file_hash\n    }\n\n    class Rnxv3ObsHeader{\n        +float rinex_version\n        +str rinex_type\n        +dict obs_types\n        +datetime first_obs\n    }</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#core-components","level":3,"title":"Core Components","text":"<p><code>Rnxv3Obs</code> -- Main reader class. Implements the <code>GNSSDataReader</code> ABC, orchestrates parsing, and builds xarray Datasets.</p> <p><code>Rnxv3ObsHeader</code> -- Header parser. A Pydantic model for the header section that validates header format and extracts metadata.</p> <p><code>Rnxv3ObsEpochRecord</code> -- Epoch parser. Parses epoch lines, extracts timestamps and satellite counts, and validates epoch flags.</p> <p><code>Observation</code> -- Individual observation. Represents a single measurement and stores the value along with LLI and SSI flags.</p>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#parsing-pipeline","level":2,"title":"Parsing Pipeline","text":"","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#step-1-initialization","level":3,"title":"Step 1: Initialization","text":"<pre><code>from pathlib import Path\nfrom canvod.readers import Rnxv3Obs\n\nreader = Rnxv3Obs(fpath=Path(\"station.24o\"))\n</code></pre> <p>During initialization, the following operations are performed:</p> <ol> <li>Pydantic validates that <code>fpath</code> exists and is readable.</li> <li>The header section is parsed immediately.</li> <li>The header is validated (RINEX version, file type, observation types).</li> <li>The data section position is cached for iteration.</li> </ol> <pre><code># Internal initialization\nclass Rnxv3Obs(BaseModel, GNSSDataReader):\n    fpath: Path\n    header: Rnxv3ObsHeader | None = None\n\n    @model_validator(mode='after')\n    def parse_header(self):\n        \"\"\"Parse header on initialization.\"\"\"\n        with open(self.fpath, 'r') as f:\n            header_lines = []\n            for line in f:\n                header_lines.append(line)\n                if \"END OF HEADER\" in line:\n                    break\n\n        self.header = Rnxv3ObsHeader.from_lines(header_lines)\n        return self\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#step-2-epoch-iteration","level":3,"title":"Step 2: Epoch Iteration","text":"<pre><code>for epoch in reader.iter_epochs():\n    print(epoch.timestamp, epoch.num_satellites)\n</code></pre> <p>Internal flow:</p> <pre><code>def iter_epochs(self) -&gt; Generator[Rnxv3ObsEpochRecord, None, None]:\n    \"\"\"Lazily iterate through epochs.\"\"\"\n    with open(self.fpath, 'r') as f:\n        # Skip to end of header\n        for line in f:\n            if \"END OF HEADER\" in line:\n                break\n\n        # Read data section\n        current_epoch = None\n        for line in f:\n            if line.startswith('&gt;'):  # Epoch marker\n                if current_epoch:\n                    yield current_epoch\n                current_epoch = Rnxv3ObsEpochRecord.from_line(line)\n            else:\n                # Observation line\n                current_epoch.add_observation(line)\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#step-3-dataset-construction","level":3,"title":"Step 3: Dataset Construction","text":"<pre><code>ds = reader.to_ds(keep_rnx_data_vars=[\"SNR\", \"Phase\"])\n</code></pre> <p>Complete implementation:</p> <pre><code>def to_ds(\n    self,\n    keep_rnx_data_vars: list[str] | None = None,\n    **kwargs\n) -&gt; xr.Dataset:\n    \"\"\"Convert RINEX to xarray.Dataset.\n\n    Parameters\n    ----------\n    keep_rnx_data_vars : list of str, optional\n        Variables to include. If None, includes all.\n\n    Returns\n    -------\n    xr.Dataset\n        Validated dataset with standardized structure.\n    \"\"\"\n    # 1. Collect all observations\n    epochs_data = []\n    for epoch in self.iter_epochs():\n        epochs_data.append(epoch)\n\n    # 2. Build Signal ID index\n    signal_mapper = SignalIDMapper()\n    all_sids = set()\n    for epoch in epochs_data:\n        for sat in epoch.satellites:\n            for obs_code in sat.observations:\n                sid = signal_mapper.create_signal_id(\n                    sat.sv, f\"{sat.sv}|{obs_code}\"\n                )\n                all_sids.add(sid)\n\n    sids = sorted(all_sids)\n\n    # 3. Create coordinate arrays\n    epochs = [e.timestamp for e in epochs_data]\n\n    # Extract SV, system, band, code from SIDs\n    sv_arr = np.array([sid.split('|')[0] for sid in sids])\n    band_arr = np.array([sid.split('|')[1] for sid in sids])\n    code_arr = np.array([sid.split('|')[2] for sid in sids])\n    system_arr = np.array([sid[0] for sid in sids])\n\n    # Get frequencies\n    freq_center = np.array([\n        signal_mapper.get_band_frequency(sid.split('|')[1])\n        for sid in sids\n    ], dtype=np.float64)\n\n    bandwidth = np.array([\n        signal_mapper.get_band_bandwidth(sid.split('|')[1])\n        for sid in sids\n    ], dtype=np.float64)\n\n    freq_min = freq_center - (bandwidth / 2.0)\n    freq_max = freq_center + (bandwidth / 2.0)\n\n    # 4. Build data arrays\n    data_vars = {}\n    if keep_rnx_data_vars is None or \"SNR\" in keep_rnx_data_vars:\n        snr_data = np.full((len(epochs), len(sids)), np.nan, dtype=np.float32)\n        # Fill with observations...\n        data_vars[\"SNR\"] = ((\"epoch\", \"sid\"), snr_data)\n\n    # Similar for Phase, Pseudorange, Doppler...\n\n    # 5. Create Dataset\n    ds = xr.Dataset(\n        data_vars=data_vars,\n        coords={\n            \"epoch\": (\"epoch\", epochs),\n            \"sid\": (\"sid\", sids),\n            \"sv\": (\"sid\", sv_arr),\n            \"system\": (\"sid\", system_arr),\n            \"band\": (\"sid\", band_arr),\n            \"code\": (\"sid\", code_arr),\n            \"freq_center\": (\"sid\", freq_center),\n            \"freq_min\": (\"sid\", freq_min),\n            \"freq_max\": (\"sid\", freq_max),\n        },\n        attrs={\n            \"Created\": datetime.now().isoformat(),\n            \"Software\": f\"canvod-readers {__version__}\",\n            \"RINEX File Hash\": self.file_hash,\n            # ... more attributes\n        }\n    )\n\n    # 6. Validate before returning\n    self.validate_output(ds, required_vars=keep_rnx_data_vars)\n\n    return ds\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#pydantic-data-models","level":2,"title":"Pydantic Data Models","text":"<p>Pydantic models are used throughout the parsing pipeline to enforce type safety and validate data at parse time rather than during downstream analysis. This approach ensures that malformed input is detected immediately, with structured error messages indicating the source of the problem.</p>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#header-model","level":3,"title":"Header Model","text":"<p>The <code>Rnxv3ObsHeader</code> model validates all header fields according to the RINEX v3.04 specification:</p> <pre><code>from pydantic import BaseModel, field_validator\nfrom datetime import datetime\n\nclass Rnxv3ObsHeader(BaseModel):\n    \"\"\"RINEX v3 header with automatic validation.\n\n    Parses and validates all header fields according to\n    RINEX v3.04 specification.\n    \"\"\"\n\n    # Required fields\n    rinex_version: float\n    rinex_type: str\n    sat_system: str | None = None\n\n    # Program information\n    pgm: str | None = None\n    run_by: str | None = None\n    date: str | None = None\n\n    # Station information\n    marker_name: str | None = None\n    marker_number: str | None = None\n    marker_type: str | None = None\n    observer: str | None = None\n    agency: str | None = None\n\n    # Receiver information\n    rec_number: str | None = None\n    rec_type: str | None = None\n    rec_vers: str | None = None\n\n    # Antenna information\n    ant_number: str | None = None\n    ant_type: str | None = None\n    approx_position_xyz: tuple[float, float, float] | None = None\n    antenna_delta_hen: tuple[float, float, float] | None = None\n\n    # Observation information\n    obs_types: dict[str, list[str]]  # system -&gt; observation types\n    signal_strength_unit: str | None = None\n    interval: float | None = None\n    time_of_first_obs: datetime | None = None\n    time_of_last_obs: datetime | None = None\n\n    # Validators\n    @field_validator('rinex_version')\n    def check_rinex_version(cls, v):\n        \"\"\"Ensure RINEX v3.x.\"\"\"\n        if not (3.0 &lt;= v &lt; 4.0):\n            raise ValueError(f\"Expected RINEX v3, got {v}\")\n        return v\n\n    @field_validator('rinex_type')\n    def check_file_type(cls, v):\n        \"\"\"Ensure observation file.\"\"\"\n        if v != 'O':\n            raise ValueError(f\"Expected observation file (O), got {v}\")\n        return v\n\n    @field_validator('obs_types')\n    def check_obs_types(cls, v):\n        \"\"\"Validate observation types.\"\"\"\n        if not v:\n            raise ValueError(\"No observation types defined\")\n\n        valid_types = {'C', 'L', 'D', 'S'}\n        for system, obs_list in v.items():\n            for obs in obs_list:\n                if obs[0] not in valid_types:\n                    raise ValueError(f\"Invalid observation type: {obs}\")\n        return v\n\n    @classmethod\n    def from_lines(cls, lines: list[str]) -&gt; 'Rnxv3ObsHeader':\n        \"\"\"Parse header from lines.\"\"\"\n        data = {}\n\n        for line in lines:\n            label = line[60:80].strip()\n\n            if label == \"RINEX VERSION / TYPE\":\n                data['rinex_version'] = float(line[0:9])\n                data['rinex_type'] = line[20]\n                data['sat_system'] = line[40] if line[40] != ' ' else None\n\n            elif label == \"PGM / RUN BY / DATE\":\n                data['pgm'] = line[0:20].strip()\n                data['run_by'] = line[20:40].strip()\n                data['date'] = line[40:60].strip()\n\n            elif label == \"MARKER NAME\":\n                data['marker_name'] = line[0:60].strip()\n\n            # ... parse all other fields\n\n            elif label == \"SYS / # / OBS TYPES\":\n                system = line[0]\n                num_obs = int(line[3:6])\n                obs_types = []\n\n                # Parse observation types (may span multiple lines)\n                start = 7\n                for i in range(num_obs):\n                    if start &gt;= 60:  # Continuation line\n                        # Read next line\n                        pass\n                    obs_types.append(line[start:start+3].strip())\n                    start += 4\n\n                if 'obs_types' not in data:\n                    data['obs_types'] = {}\n                data['obs_types'][system] = obs_types\n\n        return cls(**data)\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#epoch-record-model","level":3,"title":"Epoch Record Model","text":"<pre><code>class Rnxv3ObsEpochRecord(BaseModel):\n    \"\"\"Single epoch with observations.\"\"\"\n\n    epoch_flag: int  # 0-6\n    timestamp: datetime\n    num_satellites: int\n    satellites: list[Satellite] = []\n\n    @field_validator('epoch_flag')\n    def check_epoch_flag(cls, v):\n        \"\"\"Validate epoch flag.\"\"\"\n        if not (0 &lt;= v &lt;= 6):\n            raise ValueError(f\"Invalid epoch flag: {v}\")\n        return v\n\n    @classmethod\n    def from_line(cls, line: str) -&gt; 'Rnxv3ObsEpochRecord':\n        \"\"\"Parse epoch line.\n\n        Format: &gt; YYYY MM DD HH MM SS.SSSSSSS  F NS\n        \"\"\"\n        if not line.startswith('&gt;'):\n            raise ValueError(\"Epoch line must start with '&gt;'\")\n\n        # Extract fields\n        year = int(line[2:6])\n        month = int(line[7:9])\n        day = int(line[10:12])\n        hour = int(line[13:15])\n        minute = int(line[16:18])\n        second = float(line[19:30])\n\n        epoch_flag = int(line[31])\n        num_satellites = int(line[33:36])\n\n        timestamp = datetime(\n            year, month, day, hour, minute,\n            int(second), int((second % 1) * 1e6)\n        )\n\n        return cls(\n            epoch_flag=epoch_flag,\n            timestamp=timestamp,\n            num_satellites=num_satellites\n        )\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#satellite-and-observation-models","level":3,"title":"Satellite and Observation Models","text":"<pre><code>class Observation(BaseModel):\n    \"\"\"Single observation value.\"\"\"\n\n    value: float\n    lli: int | None = None  # Loss of lock indicator\n    ssi: int | None = None  # Signal strength indicator\n\n    @field_validator('lli')\n    def check_lli(cls, v):\n        if v is not None and not (0 &lt;= v &lt;= 9):\n            raise ValueError(f\"LLI must be 0-9, got {v}\")\n        return v\n\n    @field_validator('ssi')\n    def check_ssi(cls, v):\n        if v is not None and not (0 &lt;= v &lt;= 9):\n            raise ValueError(f\"SSI must be 0-9, got {v}\")\n        return v\n\n\nclass Satellite(BaseModel):\n    \"\"\"Satellite with observations.\"\"\"\n\n    sv: str  # e.g., \"G01\", \"E05\"\n    observations: dict[str, Observation]  # obs_code -&gt; Observation\n\n    @field_validator('sv')\n    def check_sv_format(cls, v):\n        \"\"\"Validate SV format.\"\"\"\n        if len(v) != 3:\n            raise ValueError(f\"SV must be 3 characters, got {v}\")\n        if v[0] not in 'GRECJIS':\n            raise ValueError(f\"Invalid system: {v[0]}\")\n        if not v[1:].isdigit():\n            raise ValueError(f\"Invalid PRN: {v[1:]}\")\n        return v\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#signal-id-mapping","level":2,"title":"Signal ID Mapping","text":"<p>The <code>SignalIDMapper</code> converts RINEX observation codes to unique Signal IDs:</p> <pre><code>class SignalIDMapper:\n    \"\"\"Map observation codes to Signal IDs.\"\"\"\n\n    def create_signal_id(self, sv: str, obs_code: str) -&gt; str:\n        \"\"\"Create Signal ID from SV and observation code.\n\n        Parameters\n        ----------\n        sv : str\n            Satellite vehicle (e.g., \"G01\")\n        obs_code : str\n            Observation code (e.g., \"G01|S1C\")\n\n        Returns\n        -------\n        str\n            Signal ID in format \"SV|BAND|CODE\"\n\n        Examples\n        --------\n        &gt;&gt;&gt; mapper = SignalIDMapper()\n        &gt;&gt;&gt; mapper.create_signal_id(\"G01\", \"G01|S1C\")\n        'G01|L1|C'\n        \"\"\"\n        sv, observation_code = obs_code.split('|')\n        system = sv[0]\n\n        # Special case: X1 auxiliary observations\n        if observation_code == 'X1':\n            return f\"{sv}|X1|X\"\n\n        # Standard: TYPE + BAND + CODE\n        # e.g., \"S1C\" -&gt; Type=S, Band=1, Code=C\n        obs_type = observation_code[0]  # S, C, L, D\n        band_num = observation_code[1]  # 1, 2, 5, etc.\n        code = observation_code[2]      # C, P, W, etc.\n\n        # Map band number to band name\n        band_name = self.SYSTEM_BANDS[system].get(\n            band_num, f'UnknownBand{band_num}'\n        )\n\n        return f\"{sv}|{band_name}|{code}\"\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#constellation-specific-band-mapping","level":3,"title":"Constellation-Specific Band Mapping","text":"<p>Different GNSS constellations use different band numbering schemes. The mapping table is as follows:</p> <pre><code># GPS: 1 -&gt; L1, 2 -&gt; L2, 5 -&gt; L5\nSYSTEM_BANDS = {\n    'G': {'1': 'L1', '2': 'L2', '5': 'L5'},\n    'R': {'1': 'G1', '2': 'G2', '3': 'G3'},  # GLONASS\n    'E': {'1': 'E1', '5': 'E5a', '7': 'E5b', '6': 'E6'},  # Galileo\n    'C': {'2': 'B1I', '1': 'B1C', '5': 'B2a', '7': 'B2b', '6': 'B3I'},  # BeiDou\n}\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#performance-optimizations","level":2,"title":"Performance Optimizations","text":"","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#lazy-iteration","level":3,"title":"Lazy Iteration","text":"<p>The file is not loaded entirely into memory. Instead, epochs are yielded one at a time:</p> <pre><code>def iter_epochs(self):\n    \"\"\"Generator - yields one epoch at a time.\"\"\"\n    with open(self.fpath, 'r') as f:\n        # Skip header\n        for line in f:\n            if \"END OF HEADER\" in line:\n                break\n\n        # Yield epochs as parsed\n        for epoch in self._parse_data_section(f):\n            yield epoch\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#pre-allocated-arrays","level":3,"title":"Pre-allocated Arrays","text":"<p>For <code>to_ds()</code>, arrays are pre-allocated rather than grown dynamically:</p> <pre><code># Pre-allocate with NaN fill\nsnr_data = np.full(\n    (len(epochs), len(sids)),\n    np.nan,\n    dtype=np.float32\n)\n\n# Then fill in-place\nfor i, epoch in enumerate(epochs_data):\n    for sid, value in epoch.get_observations(\"SNR\"):\n        j = sid_to_index[sid]\n        snr_data[i, j] = value\n</code></pre> <p>This avoids repeated memory allocations that would occur with dynamically growing lists.</p>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#vectorized-operations","level":3,"title":"Vectorized Operations","text":"<p>NumPy vectorization is used for coordinate creation:</p> <pre><code># Vectorized\nsv_arr = np.array([sid.split('|')[0] for sid in sids])\nsystem_arr = np.char.array(sv_arr)[:, 0]\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#error-handling","level":2,"title":"Error Handling","text":"","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#validation-at-parse-time","level":3,"title":"Validation at Parse Time","text":"<pre><code>try:\n    reader = Rnxv3Obs(fpath=path)\nexcept ValidationError as e:\n    # Pydantic validation failed\n    print(f\"Invalid RINEX file: {e}\")\n\ntry:\n    ds = reader.to_ds()\nexcept ValueError as e:\n    # Dataset validation failed\n    print(f\"Invalid dataset structure: {e}\")\n</code></pre>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/readers/rinex-format/#specific-exception-types","level":3,"title":"Specific Exception Types","text":"<p>The <code>canvod.readers.gnss_specs.exceptions</code> module defines a hierarchy of exceptions for fine-grained error handling:</p> <pre><code>from canvod.readers.gnss_specs.exceptions import (\n    CorruptedFileError,\n    MissingEpochError,\n    IncompleteEpochError,\n)\n\ntry:\n    ds = reader.to_ds()\nexcept CorruptedFileError:\n    print(\"File is corrupted or malformed\")\nexcept MissingEpochError:\n    print(\"Expected epochs are missing\")\nexcept IncompleteEpochError:\n    print(\"Epoch has incomplete observations\")\n</code></pre> <p>All reader-specific exceptions inherit from <code>RinexError</code>, allowing broad exception handling when needed while preserving specific error types for targeted recovery.</p>","path":["Packages","canvod-readers","RINEX v3.04 Parsing"],"tags":[]},{"location":"packages/store/icechunk/","level":1,"title":"Icechunk Storage","text":"","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#overview","level":2,"title":"Overview","text":"<p>Icechunk is a cloud-native transactional storage format for multidimensional array data providing ACID guarantees, built-in versioning, and Zarr compatibility.</p>","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#rationale","level":2,"title":"Rationale","text":"Feature Icechunk Zarr NetCDF4 HDF5 Version control Yes No No No Cloud-native Yes Yes No No Transactions Yes No No No Chunking Yes Yes Yes Yes Compression Yes Yes Yes Yes","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#storage-structure","level":2,"title":"Storage Structure","text":"<pre><code>stores/\n  rosalia/\n    rinex/\n      .icechunk/      # Icechunk metadata\n      data/           # Chunked data files\n      versions/       # Version snapshots\n    vod/\n      .icechunk/\n      data/\n      versions/\n</code></pre>","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#chunk-strategy","level":2,"title":"Chunk Strategy","text":"<p>Default chunking for RINEX data:</p> <pre><code>{\n    \"epoch\": 34560,  # ~24 hours at 2.5s sampling\n    \"sid\": -1        # All signals in one chunk\n}\n</code></pre> <p>Epoch chunking aligns with daily processing granularity. Keeping all signal IDs in a single chunk (-1) optimizes VOD calculations that access all signals simultaneously.</p>","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#configuration","level":2,"title":"Configuration","text":"<pre><code># config/processing.yaml\nicechunk:\n  compression_algorithm: zstd\n  compression_level: 5\n  inline_threshold: 512\n  get_concurrency: 1\n</code></pre>","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#usage","level":2,"title":"Usage","text":"","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#initialize-store","level":3,"title":"Initialize Store","text":"<pre><code>from icechunk import IcechunkStore\n\nstore = IcechunkStore.open_or_create(\n    storage=\"file:///path/to/store\",\n    read_only=False\n)\n</code></pre>","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#write-with-transaction","level":3,"title":"Write with Transaction","text":"<pre><code>with store.transaction() as txn:\n    ds = preprocess_dataset(raw_data)\n    ds.to_zarr(store, mode=\"a\")\n    txn.commit(message=\"Added 2024-01-15 data\")\n</code></pre>","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#version-control","level":3,"title":"Version Control","text":"<pre><code>versions = store.list_versions()\n\nstore_v1 = IcechunkStore.open(\n    storage=\"file:///path/to/store\",\n    version=versions[0]\n)\nds = xr.open_zarr(store_v1)\n</code></pre>","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#query-time-range","level":3,"title":"Query Time Range","text":"<pre><code>ds = xr.open_zarr(store)\nsubset = ds.sel(epoch=slice(\"2024-01-01\", \"2024-01-31\"))\n</code></pre>","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#cloud-deployment","level":2,"title":"Cloud Deployment","text":"","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#s3-backend","level":3,"title":"S3 Backend","text":"<pre><code>store = IcechunkStore.open_or_create(\n    storage=\"s3://bucket-name/path/to/store\",\n    storage_config={\"region\": \"us-east-1\", \"credentials\": \"auto\"}\n)\n</code></pre>","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/icechunk/#azure-backend","level":3,"title":"Azure Backend","text":"<pre><code>store = IcechunkStore.open_or_create(\n    storage=\"az://container/path/to/store\",\n    storage_config={\"account_name\": \"myaccount\", \"account_key\": \"...\"}\n)\n</code></pre>","path":["Packages","canvod-store","Icechunk Storage"],"tags":[]},{"location":"packages/store/overview/","level":1,"title":"canvod-store","text":"","path":["Packages","canvod-store"],"tags":[]},{"location":"packages/store/overview/#purpose","level":2,"title":"Purpose","text":"<p>The <code>canvod-store</code> package provides storage management for GNSS vegetation optical depth data using Icechunk, a cloud-native transactional format for multidimensional arrays. It handles dataset storage, preprocessing for storage compatibility, configurable write strategies, and query interfaces.</p>","path":["Packages","canvod-store"],"tags":[]},{"location":"packages/store/overview/#architecture","level":2,"title":"Architecture","text":"<pre><code>graph LR\n    A[RINEX Data] --&gt; B[Preprocessing]\n    B --&gt; C[Icechunk Store]\n    C --&gt; D[Query Interface]\n    D --&gt; E[VOD Analysis]\n</code></pre>","path":["Packages","canvod-store"],"tags":[]},{"location":"packages/store/overview/#core-components","level":2,"title":"Core Components","text":"","path":["Packages","canvod-store"],"tags":[]},{"location":"packages/store/overview/#storage-manager","level":3,"title":"Storage Manager","text":"<pre><code>from canvod.store import MyIcechunkStore\n\nstore = MyIcechunkStore(store_path, strategy=\"append\")\nstore.write(dataset)\n</code></pre>","path":["Packages","canvod-store"],"tags":[]},{"location":"packages/store/overview/#preprocessing-pipeline","level":3,"title":"Preprocessing Pipeline","text":"<pre><code>from canvod.store.preprocessing import IcechunkPreprocessor\n\npreprocessed = IcechunkPreprocessor.prep_aux_ds(raw_dataset)\n</code></pre>","path":["Packages","canvod-store"],"tags":[]},{"location":"packages/store/overview/#reader-interface","level":3,"title":"Reader Interface","text":"<pre><code>from canvod.store import IcechunkDataReader\n\nreader = IcechunkDataReader(store_path)\nds = reader.read(time_range=(\"2024-01-01\", \"2024-12-31\"))\n</code></pre>","path":["Packages","canvod-store"],"tags":[]},{"location":"packages/store/overview/#data-flow","level":2,"title":"Data Flow","text":"<ol> <li>Ingest: Raw GNSS data (RINEX, SP3, CLK)</li> <li>Preprocess: Convert to standard format (sid dimension, padding)</li> <li>Store: Write to Icechunk with compression</li> <li>Query: Retrieve by time range or signal selection</li> <li>Analyze: VOD calculation and analysis</li> </ol>","path":["Packages","canvod-store"],"tags":[]},{"location":"packages/store/overview/#storage-format","level":2,"title":"Storage Format","text":"<p>Data is stored in Icechunk format with: - Chunking optimized for time-series access - Zstd/LZ4 compression - Built-in version control (git-like semantics for data) - S3-compatible cloud backends</p>","path":["Packages","canvod-store"],"tags":[]},{"location":"packages/store/storage-strategies/","level":1,"title":"Storage Strategies","text":"","path":["Packages","canvod-store","Storage Strategies"],"tags":[]},{"location":"packages/store/storage-strategies/#overview","level":2,"title":"Overview","text":"<p>Three storage strategies control how existing data is handled during writes:</p> Strategy Behavior Use Case Skip No write if data exists for the time range Initial ingestion, pipeline restarts Overwrite Replace existing data for the time range Reprocessing, algorithm updates Append Merge new data with existing Continuous monitoring, extending time series","path":["Packages","canvod-store","Storage Strategies"],"tags":[]},{"location":"packages/store/storage-strategies/#skip-strategy","level":2,"title":"Skip Strategy","text":"<pre><code>store = MyIcechunkStore(store_path, strategy=\"skip\")\nstore.write(dataset)  # No-op if data exists\n</code></pre> <p>Checks whether data exists for the given time range before writing. Suitable for raw RINEX observations, which do not change after initial ingestion.</p>","path":["Packages","canvod-store","Storage Strategies"],"tags":[]},{"location":"packages/store/storage-strategies/#overwrite-strategy","level":2,"title":"Overwrite Strategy","text":"<pre><code>store = MyIcechunkStore(store_path, strategy=\"overwrite\")\nstore.write(dataset)  # Replaces existing data\n</code></pre> <p>Deletes existing data for the time range and writes new data with a new version snapshot. Suitable for processed results that may be recomputed with improved algorithms.</p>","path":["Packages","canvod-store","Storage Strategies"],"tags":[]},{"location":"packages/store/storage-strategies/#append-strategy","level":2,"title":"Append Strategy","text":"<pre><code>store = MyIcechunkStore(store_path, strategy=\"append\")\nstore.write(dataset)  # Merges with existing\n</code></pre> <p>Merges new data with existing data, handling overlapping time ranges. Suitable for live monitoring stations.</p>","path":["Packages","canvod-store","Storage Strategies"],"tags":[]},{"location":"packages/store/storage-strategies/#configuration","level":2,"title":"Configuration","text":"<p>Strategy can be set via constructor, configuration file, or environment variable:</p> <pre><code>store = MyIcechunkStore(store_path, strategy=\"append\")\n</code></pre> <pre><code># config/processing.yaml\nstorage:\n  rinex_store_strategy: skip\n  vod_store_strategy: overwrite\n</code></pre> <pre><code>export CANVOD_STORE_STRATEGY=append\n</code></pre>","path":["Packages","canvod-store","Storage Strategies"],"tags":[]},{"location":"packages/store/storage-strategies/#recommended-defaults","level":2,"title":"Recommended Defaults","text":"<ul> <li>RINEX data (raw observations): <code>skip</code> -- raw data is immutable</li> <li>VOD data (processed results): <code>overwrite</code> -- recompute as algorithms improve</li> <li>Continuous monitoring: <code>append</code> -- extend time series daily</li> </ul>","path":["Packages","canvod-store","Storage Strategies"],"tags":[]},{"location":"packages/store/storage-strategies/#performance-characteristics","level":2,"title":"Performance Characteristics","text":"Strategy Write Speed Storage Efficiency Data Safety Skip Fast High High Overwrite Medium Moderate Medium Append Slow Lower Lower","path":["Packages","canvod-store","Storage Strategies"],"tags":[]},{"location":"packages/utils/overview/","level":1,"title":"canvod-utils","text":"","path":["Packages","canvod-utils"],"tags":[]},{"location":"packages/utils/overview/#purpose","level":2,"title":"Purpose","text":"<p>The <code>canvod-utils</code> package provides configuration management and command-line tools for the canVODpy ecosystem. It implements a YAML-based configuration system with Pydantic validation and a CLI for managing settings.</p>","path":["Packages","canvod-utils"],"tags":[]},{"location":"packages/utils/overview/#configuration-system","level":2,"title":"Configuration System","text":"","path":["Packages","canvod-utils"],"tags":[]},{"location":"packages/utils/overview/#file-structure","level":3,"title":"File Structure","text":"<pre><code>canvodpy/\n  config/                              # User configuration (gitignored)\n    processing.yaml                    # Processing parameters\n    sites.yaml                         # Research site definitions\n    sids.yaml                          # Signal ID selection\n    processing.yaml.example            # Template\n  packages/canvod-utils/\n    src/canvod/utils/config/\n      defaults/                        # Package defaults\n        processing.yaml\n        sites.yaml\n        sids.yaml\n</code></pre> <p>User configuration overrides package defaults for any specified values.</p>","path":["Packages","canvod-utils"],"tags":[]},{"location":"packages/utils/overview/#processing-configuration","level":3,"title":"Processing Configuration","text":"<pre><code># config/processing.yaml\nmetadata:\n  author: Nicolas Francois Bader\n  email: nicolas.bader@tuwien.ac.at\n  institution: TU Wien\n\ncredentials:\n  cddis_mail: your.email@example.com\n  gnss_root_dir: /path/to/gnss/data\n\naux_data:\n  agency: COD\n  product_type: final\n\nprocessing:\n  time_aggregation_seconds: 15\n  n_max_threads: 20\n  keep_rnx_vars: [SNR]\n\nicechunk:\n  compression_level: 5\n  compression_algorithm: zstd\n  chunk_strategies:\n    rinex_store:\n      epoch: 34560\n      sid: -1\n\nstorage:\n  stores_root_dir: /path/to/stores\n  rinex_store_strategy: skip\n  vod_store_strategy: overwrite\n</code></pre>","path":["Packages","canvod-utils"],"tags":[]},{"location":"packages/utils/overview/#sites-configuration","level":3,"title":"Sites Configuration","text":"<pre><code># config/sites.yaml\nsites:\n  rosalia:\n    base_dir: /path/to/rosalia\n    receivers:\n      reference_01:\n        type: reference\n        directory: 01_reference/01_GNSS/01_raw\n      canopy_01:\n        type: canopy\n        directory: 02_canopy/01_GNSS/01_raw\n    vod_analyses:\n      canopy_01_vs_reference_01:\n        canopy_receiver: canopy_01\n        reference_receiver: reference_01\n</code></pre>","path":["Packages","canvod-utils"],"tags":[]},{"location":"packages/utils/overview/#loading-configuration","level":3,"title":"Loading Configuration","text":"<pre><code>from canvod.utils.config import load_config\n\nconfig = load_config()\nauthor = config.processing.metadata.author\nagency = config.processing.aux_data.agency\n</code></pre>","path":["Packages","canvod-utils"],"tags":[]},{"location":"packages/utils/overview/#validation","level":3,"title":"Validation","text":"<p>All configuration values are validated by Pydantic models at load time. Invalid emails, nonexistent paths, and out-of-range parameters produce structured error messages.</p> <pre><code>from canvod.utils.config.models import ProcessingParams\n\nparams = ProcessingParams(\n    time_aggregation_seconds=15,  # Valid: 1-300\n    n_max_threads=20,             # Valid: 1-100\n)\n</code></pre>","path":["Packages","canvod-utils"],"tags":[]},{"location":"packages/utils/overview/#cli-tools","level":2,"title":"CLI Tools","text":"<p>The <code>canvodpy</code> command-line interface manages configuration files.</p>","path":["Packages","canvod-utils"],"tags":[]},{"location":"packages/utils/overview/#commands","level":3,"title":"Commands","text":"<pre><code>canvodpy config init       # Copy templates to config/\ncanvodpy config validate   # Validate all configuration files\ncanvodpy config show       # Display current settings\ncanvodpy config edit processing  # Open in $EDITOR\n</code></pre>","path":["Packages","canvod-utils"],"tags":[]},{"location":"packages/utils/overview/#initial-setup-workflow","level":3,"title":"Initial Setup Workflow","text":"<pre><code>canvodpy config init\ncanvodpy config edit processing   # Set metadata, paths, agency\ncanvodpy config edit sites        # Define research sites\ncanvodpy config validate          # Check for errors\n</code></pre>","path":["Packages","canvod-utils"],"tags":[]},{"location":"packages/viz/overview/","level":1,"title":"canvod-viz","text":"","path":["Packages","canvod-viz"],"tags":[]},{"location":"packages/viz/overview/#purpose","level":2,"title":"Purpose","text":"<p>The <code>canvod-viz</code> package provides 2D and 3D hemispheric visualization for GNSS-T grids, SNR data, and VOD results. It wraps matplotlib (publication-quality 2D polar plots) and plotly (interactive 3D hemispheres) behind a unified API.</p>","path":["Packages","canvod-viz"],"tags":[]},{"location":"packages/viz/overview/#components","level":2,"title":"Components","text":"Class / Function Purpose <code>HemisphereVisualizer</code> Unified API combining 2D and 3D <code>HemisphereVisualizer2D</code> Matplotlib polar projection plots <code>HemisphereVisualizer3D</code> Plotly interactive 3D hemispheres <code>visualize_grid</code> One-call 2D grid plot <code>visualize_grid_3d</code> One-call 3D grid plot <code>add_tissot_indicatrix</code> Overlay angular distortion circles <code>PlotStyle</code> / <code>PolarPlotStyle</code> Styling configuration <code>create_publication_style</code> Pre-configured publication settings <code>create_interactive_style</code> Pre-configured interactive settings","path":["Packages","canvod-viz"],"tags":[]},{"location":"packages/viz/overview/#usage","level":2,"title":"Usage","text":"","path":["Packages","canvod-viz"],"tags":[]},{"location":"packages/viz/overview/#unified-api-recommended","level":3,"title":"Unified API (recommended)","text":"<pre><code>from canvod.grids import create_hemigrid\nfrom canvod.viz import HemisphereVisualizer\n\ngrid = create_hemigrid(\"equal_area\", angular_resolution=10.0)\nviz = HemisphereVisualizer(grid)\n\n# 2D publication plot\nfig_2d, ax_2d = viz.plot_2d(data=vod_data, title=\"VOD Distribution\")\n\n# 3D interactive plot\nfig_3d = viz.plot_3d(data=vod_data, title=\"Interactive VOD\")\nfig_3d.show()\n</code></pre>","path":["Packages","canvod-viz"],"tags":[]},{"location":"packages/viz/overview/#convenience-functions","level":3,"title":"Convenience functions","text":"<pre><code>from canvod.viz import visualize_grid, visualize_grid_3d, add_tissot_indicatrix\n\nfig, ax = visualize_grid(grid, data=vod_data, cmap=\"viridis\")\nadd_tissot_indicatrix(ax, grid, n_sample=5)\n\nfig_3d = visualize_grid_3d(grid, data=vod_data)\n</code></pre>","path":["Packages","canvod-viz"],"tags":[]},{"location":"packages/viz/overview/#specialized-visualizers","level":3,"title":"Specialized visualizers","text":"<pre><code>from canvod.viz import HemisphereVisualizer2D, HemisphereVisualizer3D\n\nviz2d = HemisphereVisualizer2D(grid)\nfig, ax = viz2d.plot_grid_patches(data=vod_data, title=\"VOD\")\n\nviz3d = HemisphereVisualizer3D(grid)\nfig = viz3d.plot_hemisphere_surface(data=vod_data, title=\"Interactive VOD\")\n</code></pre>","path":["Packages","canvod-viz"],"tags":[]},{"location":"packages/viz/overview/#styling","level":3,"title":"Styling","text":"<pre><code>from canvod.viz import HemisphereVisualizer, create_publication_style, create_interactive_style\n\nviz = HemisphereVisualizer(grid)\n\n# Publication quality\nviz.set_style(create_publication_style())\nfig, ax = viz.plot_2d(data=vod_data)\n\n# Interactive dark mode\nviz.set_style(create_interactive_style(dark_mode=True))\nfig = viz.plot_3d(data=vod_data)\n</code></pre>","path":["Packages","canvod-viz"],"tags":[]},{"location":"packages/viz/overview/#comparison-and-export","level":3,"title":"Comparison and export","text":"<pre><code>viz = HemisphereVisualizer(grid)\n\n# Side-by-side 2D + 3D\n(fig_2d, ax_2d), fig_3d = viz.create_comparison_plot(data=vod_data)\n\n# Publication figure with custom DPI\nfig, ax = viz.create_publication_figure(\n    data=vod_data,\n    title=\"VOD Distribution\",\n    save_path=\"figure_3.png\",\n    dpi=600,\n)\n\n# Interactive HTML export\nfig = viz.create_interactive_explorer(\n    data=vod_data,\n    dark_mode=True,\n    save_html=\"explorer.html\",\n)\n</code></pre>","path":["Packages","canvod-viz"],"tags":[]},{"location":"packages/viz/overview/#dependencies","level":2,"title":"Dependencies","text":"<p>canvod-viz depends on canvod-grids for grid geometry. 2D plots use matplotlib; 3D plots use plotly.</p>","path":["Packages","canvod-viz"],"tags":[]},{"location":"packages/vod/overview/","level":1,"title":"canvod-vod","text":"","path":["Packages","canvod-vod"],"tags":[]},{"location":"packages/vod/overview/#purpose","level":2,"title":"Purpose","text":"<p>The <code>canvod-vod</code> package implements vegetation optical depth (VOD) estimation from GNSS signal-to-noise ratio (SNR) data. It provides the core scientific algorithms for the canVODpy analysis pipeline.</p>","path":["Packages","canvod-vod"],"tags":[]},{"location":"packages/vod/overview/#theoretical-background","level":2,"title":"Theoretical Background","text":"<p>The package implements the zeroth-order tau-omega radiative transfer model for GNSS-transmissometry, following Humphrey and Frankenberg (2022). In this model, the attenuation of GNSS signals passing through a vegetation canopy is related to the optical depth (tau) of the canopy layer.</p> <p>The zeroth-order model assumes: - Single-scattering approximation (no multiple scattering between canopy elements) - Plane-parallel canopy layer - Signal attenuation proportional to the path length through the canopy</p>","path":["Packages","canvod-vod"],"tags":[]},{"location":"packages/vod/overview/#multi-receiver-scs-expansion","level":2,"title":"Multi-Receiver SCS Expansion","text":"<p>When a reference receiver serves multiple canopy receivers, its satellite geometry must be recomputed relative to each canopy position. The <code>scs_from</code> configuration controls this expansion, creating separate store groups and matched VOD analysis pairs.</p> <pre><code>flowchart TD\n    subgraph FIELD[\"Field Setup\"]\n        C1[\"Canopy Receiver 1\\n(below canopy, NE)\"]\n        C2[\"Canopy Receiver 2\\n(below canopy, SW)\"]\n        R1[\"Reference Receiver\\n(open sky)\"]\n    end\n\n    subgraph SCS_CONFIG[\"SCS Configuration (sites.yaml)\"]\n        C1_CFG[\"canopy_01\\ntype: canopy\\nscs_from: null\\n(uses own position)\"]\n        C2_CFG[\"canopy_02\\ntype: canopy\\nscs_from: null\\n(uses own position)\"]\n        R1_CFG[\"reference_01\\ntype: reference\\nscs_from:\\n  - canopy_01\\n  - canopy_02\"]\n    end\n\n    subgraph EXPANSION[\"SCS Processing Expansion\"]\n        C1_PROC[\"Process canopy_01\\nposition: canopy_01\"]\n        C2_PROC[\"Process canopy_02\\nposition: canopy_02\"]\n        R1_C1[\"Process reference_01\\nposition: canopy_01\\n(satellite geometry\\nrelative to canopy_01)\"]\n        R1_C2[\"Process reference_01\\nposition: canopy_02\\n(satellite geometry\\nrelative to canopy_02)\"]\n    end\n\n    subgraph STORE_GROUPS[\"RINEX Store Groups\"]\n        SG1[\"canopy_01\"]\n        SG2[\"canopy_02\"]\n        SG3[\"reference_01_canopy_01\"]\n        SG4[\"reference_01_canopy_02\"]\n    end\n\n    subgraph VOD_PAIRS[\"VOD Analysis Pairs\"]\n        VP1[\"canopy_01\\nvs\\nreference_01_canopy_01\"]\n        VP2[\"canopy_02\\nvs\\nreference_01_canopy_02\"]\n    end\n\n    subgraph VOD_OUT[\"VOD Output\"]\n        V1[\"VOD (canopy_01)\\nSNR attenuation by\\ncanopy at position 1\"]\n        V2[\"VOD (canopy_02)\\nSNR attenuation by\\ncanopy at position 2\"]\n    end\n\n    C1 --&gt; C1_CFG\n    C2 --&gt; C2_CFG\n    R1 --&gt; R1_CFG\n\n    C1_CFG --&gt; C1_PROC\n    C2_CFG --&gt; C2_PROC\n    R1_CFG --&gt; R1_C1\n    R1_CFG --&gt; R1_C2\n\n    C1_PROC --&gt; SG1\n    C2_PROC --&gt; SG2\n    R1_C1 --&gt; SG3\n    R1_C2 --&gt; SG4\n\n    SG1 --&gt; VP1\n    SG3 --&gt; VP1\n    SG2 --&gt; VP2\n    SG4 --&gt; VP2\n\n    VP1 --&gt; V1\n    VP2 --&gt; V2</code></pre>","path":["Packages","canvod-vod"],"tags":[]},{"location":"packages/vod/overview/#usage","level":2,"title":"Usage","text":"","path":["Packages","canvod-vod"],"tags":[]},{"location":"packages/vod/overview/#direct-instantiation","level":3,"title":"Direct instantiation","text":"<pre><code>from canvod.vod import TauOmegaZerothOrder\n\ncalculator = TauOmegaZerothOrder(canopy_ds=canopy_ds, sky_ds=sky_ds)\nvod_result = calculator.calculate_vod()\n</code></pre>","path":["Packages","canvod-vod"],"tags":[]},{"location":"packages/vod/overview/#from-aligned-datasets","level":3,"title":"From aligned datasets","text":"<pre><code>from canvod.vod import TauOmegaZerothOrder\n\nvod_result = TauOmegaZerothOrder.from_datasets(\n    canopy_ds=canopy_ds,\n    sky_ds=sky_ds,\n    align=True,\n)\n</code></pre>","path":["Packages","canvod-vod"],"tags":[]},{"location":"packages/vod/overview/#from-icechunk-store","level":3,"title":"From Icechunk store","text":"<pre><code>from canvod.vod import TauOmegaZerothOrder\n\nvod_result = TauOmegaZerothOrder.from_icechunkstore(\n    icechunk_store_pth=\"path/to/store\",\n    canopy_group=\"canopy_01\",\n    sky_group=\"reference_01\",\n)\n</code></pre> <p>The calculator requires:</p> <ul> <li>Canopy dataset (<code>canopy_ds</code>): RINEX observations from a receiver beneath vegetation</li> <li>Sky dataset (<code>sky_ds</code>): RINEX observations from a nearby open-sky receiver</li> <li>Both datasets must contain an <code>SNR</code> data variable</li> <li>Both datasets should be augmented with spherical coordinates (from canvod-auxiliary) and assigned to grid cells (from canvod-grids)</li> </ul>","path":["Packages","canvod-vod"],"tags":[]},{"location":"packages/vod/overview/#output","level":2,"title":"Output","text":"<p><code>calculate_vod()</code> returns an <code>xr.Dataset</code> containing:</p> <ul> <li><code>VOD</code> — vegetation optical depth values</li> <li><code>phi</code> — azimuth angles (from canopy dataset)</li> <li><code>theta</code> — polar angles from zenith (from canopy dataset)</li> </ul>","path":["Packages","canvod-vod"],"tags":[]},{"location":"packages/vod/overview/#references","level":2,"title":"References","text":"<p>Humphrey, V. and Frankenberg, C. (2022). GNSS-transmissometry: A new approach for vegetation optical depth estimation. Remote Sensing of Environment.</p>","path":["Packages","canvod-vod"],"tags":[]}]}