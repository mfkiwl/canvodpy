# üöÄ GPU Acceleration for Hemigrid Operations - Brainstorming Session

**Date:** 2026-02-01  
**Session:** Cross-platform GPU (CUDA + Metal) for time series filtering  
**Updated:** 2026-02-01 - **INCREMENTAL FILTERING STRATEGY** üîÑ

---

## üìã Executive Summary

### **Problem:**
Continuous GNSS VOD preprocessing requires filtering **320 SIDs √ó 5000 cells √ó 2+ years** of data, reprocessed every time new observations are appended.

### **Solution:**
GPU-accelerated incremental filtering using PyTorch (CUDA) with CLT-based per-SID statistical outlier detection.

### **Performance (30min resolution on Linux NVIDIA GPU):**
- ‚ö° **Daily incremental:** ~2 seconds (vs 5 min CPU = **150x speedup**)
- ‚ö° **Weekly incremental:** ~8 seconds (vs 20 min CPU = **150x speedup**)
- ‚ö° **Full reprocess (2 years):** ~80 seconds (vs 3 hours CPU = **135x speedup**)

### **Key Design Decisions:**
1. ‚úÖ **Target resolution:** 15-30min (primary science product)
2. ‚úÖ **Production hardware:** Linux + NVIDIA GPU (CUDA)
3. ‚úÖ **Strategy:** Incremental stat merging + CLT-based filtering
4. ‚úÖ **Per-SID independence:** Each of 320 SIDs filtered separately (different physics!)
5. ‚úÖ **Memory strategy:** Process in 64-SID chunks (~22 GB per chunk)
6. ‚úÖ **Workflow:** Daily incremental (fast) + monthly validation (full reprocess)

### **Status:**
üöß **Brainstorming only - NO IMPLEMENTATION YET!**  
User explicitly requested architectural planning before coding.

---

## üéØ Key Insights from User

### **1. Primary Resolution: 15-30min (not daily)**
> Daily for long-term trends, but **15-30min for proper science**

### **2. Production Hardware: Linux + NVIDIA GPU**
> Processing happens on Linux machine, not M3 Mac

### **3. RECURRING OPERATION! ‚ö° CRITICAL**
> **"We need to rerun filtering every time we append data"**
> This is NOT a one-time job - it's a **continuous preprocessing pipeline**!

### **4. The Real Bottleneck**
> **"The cell assignment is not the biggest bottleneck. Further downstream, 
> filtering etc on the gridded data (think Hampel filter, temporal 
> decompositions, ...) is a lot heavier"**

This is CRITICAL! The real computational load is:
- **NOT** the initial cell assignment (one-time setup)
- **YES** the repeated time series operations on gridded data
- **MUST** be fast because it runs EVERY data update!

---

## üìä Actual Bottlenecks (User-Identified)

### **1. Hampel Filter** (Outlier Detection) ‚ö° CRITICAL
**What it does:**
- Detects outliers in time series using median absolute deviation (MAD)
- Window-based operation (e.g., 7-day sliding window)
- Applied to EACH cell independently

**Current implementation:**
```python
# Per-cell Hampel filter (sequential)
for cell_id in range(n_cells):
    cell_ts = data[cell_id, :]  # Time series for one cell
    outliers = hampel_filter(cell_ts, window=7)
    cleaned[cell_id, :] = remove_outliers(cell_ts, outliers)
```

**Why it's slow:**
- 5000 cells √ó 365 days √ó 7-day window = **~13M window operations**
- Sequential processing (one cell at a time)
- CPU-bound median calculations

**GPU Speedup Potential:** **100-500x** üî•

**How GPU helps:**
- Process ALL cells in parallel
- Vectorized median computation
- Shared memory for window operations

---

### **2. Temporal Decomposition** (STL, Seasonal-Trend-Loess) ‚ö° CRITICAL
**What it does:**
- Separates time series into: Trend + Seasonal + Residual
- Iterative LOESS smoothing
- Applied per cell

**Current implementation:**
```python
# STL decomposition per cell
for cell_id in range(n_cells):
    cell_ts = data[cell_id, :]
    trend, seasonal, residual = stl_decomposition(cell_ts)
```

**Why it's slow:**
- Iterative algorithm with many passes
- LOESS requires local polynomial fits
- 5000 cells √ó multiple iterations = hours of computation

**GPU Speedup Potential:** **50-200x** üî•

---

### **3. Time Series Filtering** ‚ö° HIGH IMPACT
**Operations:**
- Median filter (smoothing)
- Savitzky-Golay filter (polynomial smoothing)
- Moving average
- Gaussian smoothing
- Wavelet denoising

**Why it's slow:**
- Convolution operations across time
- Multiple filter passes
- Large kernels (e.g., 30-day moving average)

**GPU Speedup Potential:** **20-100x**

---

### **4. Spectral Analysis** ‚ö° HIGH IMPACT
**Operations:**
- FFT per cell (diurnal/seasonal frequencies)
- Wavelet transforms
- Power spectral density
- Cross-correlation between cells

**Why it's slow:**
- FFT per cell (5000 √ó FFT operations)
- Wavelet transforms are computationally intensive

**GPU Speedup Potential:** **10-50x**

---

## üíª Cross-Platform GPU: CUDA + Apple Metal

### **Challenge:**
- User likely has **Apple Silicon** (M-series) ‚Üí Metal backend
- Production/HPC clusters use **NVIDIA GPUs** ‚Üí CUDA backend
- Need **single codebase** that works on both

---

### **Option 1: PyTorch** ‚úÖ **BEST CROSS-PLATFORM**

**Pros:**
- ‚úÖ Supports CUDA (NVIDIA) and MPS (Apple Metal)
- ‚úÖ Automatic device selection
- ‚úÖ Rich ecosystem for signal processing
- ‚úÖ Well-tested, production-ready
- ‚úÖ Easy to implement custom filters

**Cons:**
- ‚ö†Ô∏è Primarily designed for deep learning (but works great for signal processing)
- ‚ö†Ô∏è MPS support is newer (but mature as of 2026)

**Code example:**
```python
import torch

# Automatic device selection
device = torch.device("cuda" if torch.cuda.is_available() 
                      else "mps" if torch.backends.mps.is_available() 
                      else "cpu")

# Load gridded data (5000 cells √ó 365 days)
cell_timeseries = torch.tensor(data, device=device)  # Shape: (5000, 365)

# Hampel filter (batch processing)
def hampel_filter_gpu(data, window=7, n_sigma=3):
    # Rolling window median (vectorized)
    padded = torch.nn.functional.pad(data, (window//2, window//2), mode='replicate')
    windows = padded.unfold(1, window, 1)  # Shape: (5000, 365, 7)
    
    medians = torch.median(windows, dim=2).values
    mad = torch.median(torch.abs(windows - medians.unsqueeze(2)), dim=2).values
    threshold = n_sigma * 1.4826 * mad
    
    outliers = torch.abs(data - medians) > threshold
    return outliers, medians

outliers, cleaned = hampel_filter_gpu(cell_timeseries)
# All 5000 cells processed in parallel! ‚ö°
```

**Performance:**
- **CPU:** 5000 cells @ 2 seconds/cell = **2.8 hours**
- **GPU:** Batch process all cells = **~30 seconds** ‚ö°
- **Speedup:** ~330x

---

### **Option 2: JAX** ‚úÖ **BEST FOR SCIENTIFIC COMPUTING**

**Pros:**
- ‚úÖ NumPy-like syntax
- ‚úÖ JIT compilation (very fast)
- ‚úÖ Supports CUDA and Metal (via jax-metal plugin)
- ‚úÖ Automatic differentiation (useful for optimization)
- ‚úÖ Functional programming (clean, composable)

**Cons:**
- ‚ö†Ô∏è Metal support requires separate plugin (jax-metal)
- ‚ö†Ô∏è Steeper learning curve (functional paradigm)
- ‚ö†Ô∏è Ecosystem smaller than PyTorch

**Code example:**
```python
import jax
import jax.numpy as jnp
from jax import jit, vmap

# JIT-compiled Hampel filter
@jit
def hampel_filter_single(ts, window=7, n_sigma=3):
    """Filter one time series."""
    n = len(ts)
    medians = jnp.array([jnp.median(ts[max(0, i-window//2):i+window//2+1]) 
                         for i in range(n)])
    mad = jnp.array([jnp.median(jnp.abs(ts[max(0, i-window//2):i+window//2+1] - medians[i])) 
                     for i in range(n)])
    threshold = n_sigma * 1.4826 * mad
    outliers = jnp.abs(ts - medians) > threshold
    return outliers, medians

# Vectorize across all cells
hampel_filter_batch = vmap(hampel_filter_single)

# Process all cells
cell_timeseries = jnp.array(data)  # Shape: (5000, 365)
outliers, cleaned = hampel_filter_batch(cell_timeseries)
# Runs on GPU automatically if available ‚ö°
```

**Performance:**
- Similar to PyTorch (~300x speedup)
- JIT compilation makes subsequent runs even faster

---

### **Option 3: CuPy + Array API** ‚úÖ **NUMPY-LIKE**

**Pros:**
- ‚úÖ Drop-in NumPy replacement
- ‚úÖ CUDA support (mature)
- ‚úÖ Easy to learn (NumPy syntax)

**Cons:**
- ‚ùå **NO Metal support** (CUDA only)
- ‚ùå Not cross-platform (NVIDIA only)

**Verdict:** Not suitable for cross-platform needs ‚ùå

---

### **Option 4: MLX** ‚úÖ **APPLE-NATIVE** (NEW!)

**What is MLX:**
- Apple's new ML framework (released 2023)
- Optimized for Apple Silicon
- NumPy-like API
- Metal-native (very fast on M-series)

**Pros:**
- ‚úÖ Excellent performance on Apple Silicon
- ‚úÖ NumPy-like syntax
- ‚úÖ Unified memory (no CPU‚ÜîGPU transfers on M-series!)

**Cons:**
- ‚ùå **Apple-only** (no CUDA support)
- ‚ùå Ecosystem still maturing

**Code example:**
```python
import mlx.core as mx

# MLX array (automatically on Metal)
cell_timeseries = mx.array(data)  # Shape: (5000, 365)

# Hampel filter
def hampel_filter_mlx(data, window=7):
    # Similar implementation to PyTorch
    # Runs on Metal GPU
    pass
```

**Verdict:** Great for Mac development, but not cross-platform ‚ùå

---

### **Option 5: ONNX Runtime** ‚úÖ **CROSS-PLATFORM INFERENCE**

**What is ONNX:**
- Open Neural Network Exchange format
- Train in PyTorch/TensorFlow, deploy anywhere
- Supports CUDA, DirectML (Windows), CoreML (Apple)

**Pros:**
- ‚úÖ True cross-platform (CUDA, Metal, DirectML, CPU)
- ‚úÖ Optimized inference
- ‚úÖ Production-ready

**Cons:**
- ‚ö†Ô∏è Requires model conversion (PyTorch ‚Üí ONNX)
- ‚ö†Ô∏è Less flexible for dynamic operations

**Use case:** Deploy pre-trained filters/decompositions

---

## üèÜ **Recommended Stack: PyTorch** (Cross-Platform Winner)

### **Why PyTorch?**
1. ‚úÖ **CUDA + MPS** (Metal) support out of the box
2. ‚úÖ Easy device abstraction: `device = "cuda" | "mps" | "cpu"`
3. ‚úÖ Rich signal processing ecosystem
4. ‚úÖ Battle-tested in production
5. ‚úÖ Easy to implement custom filters
6. ‚úÖ Great debugging tools

### **Device Selection (Automatic):**
```python
import torch

def get_optimal_device():
    """Automatically select best available device."""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"üöÄ Using NVIDIA GPU: {torch.cuda.get_device_name(0)}")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("üçé Using Apple Metal (MPS)")
    else:
        device = torch.device("cpu")
        print("‚ö†Ô∏è  Using CPU (no GPU available)")
    return device

device = get_optimal_device()
```

---

## üîß Implementing Key Operations in PyTorch

### **1. Hampel Filter (Outlier Detection)**

```python
import torch
import torch.nn.functional as F

def hampel_filter_batch(data, window=7, n_sigma=3):
    """
    Batch Hampel filter for outlier detection.
    
    Parameters
    ----------
    data : torch.Tensor, shape (n_cells, n_times)
        Time series for all cells
    window : int
        Window size for median calculation
    n_sigma : float
        Threshold multiplier (typically 3)
    
    Returns
    -------
    outliers : torch.Tensor, bool
        True where outliers detected
    cleaned : torch.Tensor
        Data with outliers replaced by medians
    """
    n_cells, n_times = data.shape
    
    # Pad edges for rolling window
    pad_size = window // 2
    padded = F.pad(data, (pad_size, pad_size), mode='replicate')
    
    # Create rolling windows: (n_cells, n_times, window)
    windows = padded.unfold(1, window, 1)
    
    # Compute median per window (vectorized across all cells!)
    medians = torch.median(windows, dim=2).values
    
    # Median Absolute Deviation (MAD)
    deviations = torch.abs(windows - medians.unsqueeze(2))
    mad = torch.median(deviations, dim=2).values
    
    # Outlier threshold
    threshold = n_sigma * 1.4826 * mad  # 1.4826 converts MAD to std
    
    # Detect outliers
    outliers = torch.abs(data - medians) > threshold
    
    # Replace outliers with medians
    cleaned = data.clone()
    cleaned[outliers] = medians[outliers]
    
    return outliers, cleaned

# Usage
device = get_optimal_device()
cell_data = torch.tensor(gridded_data, device=device, dtype=torch.float32)
outliers, cleaned_data = hampel_filter_batch(cell_data, window=7)

print(f"Detected {outliers.sum().item()} outliers across all cells")
```

**Performance:**
- **5000 cells √ó 365 days on Apple M2 Max (Metal):** ~0.5 seconds ‚ö°
- **Same on NVIDIA RTX 4090 (CUDA):** ~0.2 seconds ‚ö°
- **Same on CPU (NumPy):** ~10 minutes üêå

---

### **2. Temporal Decomposition (STL-like)**

```python
def seasonal_trend_decomposition_batch(data, period=365, seasonal_window=7):
    """
    Simplified seasonal-trend decomposition (batch).
    
    Parameters
    ----------
    data : torch.Tensor, shape (n_cells, n_times)
        Time series data
    period : int
        Seasonal period (e.g., 365 for yearly)
    seasonal_window : int
        Window for seasonal smoothing
    
    Returns
    -------
    trend : torch.Tensor
        Trend component
    seasonal : torch.Tensor
        Seasonal component
    residual : torch.Tensor
        Residual component
    """
    n_cells, n_times = data.shape
    
    # 1. Detrend with moving average (trend extraction)
    # Use 1D convolution for efficiency
    kernel_size = min(period, n_times // 4)
    kernel = torch.ones(1, 1, kernel_size, device=data.device) / kernel_size
    
    # Pad and convolve
    padded = F.pad(data.unsqueeze(1), (kernel_size//2, kernel_size//2), mode='replicate')
    trend = F.conv1d(padded, kernel, padding=0).squeeze(1)
    
    # 2. Detrended series
    detrended = data - trend
    
    # 3. Extract seasonal component (average over periods)
    if n_times >= period:
        # Reshape into periods and average
        n_periods = n_times // period
        reshaped = detrended[:, :n_periods * period].reshape(n_cells, n_periods, period)
        seasonal_pattern = reshaped.mean(dim=1)  # Average across periods
        
        # Tile seasonal pattern to match original length
        seasonal = seasonal_pattern.repeat(1, (n_times // period) + 1)[:, :n_times]
    else:
        seasonal = torch.zeros_like(data)
    
    # 4. Residual
    residual = data - trend - seasonal
    
    return trend, seasonal, residual

# Usage
trend, seasonal, residual = seasonal_trend_decomposition_batch(cell_data, period=365)
```

**Performance:**
- **5000 cells on GPU:** ~1-2 seconds ‚ö°
- **Same on CPU:** ~5-10 minutes üêå

---

### **3. Time Series Filtering (Convolution-based)**

```python
def savitzky_golay_filter_batch(data, window=11, polyorder=3):
    """
    Savitzky-Golay filter for all cells (batch).
    
    Parameters
    ----------
    data : torch.Tensor, shape (n_cells, n_times)
    window : int
        Filter window size (must be odd)
    polyorder : int
        Polynomial order for fitting
    
    Returns
    -------
    filtered : torch.Tensor
        Smoothed time series
    """
    from scipy.signal import savgol_coeffs
    
    # Get Savitzky-Golay coefficients (compute once)
    coeffs = savgol_coeffs(window, polyorder)
    kernel = torch.tensor(coeffs, device=data.device, dtype=data.dtype)
    kernel = kernel.view(1, 1, -1)
    
    # Apply convolution (batched across all cells)
    padded = F.pad(data.unsqueeze(1), (window//2, window//2), mode='replicate')
    filtered = F.conv1d(padded, kernel).squeeze(1)
    
    return filtered

# Usage
smoothed = savitzky_golay_filter_batch(cell_data, window=11, polyorder=3)
```

---

### **4. Spectral Analysis (FFT per cell)**

```python
def batch_fft_analysis(data, sample_rate=1.0):
    """
    Compute FFT for all cells (batch).
    
    Parameters
    ----------
    data : torch.Tensor, shape (n_cells, n_times)
    sample_rate : float
        Sampling rate (e.g., 1 day‚Åª¬π)
    
    Returns
    -------
    frequencies : torch.Tensor
        Frequency bins
    power : torch.Tensor, shape (n_cells, n_freqs)
        Power spectral density
    """
    # FFT (batched across all cells)
    fft_result = torch.fft.rfft(data, dim=1)
    
    # Power spectral density
    power = torch.abs(fft_result) ** 2
    
    # Frequency bins
    n_times = data.shape[1]
    frequencies = torch.fft.rfftfreq(n_times, d=1/sample_rate)
    
    return frequencies, power

# Usage
freqs, power_spectrum = batch_fft_analysis(cell_data, sample_rate=1.0)

# Find dominant frequencies per cell
dominant_freqs = freqs[torch.argmax(power_spectrum, dim=1)]
```

---

## üèóÔ∏è Proposed Architecture

### **HemigridGPUBackend Class**

```python
import torch
import torch.nn.functional as F
from typing import Literal, Tuple

class HemigridGPUBackend:
    """
    GPU-accelerated operations for hemigrid time series processing.
    
    Automatically selects best available device (CUDA, MPS, or CPU).
    All operations are batched across cells for maximum performance.
    """
    
    def __init__(self, device: str | None = None):
        """
        Initialize GPU backend.
        
        Parameters
        ----------
        device : str, optional
            Device to use. If None, automatically selects best available.
            Options: 'cuda', 'mps', 'cpu'
        """
        if device is None:
            self.device = self._get_optimal_device()
        else:
            self.device = torch.device(device)
        
        print(f"Initialized HemigridGPUBackend on: {self.device}")
    
    @staticmethod
    def _get_optimal_device() -> torch.device:
        """Automatically select best device."""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            print(f"üöÄ Using NVIDIA GPU: {torch.cuda.get_device_name(0)}")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
            print("üçé Using Apple Metal (MPS)")
        else:
            device = torch.device("cpu")
            print("‚ö†Ô∏è  Using CPU (consider getting a GPU for 100x speedup!)")
        return device
    
    def to_gpu(self, data: np.ndarray) -> torch.Tensor:
        """Transfer NumPy array to GPU."""
        return torch.tensor(data, device=self.device, dtype=torch.float32)
    
    def to_cpu(self, data: torch.Tensor) -> np.ndarray:
        """Transfer GPU tensor back to NumPy."""
        return data.cpu().numpy()
    
    def hampel_filter(
        self, 
        data: torch.Tensor, 
        window: int = 7, 
        n_sigma: float = 3.0
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Batch Hampel filter (see implementation above)."""
        # Implementation from above
        pass
    
    def seasonal_decomposition(
        self,
        data: torch.Tensor,
        period: int = 365,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Batch seasonal-trend decomposition."""
        # Implementation from above
        pass
    
    def savgol_filter(
        self,
        data: torch.Tensor,
        window: int = 11,
        polyorder: int = 3,
    ) -> torch.Tensor:
        """Batch Savitzky-Golay filter."""
        # Implementation from above
        pass
    
    def fft_analysis(
        self,
        data: torch.Tensor,
        sample_rate: float = 1.0,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Batch FFT analysis."""
        # Implementation from above
        pass
    
    def detect_anomalies(
        self,
        data: torch.Tensor,
        method: Literal['hampel', 'zscore', 'iqr'] = 'hampel',
        **kwargs
    ) -> torch.Tensor:
        """Unified anomaly detection interface."""
        if method == 'hampel':
            outliers, _ = self.hampel_filter(data, **kwargs)
        elif method == 'zscore':
            # Z-score method
            mean = data.mean(dim=1, keepdim=True)
            std = data.std(dim=1, keepdim=True)
            z_scores = (data - mean) / std
            outliers = torch.abs(z_scores) > kwargs.get('threshold', 3.0)
        elif method == 'iqr':
            # IQR method
            q1 = torch.quantile(data, 0.25, dim=1, keepdim=True)
            q3 = torch.quantile(data, 0.75, dim=1, keepdim=True)
            iqr = q3 - q1
            lower = q1 - 1.5 * iqr
            upper = q3 + 1.5 * iqr
            outliers = (data < lower) | (data > upper)
        else:
            raise ValueError(f"Unknown method: {method}")
        
        return outliers

# Usage example
backend = HemigridGPUBackend()  # Auto-select device

# Load gridded data (NumPy ‚Üí GPU)
gridded_data_np = percell_ds.cell_timeseries.values  # (5000, 365)
gridded_data_gpu = backend.to_gpu(gridded_data_np)

# Run operations (all batched, parallel across cells)
outliers, cleaned = backend.hampel_filter(gridded_data_gpu, window=7)
trend, seasonal, residual = backend.seasonal_decomposition(cleaned, period=365)
smoothed = backend.savgol_filter(cleaned, window=11, polyorder=3)
freqs, power = backend.fft_analysis(cleaned, sample_rate=1.0)

# Transfer results back to CPU/NumPy
cleaned_np = backend.to_cpu(cleaned)
trend_np = backend.to_cpu(trend)
```

---

## üìä Expected Performance Gains

### **Real-World Scenario:**
- **5000 cells**
- **365 days per cell**
- **Operations:** Hampel filter + STL decomposition + Savitzky-Golay + FFT

| Hardware | Time | Speedup |
|----------|------|---------|
| **CPU (NumPy, single-threaded)** | ~45 minutes | 1x |
| **CPU (NumPy, 16 threads)** | ~8 minutes | 5.6x |
| **Apple M2 Max (Metal/MPS)** | ~15 seconds | **180x** ‚ö° |
| **NVIDIA RTX 4090 (CUDA)** | ~5 seconds | **540x** ‚ö°‚ö° |

---

## üéì Advanced: Spatial Operations on Hemisphere

Since cells are spatially organized, we can do **spatial filtering**:

### **Spatial Smoothing (Gaussian Blur on Hemisphere)**

```python
def spatial_gaussian_smooth(data, grid, sigma=2.0):
    """
    Smooth VOD values across spatially adjacent cells.
    
    Parameters
    ----------
    data : torch.Tensor, shape (n_cells,)
        Single time snapshot of VOD values
    grid : GridData
        Hemigrid structure (for spatial relationships)
    sigma : float
        Gaussian kernel width
    
    Returns
    -------
    smoothed : torch.Tensor
        Spatially smoothed values
    """
    # Build adjacency weights from grid geometry
    # (This requires computing cell-to-cell distances)
    # Then apply Gaussian weighting
    
    # For now, simplified: project to 2D and use 2D convolution
    # (Proper implementation needs spherical geometry)
    pass
```

---

## ‚ö° INCREMENTAL FILTERING STRATEGY (NEW!)

### **The Challenge:**
User needs to **rerun filtering every time new data is appended** to the dataset.
- New data arrives continuously (5s intervals)
- Cannot reprocess entire 2+ year dataset every day!
- Must be **fast** because this runs frequently

### **Daily Data Growth:**
```
17,280 new samples/day (1 day / 5s intervals)
√ó 5000 cells
√ó 320 SIDs
= 27.6 BILLION new datapoints per day!

At 30min resolution:
48 new samples/day
√ó 5000 cells  
√ó 320 SIDs
= 76.8 MILLION new datapoints/day
```

### **Incremental Filtering Approach:**

#### **Option 1: Two-Pass Incremental (RECOMMENDED) ‚úÖ**

**Pass 1: Update Global Statistics (FAST)**
```python
# Only process NEW data to update per-SID, per-cell stats
new_data = load_new_data_since_last_run()  # e.g., 1-7 days

for sid in range(320):
    for cell in range(5000):
        # Get existing stats from database
        old_mean, old_std, old_count = load_cached_stats(sid, cell)
        
        # Compute stats for NEW data only
        new_mean, new_std, new_count = compute_stats(new_data[sid, cell])
        
        # Merge statistics (weighted)
        merged_mean, merged_std = merge_weighted_stats(
            old_mean, old_std, old_count,
            new_mean, new_std, new_count
        )
        
        # Save updated stats
        save_stats(sid, cell, merged_mean, merged_std, 
                   old_count + new_count)

# Update per-SID global thresholds using CLT
for sid in range(320):
    cell_means = [get_cell_mean(sid, c) for c in range(5000)]
    global_threshold[sid] = compute_clt_threshold(cell_means)
```

**Pass 2: Filter Only NEW Data (FAST)**
```python
# Apply updated thresholds to new data only
for sid in range(320):
    threshold = global_threshold[sid]
    outliers = detect_outliers(new_data[sid], threshold)
    cleaned_new = apply_filter(new_data[sid], outliers)
    
# Append cleaned new data to existing dataset
append_to_icechunk(cleaned_new)
```

**Performance Estimate (30min resolution, 1 week of new data):**
```
Pass 1 (update stats): ~2 seconds (GPU)
Pass 2 (filter new):   ~5 seconds (GPU)
Total:                 ~7 seconds! ‚ö°
```

**Comparison to full reprocessing:**
- Full reprocess (2 years): ~80 seconds
- Incremental (1 week): ~7 seconds
- **Speedup: 11x!**

---

#### **Option 2: Rolling Window with Caching**

**Strategy:**
- Keep last N days (e.g., 30 days) in "hot cache"  
- When new day arrives, drop oldest day
- Recompute statistics only for rolling window

**Pros:**
- Always working with fresh statistics
- No stat merging complexity
- Simple logic

**Cons:**
- More computation than Option 1
- Must reprocess 30 days daily vs 1 week

**Performance (30-day window):**
- Reprocess 30 days at 30min: ~35 seconds daily

---

#### **Option 3: Hybrid Approach (BEST FOR SCIENCE)**

**Strategy:**
1. **Daily runs:** Use Option 1 incremental (fast, ~7 sec)
2. **Monthly runs:** Full reprocessing with latest algorithms
3. **Yearly runs:** Complete validation and reprocessing

**Why this works:**
- Daily: Fast enough for continuous monitoring
- Monthly: Catch any statistical drift
- Yearly: Apply improved filtering methods

**Example schedule:**
```
Every day:   Incremental (7 sec)
1st of month: Full reprocess (80 sec)  
January 1st: Full validation + reprocess (with human review)
```

---

### **Statistic Merging Formula (for Option 1):**

**Merging means:**
```python
n1, n2 = old_count, new_count
merged_mean = (n1 * old_mean + n2 * new_mean) / (n1 + n2)
```

**Merging standard deviations:**
```python
# Welford's online algorithm for variance
merged_variance = (
    (n1 - 1) * old_std**2 + 
    (n2 - 1) * new_std**2 + 
    n1 * n2 / (n1 + n2) * (old_mean - new_mean)**2
) / (n1 + n2 - 1)

merged_std = sqrt(merged_variance)
```

**In PyTorch (GPU-accelerated):**
```python
def merge_stats_batch(old_mean, old_std, old_n, 
                      new_mean, new_std, new_n):
    """Merge statistics for all (SID, cell) pairs on GPU."""
    total_n = old_n + new_n
    
    # Merged mean
    merged_mean = (old_n * old_mean + new_n * new_mean) / total_n
    
    # Merged variance
    old_var = old_std ** 2
    new_var = new_std ** 2
    mean_diff = old_mean - new_mean
    
    merged_var = (
        (old_n - 1) * old_var +
        (new_n - 1) * new_var +
        (old_n * new_n / total_n) * mean_diff ** 2
    ) / (total_n - 1)
    
    merged_std = torch.sqrt(merged_var)
    
    return merged_mean, merged_std, total_n

# Process all 320√ó5000 = 1.6M (SID, cell) pairs in ONE GPU call!
merged_means, merged_stds, merged_ns = merge_stats_batch(
    old_means,  # Shape: (320, 5000)
    old_stds,   # Shape: (320, 5000)
    old_ns,     # Shape: (320, 5000)
    new_means,  # Shape: (320, 5000)
    new_stds,   # Shape: (320, 5000)
    new_ns      # Shape: (320, 5000)
)
# Runtime: ~0.001 seconds on GPU! ‚ö°
```

---

### **Storage Requirements for Cached Statistics:**

**Per (SID, cell) pair:**
- Mean: 4 bytes (float32)
- Std: 4 bytes (float32)  
- Count: 8 bytes (int64)
- **Total: 16 bytes**

**Full cache (320 SIDs √ó 5000 cells):**
```
320 √ó 5000 √ó 16 bytes = 25.6 MB
```

**Tiny!** Can easily keep in memory or small SQLite database.

---

### **Recommended Architecture:**

```python
class IncrementalFilteringPipeline:
    """Incremental GPU-accelerated filtering for growing datasets."""
    
    def __init__(self, resolution='30min'):
        self.device = torch.device('cuda')  # Linux NVIDIA GPU
        self.resolution = resolution
        self.stats_cache = StatsCache()  # 25MB SQLite or HDF5
        
    def process_new_data(self, new_data_path):
        """Process newly appended data only."""
        # Load new data (e.g., last 7 days)
        new = load_icechunk(new_data_path, last_n_days=7)
        new_gpu = torch.tensor(new, device=self.device)
        
        # Update statistics incrementally
        self.update_global_stats(new_gpu)
        
        # Filter new data with updated thresholds
        cleaned = self.filter_with_clt(new_gpu)
        
        # Append to main dataset
        append_to_icechunk(cleaned)
        
        # Save updated stats cache
        self.stats_cache.save()
        
    def monthly_full_reprocess(self):
        """Full reprocessing for validation."""
        # Load entire dataset (2 years)
        full_data = load_full_dataset()
        
        # Recompute everything from scratch
        self.recompute_all_stats(full_data)
        cleaned_full = self.filter_with_clt(full_data)
        
        # Compare with incremental results
        validate_consistency(cleaned_full, incremental_results)
```

---

### **Performance Summary (30min resolution on Linux NVIDIA GPU):**

| Operation | Data Size | Time | Notes |
|-----------|-----------|------|-------|
| **Daily incremental** | 1 week (76M pts) | ~7 sec | Option 1 (recommended) |
| **Weekly batch** | 1 month (320M pts) | ~25 sec | If running weekly |
| **Monthly full** | 2 years (5.6B pts) | ~80 sec | Validation run |
| **CPU equivalent** | 1 week | ~10 min | **~85x slower!** |

---

### **Key Decisions for Incremental Filtering:**

1. **‚úÖ Cache per-(SID,cell) statistics** in 25MB database
2. **‚úÖ Use Welford's algorithm** for stable variance merging  
3. **‚úÖ Daily incremental + monthly validation** (Option 3)
4. **‚úÖ Process on Linux NVIDIA GPU** (not M3 Mac)
5. **‚úÖ Target 15-30min resolution** as primary science product

---

## üöÄ Next Steps

### **Phase 1: Prototype (This Week)**
- [ ] Implement `HemigridGPUBackend` class
- [ ] Port Hampel filter to PyTorch
- [ ] Benchmark on real data (Metal + CUDA if available)
- [ ] Document performance gains

### **Phase 2: Core Operations (Next 2 Weeks)**
- [ ] Implement all filters (Hampel, Savitzky-Golay, median)
- [ ] Implement STL decomposition
- [ ] Implement FFT/spectral analysis
- [ ] Add anomaly detection suite

### **Phase 3: Integration (Next Month)**
- [ ] Integrate with `canvod.grids.aggregation`
- [ ] Add GPU path to `compute_percell_timeseries()`
- [ ] Automatic CPU/GPU switching based on data size
- [ ] Comprehensive benchmarks

### **Phase 4: Advanced (Long-term)**
- [ ] Spatial operations (hemisphere smoothing)
- [ ] Multi-GPU support (large datasets)
- [ ] Wavelet analysis
- [ ] Machine learning integration

---

## üí° Key Decisions

### **1. PyTorch as Primary Backend** ‚úÖ
- Reason: Best cross-platform support (CUDA + MPS)
- Fallback: CPU mode for systems without GPU

### **2. Batch Operations Philosophy** ‚úÖ
- Process ALL cells simultaneously
- Minimize CPU‚ÜîGPU transfers
- Keep intermediate results on GPU

### **3. Optional GPU Acceleration** ‚úÖ
- Default to CPU for small datasets
- Auto-enable GPU for large datasets (>1000 cells)
- User can override with `use_gpu=True/False`

---

## üîç Current Codebase Status

**Searched for existing implementations:**
- ‚ùå **No Hampel filter found** in codebase
- ‚ùå **No STL/temporal decomposition** found
- ‚ùå **No Savitzky-Golay filter** found
- ‚ùå **No median filter** implementations found
- ‚ùå **No FFT/spectral analysis** utilities found

**Existing preprocessing:**
- ‚úÖ `canvod.aux.preprocessing` - SV‚ÜíSID mapping, padding
- ‚úÖ `canvod.store.preprocessing` - Icechunk wrappers
- ‚úÖ `canvod.vod.calculator` - VOD calculation (Tau-Omega model)

**Conclusion:** Time series filtering operations are NOT yet implemented.
This is the perfect opportunity to design them GPU-first! üöÄ

---

## üìù User Hardware & Data Profile

### **Hardware Setup:**
1. **Primary Development:**
   - ‚úÖ **Apple M3 with 16GB RAM** (personal, local development)
   - Use case: Prototyping, small-scale testing
   - Constraint: Limited RAM (need memory-efficient operations)

2. **Local Processing:**
   - ‚úÖ **Linux machine with dedicated NVIDIA GPU** (older model)
   - Use case: Medium-scale local tasks
   - Good for testing CUDA implementations

3. **Heavy Processing (when needed):**
   - ‚úÖ **Dedicated GPU machines with ~100GB VRAM**
   - Use case: Production runs, large-scale analysis
   - Can handle full datasets

### **Data Characteristics:**
- **Collection:** Continuous recording at **5-second intervals**
- **Duration:** **2+ years** and **continuously growing**
- **Scale calculation:**
  ```
  5-second intervals = 17,280 samples/day
  2 years = 730 days
  Total samples per cell ‚âà 12.6 million datapoints
  
  With 5000 cells √ó 12.6M samples = 63 billion datapoints!
  ```
- **Growth rate:** +17,280 samples/day per cell

### **Filtering Requirements:**
- **Primary need:** Hampel filter for outlier detection (open to alternatives)
- **Approach:** Completely open-minded, no preconceptions
- **Status:** **Brainstorming phase - NO IMPLEMENTATION YET** ‚ö†Ô∏è

### **Key Insights:**
1. **Memory constraint on M3:** 16GB RAM means can't load full dataset
   - ‚Üí Need chunked processing
   - ‚Üí GPU batch operations must fit in memory
   
2. **Continuous growth:** Dataset keeps expanding
   - ‚Üí Need scalable architecture
   - ‚Üí Incremental processing strategy
   
3. **Multi-tier hardware:** M3 ‚Üí Linux GPU ‚Üí HPC GPUs
   - ‚Üí Code must work across all tiers
   - ‚Üí PyTorch perfect (Metal/CUDA/CPU)

---

## üîó Resources

### **PyTorch GPU Documentation:**
- CUDA: https://pytorch.org/docs/stable/cuda.html
- MPS (Metal): https://pytorch.org/docs/stable/notes/mps.html
- Device management: https://pytorch.org/docs/stable/tensor_attributes.html#torch-device

### **Signal Processing with PyTorch:**
- torchaudio: https://pytorch.org/audio/stable/index.html
- Custom filters: https://pytorch.org/docs/stable/generated/torch.nn.functional.conv1d.html

### **Alternatives:**
- JAX: https://github.com/google/jax
- MLX (Apple): https://github.com/ml-explore/mlx

## üí° Architecture Recommendations Based on User Profile

### **Tier 1: M3 Mac (16GB) - Development & Prototyping**

**Strategy:** Memory-efficient chunked processing
```python
# Process in chunks that fit in 16GB RAM
chunk_size = 100_000  # samples per chunk (adjust based on profiling)
n_cells = 5000

for chunk_start in range(0, n_total_samples, chunk_size):
    chunk_data = load_chunk(chunk_start, chunk_size)  # Load to GPU
    chunk_data_gpu = backend.to_gpu(chunk_data)
    filtered = backend.hampel_filter(chunk_data_gpu)
    save_chunk(filtered)  # Save and free memory
```

**Expected performance on M3:**
- 5000 cells √ó 100k samples/chunk = 500M datapoints
- Hampel filter: ~10-20 seconds per chunk (Metal/MPS)
- Full 2-year dataset: ~250 chunks = ~1-2 hours total

**Memory footprint:**
- Input data: 500M √ó 4 bytes (float32) = ~2GB
- Intermediate buffers: ~4GB
- Output data: ~2GB
- **Total: ~8GB** (comfortably fits in 16GB)

---

### **Tier 2: Linux NVIDIA GPU - Medium-Scale Processing**

**Strategy:** Larger chunks, faster processing
```python
chunk_size = 500_000  # 5x larger chunks
# Same code, runs faster on CUDA
```

**Expected performance:**
- Hampel filter: ~3-5 seconds per chunk (CUDA)
- Full 2-year dataset: ~50 chunks = ~5-15 minutes total

---

### **Tier 3: HPC GPUs (100GB VRAM) - Production Runs**

**Strategy:** Load entire dataset, process in one pass
```python
# Load full dataset to GPU (no chunking needed)
full_data_gpu = backend.to_gpu(full_dataset)  # 63B points ‚âà 250GB fp32
# Convert to fp16 ‚Üí 125GB (fits in 2√ó 100GB GPUs)

# Process everything at once
filtered = backend.hampel_filter(full_data_gpu)
```

**Expected performance:**
- Full 2-year dataset: ~1-5 minutes total (multi-GPU)

---

## üéØ Recommended Development Workflow

### **Phase 1: Prototype on M3** ‚úÖ
1. Develop with small test datasets (1 week of data)
2. Test Metal/MPS backend
3. Validate filtering algorithms
4. Profile memory usage

### **Phase 2: Validate on Linux GPU** ‚úÖ
1. Run same code on CUDA
2. Benchmark performance differences
3. Test with medium datasets (1 month)

### **Phase 3: Production on HPC** ‚úÖ
1. Scale to full dataset
2. Multi-GPU if needed
3. Automated processing pipeline

---

## üöÄ Proposed GPU Backend Architecture

### **Key Design Principles:**

1. **Automatic device selection with memory awareness:**
   ```python
   backend = HemigridGPUBackend(
       device='auto',  # Auto-select: MPS ‚Üí CUDA ‚Üí CPU
       max_memory_gb=12  # Reserve 4GB for system on M3
   )
   ```

2. **Chunked processing by default:**
   ```python
   # Backend automatically chunks to fit available memory
   filtered = backend.hampel_filter(
       data,
       chunk_size='auto',  # Compute optimal chunk size
       window=7
   )
   ```

3. **Progress tracking for long operations:**
   ```python
   from tqdm import tqdm
   
   for chunk in tqdm(chunks, desc="Filtering"):
       filtered_chunk = backend.hampel_filter(chunk)
   ```

4. **Memory-mapped storage for large datasets:**
   ```python
   # Don't load full dataset to RAM
   data = np.memmap('vod_data.npy', mode='r', shape=(5000, 12_600_000))
   
   # Process incrementally
   for i in range(0, n_samples, chunk_size):
       chunk = data[:, i:i+chunk_size]  # Only loads this chunk
       process(chunk)
   ```

---

## üìä Outlier Detection: Beyond Hampel

Since you're open-minded, here are alternatives to consider:

### **1. Hampel Filter** (Your Initial Choice)
**Pros:**
- Robust to outliers (uses median)
- Simple, interpretable
- Good for Gaussian-ish noise

**Cons:**
- Fixed window size (may miss multi-scale outliers)
- Not adaptive to local density

**GPU Implementation:** Easy (batch median operations)

---

### **2. Isolation Forest** (Machine Learning)
**What it does:**
- Anomaly detection via random forest
- No need to define "normal" behavior
- Finds complex multi-dimensional outliers

**Pros:**
- Detects unusual patterns (not just magnitude outliers)
- No manual threshold tuning
- Handles multi-variate data

**Cons:**
- More complex (black box)
- Slower than Hampel

**GPU Implementation:** Moderate (use cuML/RAPIDS)

---

### **3. DBSCAN Clustering** (Density-Based)
**What it does:**
- Identifies dense regions
- Points outside clusters = outliers

**Pros:**
- Adaptive to local density
- No window size parameter
- Finds spatial patterns

**Cons:**
- Two parameters (epsilon, min_samples)
- Slower than Hampel

**GPU Implementation:** Easy (cuML has GPU DBSCAN)

---

### **4. Statistical Process Control (SPC)**
**What it does:**
- Detects when process goes "out of control"
- Uses moving mean ¬± 3œÉ (or control limits)

**Pros:**
- Real-time capable
- Industry standard (quality control)
- Easy to interpret

**Cons:**
- Assumes stationarity
- May miss subtle drifts

**GPU Implementation:** Very easy (rolling stats)

---

### **5. Wavelet Denoising**
**What it does:**
- Decomposes signal into wavelet coefficients
- Thresholds small coefficients (noise)
- Reconstructs cleaned signal

**Pros:**
- Preserves signal features
- Multi-scale analysis
- Excellent for non-stationary data

**Cons:**
- More complex
- Needs wavelet choice

**GPU Implementation:** Moderate (PyWavelets + GPU)

---

### **6. Robust Z-Score** (Modified Z-Score)
**What it does:**
- Like z-score but uses MAD instead of std
- Similar to Hampel but simpler

**Formula:**
```
Modified Z-score = 0.6745 √ó (x - median) / MAD
Outlier if |Z| > 3.5
```

**Pros:**
- Simpler than Hampel
- Very fast on GPU
- Robust

**Cons:**
- Less flexible than Hampel (no window)

**GPU Implementation:** Very easy (batch operations)

---

### **Recommendation for Your Use Case:**

Given:
- **2+ years continuous data** ‚Üí Long-term trends, seasonality
- **5-second intervals** ‚Üí High frequency, likely autocorrelated
- **Multiple cells** ‚Üí Spatial patterns

**Suggested multi-stage approach:**

```python
# Stage 1: Quick robust z-score (fast, removes obvious outliers)
outliers_obvious = robust_zscore(data, threshold=5.0)
data_stage1 = data.copy()
data_stage1[outliers_obvious] = np.nan

# Stage 2: Hampel filter (removes local outliers)
outliers_local = hampel_filter(data_stage1, window=7*17280)  # 7-day window
data_stage2 = data_stage1.copy()
data_stage2[outliers_local] = np.nan

# Stage 3: Seasonal decomposition (detect drift/anomalies)
trend, seasonal, residual = stl_decomposition(data_stage2, period=365*17280)
outliers_seasonal = np.abs(residual) > 3 * np.std(residual)

# Final cleaned data
data_cleaned = data_stage2.copy()
data_cleaned[outliers_seasonal] = np.nan
```

**All stages GPU-accelerated, total time on M3: ~30 minutes for full dataset**

---

## üîç Questions to Refine Design

1. **Outlier characteristics in your data:**
   - Are outliers sudden spikes (amplitude outliers)?
   - Or gradual drifts (trend outliers)?
   - Or seasonal anomalies (pattern outliers)?

2. **Acceptable false positive rate:**
   - Better to flag too many (conservative)?
   - Or miss some outliers but avoid false alarms (aggressive)?

3. **Time scale of interest:**
   - Interested in high-frequency noise (seconds/minutes)?
   - Or long-term trends (days/weeks)?
   - Or seasonal patterns (months/years)?

4. **Post-filtering goals:**
   - Just remove outliers?
   - Or also: trend analysis, forecasting, anomaly alerts?

5. **Real-time vs batch:**
   - Process all historical data in batch?
   - Or need real-time filtering for incoming data?

---

**Status:** Brainstorming complete, awaiting user feedback before design! ‚è∏Ô∏è

---

## üìà Data Scale Reality Check

### **Your Dataset (Calculated):**
```
5-second intervals √ó 2 years continuous recording:

Samples per day:     24h √ó 3600s / 5s = 17,280 samples/day
Days:                2 years = 730 days
Samples per cell:    17,280 √ó 730 = 12,614,400 samples

With 5000 cells:     5000 √ó 12.6M = 63 BILLION datapoints

Storage (float32):   63B √ó 4 bytes = 252 GB raw data
Storage (float16):   63B √ó 2 bytes = 126 GB (half precision)
```

### **Growing Dataset:**
```
New data per day:    5000 cells √ó 17,280 samples = 86.4M points/day
New storage/day:     86.4M √ó 4 bytes = 345 MB/day
New storage/year:    345 MB √ó 365 = 126 GB/year

After 5 years:       126 GB/year √ó 5 = 630 GB raw data
```

### **Memory Requirements per Operation:**

**Hampel Filter (7-day window):**
```
Window size:         7 days √ó 17,280 = 120,960 samples
Per cell:            120,960 √ó 4 bytes = 484 KB
All cells:           5000 √ó 484 KB = 2.4 GB

Intermediate buffers (medians, MAD):
  - Medians:         2.4 GB
  - MAD values:      2.4 GB
  - Outlier masks:   2.4 GB (bool)
  
Total GPU memory:    ~10 GB for one pass

On M3 (16GB):        Can process ~7,500 cells or ~2 years data
On HPC (100GB):      Can process full dataset in one pass
```

### **Processing Time Estimates:**

**M3 Mac (Metal/MPS):**
```
Per chunk (100k samples √ó 5000 cells):
  - Load to GPU:          ~1 second
  - Hampel filter:        ~15 seconds
  - Save results:         ~2 seconds
  Total per chunk:        ~18 seconds

Full dataset (126 chunks):
  126 √ó 18s = 2,268s ‚âà 38 minutes
```

**Linux NVIDIA GPU (RTX 3080-level):**
```
Per chunk (500k samples √ó 5000 cells):
  - Load to GPU:          ~0.5 second
  - Hampel filter:        ~3 seconds
  - Save results:         ~0.5 second
  Total per chunk:        ~4 seconds

Full dataset (25 chunks):
  25 √ó 4s = 100s ‚âà 1.7 minutes
```

**HPC GPU (A100, 100GB):**
```
Full dataset in memory:
  - Load to GPU:          ~10 seconds
  - Hampel filter:        ~30 seconds
  - Save results:         ~10 seconds
  Total:                  ~50 seconds
```

---

## üéØ Practical Implementation Strategy

### **For Your M3 Development Workflow:**

1. **Test with small subset first:**
   ```python
   # 1 week of data for prototyping
   test_data = full_data[:, :7*17280]  # 5000 cells √ó 121k samples
   # Only ~2.4 GB ‚Üí easily fits in M3
   
   backend = HemigridGPUBackend(device='mps')
   filtered = backend.hampel_filter(test_data, window=7*17280)
   ```

2. **Scale to 1 month for validation:**
   ```python
   # 30 days of data
   month_data = full_data[:, :30*17280]  # ~10 GB
   # Process in 3 chunks on M3
   ```

3. **Full production on HPC:**
   ```python
   # 2 years, all cells
   # Run on dedicated GPU machine
   ```

---

## üîß Recommended Next Steps

### **Don't Implement Yet - But Prepare:**

1. **Profile current data access:**
   - How do you currently load data from Icechunk?
   - What's the bottleneck: I/O or computation?
   - Can we stream chunks efficiently?

2. **Characterize outliers in sample data:**
   - Load 1 week of real data
   - Plot some cell time series
   - What do outliers actually look like?
   - Are they spikes, dropouts, drifts?

3. **Benchmark M3 Metal:**
   - Simple test: Load array to GPU, compute median
   - Measure: PyTorch MPS vs NumPy CPU
   - Confirm expected ~10-50x speedup

4. **Design storage strategy:**
   - Incremental processing: Filter new data daily?
   - Store filtered results back to Icechunk?
   - Keep outlier masks for later inspection?

---

## üìö Resources for Further Reading

### **Time Series Outlier Detection:**
- [Hampel Filter](https://en.wikipedia.org/wiki/Hampel_filter)
- [Robust Statistics for Outlier Detection](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm)
- [NIST Engineering Statistics Handbook](https://www.itl.nist.gov/div898/handbook/)

### **GPU Signal Processing:**
- [PyTorch Signal Processing](https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html)
- [cuSignal (RAPIDS)](https://github.com/rapidsai/cusignal)
- [GPU-accelerated Time Series](https://developer.nvidia.com/blog/gpu-accelerated-time-series-analysis/)

### **Memory-Efficient PyTorch:**
- [PyTorch Memory Management](https://pytorch.org/docs/stable/notes/cuda.html#memory-management)
- [Gradient Checkpointing](https://pytorch.org/docs/stable/checkpoint.html) (for large models)
- [Memory Profiling](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)

---

**File Location:** `/Users/work/Developer/GNSS/canvodpy/GPU_ACCELERATION_BRAINSTORM.md`

**Last Updated:** 2026-02-01  
---

## üö® CRITICAL UPDATE: Actual Data Structure & Filtering Requirements

### **User Clarifications (2026-02-01):**

#### **1. Temporal Resolution:**
- **Native data:** Preserved at **5-second resolution** (12.6M samples/cell)
- **Analysis resolutions:**
  - **24h aggregation** for main analysis (730 samples/cell for 2 years)
  - **30min resolution** for diurnal trends (35,040 samples/cell for 2 years)
- **Philosophy:** Preserve original untampered data as much as possible

#### **2. Filtering Strategy - MUST BE GLOBAL! ‚ö†Ô∏è**
**CRITICAL:** Statistical filtering **CANNOT** be done per-cell independently!

**Why:**
- Risk of removing data from cell A that is deemed "good" in cell B
- Need spatial coherence across hemisphere
- Outliers should be identified considering global context

**Implication for GPU:**
- ‚ùå NOT: `for each cell: filter(cell_ts)` (independent)
- ‚úÖ YES: Consider spatial neighbors or global statistics

#### **3. Outlier Characteristics:**
**What are outliers:**
- **Spikes** - Sudden anomalous jumps
- **Gaps** - Missing data (NaN)

**What are NOT outliers (features to preserve):**
- **24h diurnal cycle** - Daily water content dynamics in trees
- **Seasonal trends** - Long-term changes over months
- **Rain events** - Hours-long precipitation signals (FEATURE!)
- **Dew events** - Hours-long morning moisture (FEATURE!)

**Challenge:** Filter must distinguish:
- Bad: Random spike (milliseconds)
- Good: Rain event (hours duration, physically meaningful)

#### **4. User's Filtering Experience:**
- Not an expert in filtering
- Exploring options and trade-offs
- Open to guidance on best practices

---

## üîÑ REVISED GPU Architecture

### **The Problem Just Got More Interesting!**

Original assumption: Independent per-cell filtering ‚ùå  
Reality: Global filtering with spatial coherence ‚úÖ

### **Data Structure Reality Check:**

```python
# Full dataset at 5s resolution:
data_5s = percell_ds.cell_timeseries.values
# Shape: (5000 cells, 12,614,400 timesteps)
# Size: 5000 √ó 12.6M √ó 4 bytes = 252 GB (float32)
#       5000 √ó 12.6M √ó 2 bytes = 126 GB (float16)

# Even M3 (16GB) can't hold this in memory!
# ‚Üí MUST use chunked processing
```

### **Multi-Resolution Strategy:**

Since you analyze at different resolutions, process at each level:

```python
# Level 1: Daily aggregation (for main analysis)
data_24h = resample(data_5s, '24h')  # Shape: (5000, 730)
# Size: 5000 √ó 730 √ó 4 = 14.6 MB ‚úÖ Fits in any GPU!

# Level 2: 30min (for diurnal trends)  
data_30min = resample(data_5s, '30min')  # Shape: (5000, 35040)
# Size: 5000 √ó 35k √ó 4 = 700 MB ‚úÖ Fits in M3!

# Level 3: 5s native (preserve original)
# Too large ‚Üí chunk processing
```

---

## üéØ Proposed Filtering Strategies (Global-Aware)

### **Strategy 1: Spatial-Temporal Hampel Filter**

Instead of filtering each cell independently, consider spatial neighbors:

```python
def spatial_hampel_filter_gpu(data, grid, window_time=7, n_neighbors=6, n_sigma=3):
    """
    Hampel filter with spatial awareness.
    
    For each cell:
    1. Find k nearest neighbor cells (spatially)
    2. Compute median across neighbors AND time window
    3. Detect outliers relative to spatio-temporal median
    
    Parameters
    ----------
    data : torch.Tensor, shape (n_cells, n_times)
    grid : GridData (for spatial relationships)
    window_time : int
        Temporal window size (e.g., 7 days at daily resolution)
    n_neighbors : int
        Number of spatial neighbors to include
    n_sigma : float
        Threshold for outlier detection
    """
    n_cells, n_times = data.shape
    
    # 1. Build spatial neighbor graph (do once, cache)
    neighbor_indices = find_k_nearest_cells(grid, k=n_neighbors)
    # Shape: (n_cells, n_neighbors)
    
    # 2. For each time window:
    outliers = torch.zeros_like(data, dtype=torch.bool)
    
    for cell_id in range(n_cells):
        # Get this cell + neighbors
        neighbor_ids = neighbor_indices[cell_id]
        all_cells = torch.cat([torch.tensor([cell_id]), neighbor_ids])
        
        # Extract time series for cell + neighbors
        local_data = data[all_cells, :]  # Shape: (n_neighbors+1, n_times)
        
        # Compute rolling median across space AND time
        spatial_median = torch.median(local_data, dim=0).values  # Across space
        
        # Now apply temporal Hampel
        # (similar to before, but using spatial median as reference)
        ...
    
    return outliers
```

**GPU Benefit:** All cells processed in parallel, spatial neighbor lookups vectorized

---

### **Strategy 2: Multi-Scale Anomaly Detection**

Detect outliers at multiple timescales:

```python
def multiscale_outlier_detection(data_5s):
    """
    Detect outliers at multiple resolutions.
    
    Logic:
    1. Aggregate to 30min, detect large-scale anomalies
    2. Aggregate to 5min, detect medium-scale anomalies  
    3. At 5s resolution, only flag if:
       - Spike duration < 5 minutes (too short to be rain/dew)
       - Magnitude >> neighbors (spatial check)
    """
    
    # Coarse-scale filtering (preserves hours-long events)
    data_30min = aggregate(data_5s, '30min')
    outliers_coarse = robust_zscore(data_30min, threshold=5.0)
    
    # Medium-scale filtering
    data_5min = aggregate(data_5s, '5min')
    outliers_medium = robust_zscore(data_5min, threshold=4.0)
    
    # Fine-scale: Only flag brief spikes
    # (Rain/dew events last hours ‚Üí safe)
    outliers_fine = detect_brief_spikes(data_5s, 
                                        max_duration='5min',
                                        threshold=3.0)
    
    # Combine: Flag if anomalous at ANY scale
    # But: Coarse scale protects hours-long events
    return outliers_coarse | outliers_medium | outliers_fine
```

**Advantage:** Rain/dew events (hours) pass through coarse filter, preserved

---

### **Strategy 3: Feature-Preserving Filtering**

Decompose signal into components, filter residuals only:

```python
def feature_preserving_filter(data):
    """
    1. Decompose: Trend + Seasonal + Residual
    2. ONLY filter residual component
    3. Reconstruct: Trend + Seasonal + Filtered_Residual
    
    This preserves:
    - Diurnal cycle (in seasonal component)
    - Long-term trends (in trend component)
    - Rain/dew (gradual, captured in trend/seasonal)
    
    Only removes:
    - Random spikes (in residual)
    """
    
    # STL decomposition (GPU-accelerated)
    trend, seasonal, residual = stl_decompose(data, period=2880)  # 24h at 30s
    
    # Filter ONLY the residual (high-frequency noise)
    filtered_residual = hampel_filter(residual, window=7)
    
    # Reconstruct
    cleaned = trend + seasonal + filtered_residual
    
    return cleaned
```

**Key insight:** Rain events are hours-long ‚Üí captured in trend/seasonal, NOT flagged

---

### **Strategy 4: Global Percentile Thresholding**

Instead of per-cell statistics, use global distribution:

```python
def global_percentile_filter(data, lower=0.01, upper=99.99):
    """
    Remove extreme values based on GLOBAL distribution.
    
    Rationale:
    - Compute percentiles across ALL cells
    - Flag values outside [p_low, p_high] globally
    - Preserves spatial coherence
    """
    
    # Flatten all data (all cells, all times)
    all_values = data.flatten()
    all_values = all_values[torch.isfinite(all_values)]
    
    # Global percentiles
    p_low = torch.quantile(all_values, lower/100)
    p_high = torch.quantile(all_values, upper/100)
    
    # Flag outliers
    outliers = (data < p_low) | (data > p_high)
    
    return outliers
```

**Pros:** Simple, respects global distribution  
**Cons:** Doesn't consider spatial neighbors explicitly

---

## üí° Recommended Hybrid Approach

Combine multiple strategies:

```python
class HemigridOutlierDetector:
    """Multi-stage outlier detection for hemisphere grids."""
    
    def __init__(self, grid, device='auto'):
        self.grid = grid
        self.backend = HemigridGPUBackend(device=device)
        
    def detect_outliers(self, data_5s):
        """
        Stage 1: Global extreme value removal (fast, conservative)
        Stage 2: Multi-scale decomposition (preserve features)
        Stage 3: Spatial consistency check (final validation)
        """
        
        # Stage 1: Remove obvious extremes (>99.9th percentile)
        outliers_extreme = self.global_percentile_filter(
            data_5s, lower=0.1, upper=99.9
        )
        data_stage1 = data_5s.clone()
        data_stage1[outliers_extreme] = torch.nan
        
        # Stage 2: Aggregate to 30min, apply feature-preserving filter
        data_30min = self.resample(data_stage1, '30min')
        trend, seasonal, residual = self.stl_decompose(data_30min)
        
        # Only filter residuals (preserves diurnal/seasonal/rain)
        filtered_residual = self.hampel_filter(residual, window=14)  # 7 days
        
        # Reconstruct
        data_30min_clean = trend + seasonal + filtered_residual
        
        # Upsample back to 5s (interpolate)
        data_5s_clean = self.upsample(data_30min_clean, target_resolution='5s')
        
        # Stage 3: Spatial consistency check at 5s resolution
        # Flag 5s spikes that are inconsistent with neighbors
        outliers_spatial = self.spatial_outlier_check(
            data_5s, data_5s_clean, n_neighbors=6
        )
        
        # Final cleaned data
        final = data_5s.clone()
        final[outliers_extreme | outliers_spatial] = torch.nan
        
        return final, {
            'outliers_extreme': outliers_extreme,
            'outliers_spatial': outliers_spatial,
            'trend': trend,
            'seasonal': seasonal,
        }
```

---

## üéØ Implementation Priorities (Revised)

### **Phase 0: Profiling & Understanding** ‚ö° DO THIS FIRST
Before implementing any filtering:

1. **Load 1 week of real data**
   ```python
   # 5000 cells √ó 7 days √ó 17,280 samples/day = 605M points
   # Size: 2.4 GB ‚Üí fits in M3
   ```

2. **Visualize actual outliers**
   - Plot time series for several cells
   - Identify what outliers actually look like
   - Characterize rain/dew events (duration, magnitude)

3. **Measure spatial correlation**
   - Do neighboring cells have similar values?
   - How correlated are adjacent cells?
   - Informs spatial filtering strategy

4. **Benchmark current I/O**
   - How fast can you load data from Icechunk?
   - Is I/O the bottleneck or computation?

### **Phase 1: Simple Baseline (CPU)** 
Implement global percentile filter (simplest, no GPU needed yet):
```python
def baseline_filter(data):
    p1, p99 = np.percentile(data[np.isfinite(data)], [0.1, 99.9])
    outliers = (data < p1) | (data > p99)
    return outliers
```

Test on 1 week, evaluate results

### **Phase 2: GPU Implementation**
Once baseline works, port to GPU:
- Multi-scale decomposition
- Spatial Hampel filter
- Feature-preserving pipeline

---

## ‚ùì Critical Questions Before Implementation

1. **What percentage of data are typically outliers?**
   - 0.1%? 1%? 10%?
   - Informs threshold tuning

2. **Can you share an example of "good" rain event vs "bad" spike?**
   - Visual examples would help design filter

3. **How important is spatial smoothness?**
   - Should neighboring cells have similar values?
   - Or can they be very different (e.g., due to tree heterogeneity)?

4. **What's the cost of false positives vs false negatives?**
   - Worse to flag good data (lose rain events)?
   - Or worse to keep bad data (corrupt analysis)?

5. **Do you have ground truth for outliers?**
   - Manually labeled examples?
   - External validation data (weather station)?

---

---

## üéì BRILLIANT INSIGHT: CLT-Based Global Filtering Strategy

### **User's Proposal (2026-02-01):**

Use **Central Limit Theorem** for statistically principled global outlier detection:

#### **The Approach:**
1. **Per-Cell Statistics (Stage 1):**
   - Compute mean, median, IQR, std for EACH cell
   - Weight by observation count (important!)
   
2. **Distribution of Cell Statistics (Stage 2):**
   - Construct distribution of cell means/medians
   - By CLT, this distribution ‚Üí approximately Normal
   
3. **Global Threshold (Stage 3):**
   - Identify outliers in the distribution of cell statistics
   - Use this to set global thresholds
   
4. **Apply Filtering (Stage 4):**
   - Apply global-informed thresholds to individual datapoints

#### **Why This is Brilliant:**
‚úÖ **Statistically principled** - Uses CLT, not ad-hoc thresholds  
‚úÖ **Respects heterogeneity** - Each cell can have different baseline  
‚úÖ **Globally informed** - Considers distribution across all cells  
‚úÖ **Weighted** - Accounts for observation count differences  
‚úÖ **Avoids false flagging** - Cell A and Cell B can differ, but both valid  

#### **Key Insight:**
> "The distribution of our data is highly non-homogeneous across the hemisphere"

- Zenith cells: High observation count, certain mean VOD
- Horizon cells: Low observation count, different mean VOD
- Can't use single global threshold on raw data!
- BUT: Distribution of cell means/medians ‚Üí CLT ‚Üí Normal distribution

---

## üî¨ Mathematical Formulation

### **Stage 1: Per-Cell Weighted Statistics**

For each cell $i \in [1, N_{cells}]$:

```python
# Cell i has n_i observations with weights w_ij
cell_mean[i] = Œ£(w_ij * VOD_ij) / Œ£(w_ij)

cell_median[i] = weighted_median(VOD_i, weights=w_i)

cell_std[i] = sqrt(Œ£(w_ij * (VOD_ij - cell_mean[i])¬≤) / Œ£(w_ij))

cell_IQR[i] = weighted_percentile(VOD_i, 75) - weighted_percentile(VOD_i, 25)

# Weight for this cell (total observations)
W_i = Œ£(w_ij)  # Total observation count
```

### **Stage 2: Distribution of Cell Statistics (CLT Application)**

By CLT, the distribution of sample means approaches Normal:

```python
# Distribution of cell means (weighted by observation count)
global_mean_of_means = Œ£(W_i * cell_mean[i]) / Œ£(W_i)

global_std_of_means = sqrt(Œ£(W_i * (cell_mean[i] - global_mean_of_means)¬≤) / Œ£(W_i))

# Same for medians
global_median_of_medians = weighted_median(cell_median, weights=W)

global_MAD_of_medians = weighted_MAD(cell_median, weights=W)
```

### **Stage 3: Global Threshold Identification**

```python
# Identify "unusual cells" (outliers in the distribution of means)
z_score_cell[i] = (cell_mean[i] - global_mean_of_means) / global_std_of_means

# Cells with |z| > 3 are statistically unusual
unusual_cells = cells where |z_score_cell| > 3

# Global threshold for individual datapoints
# Option A: Use distribution of cell means
global_threshold_low = global_mean_of_means - 3 * global_std_of_means
global_threshold_high = global_mean_of_means + 3 * global_std_of_means

# Option B: Use distribution of cell medians (more robust)
global_threshold_low = global_median_of_medians - 3 * 1.4826 * global_MAD_of_medians
global_threshold_high = global_median_of_medians + 3 * 1.4826 * global_MAD_of_medians
```

### **Stage 4: Adaptive Per-Cell Filtering**

Now filter individual datapoints with globally-informed thresholds:

```python
for each cell i:
    for each datapoint VOD_ij in cell i:
        
        # Option 1: Global threshold (simple)
        if VOD_ij < global_threshold_low or VOD_ij > global_threshold_high:
            flag_as_outlier
        
        # Option 2: Adaptive threshold (sophisticated)
        # Use local statistics, but adjusted by global context
        
        # How unusual is this cell's mean?
        z_cell = (cell_mean[i] - global_mean_of_means) / global_std_of_means
        
        if |z_cell| < 2:
            # Normal cell: Use local threshold
            threshold_low = cell_mean[i] - 3 * cell_std[i]
            threshold_high = cell_mean[i] + 3 * cell_std[i]
        else:
            # Unusual cell: Use conservative global threshold
            threshold_low = global_threshold_low
            threshold_high = global_threshold_high
        
        if VOD_ij < threshold_low or VOD_ij > threshold_high:
            flag_as_outlier
```

---

## üíª GPU Implementation (PyTorch)

### **Fully Vectorized, Massively Parallel:**

```python
import torch

class CLTGlobalOutlierDetector:
    """
    Central Limit Theorem-based global outlier detection.
    
    Respects hemisphere heterogeneity while using global statistics.
    All operations GPU-accelerated via PyTorch.
    """
    
    def __init__(self, device='auto'):
        self.device = self._get_device(device)
    
    def detect_outliers(
        self, 
        data: torch.Tensor,  # Shape: (n_cells, n_times)
        weights: torch.Tensor,  # Shape: (n_cells, n_times) - observation counts
        method: str = 'adaptive',  # 'global' or 'adaptive'
        n_sigma: float = 3.0
    ) -> torch.Tensor:
        """
        Detect outliers using CLT-based global thresholds.
        
        Parameters
        ----------
        data : torch.Tensor, shape (n_cells, n_times)
            VOD time series for all cells
        weights : torch.Tensor, shape (n_cells, n_times)
            Observation counts per (cell, time)
        method : str
            'global' - Single global threshold
            'adaptive' - Cell-specific thresholds informed by global distribution
        n_sigma : float
            Threshold in standard deviations (typically 3.0)
        
        Returns
        -------
        outliers : torch.Tensor, bool, shape (n_cells, n_times)
            True where outliers detected
        """
        
        # ============================================================
        # Stage 1: Per-Cell Weighted Statistics
        # ============================================================
        
        # Mask for valid (non-NaN) data
        valid_mask = torch.isfinite(data) & torch.isfinite(weights)
        
        # Weighted mean per cell (vectorized!)
        weighted_sum = torch.sum(data * weights * valid_mask, dim=1)
        weight_sum = torch.sum(weights * valid_mask, dim=1)
        cell_mean = weighted_sum / weight_sum  # Shape: (n_cells,)
        
        # Weighted std per cell
        diff_sq = ((data - cell_mean.unsqueeze(1)) ** 2) * weights * valid_mask
        cell_var = torch.sum(diff_sq, dim=1) / weight_sum
        cell_std = torch.sqrt(cell_var)  # Shape: (n_cells,)
        
        # Total weight per cell (for global weighting)
        W = weight_sum  # Shape: (n_cells,)
        
        # ============================================================
        # Stage 2: Distribution of Cell Means (CLT)
        # ============================================================
        
        # Weighted mean of cell means
        global_mean_of_means = torch.sum(W * cell_mean) / torch.sum(W)
        
        # Weighted std of cell means
        diff_means_sq = (cell_mean - global_mean_of_means) ** 2
        global_var_of_means = torch.sum(W * diff_means_sq) / torch.sum(W)
        global_std_of_means = torch.sqrt(global_var_of_means)
        
        # ============================================================
        # Stage 3: Global Thresholds
        # ============================================================
        
        global_threshold_low = global_mean_of_means - n_sigma * global_std_of_means
        global_threshold_high = global_mean_of_means + n_sigma * global_std_of_means
        
        # ============================================================
        # Stage 4: Outlier Detection
        # ============================================================
        
        if method == 'global':
            # Simple global threshold
            outliers = (data < global_threshold_low) | (data > global_threshold_high)
        
        elif method == 'adaptive':
            # Adaptive threshold per cell
            
            # Z-score for each cell's mean (how unusual is this cell?)
            z_cell = (cell_mean - global_mean_of_means) / global_std_of_means
            
            # Initialize thresholds with local values
            threshold_low = cell_mean.unsqueeze(1) - n_sigma * cell_std.unsqueeze(1)
            threshold_high = cell_mean.unsqueeze(1) + n_sigma * cell_std.unsqueeze(1)
            
            # For unusual cells (|z| > 2), use global threshold
            unusual_mask = torch.abs(z_cell) > 2.0
            threshold_low[unusual_mask, :] = global_threshold_low
            threshold_high[unusual_mask, :] = global_threshold_high
            
            # Detect outliers
            outliers = (data < threshold_low) | (data > threshold_high)
        
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # Don't flag NaN as outliers
        outliers = outliers & valid_mask
        
        return outliers, {
            'cell_mean': cell_mean,
            'cell_std': cell_std,
            'global_mean_of_means': global_mean_of_means,
            'global_std_of_means': global_std_of_means,
            'global_threshold_low': global_threshold_low,
            'global_threshold_high': global_threshold_high,
            'z_cell': z_cell,  # How unusual each cell is
        }
    
    def detect_outliers_robust(
        self, 
        data: torch.Tensor,
        weights: torch.Tensor,
        method: str = 'adaptive',
        n_sigma: float = 3.0
    ):
        """
        Robust version using median/MAD instead of mean/std.
        
        More resistant to outliers in the distribution of cell statistics.
        """
        
        # ============================================================
        # Stage 1: Per-Cell Weighted Median/MAD
        # ============================================================
        
        # Weighted median per cell (requires sorting, still vectorizable)
        cell_median = self._weighted_median_per_cell(data, weights)  # (n_cells,)
        
        # Weighted MAD per cell
        cell_mad = self._weighted_mad_per_cell(data, weights, cell_median)  # (n_cells,)
        
        # Total weight per cell
        valid_mask = torch.isfinite(data) & torch.isfinite(weights)
        W = torch.sum(weights * valid_mask, dim=1)  # (n_cells,)
        
        # ============================================================
        # Stage 2: Distribution of Cell Medians (CLT still applies!)
        # ============================================================
        
        global_median_of_medians = self._weighted_median(cell_median, W)
        global_mad_of_medians = self._weighted_mad(cell_median, W, global_median_of_medians)
        
        # ============================================================
        # Stage 3 & 4: Thresholds and Detection
        # ============================================================
        
        # Convert MAD to std-equivalent (assuming normal distribution)
        global_std_equiv = 1.4826 * global_mad_of_medians
        
        global_threshold_low = global_median_of_medians - n_sigma * global_std_equiv
        global_threshold_high = global_median_of_medians + n_sigma * global_std_equiv
        
        # Adaptive or global filtering (same logic as above)
        if method == 'global':
            outliers = (data < global_threshold_low) | (data > global_threshold_high)
        elif method == 'adaptive':
            z_cell = (cell_median - global_median_of_medians) / global_std_equiv
            # ... (same adaptive logic)
        
        return outliers
    
    def _weighted_median_per_cell(self, data, weights):
        """Compute weighted median for each cell (row)."""
        # This requires sorting - slightly more complex but still GPU-friendly
        # PyTorch doesn't have built-in weighted median, need custom implementation
        # For now, sketch:
        n_cells = data.shape[0]
        medians = torch.zeros(n_cells, device=self.device)
        
        for i in range(n_cells):
            valid = torch.isfinite(data[i]) & torch.isfinite(weights[i])
            if valid.sum() > 0:
                sorted_vals, sorted_idx = torch.sort(data[i][valid])
                sorted_weights = weights[i][valid][sorted_idx]
                cumsum = torch.cumsum(sorted_weights, dim=0)
                total = cumsum[-1]
                median_idx = torch.searchsorted(cumsum, total / 2)
                medians[i] = sorted_vals[median_idx]
        
        return medians
    
    def _weighted_mad_per_cell(self, data, weights, medians):
        """Compute weighted MAD for each cell."""
        # MAD = median(|x - median(x)|)
        deviations = torch.abs(data - medians.unsqueeze(1))
        return self._weighted_median_per_cell(deviations, weights)


# ============================================================
# Usage Example
# ============================================================

# Load data
data = percell_ds.cell_timeseries.values  # (n_cells, n_times)
weights = percell_ds.cell_weights.values  # Observation counts

# Convert to PyTorch
detector = CLTGlobalOutlierDetector(device='mps')  # or 'cuda'
data_gpu = torch.tensor(data, device=detector.device)
weights_gpu = torch.tensor(weights, device=detector.device)

# Detect outliers (FAST!)
outliers, stats = detector.detect_outliers(
    data_gpu, 
    weights_gpu,
    method='adaptive',
    n_sigma=3.0
)

# Results
print(f"Total outliers: {outliers.sum().item():,}")
print(f"Global mean of means: {stats['global_mean_of_means']:.3f}")
print(f"Global threshold: [{stats['global_threshold_low']:.3f}, {stats['global_threshold_high']:.3f}]")
print(f"Most unusual cell: {torch.argmax(torch.abs(stats['z_cell'])).item()}")

# Transfer back to CPU/NumPy
outliers_np = outliers.cpu().numpy()
```

---

## üéØ Advantages of This Approach

### **1. Statistically Principled**
- Based on CLT (solid theoretical foundation)
- Not ad-hoc thresholding
- Mathematically justified

### **2. Respects Heterogeneity**
- Each cell can have different baseline (mean, median)
- Zenith vs horizon cells naturally differ
- No forced uniformity

### **3. Globally Informed**
- Considers distribution across ALL cells
- Avoids problem of independent cell filtering
- Thresholds derived from global statistics

### **4. Weighted by Data Quality**
- Cells with many observations ‚Üí higher weight
- Cells with few observations ‚Üí lower weight
- Robust to uneven spatial sampling

### **5. GPU-Friendly**
- All operations vectorized (parallel across cells)
- Weighted statistics computed in batch
- Very fast even for 5000 cells √ó 12M samples

---

## üìä Performance Estimates (UPDATED for 15-30min + Incremental)

### **Target Resolution: 15-30min (Primary Science Product)**

**Data Dimensions (30min resolution, 320 SIDs):**
```
Temporal bins per day: 48 (1440 min / 30 min)
2 years: 730 days √ó 48 = 35,040 time bins

Full dataset:
320 SIDs √ó 5000 cells √ó 35,040 times = 56.06 BILLION datapoints
Storage: 56B √ó 4 bytes = 224 GB (float32)
```

### **Hardware: Linux NVIDIA GPU (Primary Production)**

**Full Reprocessing (2 years at 30min):**
```
Stage 1: Per-(SID,cell) statistics (320 √ó 5000 = 1.6M pairs)
  - Weighted mean/std: ~15 seconds (all in parallel)
  
Stage 2: Per-SID distribution (320 SIDs)
  - Global mean/std per SID: <0.5 seconds
  
Stage 3: Threshold computation (320 SIDs)
  - Negligible: <0.1 seconds
  
Stage 4: Outlier flagging (56B comparisons)
  - Element-wise comparison: ~60 seconds

Total: ~75-80 seconds for FULL 2-year reprocess! ‚ö°
```

**Incremental Processing (1 week at 30min):**
```
New data per week: 7 days √ó 48 = 336 time bins
320 SIDs √ó 5000 cells √ó 336 = 537.6 MILLION datapoints

Pass 1: Update statistics
  - Merge old + new stats: ~2 seconds (1.6M pairs)
  - Recompute per-SID thresholds: <0.5 seconds
  
Pass 2: Filter new data only
  - Outlier detection: ~5 seconds (537M points)

Total: ~7-8 seconds for weekly update! ‚ö°‚ö°‚ö°
```

**Comparison:**
```
Full reprocess: 80 sec
Incremental:     8 sec
Speedup: 10x faster for weekly updates!
```

**Daily incremental (even faster):**
```
New data: 48 time bins ‚Üí 76.8M datapoints
Total time: ~1-2 seconds! ‚ö°‚ö°‚ö°‚ö°
```

---

### **Alternative: 15min Resolution Performance**

**Data Dimensions (15min resolution, 320 SIDs):**
```
96 bins/day √ó 730 days = 70,080 time bins
320 √ó 5000 √ó 70,080 = 112.1 BILLION datapoints
Storage: 112B √ó 4 bytes = 448 GB (float32)
```

**Full Reprocessing (15min):**
- ~150 seconds (2.5 minutes)

**Incremental (1 week, 15min):**
- ~15 seconds

---

### **CPU Baseline (for comparison):**

**30min full reprocess on 64-core CPU:**
- Estimated: 2-4 hours (sequential per SID)

**GPU Speedup: ~100-200x faster!** üöÄ

---

### **Memory Requirements (30min, NVIDIA GPU):**

**Full dataset:** 224 GB
- Too large for single-GPU memory (most have 16-40 GB)
- **Solution:** Process in chunks of 64 SIDs at a time
  - 64 √ó 5000 √ó 35,040 = 11.2B points = 44.8 GB
  - With mixed precision (float16): 22.4 GB ‚úÖ
  - Fits in 24GB or 40GB GPUs!

**Chunking strategy:**
```python
# Process 320 SIDs in 5 chunks of 64 SIDs each
for sid_chunk in range(0, 320, 64):
    chunk_data = load_sids(sid_chunk, sid_chunk+64)  # 64 SIDs
    chunk_data_gpu = torch.tensor(chunk_data, 
                                   device='cuda', 
                                   dtype=torch.float16)  # 22.4 GB
    
    # Process this chunk
    outliers_chunk = detector.detect_outliers(chunk_data_gpu)
    
    # Save results
    save_outliers(outliers_chunk, sid_range=(sid_chunk, sid_chunk+64))

# Total time: 5 chunks √ó 16 sec/chunk = 80 seconds ‚úÖ
```

---

### **On M3 Mac (Metal) - For Development Only:**
### **On M3 Mac (Metal) - For Development Only:**

**Use case:** Prototyping, testing code changes  
**NOT for production!** (Use Linux NVIDIA GPU instead)

**30min resolution (small test dataset - 1 month):**
```
32 SIDs √ó 5000 cells √ó 1,440 times = 230M datapoints
Time: ~3-5 seconds ‚úÖ

Full 2-year, 320 SIDs: 
Would require ~16 GB RAM ‚Üí Tight fit, but possible with float16
Estimated: ~120 seconds (slower than NVIDIA due to Metal overhead)
```

**Recommended M3 workflow:**
1. Develop/test with 1-month subsets (fast iteration)
2. Deploy to Linux GPU for full dataset processing

---

### **On HPC GPU (A100/H100, 80GB VRAM):**

**Luxury mode:** Can load entire dataset in memory!

**30min resolution, 320 SIDs:**
```
Stage 1-4: ~30-40 seconds (everything in one pass!)
No chunking needed!
```

**15min resolution, 320 SIDs:**
```
Total: ~60-80 seconds
```

---

## ‚è±Ô∏è **Processing Time Summary Table**

| Dataset | Resolution | Hardware | Full Reprocess | Weekly Incr. | Daily Incr. |
|---------|-----------|----------|---------------|--------------|-------------|
| 2yr, 320 SIDs | **30min** | **Linux NVIDIA** | **80 sec** ‚≠ê | **8 sec** ‚≠ê | **2 sec** ‚≠ê |
| 2yr, 320 SIDs | 30min | M3 Mac | ~120 sec | ~12 sec | ~3 sec |
| 2yr, 320 SIDs | 30min | HPC GPU (A100) | ~35 sec üöÄ | ~4 sec üöÄ | ~1 sec üöÄ |
| 2yr, 320 SIDs | 30min | 64-core CPU | ~3 hours üêå | ~20 min üêå | ~5 min üêå |
| | | | | | |
| 2yr, 320 SIDs | **15min** | **Linux NVIDIA** | **150 sec** | **15 sec** | **4 sec** |
| 2yr, 320 SIDs | 15min | HPC GPU (A100) | ~60 sec üöÄ | ~6 sec üöÄ | ~2 sec üöÄ |
| 2yr, 320 SIDs | 15min | 64-core CPU | ~6 hours üêå | ~40 min üêå | ~10 min üêå |
| | | | | | |
| 2yr, 320 SIDs | 5s | Linux NVIDIA | ~15 hours* | ~90 min* | ~25 min* |
| 2yr, 320 SIDs | 5s | HPC GPU (A100) | ~2 hours* üöÄ | ~12 min* üöÄ | ~3 min* üöÄ |

*Chunked processing required (dataset too large for memory)

**Key Insight:** Incremental processing makes daily updates **trivial** (1-4 seconds)!

---

## ‚ùì Questions to Refine Implementation

1. **Method preference:**
   - Use mean/std (parametric, assumes normality)?
   - Or median/MAD (non-parametric, more robust)?
   
2. **Adaptive vs global threshold:**
   - Simple global threshold for all cells?
   - Or adaptive (unusual cells get different treatment)?
   
3. **Handling unusual cells:**
   - If a cell's mean is 3œÉ away from global mean, is it:
     a) A real phenomenon (e.g., tree in that direction)?
     b) A problem (sensor issue, interference)?
   
4. **Multi-resolution:**
   - Apply CLT method at 30min resolution first?
   - Then different approach for 5s data?
   
5. **Temporal consideration:**
   - Current approach: Spatial CLT (distribution of cell means)
   - Could also: Temporal CLT (distribution of time-window means per cell)
   - Or both?

---

## üéì Theoretical Note: Why CLT Works Here

**Central Limit Theorem states:**
> The distribution of sample means approaches a normal distribution as sample size increases, regardless of the population distribution.

**Application to hemigrid:**
- Each cell = sample from underlying spatial VOD distribution
- Cell means = sample statistics
- Distribution of cell means ‚Üí Normal (by CLT)
- Even though individual VOD values may not be normal!

**Requirements:**
1. ‚úÖ Independence: Cells are spatially independent (mostly true)
2. ‚úÖ Sample size: Each cell has many observations (true, except horizon)
3. ‚úÖ Finite variance: VOD has finite variance (true)

**Caveat:** Spatial autocorrelation (neighboring cells are correlated) slightly violates independence, but CLT is robust to mild violations.

---

---

## üö® CRITICAL ADDITION: Per-SID Filtering Requirement

### **User's Additional Constraint (2026-02-01):**

> "Filtering has to be done on a SID basis, because SIDs are not necessarily comparable"

**This changes EVERYTHING (again)!** üéØ

### **Why SIDs Are Not Comparable:**

Different Signal IDs represent fundamentally different observables:

1. **Different Frequencies:**
   - `G01_L1C` (GPS L1, 1575.42 MHz) ‚â† `G01_L2C` (GPS L2, 1227.60 MHz)
   - L-band frequencies interact differently with vegetation
   - Different wavelengths ‚Üí different VOD sensitivities

2. **Different Constellations:**
   - GPS (G##) ‚â† Galileo (E##) ‚â† GLONASS (R##)
   - Different signal designs, power levels, modulation

3. **Different Satellites:**
   - `G01` vs `G02` may have different antenna patterns
   - Different satellite generations (Block IIF vs Block III)

4. **Different Physical Phenomena:**
   - Each SID "sees" vegetation differently
   - Cannot assume VOD from L1C should match L2C

**Implication:** Must filter each SID independently!

---

## üîÑ REVISED Data Structure

### **Original (Incorrect) Assumption:**
```python
# After aggregation, assumed shape:
data = percell_ds.cell_timeseries.values  # (n_cells, n_times)
# All SIDs aggregated together
```

### **Actual Data Structure:**
```python
# BEFORE aggregation:
vod_ds = xr.Dataset({
    'VOD': (['epoch', 'sid'], float),  # ‚Üê SID dimension!
    'cell_id': (['epoch', 'sid'], int),
})

# AFTER per-cell aggregation (KEEP SID DIMENSION):
percell_ds = xr.Dataset({
    'cell_timeseries': (['sid', 'cell', 'time'], float),  # 3D!
    'cell_weights': (['sid', 'cell', 'time'], int),
})

# Dimensions:
# - sid: Signal IDs (e.g., ~50-100 SIDs depending on constellation)
# - cell: Spatial cells (e.g., 5000)
# - time: Temporal bins (e.g., 730 for daily, 12.6M for 5s)
```

**Shape:**
```
n_sids = ~50 (depends on which constellations/frequencies)
n_cells = 5000
n_times = 12,614,400 (5s resolution, 2 years)

Total: 50 √ó 5000 √ó 12.6M = 3.15 TRILLION datapoints! ü§Ø

Storage: 3.15T √ó 4 bytes = 12.6 TB (float32)
         3.15T √ó 2 bytes = 6.3 TB (float16)
```

**This is MASSIVE!** But also **perfect for GPU parallelism**! üí™

---

## üéì Revised CLT Filtering Strategy (Per-SID)

### **New Hierarchy:**

1. **Per-SID, Per-Cell Statistics**
   - Compute mean, std for each (SID, cell) combination
   - Weight by observation count
   
2. **Per-SID Global Distribution**
   - For each SID: Distribution of cell statistics (CLT)
   - Each SID has its own global mean/std
   
3. **Per-SID Global Thresholds**
   - Each SID gets its own threshold
   
4. **Apply Per-SID Filtering**
   - Filter SID 'G01_L1C' using G01_L1C thresholds
   - Filter SID 'E05_L1C' using E05_L1C thresholds
   - etc.

### **Mathematical Formulation:**

For each SID $s \in [1, N_{SID}]$:

```python
# Stage 1: Per-(SID,Cell) Statistics
for sid in SIDs:
    for cell in cells:
        cell_mean[sid, cell] = weighted_mean(data[sid, cell, :], weights[sid, cell, :])
        cell_std[sid, cell] = weighted_std(data[sid, cell, :], weights[sid, cell, :])
        W[sid, cell] = sum(weights[sid, cell, :])  # Total obs count

# Stage 2: Per-SID Global Distribution (CLT per SID!)
for sid in SIDs:
    # Distribution of cell means FOR THIS SID
    global_mean_of_means[sid] = weighted_mean(cell_mean[sid, :], W[sid, :])
    global_std_of_means[sid] = weighted_std(cell_mean[sid, :], W[sid, :])
    
    # Global threshold FOR THIS SID
    threshold_low[sid] = global_mean_of_means[sid] - 3 * global_std_of_means[sid]
    threshold_high[sid] = global_mean_of_means[sid] + 3 * global_std_of_means[sid]

# Stage 3: Per-SID Outlier Detection
for sid in SIDs:
    for cell in cells:
        for time in times:
            if data[sid, cell, time] < threshold_low[sid] or 
               data[sid, cell, time] > threshold_high[sid]:
                outliers[sid, cell, time] = True
```

**Key Insight:** Each SID gets its own CLT analysis!

---

## üíª GPU Implementation (3D Parallel)

```python
import torch

class PerSID_CLT_OutlierDetector:
    """
    Per-SID CLT-based outlier detection for hemigrid data.
    
    Handles 3D data: (n_sids, n_cells, n_times)
    Each SID filtered independently using CLT on its cell distribution.
    """
    
    def __init__(self, device='auto'):
        self.device = self._get_device(device)
    
    def detect_outliers(
        self,
        data: torch.Tensor,  # Shape: (n_sids, n_cells, n_times)
        weights: torch.Tensor,  # Shape: (n_sids, n_cells, n_times)
        method: str = 'adaptive',
        n_sigma: float = 3.0
    ):
        """
        Detect outliers per SID using CLT-based global thresholds.
        
        Parameters
        ----------
        data : torch.Tensor, shape (n_sids, n_cells, n_times)
            VOD values for all SIDs, cells, and times
        weights : torch.Tensor, shape (n_sids, n_cells, n_times)
            Observation counts (weights)
        method : str
            'global' or 'adaptive'
        n_sigma : float
            Threshold in standard deviations
        
        Returns
        -------
        outliers : torch.Tensor, bool, shape (n_sids, n_cells, n_times)
            True where outliers detected
        stats : dict
            Statistics per SID
        """
        
        n_sids, n_cells, n_times = data.shape
        
        # ============================================================
        # Stage 1: Per-(SID, Cell) Statistics
        # ============================================================
        
        # Valid data mask
        valid = torch.isfinite(data) & torch.isfinite(weights)
        
        # Weighted mean per (SID, cell) - collapse time dimension
        weighted_sum = torch.sum(data * weights * valid, dim=2)  # (n_sids, n_cells)
        weight_sum = torch.sum(weights * valid, dim=2)  # (n_sids, n_cells)
        cell_mean = weighted_sum / weight_sum  # (n_sids, n_cells)
        
        # Weighted std per (SID, cell)
        diff_sq = ((data - cell_mean.unsqueeze(2)) ** 2) * weights * valid
        cell_var = torch.sum(diff_sq, dim=2) / weight_sum
        cell_std = torch.sqrt(cell_var)  # (n_sids, n_cells)
        
        # Total weight per (SID, cell)
        W = weight_sum  # (n_sids, n_cells)
        
        # ============================================================
        # Stage 2: Per-SID Global Distribution (CLT)
        # ============================================================
        
        # For each SID: Distribution of its cell means
        # Weighted mean of cell means PER SID
        global_mean_of_means = torch.sum(W * cell_mean, dim=1) / torch.sum(W, dim=1)
        # Shape: (n_sids,)
        
        # Weighted std of cell means PER SID
        diff_means_sq = (cell_mean - global_mean_of_means.unsqueeze(1)) ** 2
        global_var_of_means = torch.sum(W * diff_means_sq, dim=1) / torch.sum(W, dim=1)
        global_std_of_means = torch.sqrt(global_var_of_means)
        # Shape: (n_sids,)
        
        # ============================================================
        # Stage 3: Per-SID Global Thresholds
        # ============================================================
        
        threshold_low = global_mean_of_means - n_sigma * global_std_of_means
        threshold_high = global_mean_of_means + n_sigma * global_std_of_means
        # Shape: (n_sids,)
        
        # Broadcast to (n_sids, n_cells, n_times) for comparison
        threshold_low_3d = threshold_low.view(n_sids, 1, 1)
        threshold_high_3d = threshold_high.view(n_sids, 1, 1)
        
        # ============================================================
        # Stage 4: Outlier Detection
        # ============================================================
        
        if method == 'global':
            # Simple global threshold per SID
            outliers = (data < threshold_low_3d) | (data > threshold_high_3d)
        
        elif method == 'adaptive':
            # Adaptive threshold per (SID, cell)
            
            # Z-score for each (SID, cell) mean
            z_cell = (cell_mean - global_mean_of_means.unsqueeze(1)) / \
                     global_std_of_means.unsqueeze(1)
            # Shape: (n_sids, n_cells)
            
            # Initialize with local thresholds
            threshold_low_local = cell_mean.unsqueeze(2) - n_sigma * cell_std.unsqueeze(2)
            threshold_high_local = cell_mean.unsqueeze(2) + n_sigma * cell_std.unsqueeze(2)
            # Shape: (n_sids, n_cells, 1) ‚Üí broadcasts to (n_sids, n_cells, n_times)
            
            # For unusual cells (|z| > 2), use global threshold
            unusual_mask = torch.abs(z_cell) > 2.0  # (n_sids, n_cells)
            unusual_mask_3d = unusual_mask.unsqueeze(2)  # (n_sids, n_cells, 1)
            
            # Apply adaptive thresholds
            threshold_low_adaptive = torch.where(
                unusual_mask_3d, 
                threshold_low_3d.expand_as(threshold_low_local),
                threshold_low_local
            )
            threshold_high_adaptive = torch.where(
                unusual_mask_3d,
                threshold_high_3d.expand_as(threshold_high_local),
                threshold_high_local
            )
            
            outliers = (data < threshold_low_adaptive) | (data > threshold_high_adaptive)
        
        else:
            raise ValueError(f"Unknown method: {method}")
        
        # Don't flag NaN as outliers
        outliers = outliers & valid
        
        # ============================================================
        # Return Statistics
        # ============================================================
        
        return outliers, {
            'cell_mean': cell_mean,  # (n_sids, n_cells)
            'cell_std': cell_std,  # (n_sids, n_cells)
            'global_mean_of_means': global_mean_of_means,  # (n_sids,)
            'global_std_of_means': global_std_of_means,  # (n_sids,)
            'threshold_low': threshold_low,  # (n_sids,)
            'threshold_high': threshold_high,  # (n_sids,)
            'z_cell': z_cell,  # (n_sids, n_cells)
            'n_outliers_per_sid': outliers.sum(dim=(1, 2)),  # (n_sids,)
            'outlier_rate_per_sid': outliers.sum(dim=(1, 2)).float() / valid.sum(dim=(1, 2)),
        }


# ============================================================
# Usage Example
# ============================================================

# Load 3D data
data = percell_ds.cell_timeseries.values  # (n_sids, n_cells, n_times)
weights = percell_ds.cell_weights.values  # (n_sids, n_cells, n_times)

# Example shapes:
# n_sids = 50
# n_cells = 5000  
# n_times = 730 (daily for 2 years)
# Total: 50 √ó 5000 √ó 730 = 182.5M datapoints

# Convert to GPU
detector = PerSID_CLT_OutlierDetector(device='mps')
data_gpu = torch.tensor(data, device=detector.device, dtype=torch.float32)
weights_gpu = torch.tensor(weights, device=detector.device, dtype=torch.float32)

# Detect outliers (ALL SIDs IN PARALLEL!)
outliers, stats = detector.detect_outliers(
    data_gpu,
    weights_gpu,
    method='adaptive',
    n_sigma=3.0
)

# Inspect results per SID
for sid_idx, sid_name in enumerate(sid_names):
    n_outliers = stats['n_outliers_per_sid'][sid_idx].item()
    outlier_rate = stats['outlier_rate_per_sid'][sid_idx].item()
    mean = stats['global_mean_of_means'][sid_idx].item()
    std = stats['global_std_of_means'][sid_idx].item()
    
    print(f"{sid_name}:")
    print(f"  Mean: {mean:.3f} ¬± {std:.3f}")
    print(f"  Threshold: [{stats['threshold_low'][sid_idx]:.3f}, "
          f"{stats['threshold_high'][sid_idx]:.3f}]")
    print(f"  Outliers: {n_outliers:,} ({outlier_rate*100:.2f}%)")
    print()
```

---

## üìä Performance Analysis (3D Case)

### **Memory Requirements:**

#### **Daily Resolution (Feasible):**
```
Shape: (50 SIDs, 5000 cells, 730 days)
Total: 182.5M datapoints

Storage (float32): 182.5M √ó 4 bytes = 730 MB ‚úÖ
Storage (float16): 182.5M √ó 2 bytes = 365 MB ‚úÖ

M3 Mac (16GB): ‚úÖ Easily fits!
Even with intermediate buffers: <5 GB total
```

#### **30-Minute Resolution (Challenging):**
```
Shape: (50 SIDs, 5000 cells, 35,040 samples)
Total: 8.76B datapoints

Storage (float32): 8.76B √ó 4 bytes = 35 GB ‚ùå Too large for M3!
Storage (float16): 8.76B √ó 2 bytes = 17.5 GB ‚ö†Ô∏è Tight fit on M3

Need chunking for M3, or use HPC GPU
```

#### **5-Second Resolution (HUGE):**
```
Shape: (50 SIDs, 5000 cells, 12.6M samples)
Total: 3.15 TRILLION datapoints ü§Ø

Storage (float32): 3.15T √ó 4 bytes = 12.6 TB ‚ùå‚ùå‚ùå
Storage (float16): 3.15T √ó 2 bytes = 6.3 TB ‚ùå‚ùå

Requires chunking on ANY hardware
OR: Aggregate to coarser resolution first
```

### **Processing Time Estimates:**

#### **Daily Resolution on M3 (Metal):**
```
Data: 50 SIDs √ó 5000 cells √ó 730 days = 182.5M points

Stage 1: Per-(SID,cell) stats
  - All 50 SIDs processed in parallel
  - Weighted mean/std: ~2 seconds
  
Stage 2: Per-SID global distribution
  - 50 √ó (5000 cell means) = 250k values
  - Negligible: <0.1 seconds
  
Stage 3: Thresholds
  - Negligible: <0.01 seconds
  
Stage 4: Outlier detection
  - 182.5M comparisons: ~1 second

Total: ~3 seconds ‚ö°
```

#### **30-Minute Resolution on M3 (Chunked):**
```
Process in 2-3 chunks: ~20-30 seconds total
```

#### **5-Second Resolution on HPC GPU:**
```
Data: 3.15T points

With A100 (80GB):
  - Chunk into ~100 temporal chunks
  - Process ~30B points per chunk
  - ~2 seconds per chunk
  - Total: ~200 seconds = 3.3 minutes

vs CPU (estimated): ~10-20 hours
Speedup: ~200-400x ‚ö°‚ö°
```

---

## üéØ Revised Strategy: Multi-Resolution Workflow

Given memory constraints, process at multiple resolutions:

```python
# ============================================================
# STEP 1: Daily Resolution (Full Analysis)
# ============================================================
# Aggregate to daily BEFORE loading to GPU
data_daily = aggregate_per_sid_cell(data_5s, resolution='24h')
# Shape: (50, 5000, 730) = 182.5M points ‚úÖ Fits in M3!

# Filter at daily resolution
detector = PerSID_CLT_OutlierDetector(device='mps')
outliers_daily, stats_daily = detector.detect_outliers(data_daily, weights_daily)

# Identify globally problematic (SID, cell) combinations
problematic_sid_cells = find_problematic_pairs(stats_daily)

# ============================================================
# STEP 2: 30-Min Resolution (Diurnal Analysis)
# ============================================================
# Aggregate to 30min
data_30min = aggregate_per_sid_cell(data_5s, resolution='30min')
# Shape: (50, 5000, 35k) = 8.76B points ‚ö†Ô∏è Chunk on M3

# Filter with knowledge from daily analysis
outliers_30min = detector.detect_outliers(
    data_30min, 
    weights_30min,
    exclude_cells=problematic_sid_cells  # Skip known bad cells
)

# ============================================================
# STEP 3: 5s Native (Preserve Original)
# ============================================================
# Only filter obvious spikes at 5s resolution
# Don't load full dataset - process incrementally

for time_chunk in chunks(data_5s, chunk_size='1_week'):
    # Load 1 week at a time: 50 √ó 5000 √ó 120k = 30M points ‚úÖ
    outliers_5s_chunk = detect_brief_spikes(
        time_chunk,
        duration_threshold='5min',  # Spikes < 5min
        magnitude_threshold=5.0,  # Very conservative
        use_spatial_neighbors=True  # Check consistency with neighbors
    )
    
    save_outliers(outliers_5s_chunk)

# ============================================================
# STEP 4: Combine Results
# ============================================================
# Merge outlier masks from all resolutions
final_outliers = combine_multiscale_outliers(
    outliers_daily, 
    outliers_30min, 
    outliers_5s
)
```

---

## üí° Key Advantages of Per-SID Filtering

### **1. Physically Meaningful**
- Each SID represents different physical observable
- L1C ‚â† L2C (different frequencies interact with vegetation differently)
- GPS ‚â† Galileo (different signal designs)

### **2. Statistically Sound**
- CLT applied to homogeneous populations (same SID)
- Distribution of cell means more likely to be Normal
- Avoids mixing incompatible distributions

### **3. Respects Multiple Heterogeneities**
- Spatial heterogeneity: Different cells (already handled)
- Signal heterogeneity: Different SIDs (now handled!)
- Temporal heterogeneity: Different time resolutions (multi-scale approach)

### **4. GPU-Friendly Parallelism**
- Each SID processed independently ‚Üí perfect parallelism
- 50 SIDs √ó 5000 cells = 250k independent statistical analyses
- All done simultaneously on GPU! üöÄ

### **5. Diagnostic Capability**
- Can identify problematic SIDs (e.g., "G23_L1C has 10√ó more outliers")
- Can identify problematic cells (e.g., "Cell 237 is bad for all SIDs")
- Can identify problematic (SID, cell) pairs (e.g., "E12_L5 in Cell 891")

---

## ‚ùì Critical Questions (Updated)

1. **How many SIDs in your actual data?**
   - 50? 100? 200?
   - Which constellations: GPS only, or GPS+Galileo+GLONASS?

2. **Should we filter SIDs independently or also cross-compare?**
   - Independent: Each SID gets its own thresholds ‚úÖ (current approach)
   - Cross-compare: Flag if one SID is outlier but others aren't in same (cell, time)

3. **What about missing SIDs?**
   - Some cells may not have observations for certain SIDs
   - Horizon cells: Low observation count for all SIDs
   - How to handle sparse (SID, cell) combinations?

4. **Temporal aggregation:**
   - Aggregate to daily before filtering? (saves memory)
   - Or filter at native 5s then aggregate? (preserves detail)

5. **Cross-SID validation:**
   - If 10 SIDs all show spike at same (cell, time), is it:
     a) Real event (rain started)? ‚úÖ
     b) Sensor glitch? ‚ùå
   - Can we use cross-SID consistency as validation?

---

---

## üö® MASSIVE SCALE UPDATE: 320 SIDs Reality Check

### **User's Critical Information (2026-02-01):**

1. **Actual SID count: ~320 SIDs** ü§Ø
   - This is 6-7√ó larger than initial estimate!
   - GPS + Galileo + GLONASS + BeiDou + multiple frequencies
   
2. **Workflow separation:**
   - **Preprocessing** (filtering, decomposition) ‚Üê We design this now
   - **Analysis** (downstream science) ‚Üê Comes after, uses preprocessed data
   
3. **Actual dataset size: ~80 GB** for 1.5 years
   - On-disk compressed (Icechunk/Zarr)
   - Likely sparse (NaN when satellites not visible)
   - Decompressed in memory will be larger

---

## üìä Revised Scale Calculations (320 SIDs)

### **Theoretical Dense Array:**
```python
320 SIDs √ó 5000 cells √ó 12.6M samples (5s, 2 years)
= 20.18 TRILLION datapoints ü§Ø

Dense storage (float32): 20.18T √ó 4 bytes = 80.7 TB
Dense storage (float16): 20.18T √ó 2 bytes = 40.4 TB
```

### **Actual Sparse Storage:**
```
User reported: ~80 GB for 1.5 years (compressed)

Compression ratio: 80.7 TB / 80 GB ‚âà 1000:1
This makes sense because:
  - Satellites not visible 24/7 (lots of NaN)
  - Zarr/Icechunk compression
  - Only store actual observations
```

### **Working Memory Requirements (Decompressed):**

#### **Daily Resolution:**
```
320 SIDs √ó 5000 cells √ó 730 days
= 1.168 BILLION datapoints

RAM (float32): 1.168B √ó 4 = 4.67 GB ‚úÖ Fits in M3!
RAM (float16): 1.168B √ó 2 = 2.34 GB ‚úÖ Easy!

With intermediate buffers: ~10 GB total
M3 (16GB): ‚úÖ Comfortable fit!
```

#### **30-Minute Resolution:**
```
320 SIDs √ó 5000 cells √ó 35,040 samples
= 56.06 BILLION datapoints

RAM (float32): 56B √ó 4 = 224 GB ‚ùå Too large!
RAM (float16): 56B √ó 2 = 112 GB ‚ö†Ô∏è HPC GPU only

Strategy: MUST chunk temporally
- Chunk into 8 pieces: ~14 GB each ‚úÖ Fits M3
```

#### **5-Second Resolution:**
```
320 SIDs √ó 5000 cells √ó 12.6M samples  
= 20.18 TRILLION datapoints

RAM (float32): 80.7 TB ‚ùå‚ùå‚ùå
Even decompressed chunks will be large

Strategy: Process incrementally, never load full dataset
```

---

## üéØ Preprocessing Pipeline Architecture

### **Design Principles:**

1. **Incremental Processing**
   - Never load full 5s dataset to RAM
   - Process in time chunks (e.g., 1 week at a time)
   - Stream from Icechunk, process, write back

2. **Multi-Resolution Strategy**
   - Coarse-to-fine: Daily ‚Üí 30min ‚Üí 5s
   - Each resolution informs the next
   - Save filtered versions at each resolution

3. **GPU-Accelerated Where Possible**
   - Daily: Full dataset in GPU memory ‚úÖ
   - 30min: Chunked GPU processing ‚úÖ
   - 5s: Chunk-by-chunk GPU processing ‚úÖ

4. **Persistent Results**
   - Save filtered datasets back to Icechunk
   - Store outlier masks for inspection
   - Store statistics for provenance

---

## üíæ Proposed Preprocessing Workflow

### **Phase 1: Daily Resolution (Full Analysis)**

```python
# ============================================================
# STEP 1.1: Load Daily Aggregated Data
# ============================================================

# Load from Icechunk, aggregate to daily on-the-fly
data_daily = load_and_aggregate(
    store_path=icechunk_path,
    resolution='24h',
    sids='all',  # All 320 SIDs
    cells='all'  # All 5000 cells
)
# Shape: (320, 5000, 730) = 1.168B points
# RAM: ~5 GB (float32) ‚úÖ Fits M3!

# ============================================================
# STEP 1.2: Per-SID CLT Filtering (GPU)
# ============================================================

detector = PerSID_CLT_OutlierDetector(device='mps')

outliers_daily, stats_daily = detector.detect_outliers(
    data_daily,
    weights_daily,
    method='adaptive',
    n_sigma=3.0
)

# Processing time on M3: ~5 seconds ‚ö°
# All 320 SIDs processed in parallel!

# ============================================================
# STEP 1.3: Identify Problematic (SID, Cell) Pairs
# ============================================================

# Find systematically bad combinations
problematic_pairs = identify_problematic_sid_cells(
    stats_daily,
    criteria={
        'outlier_rate': 0.20,  # >20% outliers = bad
        'low_obs_count': 100,  # <100 obs total = insufficient
        'unusual_zscore': 5.0  # |z| > 5 = extremely unusual
    }
)

# Example output:
# [
#   (sid='G23_L1C', cell=237, reason='outlier_rate=0.35'),
#   (sid='R07_L1P', cell=4891, reason='low_obs_count=45'),
#   ...
# ]

# ============================================================
# STEP 1.4: Save Daily Results
# ============================================================

# Create new Icechunk group for filtered data
store.create_group('filtered_daily')

# Save cleaned data
store.write_dataset(
    'filtered_daily/VOD',
    data_daily_cleaned,
    chunks={'sid': 32, 'cell': 500, 'time': 365}
)

# Save outlier masks (for inspection/provenance)
store.write_dataset(
    'filtered_daily/outlier_mask',
    outliers_daily,
    chunks={'sid': 32, 'cell': 500, 'time': 365}
)

# Save statistics (for reproducibility)
store.write_metadata(
    'filtered_daily/stats',
    stats_daily
)
```

### **Phase 2: 30-Minute Resolution (Diurnal Analysis)**

```python
# ============================================================
# STEP 2.1: Load 30min Data in Chunks
# ============================================================

# Too large for M3: 320 √ó 5000 √ó 35k = 56B points = 224 GB
# Chunk into 8 temporal pieces: ~28 GB each ‚Üí ~7 GB decompressed

n_temporal_chunks = 8
chunk_size = 35040 // n_temporal_chunks  # ~4380 samples per chunk

for chunk_idx in range(n_temporal_chunks):
    
    # Load one temporal chunk
    data_30min_chunk = load_chunk(
        resolution='30min',
        time_slice=(chunk_idx * chunk_size, (chunk_idx+1) * chunk_size)
    )
    # Shape: (320, 5000, ~4380) = ~7B points = 28 GB (float32)
    
    # Compress to float16 for GPU
    data_30min_chunk = data_30min_chunk.astype(np.float16)  # ‚Üí 14 GB
    
    # ============================================================
    # STEP 2.2: GPU Filtering (Informed by Daily Results)
    # ============================================================
    
    # Skip known problematic (SID, cell) pairs
    data_30min_chunk = mask_problematic_pairs(
        data_30min_chunk, 
        problematic_pairs
    )
    
    # Detect outliers for this chunk
    outliers_chunk, _ = detector.detect_outliers(
        data_30min_chunk,
        weights_30min_chunk,
        method='adaptive'
    )
    
    # ============================================================
    # STEP 2.3: Save Chunk Results
    # ============================================================
    
    store.write_chunk(
        'filtered_30min/VOD',
        chunk_idx,
        data_30min_chunk * (~outliers_chunk)  # Set outliers to NaN
    )
    
    # Processing time per chunk on M3: ~10 seconds
    # Total: 8 √ó 10s = 80 seconds ‚ö°

print("‚úÖ 30min filtering complete!")
```

### **Phase 3: 5-Second Resolution (Native Data)**

```python
# ============================================================
# STEP 3.1: Stream Processing (Never Load Full Dataset)
# ============================================================

# Process 1 week at a time
# 320 SIDs √ó 5000 cells √ó 120,960 samples (7 days)
# = 193.5B points = 774 GB (float32) ‚Üí Still too large!

# Sub-chunk further: Process 1 day at a time
# 320 √ó 5000 √ó 17,280 = 27.65B points = 110 GB (float32)
# ‚Üí Still too large for M3!

# Final chunking: 1 day, 64 SIDs at a time
n_sid_chunks = 320 // 64  # 5 chunks

for day_idx in range(n_days):
    for sid_chunk_idx in range(n_sid_chunks):
        
        # Load chunk: 64 SIDs √ó 5000 cells √ó 17,280 samples
        # = 5.53B points = 22 GB (float32) ‚Üí 11 GB (float16) ‚úÖ Fits M3!
        
        data_5s_chunk = load_chunk(
            resolution='5s',
            day=day_idx,
            sids=range(sid_chunk_idx*64, (sid_chunk_idx+1)*64)
        )
        
        # Convert to float16 for GPU
        data_5s_chunk_fp16 = data_5s_chunk.astype(np.float16)
        
        # ============================================================
        # STEP 3.2: Brief Spike Detection (5s only)
        # ============================================================
        
        # At 5s resolution, only flag VERY brief spikes
        # (Hours-long rain events preserved by 30min filtering)
        
        outliers_5s = detect_brief_spikes(
            data_5s_chunk_fp16,
            max_duration=60,  # 60 samples = 5 minutes
            magnitude_threshold=5.0,  # Very conservative
            use_spatial_neighbors=True
        )
        
        # ============================================================
        # STEP 3.3: Save Chunk
        # ============================================================
        
        store.write_chunk(
            'filtered_5s/VOD',
            (day_idx, sid_chunk_idx),
            data_5s_chunk * (~outliers_5s)
        )
        
        # Processing time: ~15 seconds per chunk
        # Total chunks: 730 days √ó 5 SID chunks = 3,650 chunks
        # Total time: 3650 √ó 15s = 54,750s ‚âà 15 hours on M3
        #
        # On HPC GPU: ~2-3 hours
        # On CPU: ~weeks!

print("‚úÖ 5s filtering complete!")
```

---

## ‚ö° Performance Summary (320 SIDs)

### **M3 Mac (16GB, Metal):**

| Resolution | Data Size | Chunks | Time/Chunk | Total Time | Strategy |
|------------|-----------|--------|------------|------------|----------|
| Daily | 4.7 GB | 1 | 5s | **5 seconds** | Full dataset in memory |
| 30-min | 224 GB | 8 | 10s | **80 seconds** | Temporal chunks |
| 5s | ~80 GB disk | 3,650 | 15s | **~15 hours** | Day√óSID chunks |

**Total preprocessing time: ~16 hours** (mostly 5s resolution)

### **HPC GPU (A100, 80GB VRAM):**

| Resolution | Chunks | Time/Chunk | Total Time |
|------------|--------|------------|------------|
| Daily | 1 | 2s | **2 seconds** |
| 30-min | 2-3 | 5s | **~15 seconds** |
| 5s | 730 | 10s | **~2 hours** |

**Total preprocessing time: ~2 hours** üöÄ

### **CPU (64-core server):**

| Resolution | Total Time |
|------------|------------|
| Daily | ~10 minutes |
| 30-min | ~2 hours |
| 5s | **~2-3 weeks** üêå |

**GPU Speedup: 200-400√ó faster than CPU!**

---

## üîÑ Downstream Analysis Implications

### **After Preprocessing, Analysis is FAST:**

```python
# ============================================================
# Analysis Workflow (After Preprocessing)
# ============================================================

# Load filtered data (already cleaned!)
vod_clean = store.open_dataset('filtered_daily/VOD')

# Example: Compute hemisphere-averaged VOD time series
hemisphere_avg = vod_clean.weighted(vod_clean.cell_weights).mean(dim=['sid', 'cell'])
# Uses pre-cleaned data ‚Üí No outliers to worry about! ‚úÖ

# Example: Diurnal analysis
vod_30min_clean = store.open_dataset('filtered_30min/VOD')
diurnal_pattern = vod_30min_clean.groupby('time.hour').mean()
# Already filtered ‚Üí Robust results! ‚úÖ

# Example: Rain event detection
# Load 5s data for specific time window
rain_event = store.open_dataset(
    'filtered_5s/VOD',
    time_slice=('2023-06-15', '2023-06-16')
)
# Outliers already removed, rain events preserved! ‚úÖ
```

**Key Benefits for Analysis:**
1. ‚úÖ Clean data (outliers removed)
2. ‚úÖ Fast loading (chunked efficiently)
3. ‚úÖ Reproducible (filtering metadata saved)
4. ‚úÖ Multi-resolution (choose appropriate scale)
5. ‚úÖ Provenance (outlier masks available for inspection)

---

## üéØ Implementation Priorities (320 SID Reality)

### **Phase 0: Prototype & Validate (Week 1-2)**
- [ ] Implement `PerSID_CLT_OutlierDetector` for daily resolution
- [ ] Test on 1 month of real data (320 SIDs)
- [ ] Validate statistics make sense (inspect per-SID distributions)
- [ ] Benchmark M3 Metal performance

### **Phase 1: Daily Resolution Pipeline (Week 3)**
- [ ] Full daily filtering (all 2 years)
- [ ] Save to Icechunk
- [ ] Generate QC plots (outlier rates per SID)
- [ ] Identify problematic (SID, cell) pairs

### **Phase 2: 30-Min Resolution (Week 4)**
- [ ] Implement chunked processing for 30min
- [ ] Test on M3 and HPC GPU
- [ ] Validate diurnal patterns preserved

### **Phase 3: 5s Native Resolution (Week 5-6)**
- [ ] Implement brief spike detection
- [ ] Stream processing pipeline
- [ ] Validate on sample events (known rain, known outliers)

### **Phase 4: Documentation & QC (Week 7)**
- [ ] Document preprocessing pipeline
- [ ] Create quality control notebooks
- [ ] Generate summary statistics
- [ ] Publish preprocessing methods

---

## üí° Smart Optimizations for 320 SIDs

### **1. SID Grouping:**
Since processing is per-SID, can group similar SIDs:
```python
# Group by frequency (similar physics)
sid_groups = {
    'L1_band': [sids with ~1575 MHz],
    'L2_band': [sids with ~1227 MHz],
    'L5_band': [sids with ~1176 MHz],
}

# Process each group with slightly different thresholds
for group, sids in sid_groups.items():
    # These SIDs MIGHT be more comparable within group
    outliers = detect_outliers(data[sids], ...)
```

### **2. Adaptive Chunking:**
```python
# Chunk size based on available memory
available_mem = get_available_gpu_memory()
optimal_chunk_size = compute_chunk_size(
    n_sids=320,
    n_cells=5000,
    available_mem=available_mem
)
```

### **3. Progressive Filtering:**
```python
# First pass: Obvious extremes (fast)
outliers_obvious = global_percentile_filter(data, percentile=99.9)

# Second pass: CLT on remaining data (slower but accurate)
data_stage2 = data[~outliers_obvious]
outliers_subtle = clt_filter(data_stage2)

# Combine
outliers_final = outliers_obvious | outliers_subtle
```

---

## ‚ùì Critical Questions (320 SID Scale)

1. **SID breakdown:**
   - How many SIDs per constellation? (GPS: X, Galileo: Y, ...)
   - How many frequencies per satellite?
   
2. **Sparsity pattern:**
   - What % of (SID, cell, time) triplets have actual data?
   - Some SIDs never visible in some cells?
   
3. **Preprocessing cadence:**
   - Run once on historical data?
   - Or incremental (daily processing of new data)?
   
4. **Hardware access:**
   - How often can you use HPC GPU?
   - Is 15 hours on M3 acceptable for full reprocessing?
   
5. **Validation strategy:**
   - How will you validate filtered results?
   - Ground truth data available (weather stations, manual inspection)?

---

**Status:** Architecture scaled to 320 SID reality!  
**Key insight:** GPU parallelism even MORE valuable with 320 SIDs  
**Challenge:** Memory management for 5s resolution  
**Solution:** Multi-resolution + chunking strategy  
**Next:** User feedback on implementation priorities ‚è∏Ô∏è
