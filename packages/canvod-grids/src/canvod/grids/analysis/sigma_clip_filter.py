"""Vectorised numba-JIT Hampel filter with astropy sigma-clipping fallback.

Two complementary high-performance filtering strategies for gridded
VOD data:

* **Vectorised + numba** (``astropy_hampel_vectorized_fast``) –
  sliding-window Hampel filter compiled with ``numba.jit``.  Processes
  temporal chunks in cell batches; targets sub-5-minute runtimes for
  ~1.5 years of data.
* **Ultra-fast** (``astropy_hampel_ultra_fast``) – pure-numpy
  vectorisation backed by ``astropy.stats.sigma_clip``.  Drops the
  per-window granularity in exchange for extreme throughput.

Functions
---------
``vectorized_sliding_window_hampel``        – numba-compiled core loop.
``process_cell_batch_vectorized``           – batch dispatcher for one temporal chunk.
``astropy_hampel_vectorized_fast``          – full pipeline (numba path).
``astropy_hampel_ultra_fast``               – full pipeline (astropy path).

Notes
-----
* ``vectorized_sliding_window_hampel`` uses ``numba.prange`` so the
  outer loop is distributed across all available cores automatically.
* The 1.4826 MAD scaling factor matches the convention used by
  ``astropy.stats.mad_std``.
* Both top-level functions expect a ``cell_id_<grid_name>`` variable
  already present in the input dataset (see
  :func:`canvod.grids.add_cell_ids_to_vod_fast`).

"""

from __future__ import annotations

import logging
import time
import warnings

import numpy as np
import xarray as xr
from astropy.stats import mad_std, sigma_clip
from numba import jit, prange
from tqdm.auto import tqdm

warnings.filterwarnings("ignore")

logger = logging.getLogger(__name__)


# ----------------------------------------------------------------------
# Numba core
# ----------------------------------------------------------------------


@jit(nopython=True, parallel=True)
def vectorized_sliding_window_hampel(
    data: np.ndarray,
    times: np.ndarray,
    window_ns: int,
    sigma_threshold: float = 3.0,
    min_points: int = 5,
) -> tuple[np.ndarray, np.ndarray]:
    """Sliding-window Hampel filter compiled with numba.

    Each point is compared against the robust statistics (median,
    scaled MAD) of its temporal neighbourhood.  Points whose deviation
    exceeds *sigma_threshold* × 1.4826 × MAD are flagged as outliers
    and set to ``NaN``.

    Parameters
    ----------
    data : np.ndarray
        1-D array of values to filter.
    times : np.ndarray
        1-D array of timestamps as ``int64`` nanoseconds.
    window_ns : int
        Half-window size in nanoseconds.
    sigma_threshold : float, optional
        Number of scaled-MAD units for the outlier boundary.
    min_points : int, optional
        Minimum number of finite points required in the window to
        attempt filtering; points in smaller windows are left unchanged.

    Returns
    -------
    filtered_data : np.ndarray
        Copy of *data* with outliers replaced by ``NaN``.
    outlier_mask : np.ndarray
        Boolean mask; ``True`` where an outlier was detected.

    """
    n_points = len(data)
    filtered_data = data.copy()
    outlier_mask = np.zeros(n_points, dtype=np.bool_)

    for i in prange(n_points):
        if not np.isfinite(data[i]):
            continue

        current_time = times[i]
        window_start = current_time - window_ns
        window_end = current_time + window_ns

        # Points inside the temporal window with finite values
        window_indices = np.where(
            (times >= window_start) & (times <= window_end) & np.isfinite(data)
        )[0]

        if len(window_indices) < min_points:
            continue

        window_data = data[window_indices]

        median_val = np.median(window_data)
        mad_val = np.median(np.abs(window_data - median_val))

        if mad_val > 0:
            threshold_value = sigma_threshold * 1.4826 * mad_val
            if np.abs(data[i] - median_val) > threshold_value:
                outlier_mask[i] = True
                filtered_data[i] = np.nan

    return filtered_data, outlier_mask


# ----------------------------------------------------------------------
# Batch dispatcher
# ----------------------------------------------------------------------


def process_cell_batch_vectorized(
    cell_batch: np.ndarray,
    vod_chunk: np.ndarray,
    times_chunk: np.ndarray,
    cell_ids_chunk: np.ndarray,
    window_hours: float,
    sigma_threshold: float,
    min_points: int,
) -> tuple[np.ndarray, np.ndarray]:
    """Apply the numba Hampel filter to a subset of cells in one temporal chunk.

    Parameters
    ----------
    cell_batch : np.ndarray
        1-D array of cell IDs to process in this batch.
    vod_chunk : np.ndarray
        2-D ``(epoch, sid)`` VOD values for the current temporal chunk.
    times_chunk : np.ndarray
        1-D ``datetime64`` timestamps for the chunk.
    cell_ids_chunk : np.ndarray
        2-D ``(epoch, sid)`` cell-ID array matching *vod_chunk*.
    window_hours : float
        Half-window size in hours.
    sigma_threshold : float
        Outlier threshold in scaled-MAD units.
    min_points : int
        Minimum finite points required per window.

    Returns
    -------
    filtered_chunk : np.ndarray
        Filtered copy of *vod_chunk*.
    outlier_chunk : np.ndarray
        Boolean outlier mask with the same shape as *vod_chunk*.

    """
    epochs, sids = vod_chunk.shape
    filtered_chunk = vod_chunk.copy()
    outlier_chunk = np.zeros((epochs, sids), dtype=bool)

    window_ns = int(window_hours * 3600 * 1e9)
    times_ns = times_chunk.astype("datetime64[ns]").astype(np.int64)

    for sid_idx in range(sids):
        sid_vod = vod_chunk[:, sid_idx]
        sid_cells = cell_ids_chunk[:, sid_idx]

        valid_mask = np.isfinite(sid_vod) & np.isfinite(sid_cells)
        if not np.any(valid_mask):
            continue

        for cell_id in cell_batch:
            cell_mask = valid_mask & (sid_cells == cell_id)
            if not np.any(cell_mask):
                continue

            cell_indices = np.where(cell_mask)[0]
            cell_vod = sid_vod[cell_indices]
            cell_times = times_ns[cell_indices]

            if len(cell_vod) < min_points:
                continue

            try:
                filtered_cell, outliers_cell = vectorized_sliding_window_hampel(
                    cell_vod, cell_times, window_ns, sigma_threshold, min_points
                )
                filtered_chunk[cell_indices, sid_idx] = filtered_cell
                outlier_chunk[cell_indices, sid_idx] = outliers_cell
            except Exception:
                # Keep original data if filtering fails for this cell-SID
                continue

    return filtered_chunk, outlier_chunk


# ----------------------------------------------------------------------
# Vectorised pipeline (numba path)
# ----------------------------------------------------------------------


def astropy_hampel_vectorized_fast(
    vod_ds: xr.Dataset,
    grid_name: str = "equal_area_2deg",
    window_hours: float = 1.0,
    sigma_threshold: float = 3.0,
    min_points: int = 5,
    cell_batch_size: int = 200,
    n_workers: int | None = None,
) -> xr.Dataset:
    """Numba-accelerated sliding-window Hampel filter over a VOD dataset.

    Temporal chunks (as stored in the dask-backed dataset) are iterated
    sequentially; within each chunk the unique cells are split into
    batches of *cell_batch_size* and dispatched to
    :func:`process_cell_batch_vectorized`, which in turn calls the
    numba-compiled :func:`vectorized_sliding_window_hampel`.

    Parameters
    ----------
    vod_ds : xr.Dataset
        VOD dataset containing a ``cell_id_<grid_name>`` variable.
    grid_name : str, optional
        Suffix used to locate the cell-ID variable
        (``cell_id_<grid_name>``).
    window_hours : float, optional
        Half-window size in hours for the sliding window.
    sigma_threshold : float, optional
        Outlier boundary in scaled-MAD units.
    min_points : int, optional
        Minimum finite points required in a window.
    cell_batch_size : int, optional
        Number of cells per batch (trades memory for cache locality).
    n_workers : int or None, optional
        Unused in this implementation; reserved for future parallel
        chunk processing.

    Returns
    -------
    result_ds : xr.Dataset
        Copy of *vod_ds* with two additional variables:

        ``VOD_filtered``
            Filtered VOD (outliers → NaN).
        ``hampel_outlier_mask``
            Boolean mask; ``True`` at outlier positions.

    Raises
    ------
    ValueError
        If the expected cell-ID variable is missing from *vod_ds*.

    """
    cell_id_var = f"cell_id_{grid_name}"
    if cell_id_var not in vod_ds:
        raise ValueError(
            f"Cell ID variable '{cell_id_var}' not found in dataset. "
            "Run add_cell_ids_to_vod_fast() first."
        )

    logger.info(
        "vectorized hampel: window=%.1fh, sigma=%.1f, batch_size=%d, shape=%s",
        window_hours,
        sigma_threshold,
        cell_batch_size,
        vod_ds.VOD.shape,
    )

    start_time = time.time()

    vod_data = vod_ds.VOD.data
    cell_ids = vod_ds[cell_id_var].data
    times = vod_ds.epoch.values

    logger.info("processing %d temporal chunks ...", vod_data.numblocks[0])

    def _process_temporal_chunk(chunk_idx: int) -> tuple[np.ndarray, np.ndarray]:
        """Process a single temporal chunk.

        Parameters
        ----------
        chunk_idx : int
            Chunk index along time axis.

        Returns
        -------
        tuple[np.ndarray, np.ndarray]
            Filtered values and outlier mask.

        """
        chunk_start = chunk_idx * vod_data.chunksize[0]
        chunk_end = min(chunk_start + vod_data.chunksize[0], vod_data.shape[0])

        vod_chunk = vod_data[chunk_start:chunk_end].compute()
        cell_chunk = cell_ids[chunk_start:chunk_end].compute()
        times_chunk = times[chunk_start:chunk_end]

        unique_cells = np.unique(cell_chunk[np.isfinite(cell_chunk)])
        if len(unique_cells) == 0:
            return vod_chunk, np.zeros_like(vod_chunk, dtype=bool)

        n_cell_batches = int(np.ceil(len(unique_cells) / cell_batch_size))

        filtered_chunk = vod_chunk.copy()
        outlier_chunk = np.zeros_like(vod_chunk, dtype=bool)

        for batch_idx in range(n_cell_batches):
            batch_start = batch_idx * cell_batch_size
            batch_end = min(batch_start + cell_batch_size, len(unique_cells))
            cell_batch = unique_cells[batch_start:batch_end]

            batch_filtered, batch_outliers = process_cell_batch_vectorized(
                cell_batch=cell_batch,
                vod_chunk=vod_chunk,
                times_chunk=times_chunk,
                cell_ids_chunk=cell_chunk,
                window_hours=window_hours,
                sigma_threshold=sigma_threshold,
                min_points=min_points,
            )

            for cell_id in cell_batch:
                cell_mask = cell_chunk == cell_id
                filtered_chunk[cell_mask] = batch_filtered[cell_mask]
                outlier_chunk[cell_mask] = batch_outliers[cell_mask]

        return filtered_chunk, outlier_chunk

    filtered_chunks: list[np.ndarray] = []
    outlier_chunks: list[np.ndarray] = []

    for chunk_idx in tqdm(range(vod_data.numblocks[0]), desc="Temporal chunks"):
        f_chunk, o_chunk = _process_temporal_chunk(chunk_idx)
        filtered_chunks.append(f_chunk)
        outlier_chunks.append(o_chunk)

    logger.info("assembling results ...")
    filtered_data = np.concatenate(filtered_chunks, axis=0)
    outlier_data = np.concatenate(outlier_chunks, axis=0)

    total_time = time.time() - start_time

    # Build result dataset
    result_ds = vod_ds.copy()
    result_ds["VOD_filtered"] = (("epoch", "sid"), filtered_data)
    result_ds["hampel_outlier_mask"] = (("epoch", "sid"), outlier_data)

    total_obs = int(np.isfinite(vod_ds.VOD.values).sum())
    outliers_detected = int(outlier_data.sum())
    outlier_pct = (outliers_detected / total_obs * 100) if total_obs > 0 else 0.0

    result_ds.attrs.update(
        {
            "hampel_method": "vectorized_astropy_equivalent",
            "hampel_window_hours": window_hours,
            "hampel_sigma_threshold": sigma_threshold,
            "hampel_min_points": min_points,
            "hampel_processing_time_s": total_time,
            "hampel_vectorized_optimized": True,
            "performance_target_achieved": total_time < 300,
        }
    )
    result_ds["VOD_filtered"].attrs.update(
        {
            "long_name": "VOD filtered with vectorized Hampel method",
            "method": "vectorized_sliding_window_with_numba_jit",
            "temporal_window_hours": window_hours,
            "outlier_threshold_sigma": sigma_threshold,
        }
    )

    logger.info(
        "done in %.1fs | obs=%d outliers=%d (%.2f%%) | target_met=%s",
        total_time,
        total_obs,
        outliers_detected,
        outlier_pct,
        total_time < 300,
    )

    return result_ds


# ----------------------------------------------------------------------
# Ultra-fast pipeline (astropy path)
# ----------------------------------------------------------------------


def astropy_hampel_ultra_fast(
    vod_ds: xr.Dataset,
    grid_name: str = "equal_area_2deg",
    window_hours: float = 1.0,
    sigma_threshold: float = 3.0,
    min_points: int = 5,
) -> xr.Dataset:
    """Pure-numpy sigma-clipping filter via ``astropy.stats``.

    Each (cell, SID) time series is clipped in one shot using
    :func:`astropy.stats.sigma_clip` with median centering and MAD
    scale estimation.  No per-window temporal granularity is applied;
    this trades precision for throughput.

    Parameters
    ----------
    vod_ds : xr.Dataset
        VOD dataset containing a ``cell_id_<grid_name>`` variable.
    grid_name : str, optional
        Suffix for the cell-ID variable.
    window_hours : float, optional
        Unused in ultra-fast mode; kept for API compatibility.
    sigma_threshold : float, optional
        Sigma-clipping threshold passed to ``astropy.stats.sigma_clip``.
    min_points : int, optional
        Minimum finite observations required to attempt clipping.

    Returns
    -------
    result_ds : xr.Dataset
        Copy of *vod_ds* with ``VOD_filtered`` and
        ``hampel_outlier_mask`` variables added.

    Raises
    ------
    ValueError
        If the expected cell-ID variable is missing from *vod_ds*.

    """
    cell_id_var = f"cell_id_{grid_name}"
    if cell_id_var not in vod_ds:
        raise ValueError(
            f"Cell ID variable '{cell_id_var}' not found in dataset. "
            "Run add_cell_ids_to_vod_fast() first."
        )

    logger.info(
        "ultra-fast hampel: sigma=%.1f, min_points=%d, shape=%s",
        sigma_threshold,
        min_points,
        vod_ds.VOD.shape,
    )

    start_time = time.time()

    vod_values = vod_ds.VOD.values
    cell_ids = vod_ds[cell_id_var].values

    filtered_data = vod_values.copy()
    outlier_mask = np.zeros_like(vod_values, dtype=bool)

    unique_cells = np.unique(cell_ids[np.isfinite(cell_ids)])
    logger.info(
        "processing %d cells × %d SIDs ...", len(unique_cells), vod_values.shape[1]
    )

    for cell_id in tqdm(unique_cells, desc="Cells"):
        for sid_idx in range(vod_values.shape[1]):
            cell_mask = (cell_ids[:, sid_idx] == cell_id) & np.isfinite(
                vod_values[:, sid_idx]
            )
            if not np.any(cell_mask):
                continue

            cell_data = vod_values[cell_mask, sid_idx]
            if len(cell_data) < min_points:
                continue

            try:
                clipped = sigma_clip(
                    cell_data,
                    sigma=sigma_threshold,
                    cenfunc="median",
                    stdfunc=mad_std,
                    maxiters=1,
                    masked=True,
                )
                cell_indices = np.where(cell_mask)[0]
                filtered_data[cell_indices, sid_idx] = clipped.data
                outlier_mask[cell_indices, sid_idx] = clipped.mask
            except Exception:
                continue

    total_time = time.time() - start_time

    result_ds = vod_ds.copy()
    result_ds["VOD_filtered"] = (("epoch", "sid"), filtered_data)
    result_ds["hampel_outlier_mask"] = (("epoch", "sid"), outlier_mask)

    total_obs = int(np.isfinite(vod_values).sum())
    outliers = int(outlier_mask.sum())

    result_ds.attrs.update(
        {
            "hampel_method": "ultra_fast_astropy_sigma_clip",
            "hampel_sigma_threshold": sigma_threshold,
            "hampel_min_points": min_points,
            "hampel_processing_time_s": total_time,
        }
    )
    result_ds["VOD_filtered"].attrs.update(
        {
            "long_name": "VOD filtered with ultra-fast astropy sigma clipping",
            "method": "astropy_sigma_clip_per_cell_sid",
            "outlier_threshold_sigma": sigma_threshold,
        }
    )

    logger.info(
        "done in %.1fs | obs=%d outliers=%d (%.2f%%)",
        total_time,
        total_obs,
        outliers,
        (outliers / total_obs * 100) if total_obs > 0 else 0.0,
    )

    return result_ds
