# Icechunk Store Reading Fix - Explicit Chunking for String Coordinates

## Problem

When reading datasets from Icechunk stores, two issues occurred:

### 1. Fatal Error: Auto-Chunking with Object Dtype

**Error:**
```python
ds = mystore.read_group(branch='main', group_name='canopy_01')

NotImplementedError: Can not use auto rechunking with object dtype. 
We are unable to estimate the size in bytes of object data
```

**Stack trace:**
```
xarray/backends/zarr.py:1592 in open_zarr
  ‚Üí chunks='auto' 
dask/array/core.py:3302 in auto_chunks
  ‚Üí raise NotImplementedError(...)
```

### 2. Warning: Unstable Zarr V3 String Specification

**Warning:**
```
/path/to/zarr/core/dtype/npy/string.py:249: UnstableSpecificationWarning: 
The data type (FixedLengthUTF32(length=9, endianness='little')) does not have 
a Zarr V3 specification. That means that the representation of arrays saved 
with this data type may change without warning in a future version of Zarr Python.
```

---

## Root Cause Analysis

### Issue 1: Auto-Chunking Failure

**File:** `packages/canvod-store/src/canvod/store/store.py`

**Before (broken):**
```python
def read_group(self, group_name: str, branch: str = "main", ...):
    with self.readonly_session(branch) as session:
        if chunks is None:
            # chunks = ICECHUNK_CHUNK_STRATEGIES.get(self.store_type, {})
            chunks = 'auto'  # ‚ùå Causes error with string dtypes!
        
        ds = xr.open_zarr(
            session.store,
            group=group_name,
            chunks=chunks,  # 'auto' tries to chunk ALL variables
            consolidated=False,
        )
```

**Problem:**
- `chunks='auto'` tells dask to automatically determine chunk sizes for ALL arrays
- String coordinates (like `sid`, `band`, `code`, `system`, `sv`) have object/string dtype
- Dask cannot estimate memory size for object dtypes
- Result: `NotImplementedError`

**What should happen:**
- Numeric data arrays (SNR, phi, theta, etc.) should be chunked according to strategy
- Coordinate arrays should be loaded entirely (no chunking)
- String coordinates especially should not be chunked

### Issue 2: FixedLengthUTF32 Warning

**Source:** Zarr library when encoding string coordinates

**Why it happens:**
- Xarray encodes string coordinates using fixed-length UTF-32 encoding
- This encoding is not yet part of the official Zarr V3 specification
- Zarr warns that the encoding format might change in future versions

**Impact:**
- ‚ö†Ô∏è Warning only - data still works correctly
- Data may not be readable by other Zarr libraries
- Format may change in future zarr-python versions

---

## Solution

### Fix 1: Use Explicit Chunking Strategy ‚úÖ

**File:** `packages/canvod-store/src/canvod/store/store.py` (line ~383)

**After (fixed):**
```python
def read_group(self, group_name: str, branch: str = "main", ...):
    with self.readonly_session(branch) as session:
        if chunks is None:
            # Use explicit chunk strategy from globals
            chunks = ICECHUNK_CHUNK_STRATEGIES.get(self.store_type, {
                "epoch": 34560,  # Chunk time dimension
                "sid": -1        # Don't chunk satellite dimension
            })
        
        ds = xr.open_zarr(
            session.store,
            group=group_name,
            chunks=chunks,  # ‚úÖ Explicit chunks for dimensions only
            consolidated=False,
        )
```

**How it works:**
```python
# From canvodpy/globals.py
ICECHUNK_CHUNK_STRATEGIES = {
    "rinex_store": {
        "epoch": 34560,  # Chunk along time (2 days of 5s data)
        "sid": -1        # Don't chunk satellites (load all)
    },
    "vod_store": {
        "epoch": 34560,
        "sid": -1
    }
}
```

**Key points:**
- Only specifies chunks for **dimensions** (epoch, sid)
- Does not specify chunks for **variables** or **coordinates**
- Variables inherit chunking from their dimensions
- Coordinates with no dimension chunking are loaded fully
- String coordinates avoid auto-chunking entirely

### Fix 2: FixedLengthUTF32 Warning ‚ö†Ô∏è

**Status:** No fix needed - warning only

**Reasoning:**
- This is a limitation of Zarr V3 specification
- String encoding in Zarr V3 is still being standardized
- The warning informs users of potential future incompatibility
- Data works correctly despite the warning

**If warning is problematic:**
1. **Suppress the warning** (not recommended):
   ```python
   import warnings
   warnings.filterwarnings('ignore', category=zarr.core.dtype.npy.string.UnstableSpecificationWarning)
   ```

2. **Use Zarr V2** (not recommended - loses Icechunk features):
   ```python
   zarr_format = 2  # In store configuration
   ```

3. **Wait for Zarr V3 spec finalization** (recommended):
   - Monitor: https://github.com/zarr-developers/zarr-extensions/tree/main/data-types
   - Future zarr-python versions will stabilize string handling

---

## Verification

### Test 1: Read Dataset Without Error

```python
from canvod.store import GnssResearchSite

site = GnssResearchSite('Rosalia')

# Should work without NotImplementedError
ds = site.rinex_store.read_group(branch='main', group_name='canopy_01')

print(f'‚úÖ Successfully read dataset!')
print(f'Shape: {dict(ds.sizes)}')
print(f'Variables: {list(ds.data_vars)}')
print(f'Coordinates: {list(ds.coords)}')
```

**Output:**
```
‚úÖ Successfully read dataset!
Shape: {'epoch': 2160, 'sid': 321}
Variables: ['r', 'SNR', 'phi', 'theta']
Coordinates: ['code', 'freq_center', 'freq_min', 'band', 'epoch', 'freq_max', 'sid', 'system', 'sv']
```

### Test 2: Verify Chunking Strategy

```python
import dask.array as da

# Check data array chunking
print(f"SNR chunks: {ds['SNR'].chunks}")
# Expected: ((34560, ...), (-1,)) for (epoch, sid)

# Check coordinate loading
print(f"sid type: {type(ds.coords['sid'].data)}")
# Expected: numpy array (loaded, not dask array)

print(f"band type: {type(ds.coords['band'].data)}")
# Expected: numpy array (string coordinates loaded fully)
```

### Test 3: Marimo Notebook Usage

**File:** `demo/read_icechunk_store.py`

```python
import marimo as mo
from canvod.store import GnssResearchSite

# Initialize store
site = GnssResearchSite('Rosalia')
mystore = site.rinex_store

# Read dataset - now works!
canopy_rinex_ds = mystore.read_group(branch='main', group_name='canopy_01')

# Display info
mo.md(f"""
## Dataset Loaded Successfully
- Shape: {dict(canopy_rinex_ds.sizes)}
- Variables: {', '.join(canopy_rinex_ds.data_vars)}
- Time range: {canopy_rinex_ds.epoch.min().values} to {canopy_rinex_ds.epoch.max().values}
""")
```

---

## Technical Details

### Chunking Behavior

**Dimension chunking** (specified in strategy):
```python
chunks = {"epoch": 34560, "sid": -1}
```

**Result:**
- **SNR[epoch, sid]** ‚Üí chunked as (34560, -1) meaning:
  - epoch: 34560 elements per chunk (2 days)
  - sid: -1 = all satellites in one chunk
- **phi[epoch, sid]** ‚Üí same chunking
- **theta[epoch, sid]** ‚Üí same chunking

**Coordinate behavior:**
- **epoch[epoch]** ‚Üí dimension coordinate, respects epoch chunking
- **sid[sid]** ‚Üí dimension coordinate, loaded fully (sid=-1)
- **band[sid]** ‚Üí string coordinate, loaded fully (no chunking specified)
- **code[sid]** ‚Üí string coordinate, loaded fully
- **system[sid]** ‚Üí string coordinate, loaded fully

### Why This Works

**Dask chunking rules:**
1. If a dimension has explicit chunk size ‚Üí use it
2. If a dimension has chunk size -1 ‚Üí load all data
3. If a variable has no chunk specification ‚Üí inherit from dimensions
4. If a coordinate has no dimension chunking ‚Üí load fully

**Our strategy:**
```python
{"epoch": 34560, "sid": -1}
```

**Applies to:**
- ‚úÖ Data variables: SNR, phi, theta, r ‚Üí chunked as (34560, -1)
- ‚úÖ Dimension coords: epoch, sid ‚Üí handled appropriately
- ‚úÖ String coords: band, code, system ‚Üí no chunking spec = loaded fully
- ‚úÖ Numeric coords: freq_center, freq_min, freq_max ‚Üí loaded fully

**Avoids:**
- ‚ùå Auto-chunking with 'auto' ‚Üí would fail on strings
- ‚ùå Explicit chunks for every variable ‚Üí unnecessary complexity

---

## Best Practices

### 1. Always Use Explicit Chunk Strategies

**Good:**
```python
chunks = {"epoch": 34560, "sid": -1}
ds = xr.open_zarr(store, chunks=chunks)
```

**Bad:**
```python
chunks = 'auto'  # ‚ùå Fails with string coordinates
ds = xr.open_zarr(store, chunks=chunks)
```

### 2. Define Chunks at Dimension Level

**Good:**
```python
# Specify dimension chunks, variables inherit
chunks = {"time": 1000, "space": -1}
```

**Unnecessary:**
```python
# Don't specify chunks for every variable
chunks = {
    "temperature": (1000, -1),
    "pressure": (1000, -1),
    "humidity": (1000, -1),
    # ... tedious and error-prone
}
```

### 3. Use -1 for "Don't Chunk"

**Correct:**
```python
chunks = {"sid": -1}  # Load all satellites at once
```

**Also works:**
```python
chunks = {"sid": None}  # Equivalent to -1
```

### 4. Coordinate Arrays Should Be Small

**String coordinates like satellite IDs should be loaded fully:**
- sid: 321 satellites ‚Üí small, load all
- band: ~10 bands ‚Üí very small, load all
- code: ~20 codes ‚Üí very small, load all

**Only chunk large dimensions:**
- epoch: 100,000+ time points ‚Üí chunk it!

---

## Performance Impact

### Before Fix (Auto-Chunking)

**Behavior:**
- ‚ùå Attempts to chunk string coordinates
- ‚ùå Fails with NotImplementedError
- ‚ùå Dataset cannot be read at all

### After Fix (Explicit Chunking)

**Behavior:**
- ‚úÖ Numeric arrays chunked efficiently (34560 time points per chunk)
- ‚úÖ Satellite dimension loaded fully (-1)
- ‚úÖ String coordinates loaded fully (automatic)
- ‚úÖ Fast coordinate lookups (in memory)

**Memory usage:**
- String coordinates: ~100 KB (negligible)
- Satellite dimension: 321 elements (tiny)
- Only time series data is chunked ‚Üí efficient memory use

**Read performance:**
- Coordinates: Instant (in memory)
- Time slice: Fast (only loads needed chunks)
- Full dataset: Efficient (streams chunks as needed)

---

## Related Configuration

### Chunk Strategy Definition

**File:** `canvodpy/src/canvodpy/globals.py`

```python
ICECHUNK_CHUNK_STRATEGIES: dict[str, dict[str, int]] = {
    "rinex_store": {
        "epoch": 34560,  # Two days of 5-second data
        "sid": -1        # Don't chunk along satellite dimension
    },
    "vod_store": {
        "epoch": 34560,  # Same for VOD products
        "sid": -1
    }
}
```

### Chunk Size Calculation

**Why 34560?**
```python
# Two days of data at 5-second resolution
seconds_per_day = 24 * 60 * 60 = 86,400
samples_per_day = 86,400 / 5 = 17,280
two_days = 17,280 * 2 = 34,560 samples
```

**Advantages of 2-day chunks:**
- Reasonable memory footprint (~50-100 MB per chunk)
- Efficient for daily or multi-day analyses
- Good balance of I/O vs computation

---

## Future Improvements

### String Encoding Stability

**Monitor Zarr V3 specification:**
- https://github.com/zarr-developers/zarr-extensions/tree/main/data-types
- Watch for stable UTF-8 string encoding specification

**When stable encoding is available:**
- Update zarr-python to version with stable spec
- Warning will disappear automatically
- No code changes needed

### Alternative String Storage

**If stability is critical:**

**Option 1: Use string enum/categories**
```python
# Convert to categorical codes
ds['band'] = ds['band'].astype('category')
# Stores as integers + lookup table
```

**Option 2: Use fixed-width ASCII**
```python
# Store as S9 (9-char ASCII)
ds['band'] = ds['band'].astype('S9')
# More stable, but loses UTF-8 support
```

**Not recommended:** These lose flexibility and readability

---

## Summary

| Issue | Status | Solution |
|-------|--------|----------|
| **NotImplementedError** | ‚úÖ Fixed | Use explicit chunk strategy |
| **FixedLengthUTF32 warning** | ‚ö†Ô∏è Warning only | Wait for Zarr V3 spec |

**Changes made:**
1. ‚úÖ Updated `read_group()` to use `ICECHUNK_CHUNK_STRATEGIES`
2. ‚úÖ Removed `chunks='auto'` that caused the error
3. ‚úÖ Verified dataset reading works correctly

**The store can now be read successfully with proper chunking!** üéâ

**The FixedLengthUTF32 warning is expected and harmless - it just indicates that the Zarr V3 spec for strings is still being finalized. Data works correctly.**
